[
  {
    "function": "CREATE FUNCTION",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-function",
    "details": [
      {
        "heading": "CREATE FUNCTION",
        "tables": [
          {
            "headers": [
              "Language",
              "Handler Location"
            ],
            "rows": [
              [
                "Java",
                "In-line or staged"
              ],
              [
                "JavaScript",
                "In-line"
              ],
              [
                "Python",
                "In-line or staged"
              ],
              [
                "Scala",
                "In-line or staged"
              ],
              [
                "SQL",
                "In-line"
              ]
            ]
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nThe syntax for CREATE FUNCTION varies depending on which language youre using as the UDF handler."
      },
      {
        "heading": "Java handler",
        "description": "\nUse the syntax below if the source code is in-line:\nUse the following syntax if the handler code will be referenced on a stage (such as in a JAR):",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <java_jdk_version> ]\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name_and_version>' [ , ... ] ) ]\n  HANDLER = '<path_to_method>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  AS '<function_definition>'",
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <java_jdk_version> ]\n  [ COMMENT = '<string_literal>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  HANDLER = '<path_to_method>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]"
        ]
      },
      {
        "heading": "JavaScript handler",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVASCRIPT\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ COMMENT = '<string_literal>' ]\n  AS '<function_definition>'"
        ]
      },
      {
        "heading": "Python handler",
        "description": "\nPreview Feature  Open\nUsing Python to write a handler for a user-defined aggregate function (UDAF) with the AGGREGATE parameter is a preview feature that is\navailable to all accounts.\nUse the syntax below if the source code is in-line:\nUse the following syntax if the handler code will be referenced on a stage (such as in a module):",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] [ AGGREGATE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE PYTHON\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  RUNTIME_VERSION = <python_version>\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name>[==<version>]' [ , ... ] ) ]\n  [ ARTIFACT_REPOSITORY = '<repository_name>' ]\n  HANDLER = '<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  AS '<function_definition>'",
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] [ AGGREGATE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE PYTHON\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  RUNTIME_VERSION = <python_version>\n  [ COMMENT = '<string_literal>' ]\n  [ ARTIFACT_REPOSITORY = '<repository_name>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  [ PACKAGES = ( '<package_name>[==<version>]' [ , ... ] ) ]\n  HANDLER = '<module_file_name>.<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]"
        ]
      },
      {
        "heading": "Scala handler",
        "description": "\nPreview Feature  Open\nUsing Scala to write a handler for a user-defined function (UDF) is a preview feature that is available to all accounts. Note that\nreturning a TABLE is not yet available for Scala UDF handlers.\nUse the syntax below if the source code is in-line:\nUse the following syntax if the handler code will be referenced on a stage (such as in a JAR):",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  LANGUAGE SCALA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <scala_version> ]\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name_and_version>' [ , ... ] ) ]\n  HANDLER = '<path_to_method>'\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  AS '<function_definition>'",
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  LANGUAGE SCALA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <scala_version> ]\n  [ COMMENT = '<string_literal>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  HANDLER = '<path_to_method>'"
        ]
      },
      {
        "heading": "SQL handler",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ MEMOIZABLE ]\n  [ COMMENT = '<string_literal>' ]\n  AS '<function_definition>'"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE OR ALTER FUNCTION",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nCreates a new function if it doesnt already exist, or transforms an existing function into the function defined in the statement.\nA CREATE OR ALTER FUNCTION statement follows the syntax rules of a CREATE FUNCTION statement and has the same limitations as an\nALTER FUNCTION statement.\nSupported function alterations include:\nChange function properties and parameters. For example, SECURE, MAX_BATCH_ROWS, LOG_LEVEL, or COMMENT.\nChange function properties and parameters. For example, SECURE, MAX_BATCH_ROWS, LOG_LEVEL, or COMMENT.\nFor more information, see CREATE OR ALTER FUNCTION usage notes.\nNote\nThe COPY GRANTS parameter is not supported with this variant syntax.",
        "syntax": [
          "CREATE [ OR ALTER ] FUNCTION ..."
        ]
      },
      {
        "heading": "Required parameters"
      },
      {
        "heading": "All languages",
        "definitions": [
          {
            "term": "name ( [ arg_name arg_data_type [ DEFAULT default_value ] ] [ , ... ] )",
            "definition": "Specifies the identifier (name), any input arguments, and the default values for any optional arguments for the UDF. For the identifier: The identifier does not need to be unique for the schema in which the function is created because UDFs are\nidentified and resolved by the combination of the name and argument types. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, My object). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements. For the input arguments: For arg_name, specify the name of the input argument. For arg_data_type, use the Snowflake data type that corresponds to the handler language that you are using. For Java handlers, see SQL-Java Data Type Mappings. For JavaScript handlers, see SQL and JavaScript data type mapping. For Python handlers, see SQL-Python Data Type Mappings. For Scala handlers, see SQL-Scala Data Type Mappings. To indicate that an argument is optional, use DEFAULT default_value to specify the default value of the argument.\nFor the default value, you can use a literal or an expression. If you specify any optional arguments, you must place these after the required arguments. If a function has optional arguments, you cannot define additional functions with the same name and different signatures. For details, see Specify optional arguments."
          },
          {
            "term": "RETURNS ...",
            "definition": "Specifies the results returned by the UDF, which determines the UDF type: result_data_type: Creates a scalar UDF that returns a single value with the specified data type. Note For UDF handlers written in Java, Python, or Scala, the result_data_type must be in the SQL Data Type column of the\nfollowing table corresponding to the handler language: SQL-Java Type Mappings table SQL-Python Type Mappings table SQL-Scala Type Mappings table TABLE ( col_name col_data_type , ... ): Creates a table UDF that returns tabular results with the specified table column(s)\nand column type(s). Note For Scala UDFs, the TABLE return type is not supported."
          },
          {
            "term": "AS function_definition",
            "definition": "Defines the handler code executed when the UDF is called. The function_definition value must be source code in one of the\nlanguages supported for handlers. The code may be: Java. For more information, see Introduction to Java UDFs. JavaScript. For more information, see Introduction to JavaScript UDFs. Python. For more information, see Introduction to Python UDFs. Scala. For more information, see Introduction to Scala UDFs. A SQL expression. For more information, see Introduction to SQL UDFs. For more details, see General usage notes (in this topic). Note The AS clause is not required when the UDF handler code is referenced on a stage with the IMPORTS clause."
          }
        ]
      },
      {
        "heading": "Java",
        "definitions": [
          {
            "term": "LANGUAGE JAVA",
            "definition": "Specifies that the code is in the Java language."
          },
          {
            "term": "RUNTIME_VERSION = java_jdk_version",
            "definition": "Specifies the Java JDK runtime version to use. The supported versions of Java are: 11.x 17.x If RUNTIME_VERSION is not set, Java JDK 11 is used."
          },
          {
            "term": "IMPORTS = ( 'stage_path_and_file_name_to_read' [ , ... ] )",
            "definition": "The location (stage), path, and name of the file(s) to import. A file can be a JAR file or another type of file. If the file is a JAR file, it can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode). Java UDFs can also read non-JAR files. For an example, see Reading a file specified statically in IMPORTS. If you plan to copy a file (JAR file or other file) to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file\nto a stage. External stages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom each file name in the IMPORTS clause, even if the files are in different subdirectories or different stages. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an existing file. For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF. For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files. For Snowflake system packages, such the Snowpark package,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value. In-line Java In-line Java UDFs require a function definition."
          },
          {
            "term": "AS function_definition",
            "definition": "In-line Java UDFs require a function definition."
          },
          {
            "term": "HANDLER = handler_name",
            "definition": "The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod. If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class."
          },
          {
            "term": "AS function_definition",
            "definition": "In-line Java UDFs require a function definition."
          }
        ]
      },
      {
        "heading": "JavaScript",
        "definitions": [
          {
            "term": "LANGUAGE JAVASCRIPT",
            "definition": "Specifies that the code is in the JavaScript language."
          }
        ]
      },
      {
        "heading": "Python",
        "definitions": [
          {
            "term": "LANGUAGE PYTHON",
            "definition": "Specifies that the code is in the Python language."
          },
          {
            "term": "RUNTIME_VERSION = python_version",
            "definition": "Specifies the Python version to use. The supported versions of Python are: 3.9 3.10 3.11 3.12"
          },
          {
            "term": "IMPORTS = ( 'stage_path_and_file_name_to_read' [ , ... ] )",
            "definition": "The location (stage), path, and name of the file(s) to import. A file can be a .py file or another type of file. Python UDFs can also read non-Python files, such as text files. For an example, see Reading a file. If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a file\nto a stage. External stages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. When the handler code is stored in a stage, you must use the IMPORTS clause to specify the handler codes location. For an in-line Python UDF, the IMPORTS clause is needed only if the UDF handler needs to access other files, such as\npackages or text files. For packages included on the Snowflake system, such numpy,\nyou can specify the package with the PACKAGES clause alone, omitting the packages source as an IMPORTS value."
          },
          {
            "term": "HANDLER = handler_name",
            "definition": "The name of the handler function or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a function name. If the handler code\nis in-line with the CREATE FUNCTION statement, you can use the function name alone. When the handler code is referenced at a stage, this\nvalue should be qualified with the module name, as in the following form: my_module.my_function. If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class."
          }
        ]
      },
      {
        "heading": "Scala",
        "definitions": [
          {
            "term": "LANGUAGE SCALA",
            "definition": "Specifies that the code is in the Scala language."
          },
          {
            "term": "RUNTIME_VERSION = scala_version",
            "definition": "Specifies the Scala runtime version to use. The supported versions of Scala are: 2.12 If RUNTIME_VERSION is not set, Scala 2.12 is used."
          },
          {
            "term": "IMPORTS = ( 'stage_path_and_file_name_to_read' [ , ... ] )",
            "definition": "The location (stage), path, and name of the file(s) to import, such as a JAR or other kind of file. The JAR file might contain handler dependency libraries. It can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode). A non-JAR file might a file read by handler code. For an example, see Reading a file specified statically in IMPORTS. If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the PUT command supports\ncopying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file to a stage. External\nstages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different stage subdirectories or different stages. If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom that of any file listed in the IMPORTS clause, even if the files are in different stage subdirectories or different stages. For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF. For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files. For Snowflake system packages, such the Snowpark package,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value. In-line Scala UDFs with in-line Scala handler code require a function definition."
          },
          {
            "term": "AS function_definition",
            "definition": "UDFs with in-line Scala handler code require a function definition."
          },
          {
            "term": "HANDLER = handler_name",
            "definition": "The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod."
          },
          {
            "term": "AS function_definition",
            "definition": "UDFs with in-line Scala handler code require a function definition."
          }
        ]
      },
      {
        "heading": "Optional parameters"
      },
      {
        "heading": "All languages",
        "definitions": [
          {
            "term": "SECURE",
            "definition": "Specifies that the function is secure. For more information about secure functions, see Protecting Sensitive Information with Secure UDFs and Stored Procedures."
          },
          {
            "term": "{ TEMP | TEMPORARY }",
            "definition": "Specifies that the function persists only for the duration of the session that you created it in. A\ntemporary function is dropped at the end of the session. Default: No value. If a function is not declared as TEMPORARY, the function is permanent. You cannot create temporary user-defined functions that have the same name as a function that already\nexists in the schema."
          },
          {
            "term": "[ [ NOT ] NULL ]",
            "definition": "Specifies whether the function can return NULL values or must return only NON-NULL values. The default is NULL (i.e. the function can\nreturn NULL). Note Currently, the NOT NULL clause is not enforced for SQL UDFs.\nSQL UDFs declared as NOT NULL can return NULL values. Snowflake recommends avoiding NOT NULL\nfor SQL UDFs unless the code in the function is written to ensure that NULL values are never returned."
          },
          {
            "term": "CALLED ON NULL INPUT or . { RETURNS NULL ON NULL INPUT | STRICT }",
            "definition": "Specifies the behavior of the UDF when called with null inputs. In contrast to system-defined functions, which always return null when any\ninput is null, UDFs can handle null inputs, returning non-null values even when an input is null: CALLED ON NULL INPUT will always call the UDF with null inputs. It is up to the UDF to handle such values appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT) will not call the UDF if any input is null. Instead, a null value\nwill always be returned for that row. Note that the UDF might still return null for non-null inputs. Note RETURNS NULL ON NULL INPUT (STRICT) is not supported for SQL UDFs. SQL UDFs effectively use\nCALLED ON NULL INPUT. In your SQL UDFs, you must handle null input values. Default: CALLED ON NULL INPUT"
          },
          {
            "term": "{ VOLATILE | IMMUTABLE }",
            "definition": "Specifies the behavior of the UDF when returning results: VOLATILE: UDF might return different values for different rows, even for the same input (e.g. due to non-determinism and\nstatefullness). IMMUTABLE: UDF assumes that the function, when called with the same inputs, will always return the same result. This guarantee\nis not checked. Specifying IMMUTABLE for a UDF that returns different values for the same input will result in undefined\nbehavior. Default: VOLATILE Note IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment for the UDF, which is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS\noutput. Default: user-defined function"
          },
          {
            "term": "COPY GRANTS",
            "definition": "Specifies to retain the access privileges from the original function when a new function is created using CREATE OR REPLACE FUNCTION. The parameter copies all privileges, except OWNERSHIP, from the existing function to the new function. The new function will\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE FUNCTION\nstatement owns the new function. Note: With data sharing, if the existing function was shared to another account, the replacement function is\nalso shared. The SHOW GRANTS output for the replacement function lists the grantee for the copied privileges as the\nrole that executed the CREATE FUNCTION statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE FUNCTION command (i.e. within the same transaction)."
          }
        ]
      },
      {
        "heading": "Java",
        "syntax": [
          "-- Use version 1.2.0 of the Snowpark package.\nPACKAGES=('com.snowflake:snowpark:1.2.0')\n\n-- Use the latest version of the Snowpark package.\nPACKAGES=('com.snowflake:snowpark:latest')",
          "SELECT * FROM INFORMATION_SCHEMA.PACKAGES WHERE LANGUAGE = 'java';",
          "TARGET_PATH = '@handlers/myhandler.jar'",
          "REMOVE @handlers/myhandler.jar;"
        ],
        "definitions": [
          {
            "term": "PACKAGES = ( 'package_name_and_version' [ , ... ] )",
            "definition": "The name and version number of Snowflake system packages required as dependencies. The value should be of the form\npackage_name:version_number, where package_name is snowflake_domain:package. Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause. In-line Java Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this UDF to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          },
          {
            "term": "TARGET_PATH = stage_path_and_file_name_to_write",
            "definition": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this UDF to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nfunctions handler code to access external networks. An external access integration specifies network rules and\nsecrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration\nspecified as a value of this CREATE FUNCTION commands EXTERNAL_ACCESS_INTEGRATIONS parameter This parameters value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure."
          },
          {
            "term": "TARGET_PATH = stage_path_and_file_name_to_write",
            "definition": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this UDF to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          }
        ]
      },
      {
        "heading": "Python",
        "description": "\nARTIFACT_REPOSITORY = repository_name\nSpecifies the name of the repository to use for installing PyPI packages for use by your function.\nSnowflake installs these packages from the artifact repository.\nSpecify a list of the names of the packages that you want to install and use in your function.\nSnowflake installs these packages from the artifact repository.",
        "syntax": [
          "-- Use version 1.2.2 of the NumPy package.\nPACKAGES=('numpy==1.2.2')\n\n-- Use the latest version of the NumPy package.\nPACKAGES=('numpy')",
          "SELECT * FROM INFORMATION_SCHEMA.PACKAGES WHERE LANGUAGE = 'python';",
          "-- Use version 1.2.3 or higher of the NumPy package.\nPACKAGES=('numpy>=1.2.3')"
        ],
        "definitions": [
          {
            "term": "AGGREGATE",
            "definition": "Specifies that the function is an aggregate function. For more information about user-defined aggregate functions, see\nPython user-defined aggregate functions. Preview Feature  Open Using Python to write a handler for a user-defined aggregate function (UDAF) is a preview feature that is available to all accounts. Note IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default."
          },
          {
            "term": "PACKAGES = ( 'package_name_and_version' [ , ... ] )",
            "definition": "The name and version number of packages required as dependencies. The value should be of the form\npackage_name==version_number. If you omit the version number, Snowflake will use the latest package available on the\nsystem. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For more information about included packages, see Using third-party packages. Preview Feature  Open Specifying a range of Python package versions is available as a preview feature to all accounts. You can specify package versions by using these version\nspecifiers: ==, <=, >=, <,or >. For example:"
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nfunctions handler code to access external networks. An external access integration specifies network rules and\nsecrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration\nspecified as a value of this CREATE FUNCTION commands EXTERNAL_ACCESS_INTEGRATIONS parameter This parameters value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure."
          }
        ]
      },
      {
        "heading": "SQL",
        "definitions": [
          {
            "term": "MEMOIZABLE",
            "definition": "Specifies that the function is memoizable. For details, see Memoizable UDFs."
          }
        ]
      },
      {
        "heading": "Scala",
        "syntax": [
          "-- Use version 1.7.0 of the Snowpark package.\nPACKAGES=('com.snowflake:snowpark:1.7.0')\n\n-- Use the latest version of the Snowpark package.\nPACKAGES=('com.snowflake:snowpark:latest')",
          "SELECT * FROM INFORMATION_SCHEMA.PACKAGES WHERE LANGUAGE = 'scala';",
          "TARGET_PATH = '@handlers/myhandler.jar'",
          "REMOVE @handlers/myhandler.jar;"
        ],
        "definitions": [
          {
            "term": "PACKAGES = ( 'package_name_and_version' [ , ... ] )",
            "definition": "The name and version number of Snowflake system packages required as dependencies. The value should be of the form\npackage_name:version_number, where package_name is snowflake_domain:package. Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause. Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this UDF to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          },
          {
            "term": "TARGET_PATH = stage_path_and_file_name_to_write",
            "definition": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this UDF to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          },
          {
            "term": "TARGET_PATH = stage_path_and_file_name_to_write",
            "definition": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this UDF to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE FUNCTION",
                "Schema",
                "The privilege only enables the creation of user-defined functions in the schema.\nIf you want to enable the creation of data metric functions, the role must have the CREATE DATA METRIC FUNCTION privilege."
              ],
              [
                "USAGE",
                "Function",
                "Granting the USAGE privilege on the newly created function to a role allows users with that role to call the function elsewhere in\nSnowflake (such as masking policy owner role for External Tokenization)."
              ],
              [
                "USAGE",
                "External access integration",
                "Required on integrations, if any, specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter. For more information, see\nCREATE EXTERNAL ACCESS INTEGRATION."
              ],
              [
                "READ",
                "Secret",
                "Required on secrets, if any, specified by the SECRETS parameter. For more information, see\nCreating a secret to represent credentials and Using the external access integration in a function or procedure."
              ],
              [
                "USAGE",
                "Schema",
                "Required on schemas containing secrets, if any, specified by the SECRETS parameter. For more information,\nsee Creating a secret to represent credentials and Using the external access integration in a function or procedure."
              ]
            ]
          }
        ]
      },
      {
        "heading": "General usage notes"
      },
      {
        "heading": "All languages",
        "description": "\nfunction_definition has size restrictions. The maximum allowable size is subject to change.\nThe delimiters around the function_definition can be either single quotes or a pair of dollar signs.\nUsing $$ as the delimiter makes it easier to write functions that contain single quotes.\nIf the delimiter for the body of the function is the single quote character,\nthen any single quotes within function_definition (such as string\nliterals) must be escaped by single quotes.\nIf using a UDF in a masking policy, ensure the data type of the column, UDF, and masking policy match. For\nmore information, see User-defined functions in a masking policy.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\nhandler code of the UDF, the function returns the database or schema that contains the UDF, not the database or schema in use for\nthe session.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nfunction_definition has size restrictions. The maximum allowable size is subject to change.\nThe delimiters around the function_definition can be either single quotes or a pair of dollar signs.\nUsing $$ as the delimiter makes it easier to write functions that contain single quotes.\nIf the delimiter for the body of the function is the single quote character,\nthen any single quotes within function_definition (such as string\nliterals) must be escaped by single quotes.\nIf using a UDF in a masking policy, ensure the data type of the column, UDF, and masking policy match. For\nmore information, see User-defined functions in a masking policy.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\nhandler code of the UDF, the function returns the database or schema that contains the UDF, not the database or schema in use for\nthe session.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
      },
      {
        "heading": "Java",
        "description": "\nIn Java, primitive data types dont allow NULL values, so passing a NULL for an argument of such a type results in\nan error.\nIn the HANDLER clause, the method name is case-sensitive.\nIn the IMPORTS and TARGET_PATH clauses:\n\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nYou can use the PACKAGES clause to specify package names and version numbers for Snowflake system-defined dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency JAR files with the IMPORTS clause.\nSnowflake validates that:\n\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Java method.\n\nValidation can be done at creation time or execution time, depending on whether you are connected to an active Snowflake warehouse.\n\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Java method.\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.\nIn Java, primitive data types dont allow NULL values, so passing a NULL for an argument of such a type results in\nan error.\nIn the HANDLER clause, the method name is case-sensitive.\nIn the IMPORTS and TARGET_PATH clauses:\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nYou can use the PACKAGES clause to specify package names and version numbers for Snowflake system-defined dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency JAR files with the IMPORTS clause.\nSnowflake validates that:\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Java method.\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Java method.\nValidation can be done at creation time or execution time, depending on whether you are connected to an active Snowflake warehouse.\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse."
      },
      {
        "heading": "JavaScript",
        "description": "\nSnowflake does not validate JavaScript code at UDF creation time. In other words, creation of the UDF succeeds regardless of whether\nthe code is valid. If the code is not valid, Snowflake returns errors when the UDF is called at query time.\nSnowflake does not validate JavaScript code at UDF creation time. In other words, creation of the UDF succeeds regardless of whether\nthe code is valid. If the code is not valid, Snowflake returns errors when the UDF is called at query time."
      },
      {
        "heading": "Python",
        "description": "\nIn the HANDLER clause, the handler function name is case-sensitive.\nIn the IMPORTS clause:\n\nFile name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nFile name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nYou can use the PACKAGES clause to specify package names and version numbers for dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency files with the IMPORTS clause.\nSnowflake validates that:\n\nThe function or class specified in the CREATE FUNCTION statements HANDLER exists.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the handler.\nThe function or class specified in the CREATE FUNCTION statements HANDLER exists.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the handler.\nIn the HANDLER clause, the handler function name is case-sensitive.\nIn the IMPORTS clause:\nFile name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nFile name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nYou can use the PACKAGES clause to specify package names and version numbers for dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency files with the IMPORTS clause.\nSnowflake validates that:\nThe function or class specified in the CREATE FUNCTION statements HANDLER exists.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the handler.\nThe function or class specified in the CREATE FUNCTION statements HANDLER exists.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the handler."
      },
      {
        "heading": "Scala",
        "description": "\nIn the HANDLER clause, the method name is case-sensitive.\nIn the IMPORTS and TARGET_PATH clauses:\n\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nYou can use the PACKAGES clause to specify package names and version numbers for Snowflake system-defined dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency JAR files with the IMPORTS clause.\nSnowflake validates that:\n\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Scala method.\n\nValidation can be done at creation time or execution time, depending on whether you are connected to an active Snowflake warehouse.\n\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Scala method.\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.\nIn the HANDLER clause, the method name is case-sensitive.\nIn the IMPORTS and TARGET_PATH clauses:\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nPackage, class, and file name(s) are case-sensitive.\nStage name(s) are case-insensitive.\nYou can use the PACKAGES clause to specify package names and version numbers for Snowflake system-defined dependencies, such as those\nfrom Snowpark. For other dependencies, specify dependency JAR files with the IMPORTS clause.\nSnowflake validates that:\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Scala method.\nThe JAR file specified in the CREATE FUNCTION statements HANDLER exists and contains the specified\nclass and method.\nThe input and output types specified in the UDF declaration are compatible with the input and output types\nof the Scala method.\nValidation can be done at creation time or execution time, depending on whether you are connected to an active Snowflake warehouse.\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse.\nCreation time  If you are connected to an active Snowflake warehouse at the time the CREATE FUNCTION statement is\nexecuted, the UDF is validated at creation time.\nExecution time  If you are not connected to an active Snowflake warehouse, the UDF is created, but is not validated\nimmediately, and Snowflake returns the following message:\nFunction <name> created successfully, but could not be validated since there is no active warehouse."
      },
      {
        "heading": "SQL",
        "description": "\nCurrently, the NOT NULL clause is not enforced for SQL UDFs.\nCurrently, the NOT NULL clause is not enforced for SQL UDFs."
      },
      {
        "heading": "CREATE OR ALTER FUNCTION usage notes",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nAll limitations of the ALTER FUNCTION command apply.\nYou cannot replace or transform a FUNCTION with a PROCEDURE or a PROCEDURE with a FUNCTION.\nYou cannot replace or transform a temporary FUNCTION with a non-temporary FUNCTION, or a non-temporary FUNCTION with a temporary FUNCTION.\nYou cannot replace or transform a regular FUNCTION with an EXTERNAL FUNCTION, or an EXTERNAL FUNCTION with a regular FUNCTION.\nChanging the LANGUAGE, IMPORTS, RETURNS, HANDLER, RUNTIME_VERSION,  PACKAGES, VOLATILITY, NULL_HANDLING, and TARGET_PATH properties is not supported.\nSetting or unsetting a tag is not supported. Existing tags are not altered by a CREATE OR ALTER FUNCTION statement and remain unchanged.\nAll limitations of the ALTER FUNCTION command apply.\nYou cannot replace or transform a FUNCTION with a PROCEDURE or a PROCEDURE with a FUNCTION.\nYou cannot replace or transform a temporary FUNCTION with a non-temporary FUNCTION, or a non-temporary FUNCTION with a temporary FUNCTION.\nYou cannot replace or transform a regular FUNCTION with an EXTERNAL FUNCTION, or an EXTERNAL FUNCTION with a regular FUNCTION.\nChanging the LANGUAGE, IMPORTS, RETURNS, HANDLER, RUNTIME_VERSION,  PACKAGES, VOLATILITY, NULL_HANDLING, and TARGET_PATH properties is not supported.\nSetting or unsetting a tag is not supported. Existing tags are not altered by a CREATE OR ALTER FUNCTION statement and remain unchanged."
      },
      {
        "heading": "Examples"
      },
      {
        "heading": "Java",
        "description": "\nHere is a basic example of CREATE FUNCTION with an in-line handler:\nHere is a basic example of CREATE FUNCTION with a reference to a staged handler:\nFor more examples of Java UDFs, see examples.",
        "syntax": [
          "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE JAVA\n  CALLED ON NULL INPUT\n  HANDLER = 'TestFunc.echoVarchar'\n  TARGET_PATH = '@~/testfunc.jar'\n  AS\n  'class TestFunc {\n    public static String echoVarchar(String x) {\n      return x;\n    }\n  }';",
          "create function my_decrement_udf(i numeric(9, 0))\n    returns numeric\n    language java\n    imports = ('@~/my_decrement_udf_package_dir/my_decrement_udf_jar.jar')\n    handler = 'my_decrement_udf_package.my_decrement_udf_class.my_decrement_udf_method'\n    ;"
        ]
      },
      {
        "heading": "JavaScript",
        "description": "\nCreate a JavaScript UDF named js_factorial:",
        "syntax": [
          "CREATE OR REPLACE FUNCTION js_factorial(d double)\n  RETURNS double\n  LANGUAGE JAVASCRIPT\n  STRICT\n  AS '\n  if (D <= 0) {\n    return 1;\n  } else {\n    var result = 1;\n    for (var i = 2; i <= D; i++) {\n      result = result * i;\n    }\n    return result;\n  }\n  ';"
        ]
      },
      {
        "heading": "Python",
        "description": "\nCode in the following example creates a py_udf function whose handler code is in-line as udf.\nCode in the following example creates a dream function whose handler is in a sleepy.py file located on the\n@my_stage stage.",
        "syntax": [
          "CREATE OR REPLACE FUNCTION py_udf()\n  RETURNS VARIANT\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.10'\n  PACKAGES = ('numpy','pandas','xgboost==1.5.0')\n  HANDLER = 'udf'\nAS $$\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\ndef udf():\n    return [np.__version__, pd.__version__, xgb.__version__]\n$$;",
          "CREATE OR REPLACE FUNCTION dream(i int)\n  RETURNS VARIANT\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.10'\n  HANDLER = 'sleepy.snore'\n  IMPORTS = ('@my_stage/sleepy.py')"
        ]
      },
      {
        "heading": "Scala",
        "description": "\nHere is a basic example of CREATE FUNCTION with an in-line handler:\nHere is a basic example of CREATE FUNCTION with a reference to a staged handler:\nFor more examples of Scala UDFs, see Scala UDF handler examples.",
        "syntax": [
          "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE SCALA\n  RUNTIME_VERSION = 2.12\n  HANDLER='Echo.echoVarchar'\n  AS\n  $$\n  class Echo {\n    def echoVarchar(x : String): String = {\n      return x\n    }\n  }\n  $$;",
          "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE SCALA\n  RUNTIME_VERSION = 2.12\n  IMPORTS = ('@udf_libs/echohandler.jar')\n  HANDLER='Echo.echoVarchar';"
        ]
      },
      {
        "heading": "SQL",
        "description": "\nCreate a simple SQL scalar UDF that returns a hard-coded approximation of the\nmathematical constant pi:\nCreate a simple SQL table UDF that returns hard-coded values:\nOutput:\nCreate a UDF that accepts multiple parameters:\nCreate a SQL table UDF named get_countries_for_user that returns the results of a query:",
        "syntax": [
          "CREATE FUNCTION pi_udf()\n  RETURNS FLOAT\n  AS '3.141592654::FLOAT'\n  ;",
          "CREATE FUNCTION simple_table_function ()\n  RETURNS TABLE (x INTEGER, y INTEGER)\n  AS\n  $$\n    SELECT 1, 2\n    UNION ALL\n    SELECT 3, 4\n  $$\n  ;",
          "SELECT * FROM TABLE(simple_table_function());",
          "SELECT * FROM TABLE(simple_table_function());\n+---+---+\n| X | Y |\n|---+---|\n| 1 | 2 |\n| 3 | 4 |\n+---+---+",
          "CREATE FUNCTION multiply1 (a number, b number)\n  RETURNS number\n  COMMENT='multiply two numbers'\n  AS 'a * b';",
          "CREATE OR REPLACE FUNCTION get_countries_for_user ( id NUMBER )\n  RETURNS TABLE (country_code CHAR, country_name VARCHAR)\n  AS 'SELECT DISTINCT c.country_code, c.country_name\n      FROM user_addresses a, countries c\n      WHERE a.user_id = id\n      AND c.country_code = a.country_code';"
        ]
      },
      {
        "heading": "Create and alter a simple function using the CREATE OR ALTER FUNCTION command",
        "description": "\nCreate a function multiply that accepts two numbers:\nAlter multiply to add a comment and make the function secure:\nOn this page\nSyntax\nVariant syntax\nRequired parameters\nOptional parameters\nAccess control requirements\nGeneral usage notes\nCREATE OR ALTER FUNCTION usage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR ALTER FUNCTION multiply(a NUMBER, b NUMBER)\n  RETURNS NUMBER\n  AS 'a * b';",
          "CREATE OR ALTER SECURE FUNCTION multiply(a NUMBER, b NUMBER)\n  RETURNS NUMBER\n  COMMENT = 'Multiply two numbers.'\n  AS 'a * b';"
        ]
      }
    ]
  },
  {
    "function": "ALTER FUNCTION",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-function",
    "details": [
      {
        "heading": "ALTER FUNCTION",
        "description": "\nModifies the properties of an existing user-defined or external function.\nTo make any other changes to a UDF, you must drop the function (using DROP FUNCTION) and then recreate it.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "Writing external functions, User-defined functions overview, CREATE FUNCTION , DROP FUNCTION ,\nSHOW USER FUNCTIONS , DESCRIBE FUNCTION,  CREATE EXTERNAL FUNCTION ,\nDESCRIBE FUNCTION , DROP FUNCTION , SHOW EXTERNAL FUNCTIONS"
          }
        ]
      },
      {
        "heading": "Syntax"
      },
      {
        "heading": "User-defined and external functions",
        "description": "\nThe syntax for ALTER FUNCTION varies depending on which language youre using as the UDF handler.",
        "syntax": [
          "ALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , <integration_name> ... ] ) ]\n  [ SECRETS = ( '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]",
          "ALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]",
          "ALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , <integration_name> ... ] ) ]\n  [ SECRETS = ( '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]",
          "ALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , <integration_name> ... ] ) ]\n  [ SECRETS = ( '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]",
          "ALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]"
        ]
      },
      {
        "heading": "External functions",
        "syntax": [
          "ALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET API_INTEGRATION = <api_integration_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET HEADERS = ( [ '<header_1>' = '<value>' [ , '<header_2>' = '<value>' ... ] ] )\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET CONTEXT_HEADERS = ( [ <context_function_1> [ , <context_function_2> ...] ] )\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET MAX_BATCH_ROWS = <integer>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET COMPRESSION = <compression_type>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET { REQUEST_TRANSLATOR | RESPONSE_TRANSLATOR } = <udf_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET\n              { COMMENT | HEADERS | CONTEXT_HEADERS | MAX_BATCH_ROWS | COMPRESSION | SECURE | REQUEST_TRANSLATOR | RESPONSE_TRANSLATOR }"
        ]
      },
      {
        "heading": "Parameters"
      },
      {
        "heading": "User-defined and external functions",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the UDF to alter. The identifier can contain the schema name and database name, as well as the function name.\nIf the identifier contains spaces or special characters, the entire string must be enclosed in double quotes. Identifiers enclosed in\ndouble quotes are also case-sensitive."
          },
          {
            "term": "arg_data_type [ , ... ]",
            "definition": "Specifies the arguments/input data types for the external function. If the function accepts arguments, then the ALTER command must specify the argument types because functions support\nname overloading (i.e. two functions in the same schema can have the same name), and the argument types are used to\nidentify the function."
          },
          {
            "term": "SET ...",
            "definition": "Specifies the properties to set for the function: Specifies whether a function is secure. For more details, see Protecting Sensitive Information with Secure UDFs and Stored Procedures. Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL. For information about setting log level, see\nSetting levels for logging, metrics, and tracing. Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL. For information about setting trace level, see\nSetting levels for logging, metrics, and tracing. The names of external access integrations needed in order for this\nfunctions handler code to access external networks. An external access integration contains network rules and\nsecrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview. Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameters value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an\nexternal access integrations ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integrations name must, in turn, be specified as a value of this CREATE FUNCTION calls\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret. Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION\ncolumn in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "SECURE",
            "definition": "Specifies whether a function is secure. For more details, see Protecting Sensitive Information with Secure UDFs and Stored Procedures."
          },
          {
            "term": "LOG_LEVEL = 'log_level'",
            "definition": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL. For information about setting log level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "TRACE_LEVEL = 'trace_level'",
            "definition": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL. For information about setting trace level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nfunctions handler code to access external networks. An external access integration contains network rules and\nsecrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameters value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an\nexternal access integrations ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integrations name must, in turn, be specified as a value of this CREATE FUNCTION calls\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION\ncolumn in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output."
          },
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies the properties to unset for the function, which resets them to the defaults."
          },
          {
            "term": "SECURE",
            "definition": "Specifies whether a function is secure. For more details, see Protecting Sensitive Information with Secure UDFs and Stored Procedures."
          },
          {
            "term": "LOG_LEVEL = 'log_level'",
            "definition": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL. For information about setting log level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "TRACE_LEVEL = 'trace_level'",
            "definition": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL. For information about setting trace level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nfunctions handler code to access external networks. An external access integration contains network rules and\nsecrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameters value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an\nexternal access integrations ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integrations name must, in turn, be specified as a value of this CREATE FUNCTION calls\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION\ncolumn in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output."
          },
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          }
        ]
      },
      {
        "heading": "User-defined functions",
        "definitions": [
          {
            "term": "RENAME TO new_name",
            "definition": "Specifies the new identifier for the UDF; the combination of the identifier and existing argument data types must be unique for the schema. For more details, see Identifier requirements. Note When specifying the new name for the UDF, do not specify argument data types or parentheses; specify only the new name. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
          }
        ]
      },
      {
        "heading": "External functions",
        "definitions": [
          {
            "term": "RENAME TO new_name",
            "definition": "Specifies the new identifier for the function. The identifier does not need to be unique for the schema in which the function is created because functions are\nidentified and resolved by their name and argument types. However, the signature (name and parameter data types)\nmust be unique within the schema. The name must follow the rules for Snowflake identifiers.\nFor more details, see Identifier requirements. Note When specifying the new name for the external function, do not specify argument data types or parentheses;\nthe function will continue using the same arguments as before."
          },
          {
            "term": "api_integration_name",
            "definition": "This is the name of the API integration object that should be used to authenticate the call to the proxy service. More details about this parameter are in CREATE EXTERNAL FUNCTION."
          },
          {
            "term": "HEADERS = ( 'header_1' = 'value' [ , 'header_2' = 'value' ... ] )",
            "definition": "This clause allows users to attach key-value metadata that is sent with every request. The value must be a constant string, not an expression. More details about this parameter are in CREATE EXTERNAL FUNCTION."
          },
          {
            "term": "CONTEXT_HEADERS = ( [ context_function_1 [ , context_function_2 ... ] ] )",
            "definition": "This is similar to HEADERS, but instead of allowing only constant strings, it allows binding Snowflake\ncontext function results to HTTP headers. Each value must be the name of a context function. The names should not be quoted. More details about this parameter are in CREATE EXTERNAL FUNCTION."
          },
          {
            "term": "COMPRESSION = compression_type",
            "definition": "If this clause is specified, the JSON payload is compressed using the specified format when sent from Snowflake to\nthe proxy service, and when sent back from the proxy service to Snowflake. For more details about valid values of compression_type, see CREATE EXTERNAL FUNCTION."
          },
          {
            "term": "{ REQUEST_TRANSLATOR | RESPONSE_TRANSLATOR } = udf_name",
            "definition": "Add a request translator or a response translator if the external function does not already have one or replace an existing request translator\nor response translator by specifying the name of a previously-created JavaScript UDF.\nFor more information, see Using request and response translators with data for a remote service."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "USAGE",
                "Function",
                "Enables calling a UDF or external function."
              ],
              [
                "APPLY",
                "Tag",
                "Enables setting a tag on the UDF or external function."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "User-defined functions",
        "description": "\nIf using a UDF in a masking policy, ensure the data type of the column, UDF, and masking policy match. For\nmore information, see User-defined functions in a masking policy.\nIf using a UDF in a masking policy, ensure the data type of the column, UDF, and masking policy match. For\nmore information, see User-defined functions in a masking policy."
      },
      {
        "heading": "External functions",
        "description": "\nThere is no UNSET command for API_INTEGRATION. You can change the API_INTEGRATION, but you cannot unset it. For more, see\nALTER API INTEGRATION.\nThere is no UNSET command for API_INTEGRATION. You can change the API_INTEGRATION, but you cannot unset it. For more, see\nALTER API INTEGRATION."
      },
      {
        "heading": "Examples",
        "description": "\nRename the function function1 to function2:\nConvert a regular function function2 to a secure function:",
        "syntax": [
          "ALTER FUNCTION IF EXISTS function1(number) RENAME TO function2;",
          "ALTER FUNCTION function2(number) SET SECURE;"
        ]
      },
      {
        "heading": "External functions",
        "description": "\nChange the API Integration for an external function:\nSet the maximum number of rows per batch for an external function:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER FUNCTION function4(number) SET API_INTEGRATION = api_integration_2;",
          "ALTER FUNCTION function5(number) SET MAX_BATCH_ROWS = 100;"
        ]
      }
    ]
  },
  {
    "function": "DESCRIBE FUNCTION",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-function",
    "details": [
      {
        "heading": "DESCRIBE FUNCTION",
        "description": "\nDescribes the specified user-defined function (UDF) or external function, including the signature (i.e. arguments),\nreturn value, language, and body (i.e. definition).\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DROP FUNCTION , ALTER FUNCTION , CREATE FUNCTION , SHOW USER FUNCTIONS , SHOW EXTERNAL FUNCTIONS"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] FUNCTION <name> ( [ <arg_data_type> ] [ , ... ] )"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the function to describe. If the identifier contains spaces or special characters, the entire string must\nbe enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          },
          {
            "term": "arg_data_type [ , ... ]",
            "definition": "Specifies the data type of the argument(s), if any, for the function. The argument data types are necessary because functions support\nname overloading (i.e. two functions in the same schema can have the same name) and the argument data types are used to identify the\nfunction."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nThis demonstrates the DESCRIBE FUNCTION command:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESC FUNCTION multiply(number, number);\n\n-----------+----------------------------------+\n property  |              value               |\n-----------+----------------------------------+\n signature | (a NUMBER(38,0), b NUMBER(38,0)) |\n returns   | NUMBER(38,0)                     |\n language  | SQL                              |\n body      | a * b                            |\n-----------+----------------------------------+"
        ]
      }
    ]
  },
  {
    "function": "DROP FUNCTION",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-function",
    "details": [
      {
        "heading": "DROP FUNCTION",
        "description": "\nRemoves the specified user-defined function (UDF) or external function from the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE FUNCTION , ALTER FUNCTION , SHOW FUNCTIONS, DESCRIBE FUNCTION"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] )"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the UDF to drop. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          },
          {
            "term": "arg_data_type [ , ... ]",
            "definition": "Specifies the data type of the argument(s), if any, for the UDF. The argument types are necessary because UDFs support name\noverloading (i.e. two UDFs in the same schema can have the same name) and the argument types are used to identify the UDF you\nwish to drop."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nAll Languages\nDropped functions cant be recovered; they must be recreated.\nDropped functions cant be recovered; they must be recreated.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nJava, Python, and Scala\nFor UDFs that store code in a file (such as a .jar file or .py file) in a stage, the DROP FUNCTION command does not remove\nthe file. Different UDFs can use different functions/methods in the same file, so the file should not be removed while any UDF\nrefers to it. Snowflake does not store a count of the number of references to each staged file and does not remove that staged\nfile when there are no remaining references.\nFor UDFs that store code in a file (such as a .jar file or .py file) in a stage, the DROP FUNCTION command does not remove\nthe file. Different UDFs can use different functions/methods in the same file, so the file should not be removed while any UDF\nrefers to it. Snowflake does not store a count of the number of references to each staged file and does not remove that staged\nfile when there are no remaining references."
      },
      {
        "heading": "Examples",
        "description": "\nThis demonstrates the DROP FUNCTION command:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP FUNCTION multiply(number, number);\n\n--------------------------------+\n             status             |\n--------------------------------+\n MULTIPLY successfully dropped. |\n--------------------------------+"
        ]
      }
    ]
  },
  {
    "function": "SHOW USER FUNCTIONS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-user-functions",
    "details": [
      {
        "heading": "SHOW USER FUNCTIONS",
        "description": "\nLists all user-defined functions (UDFs) for which you have access privileges. Use this command to list the UDFs for a specified\ndatabase or schema (or the current database/schema for the session), or across your entire account.\nFor a command that lists all functions, including built-in functions, see SHOW FUNCTIONS.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "SHOW FUNCTIONS, SHOW EXTERNAL FUNCTIONS, FUNCTIONS view (Information Schema),\nFUNCTIONS view (Account Usage)"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW USER FUNCTIONS [ LIKE '<pattern>' ]\n  [ IN\n    {\n      ACCOUNT                                         |\n\n      DATABASE                                        |\n      DATABASE <database_name>                        |\n\n      SCHEMA                                          |\n      SCHEMA <schema_name>                            |\n      <schema_name>\n\n      APPLICATION <application_name>                  |\n      APPLICATION PACKAGE <application_package_name>  |\n    }\n  ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Timestamp at which the user-defined function (UDF) was created."
              ],
              [
                "name",
                "Name of the UDF."
              ],
              [
                "schema_name",
                "Name of the schema in which the UDF exists."
              ],
              [
                "is_builtin",
                "Always N for user-defined functions. See SHOW FUNCTIONS for a command to list all functions, including built-in functions."
              ],
              [
                "is_aggregate",
                "Y if the function is an aggregate function; N otherwise."
              ],
              [
                "is_ansi",
                "Not applicable currently."
              ],
              [
                "min_num_arguments",
                "Minimum number of arguments to the UDF."
              ],
              [
                "max_num_arguments",
                "Maximum number of arguments to the UDF."
              ],
              [
                "arguments",
                "Data types of the arguments and return value."
              ],
              [
                "description",
                "Description of the UDF."
              ],
              [
                "catalog_name",
                "Name of the database in which the UDF exists."
              ],
              [
                "is_table_function",
                "Y if the UDF is a table function; N otherwise."
              ],
              [
                "valid_for_clustering",
                "Y if the UDF can be used in a CLUSTER BY expression; N otherwise."
              ],
              [
                "is_secure",
                "Y if the UDF is a secure UDF; N otherwise."
              ],
              [
                "secrets",
                "Map of secret values specified by the functions SECRETS parameter, where map keys are secret variable names and map values are secret object names."
              ],
              [
                "external_access_integrations",
                "Names of external access integrations specified by the functions EXTERNAL_ACCESS_INTEGRATION parameter."
              ],
              [
                "is_external_function",
                "Y if the function is an external function; N otherwise. See SHOW EXTERNAL FUNCTIONS for a command to list external functions."
              ],
              [
                "language",
                "Programming language of the UDF (for example, PYTHON or SQL)."
              ],
              [
                "is_memoizable",
                "Y if the function is memoizable; N otherwise."
              ],
              [
                "is_data_metric",
                "Y if the function is a data metric function; N otherwise."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nShow all the UDFs that you have privileges to view in the current database:\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nRelated content\nUser-defined functions overview\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW USER FUNCTIONS LIKE 'ALLOWED_REGIONS%' IN SCHEMA;",
          "---------------------------------+--------------------------+-------------+------------+--------------+---------+-------------------+-------------------+-----------------------------------------+-----------------------+----------------+-------------------+----------------------+-----------+---------+-----------------------------+----------------------+----------+---------------+----------------+\n          created_on             |           name           | schema_name | is_builtin | is_aggregate | is_ansi | min_num_arguments | max_num_arguments |                arguments                |      description      |  catalog_name  | is_table_function | valid_for_clustering | is_secure | secrets | external_access_integration | is_external_function | language | is_memoizable | is_data_metric |\n---------------------------------+--------------------------+-------------+------------+--------------+---------+-------------------+-------------------+-----------------------------------------+-----------------------+----------------+-------------------+----------------------+-----------+---------+-----------------------------+----------------------+----------+---------------+----------------+\n Fri, 23 Jun 1967 00:00:00 -0700 | ALLOWED_REGIONS          | PUBLIC      | N          | N            | N       | 0                 | 0                 | ALLOWED_REGIONS() RETURN ARRAY          | user-defined function | MEMO_FUNC_TEST | N                 | N                    | N         |         |                             | N                    | SQL      | Y             | N              |\n Fri, 23 Jun 1967 00:00:00 -0700 | ALLOWED_REGIONS_NON_MEMO | PUBLIC      | N          | N            | N       | 0                 | 0                 | ALLOWED_REGIONS_NON_MEMO() RETURN ARRAY | user-defined function | MEMO_FUNC_TEST | N                 | N                    | N         |         |                             | N                    | SQL      | N             | N              |\n---------------------------------+--------------------------+-------------+------------+--------------+---------+-------------------+-------------------+-----------------------------------------+-----------------------+----------------+-------------------+----------------------+-----------+---------+-----------------------------+----------------------+----------+---------------+----------------+"
        ]
      }
    ]
  },
  {
    "function": "CREATE DATA METRIC FUNCTION",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-data-metric-function",
    "details": [
      {
        "heading": "CREATE DATA METRIC FUNCTION",
        "description": "\nEnterprise Edition Feature\nData Quality and data metric functions (DMFs) require Enterprise Edition. To inquire about upgrading, please contact\nSnowflake Support.\nCreates a new data metric function (DMF) in the current or specified schema, or replaces an existing data metric function.\nAfter creating a DMF, apply it to a table column using an\nALTER TABLE  ALTER COLUMN command or a view column using the ALTER VIEW command.\nThis command supports the following variants:\nCREATE OR ALTER DATA METRIC FUNCTION: Creates a new data metric function if it doesnt exist or alters an existing data metric function.\nCREATE OR ALTER DATA METRIC FUNCTION: Creates a new data metric function if it doesnt exist or alters an existing data metric function.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DMF command reference, CREATE OR ALTER <object>"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CREATE [ OR REPLACE ] [ SECURE ] DATA METRIC FUNCTION [ IF NOT EXISTS ] <name>\n  ( <table_arg> TABLE( <col_arg> <data_type> [ , ... ] )\n    [ , <table_arg> TABLE( <col_arg> <data_type> [ , ... ] ) ] )\n  RETURNS NUMBER [ [ NOT ] NULL ]\n  [ LANGUAGE SQL ]\n  [ COMMENT = '<string_literal>' ]\n  AS\n  '<expression>'"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE OR ALTER DATA METRIC FUNCTION",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nCreates a new data metric function if it doesnt already exist, or transforms an existing data metric function into\nthe function defined in the statement. A CREATE OR ALTER DATA METRIC FUNCTION statement follows the syntax rules of\na CREATE DATA METRIC FUNCTION statement and has the same limitations as an ALTER FUNCTION (DMF)\nstatement.\nUnlike a CREATE OR REPLACE DATA METRIC FUNCTION command, a CREATE OR ALTER command updates the object without\ndeleting and recreating it.\nSupported function alterations include changes to the COMMENT property.\nFor more information, see CREATE OR ALTER DATA METRIC FUNCTION usage notes.",
        "syntax": [
          "CREATE [ OR ALTER ] DATA METRIC FUNCTION ..."
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Identifier for the DMF; must be unique for your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements."
          },
          {
            "term": "( table_arg TABLE( col_arg data_type [ , ... ] ) [ , table_arg TABLE( col_arg data_type [ , ... ] ) ] )",
            "definition": "The signature for the DMF, which is used as input for the expression. You must specify: An argument name for each table (table_arg). For each table, an argument name for at least one column, along with its data type (col_arg data_type). You can optionally specify arguments for additional columns and their data types. The columns must be in the same table and cannot\nreference a different table."
          },
          {
            "term": "RETURNS NUMBER",
            "definition": "The data type of the output of the function. The data type can only be NUMBER."
          },
          {
            "term": "AS expression",
            "definition": "SQL expression that determines the output of the function. The expression must be deterministic and return a scalar value. The expression\ncan reference other table objects, such as by using a WITH clause or a\nWHERE clause. The delimiters around the expression can be either single quotes or a pair of dollar signs. Using $$ as the delimiter makes\nit easier to write expressions that contain single quotes. If the delimiter for the expression is the single quote character, then any single quotes within expression\n(for example, string literals) must be escaped by single quotes. The expression does not support the following: Using nondeterministic functions (for example, CURRENT_TIME). Referencing an object that depends on a UDF or UDTF. Returning a nonscalar output."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "SECURE",
            "definition": "Specifies that the data metric function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures."
          },
          {
            "term": "LANGUAGE SQL",
            "definition": "Specifies the language used to write the expression. SQL is the only supported language."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "A comment for the DMF."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE DATA METRIC FUNCTION",
                "Schema",
                "The privilege only enables the creation of data metric functions in the schema.\nIf you want to enable the creation of user-defined functions, such as SQL or Java UDFs, the role must have the CREATE FUNCTION\nprivilege."
              ]
            ]
          }
        ]
      },
      {
        "heading": "General usage notes",
        "description": "\nIf you want to update an existing data metric function and need to see the current definition of the function, run the\nDESCRIBE FUNCTION (DMF) command or call the GET_DDL function.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nIf you want to update an existing data metric function and need to see the current definition of the function, run the\nDESCRIBE FUNCTION (DMF) command or call the GET_DDL function.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
      },
      {
        "heading": "CREATE OR ALTER DATA METRIC FUNCTION usage notes",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nThe following alterations are not supported:\n\nChanging the body of the data metric function.\nChanging the data metric and the input table definition.\nChanging the body of the data metric function.\nChanging the data metric and the input table definition.\nThe following alterations are not supported:\nChanging the body of the data metric function.\nChanging the data metric and the input table definition.\nChanging the body of the data metric function.\nChanging the data metric and the input table definition."
      },
      {
        "heading": "Example: Single table argument",
        "description": "\nCreate a DMF that calls the COUNT function to return the total number of rows that\nhave positive numbers in three columns of the table:",
        "syntax": [
          "CREATE OR REPLACE DATA METRIC FUNCTION governance.dmfs.count_positive_numbers(\n  arg_t TABLE(\n    arg_c1 NUMBER,\n    arg_c2 NUMBER,\n    arg_c3 NUMBER\n  )\n)\nRETURNS NUMBER\nAS\n$$\n  SELECT\n    COUNT(*)\n  FROM arg_t\n  WHERE\n    arg_c1>0\n    AND arg_c2>0\n    AND arg_c3>0\n$$;"
        ]
      },
      {
        "heading": "Example: Multiple table arguments",
        "description": "\nReturns the number of records where the value of a column in one table does not have a corresponding value in the column of another table:\nFor an example that uses this DMF to validate referential integrity, see Example: Using multiple table arguments to perform referential checks.",
        "syntax": [
          "CREATE OR REPLACE DATA METRIC FUNCTION governance.dmfs.referential_check(\n  arg_t1 TABLE (arg_c1 INT), arg_t2 TABLE (arg_c2 INT))\nRETURNS NUMBER\nAS\n$$\n  SELECT\n    COUNT(*)\n    FROM arg_t1\n  WHERE\n    arg_c1 NOT IN (SELECT arg_c2 FROM arg_t2)\n$$;"
        ]
      },
      {
        "heading": "Example: Alter a data metric function using the CREATE OR ALTER DATA METRIC FUNCTION command",
        "description": "\nAlters the single-table data metric function created in the example above to set security and comment.\nOn this page\nSyntax\nVariant syntax\nRequired parameters\nOptional parameters\nAccess control requirements\nGeneral usage notes\nCREATE OR ALTER DATA METRIC FUNCTION usage notes\nExample: Single table argument\nExample: Multiple table arguments\nExample: Alter a data metric function using the CREATE OR ALTER DATA METRIC FUNCTION command\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR ALTER SECURE DATA METRIC FUNCTION governance.dmfs.count_positive_numbers(\n  arg_t TABLE(\n    arg_c1 NUMBER,\n    arg_c2 NUMBER,\n    arg_c3 NUMBER\n  )\n)\nRETURNS NUMBER\nCOMMENT = \"count positive numbers\"\nAS\n$$\n  SELECT\n    COUNT(*)\n  FROM arg_t\n  WHERE\n    arg_c1>0\n    AND arg_c2>0\n    AND arg_c3>0\n$$;"
        ]
      }
    ]
  },
  {
    "function": "ALTER FUNCTION (DMF)",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-function-dmf",
    "details": [
      {
        "heading": "ALTER FUNCTION (DMF)",
        "description": "\nEnterprise Edition Feature\nData Quality and data metric functions (DMFs) require Enterprise Edition. To inquire about upgrading, please contact\nSnowflake Support.\nModifies the properties of an existing data metric function (DMF).\nTo make any other changes to a DMF, you must drop the function using a DROP FUNCTION command and\nrecreate the DMF.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DMF command reference"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "ALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET COMMENT = '<string_literal>'\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET COMMENT\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET TAG <tag_name> [ , <tag_name> ... ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the DMF to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements."
          },
          {
            "term": "TABLE( arg_data_type [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ]",
            "definition": "Specifies the data type of the column arguments for the DMF. The data types are necessary because DMFs support name\noverloading, where two DMFs in the same schema can have the same name. The data types of the arguments are used to identify the DMF you\nwant to alter."
          },
          {
            "term": "RENAME TO new_name",
            "definition": "Specifies the new identifier for the DMF; the combination of the identifier and existing argument data types must be unique for the\nschema. For more information, see Identifier requirements. Note When specifying the new name for the UDF, dont specify argument data types or parentheses; specify only the new name. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
          },
          {
            "term": "SET ...",
            "definition": "Specifies the properties to set for the DMF: Specifies whether a function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures. Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION\ncolumn in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "SECURE",
            "definition": "Specifies whether a function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION\ncolumn in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output."
          },
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies the properties to unset for the function, which resets them to the defaults. SECURE COMMENT TAG tag_name [ , tag_name ... ]"
          },
          {
            "term": "SECURE",
            "definition": "Specifies whether a function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION\ncolumn in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output."
          },
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "OWNERSHIP",
                "Data metric function",
                ""
              ],
              [
                "APPLY",
                "Tag",
                "Enables setting a tag on the DMF."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nIf you want to update an existing data metric function and need to see the current definition of the function, run the\nDESCRIBE FUNCTION (DMF) command or call the GET_DDL function.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nIf you want to update an existing data metric function and need to see the current definition of the function, run the\nDESCRIBE FUNCTION (DMF) command or call the GET_DDL function.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "Example",
        "description": "\nYou can use the ALTER FUNCTION command to make a DMF secure. For more information about what it means for a function to be secure, see\nProtecting Sensitive Information with Secure UDFs and Stored Procedures.\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExample\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER FUNCTION governance.dmfs.count_positive_numbers(\n TABLE(\n   NUMBER,\n   NUMBER,\n   NUMBER\n))\nSET SECURE;"
        ]
      }
    ]
  },
  {
    "function": "DESCRIBE FUNCTION (DMF)",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-function-dmf",
    "details": [
      {
        "heading": "DESCRIBE FUNCTION (DMF)",
        "description": "\nEnterprise Edition Feature\nData Quality and data metric functions (DMFs) require Enterprise Edition. To inquire about upgrading, please contact\nSnowflake Support.\nDescribes the specified data metric function (DMF), including the signature (arguments), return value, language, and body (definition).",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DMF command reference"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "{ DESC | DESCRIBE } FUNCTION [ IF EXISTS ] <name>(\n  TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ]\n  )"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the function to describe. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements."
          },
          {
            "term": "TABLE( arg_data_type [ , ... ] ) [ , TABLE( arg_data_type [ , ... ] ) ]",
            "definition": "Specifies the data type of the column arguments for the DMF. The data types are necessary because DMFs support name overloading\n(that is, two DMFs in the same schema can have the same name), and the data types of the argument are used to identify the DMF you want to\ndescribe."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "USAGE",
                "Data metric function",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Example",
        "description": "\nDescribe the DMF to view its properties:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExample\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESC FUNCTION governance.dmfs.count_positive_numbers(\n  TABLE(\n    NUMBER, NUMBER, NUMBER\n  )\n);",
          "+-----------+---------------------------------------------------------------------+\n| property  | value                                                               |\n+-----------+---------------------------------------------------------------------+\n| signature | (ARG_T TABLE(ARG_C1 NUMBER, ARG_C2 NUMBER, ARG_C3 NUMBER))          |\n| returns   | NUMBER(38,0)                                                        |\n| language  | SQL                                                                 |\n| body      | SELECT COUNT(*) FROM arg_t WHERE arg_c1>0 AND arg_c2>0 AND arg_c3>0 |\n+-----------+---------------------------------------------------------------------+"
        ]
      }
    ]
  },
  {
    "function": "DROP FUNCTION (DMF)",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-function-dmf",
    "details": [
      {
        "heading": "DROP FUNCTION (DMF)",
        "description": "\nEnterprise Edition Feature\nData Quality and data metric functions (DMFs) require Enterprise Edition. To inquire about upgrading, please contact\nSnowflake Support.\nRemoves the specified data metric function (DMF) from the current or specified schema."
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP FUNCTION [ IF EXISTS ] <name>(\nTABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ]\n)"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Identifier for the DMF to drop. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements."
          },
          {
            "term": "TABLE( arg_data_type [ , ... ] ) [ , TABLE( arg_data_type [ , ... ] ) ]",
            "definition": "Specifies the data type of the column arguments for the DMF. The data types are necessary because DMFs support name overloading\n(that is, two DMFs in the same schema can have the same name), and the data types of the arguments are used to identify the DMF you want to\ndrop."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "OWNERSHIP",
                "Data metric function",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Example",
        "description": "\nDrop a custom DMF from the system:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExample\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP FUNCTION governance.dmfs.count_positive_numbers(\n  TABLE(\n    NUMBER, NUMBER, NUMBER\n  )\n);"
        ]
      }
    ]
  },
  {
    "function": "SHOW DATA METRIC FUNCTIONS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-data-metric-functions",
    "details": [
      {
        "heading": "SHOW DATA METRIC FUNCTIONS",
        "description": "\nEnterprise Edition Feature\nData Quality and data metric functions (DMFs) require Enterprise Edition. To inquire about upgrading, please contact\nSnowflake Support.\nLists the data metric functions (DMFs) for which you have access privileges.\nYou can use this command to list the DMFs in the current database and schema for the session, a specified database or\nschema, or your entire account.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE DATA METRIC FUNCTION , ALTER FUNCTION (DMF), DESCRIBE FUNCTION (DMF) , DROP FUNCTION (DMF)"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW DATA METRIC FUNCTIONS\n  [ LIKE '<pattern>' ]\n  [ IN\n      {\n        ACCOUNT                  |\n\n        DATABASE                 |\n        DATABASE <database_name> |\n\n        SCHEMA                   |\n        SCHEMA <schema_name>     |\n        <schema_name>\n      }\n  ]\n  [ STARTS WITH '<name_string>' ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Timestamp at which the function was created."
              ],
              [
                "name",
                "Name of the function."
              ],
              [
                "schema_name",
                "Name of the schema that the function exists in. NULL for built-in functions."
              ],
              [
                "is_builtin",
                "Y if the function is a built-in function; N otherwise."
              ],
              [
                "is_aggregate",
                "Y if the function is an aggregate function; N otherwise."
              ],
              [
                "is_ansi",
                "Y if the function is defined as part of the ANSI SQL standard; N otherwise."
              ],
              [
                "min_num_arguments",
                "Minimum number of arguments."
              ],
              [
                "max_num_arguments",
                "Maximum number of arguments."
              ],
              [
                "arguments",
                "Shows the data types of the arguments and of the return value."
              ],
              [
                "description",
                "Description of the function."
              ],
              [
                "catalog_name",
                "Name of the database that the function exists in. NULL for built-in functions."
              ],
              [
                "is_table_function",
                "Y if the function is a table function; N otherwise."
              ],
              [
                "valid_for_clustering",
                "Y if the function can be used in a CLUSTER BY expression; N otherwise."
              ],
              [
                "is_secure",
                "Y if the function is a secure function; N otherwise."
              ],
              [
                "is_external_function",
                "Y if the function is an external function; N otherwise."
              ],
              [
                "language",
                "For built-in functions, this column shows SQL.\nFor user-defined functions, this column shows the language in which the function was written, such as JAVASCRIPT or SQL. See SHOW USER FUNCTIONS.\nFor external functions, this column shows EXTERNAL."
              ],
              [
                "is_memoizable",
                "Y if the function is memoizable; N otherwise."
              ],
              [
                "is_data_metric",
                "Y if the function is a DMF; N otherwise."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "USAGE",
                "Data metric function",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nThe following example lists the DMFs that you have the privileges to view in the dmfs schema of the\ngovernance database:\nOn this page\nSyntax\nParameters\nOutput\nAccess control requirements\nUsage notes\nExamples\nRelated content\nUse data metric functions to perform data quality checks\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "USE SCHEMA governance.dmfs;\n\nSHOW DATA METRIC FUNCTIONS;",
          "+--------------------------+------------------------+-------------+------------+--------------+---------+-------------------+-------------------+--------------------------------------------------------------------------------------------+-----------------------+--------------+-------------------+----------------------+-----------+----------------------+----------+---------------+----------------+\n| created_on               | name                   | schema_name | is_builtin | is_aggregate | is_ansi | min_num_arguments | max_num_arguments | arguments                                                                                  | description           | catalog_name | is_table_function | valid_for_clustering | is_secure | is_external_function | language | is_memoizable | is_data_metric |\n+--------------------------+------------------------+-------------+------------+--------------+---------+-------------------+-------------------+--------------------------------------------------------------------------------------------+-----------------------+--------------+-------------------+----------------------+-----------+----------------------+----------+---------------+----------------+\n| 2023-12-11T23:30:02.785Z | COUNT_POSITIVE_NUMBERS | DMFS        | N          | N            | N       | 1                 | 1                 | \"COUNT_POSITIVE_NUMBERS(TABLE(NUMBER, NUMBER, NUMBER)) RETURNS NUMBER\"                     | user-defined function | GOVERNANCE   | N                 | N                    | N         | N                    | SQL      | N             | Y              |\n+--------------------------+------------------------+-------------+------------+--------------+---------+-------------------+-------------------+--------------------------------------------------------------------------------------------+-----------------------+--------------+-------------------+----------------------+-----------+----------------------+----------+---------------+----------------+"
        ]
      }
    ]
  },
  {
    "function": "CREATE PROCEDURE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-procedure",
    "details": [
      {
        "heading": "CREATE PROCEDURE",
        "description": "\nCreates a new stored procedure.\nA procedure can be written in one of the following languages:\nJava (using Snowpark)\nJavaScript\nPython (using Snowpark)\nScala (using Snowpark)\nSnowflake Scripting\nJava (using Snowpark)\nJavaScript\nPython (using Snowpark)\nScala (using Snowpark)\nSnowflake Scripting\nNote\nWhen you want to create and call a procedure that is anonymous (rather than stored), use CALL (with anonymous procedure).\nCreating an anonymous procedure does not require a role with CREATE PROCEDURE schema privileges.\nThis command supports the following variants:\nCREATE OR ALTER PROCEDURE: Creates a new procedure if it doesnt exist or alters an existing procedure.\nCREATE OR ALTER PROCEDURE: Creates a new procedure if it doesnt exist or alters an existing procedure.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER PROCEDURE, DROP PROCEDURE , SHOW PROCEDURES , DESCRIBE PROCEDURE, CALL,\nSHOW USER PROCEDURES CREATE OR ALTER <object>"
          }
        ]
      },
      {
        "heading": "Syntax"
      },
      {
        "heading": "Java handler",
        "description": "\nYou can create a stored procedure that either includes its handler code in-line, or refers to its handler code in a JAR file. For more\ninformation, see Keeping handler code in-line or on a stage.\nFor examples of Java stored procedures, see Writing Java handlers for stored procedures created with SQL.\nPreview Feature  Open\nUsing tabular stored procedures with a Java handler is a preview feature\nthat is available to all accounts.\nFor in-line stored procedures, use the following syntax:\nFor a stored procedure that uses a precompiled handler, use the following syntax.",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '<java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'",
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '<java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]"
        ]
      },
      {
        "heading": "JavaScript handler",
        "description": "\nFor examples of JavaScript stored procedures, see Writing stored procedures in JavaScript.\nImportant\nJavaScript is case-sensitive, whereas SQL is not. See Case-sensitivity in JavaScript arguments for\nimportant information about using stored procedure argument names in the JavaScript code.",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type> [ NOT NULL ]\n  LANGUAGE JAVASCRIPT\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'"
        ]
      },
      {
        "heading": "Python handler",
        "description": "\nFor examples of Python stored procedures, see Writing stored procedures with SQL and Python.\nFor in-line stored procedures, use the following syntax:\nFor a stored procedure in which the code is in a file on a stage, use the following syntax:",
        "syntax": [
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  [ ARTIFACT_REPOSITORY = `<repository_name>` ]\n  [ PACKAGES = ( '<package_name>' [ , ... ] ) ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }]\n  AS '<procedure_definition>'",
          "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  [ ARTIFACT_REPOSITORY = `<repository_name>` ]\n  [ PACKAGES = ( '<package_name>' [ , ... ] ) ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<module_file_name>.<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]"
        ]
      },
      {
        "heading": "Scala handler",
        "description": "\nYou can create a stored procedure that either includes its handler code in-line, or refers to its handler code in a JAR file. For more\ninformation, see Keeping handler code in-line or on a stage.\nFor examples of Scala stored procedures, see Writing Scala handlers for stored procedures created with SQL.\nPreview Feature  Open\nUsing tabular stored procedures with a Scala handler via RETURNS TABLE(...)\nis a preview feature that is available to all accounts.\nFor in-line stored procedures, use the following syntax:\nFor a stored procedure that uses a precompiled handler, use the following syntax.",
        "syntax": [
          "CREATE [ OR REPLACE ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '<scala_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'",
          "CREATE [ OR REPLACE ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '<scala_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]"
        ]
      },
      {
        "heading": "Snowflake Scripting handler",
        "description": "\nFor examples of Snowflake Scripting stored procedures, see Writing stored procedures in Snowflake Scripting.\nNote\nIf you are creating a Snowflake Scripting procedure in SnowSQL or the Classic Console, you must\nuse string literal delimiters (' or $$) around\nprocedure definition. See Using Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector.",
        "syntax": [
          "CREATE [ OR REPLACE ] PROCEDURE <name> (\n    [ <arg_name> [ { IN | INPUT | OUT | OUTPUT } ] <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  [ NOT NULL ]\n  LANGUAGE SQL\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS <procedure_definition>"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE OR ALTER PROCEDURE",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nCreates a new procedure if it doesnt already exist, or transforms an existing procedure into the procedure defined in the\nstatement. A CREATE OR ALTER PROCEDURE statement follows the syntax rules of a CREATE PROCEDURE statement and has the same\nlimitations as an ALTER PROCEDURE statement.\nAlterations to the following are supported:\nLOG_LEVEL, TRACE_LEVEL, COMMENT, SECURE\nSECRETS and EXTERNAL_ACCESS_INTEGRATIONS for Python, Scala, and Java Stored Procs\nExecution Privileges (EXECUTE AS CALLER or EXECUTE AS OWNER)\nLOG_LEVEL, TRACE_LEVEL, COMMENT, SECURE\nSECRETS and EXTERNAL_ACCESS_INTEGRATIONS for Python, Scala, and Java Stored Procs\nExecution Privileges (EXECUTE AS CALLER or EXECUTE AS OWNER)\nFor more information, see CREATE OR ALTER PROCEDURE usage notes.",
        "syntax": [
          "CREATE [ OR ALTER ] PROCEDURE ..."
        ]
      },
      {
        "heading": "Required parameters"
      },
      {
        "heading": "All languages",
        "description": "\nFor RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the\nSnowflake data types of the columns in the returned table, specify the column names and\ntypes:\nCREATE OR REPLACE PROCEDURE get_top_sales()\n  RETURNS TABLE (sales_date DATE, quantity NUMBER)\n...\n\nCopy\nOtherwise (for example, if you are determining the column types during run time), you can omit the column names and types:\nCREATE OR REPLACE PROCEDURE get_top_sales()\n  RETURNS TABLE ()\n\nCopy\n\nNote\nCurrently, in the RETURNS TABLE(...) clause, you cant specify GEOGRAPHY as a column type. This\napplies whether you are creating a stored or anonymous procedure.\nCREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE(g GEOGRAPHY)\n  ...\n\nCopy\nWITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE(g GEOGRAPHY)\n  ...\nCALL test_return_geography_table_1();\n\nCopy\nIf you attempt to specify GEOGRAPHY as a column type, calling the stored procedure results in the error:\nStored procedure execution error: data type of returned table does not match expected returned table type\n\nCopy\nTo work around this issue, you can omit the column arguments and types in RETURNS TABLE().\nCREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE()\n  ...\n\nCopy\nWITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE()\n  ...\nCALL test_return_geography_table_1();\n\nCopy\n\nRETURNS TABLE() is supported only when the handler is written in the following languages:\n\nJava\nPython\nScala\nSnowflake Scripting\nFor stored procedures in JavaScript, if you are writing a string that contains newlines, you can use\nbackquotes (also called backticks) around the string.\nThe following example of a JavaScript stored procedure uses $$ and backquotes because the body of the stored procedure\ncontains single quotes and double quotes:\n\nCREATE OR REPLACE TABLE table1 (\"column 1\" VARCHAR);\n\nCopy\nCREATE or replace PROCEDURE proc3()\n  RETURNS VARCHAR\n  LANGUAGE javascript\n  AS\n  $$\n  var rs = snowflake.execute( { sqlText: \n      `INSERT INTO table1 (\"column 1\") \n           SELECT 'value 1' AS \"column 1\" ;`\n       } );\n  return 'Done.';\n  $$;\n\nCopy",
        "syntax": [
          "CREATE OR REPLACE PROCEDURE get_top_sales()\n  RETURNS TABLE (sales_date DATE, quantity NUMBER)\n...",
          "CREATE OR REPLACE PROCEDURE get_top_sales()\n  RETURNS TABLE ()",
          "CREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE(g GEOGRAPHY)\n  ...",
          "WITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE(g GEOGRAPHY)\n  ...\nCALL test_return_geography_table_1();",
          "Stored procedure execution error: data type of returned table does not match expected returned table type",
          "CREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE()\n  ...",
          "WITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE()\n  ...\nCALL test_return_geography_table_1();",
          "CREATE OR REPLACE TABLE table1 (\"column 1\" VARCHAR);",
          "CREATE or replace PROCEDURE proc3()\n  RETURNS VARCHAR\n  LANGUAGE javascript\n  AS\n  $$\n  var rs = snowflake.execute( { sqlText: \n      `INSERT INTO table1 (\"column 1\") \n           SELECT 'value 1' AS \"column 1\" ;`\n       } );\n  return 'Done.';\n  $$;"
        ],
        "definitions": [
          {
            "term": "name ( [ arg_name [ { IN | INPUT | OUT | OUTPUT } ] arg_data_type . [ DEFAULT {default_value} ] ] [ , ... ] )",
            "definition": "Specifies the identifier (name), any arguments, and the default values for any optional arguments for the\nstored procedure. For the identifier: The identifier does not need to be unique for the schema in which the procedure is created because stored procedures are\nidentified and resolved by the combination of the name and argument types. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. My object). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements. For the arguments: For arg_name, specify the name of the argument. For { IN | INPUT | OUT | OUTPUT }, specify the type of the argument (input or output). The type specification is only valid\nfor a Snowflake Scripting stored procedure. For more information, see Using arguments passed to a stored procedure. For arg_data_type, use the Snowflake data type that corresponds to the language that you are using. For Java stored procedures, see SQL-Java Data Type Mappings. For JavaScript stored procedures, see\nSQL and JavaScript data type mapping. For Python stored procedures, see\nSQL-Python Data Type Mappings. For Scala stored procedures, see SQL-Scala Data Type Mappings. For Snowflake Scripting, a SQL data type. Note For stored procedures you write in Java, Python, or Scala (which use Snowpark APIs), omit the argument for the Snowpark\nSession object. The Session argument is not a formal parameter that you specify in CREATE PROCEDURE or CALL. When you call your\nstored procedure, Snowflake automatically creates a Session object and passes it to the handler function for your\nstored procedure. To indicate that an argument is optional, use DEFAULT default_value to specify the default value of the argument.\nFor the default value, you can use a literal or an expression. If you specify any optional arguments, you must place these after the required arguments. If a procedure has optional arguments, you cannot define additional procedures with the same name and different signatures. For details, see Specify optional arguments."
          },
          {
            "term": "RETURNS { result_data_type [ [ NOT ] NULL ] | TABLE ( [ col_name col_data_type [ , ... ] ] ) }",
            "definition": "Specifies the type of the result returned by the stored procedure. For result_data_type, use the Snowflake data type that corresponds to the type of the language that you are using. For Java stored procedures, see SQL-Java Data Type Mappings. For JavaScript stored procedures, see\nSQL and JavaScript data type mapping. For Python stored procedures, see\nSQL-Python Data Type Mappings. For Scala stored procedures, see SQL-Scala Data Type Mappings. For Snowflake Scripting, a SQL data type. Note Stored procedures you write in Snowpark (Java or Scala) must have a return value. In Snowpark (Python), when a stored procedure\nreturns no value, it is considered to be returning None. Note that every CREATE PROCEDURE statement must include a RETURNS\nclause that defines a return type, even if the procedure does not explicitly return anything. For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the\nSnowflake data types of the columns in the returned table, specify the column names and\ntypes: Otherwise (for example, if you are determining the column types during run time), you can omit the column names and types: Note Currently, in the RETURNS TABLE(...) clause, you cant specify GEOGRAPHY as a column type. This\napplies whether you are creating a stored or anonymous procedure. If you attempt to specify GEOGRAPHY as a column type, calling the stored procedure results in the error: To work around this issue, you can omit the column arguments and types in RETURNS TABLE(). RETURNS TABLE() is supported only when the handler is written in the following languages: Java Python Scala Snowflake Scripting As a practical matter, outside of a Snowflake Scripting block,\nthe returned value cannot be used because the call cannot be part of an expression."
          },
          {
            "term": "LANGUAGE language",
            "definition": "Specifies the language of the stored procedure code. Note that this is optional for stored procedures written with\nSnowflake Scripting. Currently, the supported values for language include: JAVA (for Java) JAVASCRIPT (for JavaScript) PYTHON (for Python) SCALA (for Scala) SQL (for Snowflake Scripting) Default: SQL."
          },
          {
            "term": "AS procedure_definition",
            "definition": "Defines the code executed by the stored procedure. The definition can consist of any valid code. Note the following: For stored procedures for which the code is not in-line, omit the AS clause. This includes stored procedures with staged handlers. Instead, use the IMPORTS clause to specify the location of the file containing the code for the stored procedure. For\ndetails, see: Writing stored procedures with SQL and Python Writing Java handlers for stored procedures created with SQL Writing Scala handlers for stored procedures created with SQL For more information on in-line and staged handlers, see Keeping handler code in-line or on a stage. You must use string literal delimiters (' or $$) around\nprocedure definition if: You are using a language other than Snowflake Scripting. You are creating a Snowflake Scripting procedure in SnowSQL or the Classic Console. See\nUsing Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector. For stored procedures in JavaScript, if you are writing a string that contains newlines, you can use\nbackquotes (also called backticks) around the string. The following example of a JavaScript stored procedure uses $$ and backquotes because the body of the stored procedure\ncontains single quotes and double quotes: Snowflake does not completely validate the code when you execute the CREATE PROCEDURE command. For example, for Snowpark (Scala) stored procedures, the number and types of arguments are validated, but the body of\nthe function is not validated. If the number or types do not match (e.g. if the Snowflake data type NUMBER is used when the\nargument is a non-numeric type), executing the CREATE PROCEDURE command causes an error. If the code is not valid, the CREATE PROCEDURE command will succeed, and errors will be returned when the stored procedure is\ncalled. For more details about stored procedures, see Working with stored procedures."
          }
        ]
      },
      {
        "heading": "Java",
        "syntax": [
          "SELECT * FROM INFORMATION_SCHEMA.PACKAGES WHERE LANGUAGE = 'java';",
          "domain:package_name:version",
          "PACKAGES = ('com.snowflake:snowpark:latest')",
          "com.my_company.my_package.MyClass.myMethod",
          "com.my_company.my_package",
          "package com.my_company.my_package;"
        ],
        "definitions": [
          {
            "term": "RUNTIME_VERSION = 'language_runtime_version'",
            "definition": "The language runtime version to use. Currently, the supported versions are: 11"
          },
          {
            "term": "PACKAGES = ( 'snowpark_package_name' [, 'package_name' ...] )",
            "definition": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler codes\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API. By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Java, query the\nINFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: To specify the package name and version number use the following form: To specify the latest version, specify latest for version. For example, to include a package from the latest Snowpark library in Snowflake, use the following: When specifying a package from the Snowpark library, you must specify version 1.3.0 or later."
          },
          {
            "term": "HANDLER = 'fully_qualified_method_name'",
            "definition": "Use the fully qualified name of the method or function for the stored procedure. This is typically in the\nfollowing form: where: corresponds to the package containing the object or class:"
          }
        ]
      },
      {
        "heading": "Python",
        "description": "\nWhen the code is in-line, you can specify just the function name, as in the following example:\nCREATE OR REPLACE PROCEDURE MYPROC(from_table STRING, to_table STRING, count INT)\n  ...\n  HANDLER = 'run'\nAS\n$$\ndef run(session, from_table, to_table, count):\n  ...\n$$;\n\nCopy\nWhen the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name>.\nCREATE OR REPLACE PROCEDURE MYPROC(from_table STRING, to_table STRING, count INT)\n  ...\n  IMPORTS = ('@mystage/my_py_file.py')\n  HANDLER = 'my_py_file.run';\n\nCopy",
        "syntax": [
          "SELECT * FROM INFORMATION_SCHEMA.PACKAGES WHERE LANGUAGE = 'python';",
          "package_name[==version]",
          "PACKAGES = ('snowflake-snowpark-python', 'spacy==2.3.5')",
          "-- Use version 1.2.3 or higher of the NumPy package.\nPACKAGES=('numpy>=1.2.3')",
          "CREATE OR REPLACE PROCEDURE MYPROC(from_table STRING, to_table STRING, count INT)\n  ...\n  HANDLER = 'run'\nAS\n$$\ndef run(session, from_table, to_table, count):\n  ...\n$$;",
          "CREATE OR REPLACE PROCEDURE MYPROC(from_table STRING, to_table STRING, count INT)\n  ...\n  IMPORTS = ('@mystage/my_py_file.py')\n  HANDLER = 'my_py_file.run';"
        ],
        "definitions": [
          {
            "term": "RUNTIME_VERSION = 'language_runtime_version'",
            "definition": "The language runtime version to use. Currently, the supported versions are: 3.9 3.10 3.11 3.12"
          },
          {
            "term": "PACKAGES = ( 'snowpark_package_name' [, 'package_name' ...] )",
            "definition": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler codes\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API. By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Python, query the\nINFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: Snowflake includes a large number of packages available through Anaconda; for more information, see\nUsing third-party packages. To specify the package name and version number use the following form: To specify the latest version, omit the version number. For example, to include the spacy package version 2.3.5 (along with the latest version of the required Snowpark package), use the\nfollowing: When specifying a package from the Snowpark library, you must specify version 0.4.0 or later. Omit the version number to use the\nlatest version available in Snowflake. Preview Feature  Open Specifying a range of Python package versions is available as a preview feature to all accounts. You can specify package versions by using these version\nspecifiers: ==, <=, >=, <,or >. For example:"
          },
          {
            "term": "HANDLER = 'fully_qualified_method_name'",
            "definition": "Use the name of the stored procedures function or method. This can differ depending on whether the code is in-line or\nreferenced at a stage. When the code is in-line, you can specify just the function name, as in the following example: When the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name>."
          }
        ]
      },
      {
        "heading": "Scala",
        "syntax": [
          "SELECT * FROM INFORMATION_SCHEMA.PACKAGES WHERE LANGUAGE = 'scala';",
          "domain:package_name:version",
          "PACKAGES = ('com.snowflake:snowpark:latest')",
          "com.my_company.my_package.MyClass.myMethod",
          "com.my_company.my_package",
          "package com.my_company.my_package;"
        ],
        "definitions": [
          {
            "term": "RUNTIME_VERSION = 'language_runtime_version'",
            "definition": "The language runtime version to use. Currently, the supported versions are: 2.12"
          },
          {
            "term": "PACKAGES = ( 'snowpark_package_name' [, 'package_name' ...] )",
            "definition": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler codes\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API. By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Scala, query the\nINFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: To specify the package name and version number use the following form: To specify the latest version, specify latest for version. For example, to include a package from the latest Snowpark library in Snowflake, use the following: Snowflake supports using Snowpark version 0.9.0 or later in a Scala stored procedure. Note, however, that these versions have\nlimitations. For example, versions prior to 1.1.0 do not support the use of transactions in a stored procedure."
          },
          {
            "term": "HANDLER = 'fully_qualified_method_name'",
            "definition": "Use the fully qualified name of the method or function for the stored procedure. This is typically in the following form: where: corresponds to the package containing the object or class:"
          }
        ]
      },
      {
        "heading": "Optional parameters"
      },
      {
        "heading": "All languages",
        "definitions": [
          {
            "term": "SECURE",
            "definition": "Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures."
          },
          {
            "term": "{ TEMP | TEMPORARY }",
            "definition": "Specifies that the procedure persists for only the duration of the session in which you created it.\nA temporary procedure is dropped at the end of the session. Default: No value. If a procedure is not declared as TEMPORARY, it is permanent. You cannot create temporary procedures that have the same name as\na procedure that already exists in the schema. Note that creating a temporary procedure does not require the CREATE PROCEDURE privilege on the schema in which the object is created. For more information about creating temporary procedures, see Temporary procedures."
          },
          {
            "term": "[ [ NOT ] NULL ]",
            "definition": "Specifies whether the stored procedure can return NULL values or must return only NON-NULL values. The default is NULL (i.e. the stored procedure can return NULL)."
          },
          {
            "term": "CALLED ON NULL INPUT or . { RETURNS NULL ON NULL INPUT | STRICT }",
            "definition": "Specifies the behavior of the stored procedure when called with null inputs. In contrast to system-defined functions, which\nalways return null when any input is null, stored procedures can handle null inputs, returning non-null values even when an\ninput is null: CALLED ON NULL INPUT will always call the stored procedure with null inputs. It is up to the procedure to handle such\nvalues appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT) will not call the stored procedure if any input is null,\nso the statements inside the stored procedure will not be executed. Instead, a null value will always be returned. Note that\nthe procedure might still return null for non-null inputs. Default: CALLED ON NULL INPUT"
          },
          {
            "term": "VOLATILE | IMMUTABLE",
            "definition": "Deprecated Attention These keywords are deprecated for stored procedures. These keywords are not intended to apply to stored procedures. In a\nfuture release, these keywords will be removed from the documentation."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment for the stored procedure, which is displayed in the DESCRIPTION column in the SHOW PROCEDURES output. Default: stored procedure"
          },
          {
            "term": "EXECUTE AS OWNER or . EXECUTE AS CALLER or . EXECUTE AS RESTRICTED CALLER",
            "definition": "Preview Feature  Open Restricted callers rights (EXECUTE AS RESTRICTED CALLER) is a preview feature available to all accounts. Specifies whether the stored procedure executes with the privileges of the owner (an owners rights stored procedure) or with\nthe privileges of the caller (a callers rights stored procedure): If you execute CREATE PROCEDURE  EXECUTE AS OWNER, then the procedure will execute as an owners rights procedure. If you execute the statement CREATE PROCEDURE  EXECUTE AS CALLER, then in the future the procedure will execute as a\ncallers rights procedure. If you execute the statement CREATE PROCEDURE  EXECUTE AS RESTRICTED CALLER, then in the future the procedure will execute as a\ncallers rights procedure, but might not be able to run with all of the callers privileges. For more information, see\nRestricted callers rights. If EXECUTE AS ... isnt specified, the procedure runs as an owners rights stored procedure. Owners rights stored\nprocedures have less access to the callers environment (for example, the callers session variables), and Snowflake defaults to this\nhigher level of privacy and security. For more information, see Understanding callers rights and owners rights stored procedures. Default: OWNER"
          },
          {
            "term": "COPY GRANTS",
            "definition": "Specifies to retain the access privileges from the original procedure when a new procedure is created using CREATE OR REPLACE PROCEDURE. The parameter copies all privileges, except OWNERSHIP, from the existing procedure to the new procedure. The new procedure will\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE PROCEDURE\nstatement owns the new procedure. Note: The SHOW GRANTS output for the replacement procedure lists the grantee for the copied privileges as the\nrole that executed the CREATE PROCEDURE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE PROCEDURE command (i.e. within the same transaction)."
          }
        ]
      },
      {
        "heading": "Java",
        "syntax": [
          "TARGET_PATH = '@handlers/myhandler.jar'",
          "REMOVE @handlers/myhandler.jar;"
        ],
        "definitions": [
          {
            "term": "IMPORTS = ( 'stage_path_and_file_name_to_read' [, 'stage_path_and_file_name_to_read' ...] )",
            "definition": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the\nstored procedures handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
          },
          {
            "term": "TARGET_PATH = stage_path_and_file_name_to_write",
            "definition": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the procedure_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. If you specify both the IMPORTS and TARGET_PATH clauses, the file name in the TARGET_PATH clause must\nbe different from each file name in the IMPORTS clause, even if the files are in different subdirectories or different\nstages. The generated JAR file remains until you explicitly delete it, even if you drop the procedure. When you drop the procedure you should\nseparately remove the JAR file because the JAR is no longer needed to support the procedure. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this procedure to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nprocedures handler code to access external networks. An external access integration specifies network rules and\nsecrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration\nspecified as a value of this CREATE PROCEDURE commands EXTERNAL_ACCESS_INTEGRATIONS parameter This parameters value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure."
          }
        ]
      },
      {
        "heading": "Python",
        "description": "\nARTIFACT_REPOSITORY = artifact_repository\nSpecifies the name of the repository to use for installing PyPI packages for use by your procedure.\nSet this to snowflake.snowpark.pypi_shared_repository, which is the default artifact repository provided by Snowflake.\nPACKAGES = ( 'package_name' [ , ... ] )\nSpecify a list of the names of the packages that you want to install and use in your procedure. Snowflake installs these packages from the artifact repository.",
        "definitions": [
          {
            "term": "IMPORTS = ( 'stage_path_and_file_name_to_read' [, 'stage_path_and_file_name_to_read' ...] )",
            "definition": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If your stored procedures code will be on a stage, you must also include a path to the module file your code is in. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nprocedures handler code to access external networks. An external access integration specifies network rules and\nsecrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration\nspecified as a value of this CREATE PROCEDURE commands EXTERNAL_ACCESS_INTEGRATIONS parameter This parameters value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure."
          }
        ]
      },
      {
        "heading": "Scala",
        "syntax": [
          "TARGET_PATH = '@handlers/myhandler.jar'",
          "REMOVE @handlers/myhandler.jar;"
        ],
        "definitions": [
          {
            "term": "IMPORTS = ( 'stage_path_and_file_name_to_read' [, 'stage_path_and_file_name_to_read' ...] )",
            "definition": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the\nstored procedures handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
          },
          {
            "term": "TARGET_PATH = stage_path_and_file_name_to_write",
            "definition": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the procedure_definition. If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clauses value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. If you specify both the IMPORTS and TARGET_PATH clauses, the file name in the TARGET_PATH clause must\nbe different from each file name in the IMPORTS clause, even if the files are in different subdirectories or different\nstages. The generated JAR file remains until you explicitly delete it, even if you drop the procedure. When you drop the procedure you should\nseparately remove the JAR file because the JAR is no longer needed to support the procedure. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the\nhandlers stage. When you drop this procedure to remove it, youll also need to remove its handler JAR file, such as by executing the\nREMOVE command."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE PROCEDURE",
                "Schema",
                "Required to create a permanent stored procedure. Not required when creating a temporary procedure that persists for only the\nduration of the session in which the procedure was created."
              ],
              [
                "USAGE",
                "Procedure",
                "Granting the USAGE privilege on the newly created procedure to a role allows users with that role to call the procedure elsewhere\nin Snowflake."
              ],
              [
                "USAGE",
                "External access integration",
                "Required on integrations, if any, specified when creating the procedure. For more information, see\nCREATE EXTERNAL ACCESS INTEGRATION."
              ],
              [
                "READ",
                "Secret",
                "Required on secrets, if any, specified when creating the procedure. For more information, see\nCreating a secret to represent credentials and Using the external access integration in a function or procedure."
              ],
              [
                "USAGE",
                "Schema",
                "Required on schemas containing secrets, if any, specified when creating the procedure. For more information,\nsee Creating a secret to represent credentials and Using the external access integration in a function or procedure."
              ]
            ]
          }
        ]
      },
      {
        "heading": "General usage notes",
        "description": "\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nFor additional usage notes, see the following."
      },
      {
        "heading": "All handler languages",
        "description": "\nStored procedures support overloading. Two procedures can have the same\nname if they have a different number of parameters or different data types for their parameters.\nStored procedures are not atomic; if one statement in a stored procedure fails, the other statements in the stored\nprocedure are not necessarily rolled back. For information about stored procedures and transactions, see\nTransaction management.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nStored procedures support overloading. Two procedures can have the same\nname if they have a different number of parameters or different data types for their parameters.\nStored procedures are not atomic; if one statement in a stored procedure fails, the other statements in the stored\nprocedure are not necessarily rolled back. For information about stored procedures and transactions, see\nTransaction management.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nTip\nIf your organization uses a mix of callers rights and owners rights stored procedures, you might want to use a\nnaming convention for your stored procedures to indicate whether an individual stored procedure is a callers\nrights stored procedure or an owners rights stored procedure."
      },
      {
        "heading": "Java",
        "description": "\nSee the known limitations."
      },
      {
        "heading": "Javascript",
        "description": "\nA JavaScript stored procedure can return only a single value, such as a string (for example, a success/failure indicator)\nor a number (for example, an error code). If you need to return more extensive information, you can return a\nVARCHAR that contains values separated by a delimiter (such as a comma), or a semi-structured data type, such\nas VARIANT."
      },
      {
        "heading": "Python",
        "description": "\nSee the known limitations."
      },
      {
        "heading": "Scala",
        "description": "\nSee the known limitations."
      },
      {
        "heading": "CREATE OR ALTER PROCEDURE usage notes",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nAll limitations of the ALTER PROCEDURE command apply.\nAll limitations described in CREATE OR ALTER FUNCTION usage notes apply.\nAll limitations of the ALTER PROCEDURE command apply.\nAll limitations described in CREATE OR ALTER FUNCTION usage notes apply."
      },
      {
        "heading": "Examples",
        "description": "\nThis creates a trivial stored procedure that returns a hard-coded value. This is unrealistic, but shows the basic\nSQL syntax with minimal JavaScript code:\nThis shows a more realistic example that includes a call to the JavaScript API. A more extensive version of this\nprocedure could allow a user to insert data into a table that the user didnt have privileges to insert into directly.\nJavaScript statements could check the input parameters and execute the SQL INSERT only if certain requirements\nwere met.\nFor more examples, see Working with stored procedures.",
        "syntax": [
          "CREATE OR REPLACE PROCEDURE sp_pi()\n    RETURNS FLOAT NOT NULL\n    LANGUAGE JAVASCRIPT\n    AS\n    $$\n    return 3.1415926;\n    $$\n    ;",
          "CREATE OR REPLACE PROCEDURE stproc1(FLOAT_PARAM1 FLOAT)\n    RETURNS STRING\n    LANGUAGE JAVASCRIPT\n    STRICT\n    EXECUTE AS OWNER\n    AS\n    $$\n    var sql_command = \n     \"INSERT INTO stproc_test_table1 (num_col1) VALUES (\" + FLOAT_PARAM1 + \")\";\n    try {\n        snowflake.execute (\n            {sqlText: sql_command}\n            );\n        return \"Succeeded.\";   // Return a success/error indicator.\n        }\n    catch (err)  {\n        return \"Failed: \" + err;   // Return a success/error indicator.\n        }\n    $$\n    ;"
        ]
      },
      {
        "heading": "In-line handler",
        "description": "\nCode in the following example creates a procedure called my_proc with an in-line Python handler function run. Through\nthe PACKAGES clause, the code references the included Snowpark library for Python, whose Session is required when Python\nis the procedure handler language.",
        "syntax": [
          "CREATE OR REPLACE PROCEDURE my_proc(from_table STRING, to_table STRING, count INT)\n  RETURNS STRING\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.9'\n  PACKAGES = ('snowflake-snowpark-python')\n  HANDLER = 'run'\nAS\n$$\ndef run(session, from_table, to_table, count):\n  session.table(from_table).limit(count).write.save_as_table(to_table)\n  return \"SUCCESS\"\n$$;"
        ]
      },
      {
        "heading": "Staged handler",
        "description": "\nCode in the following example creates a procedure called my_proc with an staged Java handler method MyClass.myMethod.\nThrough the PACKAGES clause, the code references the included Snowpark library for Java, whose Session is required when Java\nis the procedure handler language. With the IMPORTS clause, the code references the staged JAR file containing the handler code.",
        "syntax": [
          "CREATE OR REPLACE PROCEDURE my_proc(fromTable STRING, toTable STRING, count INT)\n  RETURNS STRING\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '11'\n  PACKAGES = ('com.snowflake:snowpark:latest')\n  IMPORTS = ('@mystage/myjar.jar')\n  HANDLER = 'MyClass.myMethod';"
        ]
      },
      {
        "heading": "Create and alter a procedure using the CREATE OR ALTER PROCEDURE command",
        "description": "\nCreate an owners rights Python stored procedure with external access integrations and default OWNER privileges.\nAlter the stored procedures secrets and change the stored procedure to a callers rights procedure:\nOn this page\nSyntax\nVariant syntax\nRequired parameters\nOptional parameters\nAccess control requirements\nGeneral usage notes\nCREATE OR ALTER PROCEDURE usage notes\nExamples\nCreate and alter a procedure using the CREATE OR ALTER PROCEDURE command\nRelated content\nCreating a stored procedure\nStored procedures overview\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR ALTER PROCEDURE python_add1(A NUMBER)\n  RETURNS NUMBER\n  LANGUAGE PYTHON\n  HANDLER='main'\n  RUNTIME_VERSION=3.10\n  EXTERNAL_ACCESS_INTEGRATIONS=(example_integration)\n  PACKAGES = ('snowflake-snowpark-python')\n  EXECUTE AS OWNER\n  AS\n$$\ndef main(session, a):\n    return a+1\n$$;",
          "CREATE OR ALTER PROCEDURE python_add1(A NUMBER)\n  RETURNS NUMBER\n  LANGUAGE PYTHON\n  HANDLER='main'\n  RUNTIME_VERSION=3.10\n  EXTERNAL_ACCESS_INTEGRATIONS=(example_integration)\n  secrets=('secret_variable_name'=secret_name)\n  PACKAGES = ('snowflake-snowpark-python')\n  EXECUTE AS CALLER\n  AS\n$$\ndef main(session, a):\n    return a+1\n$$;"
        ]
      }
    ]
  },
  {
    "function": "ALTER PROCEDURE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-procedure",
    "details": [
      {
        "heading": "ALTER PROCEDURE",
        "description": "\nModifies the properties for an existing stored procedure. If you need to make any changes not supported here, use DROP PROCEDURE\ninstead and then recreate the stored procedure.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE PROCEDURE , DROP PROCEDURE , SHOW PROCEDURES , DESCRIBE PROCEDURE, SHOW USER PROCEDURES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nThe syntax for ALTER PROCEDURE varies depending on which language youre using as the UDF handler."
      },
      {
        "heading": "Java handler",
        "syntax": [
          "ALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = '<integration_name>' [ , '<integration_name>' ... ] ]\n  [ SECRETS = '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }"
        ]
      },
      {
        "heading": "JavaScript handler",
        "syntax": [
          "ALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }"
        ]
      },
      {
        "heading": "Python handler",
        "syntax": [
          "ALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = '<integration_name>' [ , '<integration_name>' ... ] ]\n  [ SECRETS = '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }"
        ]
      },
      {
        "heading": "Scala handler",
        "syntax": [
          "ALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = '<integration_name>' [ , '<integration_name>' ... ] ]\n  [ SECRETS = '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }"
        ]
      },
      {
        "heading": "Snowflake Scripting handler",
        "syntax": [
          "ALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ AUTO_EVENT_LOGGING = '<option>' ]\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the stored procedure to alter. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          },
          {
            "term": "arg_data_type [ , ... ]",
            "definition": "Specifies the data type of the argument(s) for the stored procedure, if it has arguments. The argument types are required because stored\nprocedures support name overloading (i.e. two stored procedures in the same schema can have the same name) and the argument types are used to\nidentify the procedure you wish to alter."
          },
          {
            "term": "RENAME TO new_name",
            "definition": "Specifies the new identifier for the stored procedure; the combination of the identifier and existing argument data types must be unique for\nthe schema. For more details, see Identifier requirements. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
          },
          {
            "term": "SET ...",
            "definition": "Specifies the properties to set for the stored procedure. (For Snowflake Scripting stored procedures only) Controls whether additional Snowflake Scripting log messages and trace events are\ningested automatically into the event table. For information about the options, see AUTO_EVENT_LOGGING. Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL. For information about setting log level, see\nSetting levels for logging, metrics, and tracing. Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL. For information about setting trace level, see\nSetting levels for logging, metrics, and tracing. The names of external access integrations needed in order for this\nprocedures handler code to access external networks. An external access integration contains network rules and\nsecrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview. Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameters value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an\nexternal access integrations ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integrations name must, in turn, be specified as a value of this CREATE PROCEDURE calls\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret. Adds a comment or overwrites the existing comment for the stored procedure. The value you specify is displayed in the DESCRIPTION\ncolumn in the output for SHOW PROCEDURES. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "AUTO_EVENT_LOGGING = 'option'",
            "definition": "(For Snowflake Scripting stored procedures only) Controls whether additional Snowflake Scripting log messages and trace events are\ningested automatically into the event table. For information about the options, see AUTO_EVENT_LOGGING."
          },
          {
            "term": "LOG_LEVEL = 'log_level'",
            "definition": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL. For information about setting log level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "TRACE_LEVEL = 'trace_level'",
            "definition": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL. For information about setting trace level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nprocedures handler code to access external networks. An external access integration contains network rules and\nsecrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameters value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an\nexternal access integrations ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integrations name must, in turn, be specified as a value of this CREATE PROCEDURE calls\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the stored procedure. The value you specify is displayed in the DESCRIPTION\ncolumn in the output for SHOW PROCEDURES."
          },
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies the properties to unset for the stored procedure, which resets them to the defaults. Currently, the only properties you can unset are: COMMENT, which removes the comment, if any, for the procedure. TAG tag_name [ , tag_name ... ]"
          },
          {
            "term": "EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }",
            "definition": "Preview Feature  Open Restricted callers rights (EXECUTE AS RESTRICTED CALLER) is a preview feature available to all accounts. Specifies whether the stored procedure executes with the privileges of the owner (an owners rights stored procedure) or with\nthe privileges of the caller (a callers rights stored procedure): If you execute ALTER PROCEDURE  EXECUTE AS OWNER, then in the future the procedure will execute as an owners rights procedure. If you execute the statement ALTER PROCEDURE  EXECUTE AS CALLER, then in the future the procedure will execute as a\ncallers rights procedure. If you execute the statement ALTER PROCEDURE  EXECUTE AS RESTRICTED CALLER, then in the future the procedure will execute as a\ncallers rights procedure, but might not be able to run with all of the callers privileges. For more information, see\nRestricted callers rights. If EXECUTE AS ... isnt specified, the procedure runs as an owners rights stored procedure. Owners rights stored\nprocedures have less access to the callers environment (for example, the callers session variables), and Snowflake defaults to this\nhigher level of privacy and security. For more information, see Understanding callers rights and owners rights stored procedures. Default: EXECUTE AS OWNER"
          },
          {
            "term": "AUTO_EVENT_LOGGING = 'option'",
            "definition": "(For Snowflake Scripting stored procedures only) Controls whether additional Snowflake Scripting log messages and trace events are\ningested automatically into the event table. For information about the options, see AUTO_EVENT_LOGGING."
          },
          {
            "term": "LOG_LEVEL = 'log_level'",
            "definition": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL. For information about setting log level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "TRACE_LEVEL = 'trace_level'",
            "definition": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL. For information about setting trace level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "EXTERNAL_ACCESS_INTEGRATIONS = ( integration_name [ , ... ] )",
            "definition": "The names of external access integrations needed in order for this\nprocedures handler code to access external networks. An external access integration contains network rules and\nsecrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview."
          },
          {
            "term": "SECRETS = ( 'secret_variable_name' = secret_name [ , ... ] )",
            "definition": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameters value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an\nexternal access integrations ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integrations name must, in turn, be specified as a value of this CREATE PROCEDURE calls\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isnt also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. 'secret_variable_name' as the variable that will be used in handler code when retrieving information from the secret."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the stored procedure. The value you specify is displayed in the DESCRIPTION\ncolumn in the output for SHOW PROCEDURES."
          },
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "Examples",
        "description": "\nRename stored procedure procedure1 to procedure2:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER PROCEDURE IF EXISTS procedure1(FLOAT) RENAME TO procedure2;"
        ]
      }
    ]
  },
  {
    "function": "CALL",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/call",
    "details": [
      {
        "heading": "CALL",
        "description": "\nCalls a stored procedure.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE PROCEDURE , SHOW PROCEDURES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CALL <procedure_name> ( [ [ <arg_name> => ] <arg> , ... ] )\n  [ INTO :<snowflake_scripting_variable> ]"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "procedure_name ( [ [ arg_name => ] arg , ... ] )",
            "definition": "Specifies the identifier (procedure_name) for the procedure to call and any input arguments. You can either specify the input arguments by name (arg_name => arg) or by position (arg). Note the following: You must either specify all arguments by name or by position. You cannot specify some of the arguments by name and other\narguments by position. When specifying an argument by name, you cannot use double quotes around the argument name. If two functions or two procedures have the same name but different argument types, you can use the argument names to specify\nwhich function or procedure to execute, if the argument names are different. Refer to\nOverloading procedures and functions."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "INTO :snowflake_scripting_variable",
            "definition": "Sets the specified Snowflake Scripting variable to the return value of\nthe stored procedure."
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nFor more extensive examples of creating and calling stored procedures, see Working with stored procedures.\nEach argument to a stored procedure can be a general expression:\nAn argument can be a subquery:\nYou can call only one stored procedure per CALL statement. For example, the following statement fails:\nAlso, you cannot use a stored procedure CALL as part of an expression. For example, all the following statements fail:\nHowever, inside a stored procedure, the stored procedure can call\nanother stored procedure, or call itself recursively.\nCaution\nNested calls can exceed the maximum allowed stack depth, so be careful when nesting calls,\nespecially when using recursion.\nThe following example calls a stored procedure named sv_proc1 and passes in a string literal and number as input arguments.\nThe example specifies the arguments by position:\nYou can also specify the arguments by their names:\nThe following example demonstrates how to set and pass a session variable as an input\nargument to a stored procedure:\nThe following is an example of a Snowflake Scripting block that captures the return value of a stored procedure in a Snowflake\nScripting variable.\nNote: If you use Snowflake CLI, SnowSQL, the Classic Console, or the\nexecute_stream or execute_string method in Python Connector\ncode, use this example instead (see Using Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector):\nOn this page\nSyntax\nRequired parameters\nOptional parameters\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CALL stproc1(5.14::FLOAT);",
          "CALL stproc1(2 * 5.14::FLOAT);",
          "CALL stproc1(SELECT COUNT(*) FROM stproc_test_table1);",
          "CALL proc1(1), proc2(2);                          -- Not allowed",
          "CALL proc1(1) + proc1(2);                         -- Not allowed\nCALL proc1(1) + 1;                                -- Not allowed\nCALL proc1(proc2(x));                             -- Not allowed\nSELECT * FROM (call proc1(1));                    -- Not allowed",
          "CALL sv_proc1('Manitoba', 127.4);",
          "CALL sv_proc1(province => 'Manitoba', amount => 127.4);",
          "SET Variable1 = 49;\nCALL sv_proc2($Variable1);",
          "DECLARE\n  ret1 NUMBER;\nBEGIN\n  CALL sv_proc1('Manitoba', 127.4) into :ret1;\n  RETURN ret1;\nEND;",
          "EXECUTE IMMEDIATE $$\nDECLARE\n  ret1 NUMBER;\nBEGIN\n  CALL sv_proc1('Manitoba', 127.4) into :ret1;\n  RETURN ret1;\nEND;\n$$\n;"
        ]
      }
    ]
  },
  {
    "function": "CALL (with anonymous procedure)",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/call-with",
    "details": [
      {
        "heading": "CALL (with anonymous procedure)",
        "description": "\nCreates and calls an anonymous procedure that is like a stored procedure but is not\nstored for later use.\nWith this command, you both create an anonymous procedure defined by parameters in the WITH clause and call that procedure.\nYou need not have a role with CREATE PROCEDURE schema privileges for this command.\nThe procedure runs with callers rights, which means that the procedure runs with\nthe privileges of the caller, uses the current session context, and has access to the callers session variables and parameters.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE PROCEDURE , CALL."
          }
        ]
      },
      {
        "heading": "Syntax"
      },
      {
        "heading": "Java and Scala",
        "description": "\nPreview Feature  Open\nUsing tabular stored procedures with a Java handler or\nScala handler via RETURNS TABLE(...) is a preview feature\nthat is available to all accounts.\nFor Java and Scala procedures with staged handlers, use the following syntax:",
        "syntax": [
          "WITH <name> AS PROCEDURE ([ <arg_name> <arg_data_type> ]) [ , ... ] )\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE { SCALA | JAVA }\n  RUNTIME_VERSION = '<scala_or_java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ AS '<procedure_definition>' ]\n  [ , <cte_nameN> [ ( <cte_column_list> ) ] AS ( SELECT ...  ) ]\nCALL <name> ( [ [ <arg_name> => ] <arg> , ... ] )\n  [ INTO :<snowflake_scripting_variable> ]",
          "WITH <name> AS PROCEDURE ([ <arg_name> <arg_data_type> ]) [ , ... ] )\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE { SCALA | JAVA }\n  RUNTIME_VERSION = '<scala_or_java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ , <cte_nameN> [ ( <cte_column_list> ) ] AS ( SELECT ...  ) ]\nCALL <name> ( [ [ <arg_name> => ] <arg> , ... ] )\n  [ INTO :<snowflake_scripting_variable> ]"
        ]
      },
      {
        "heading": "JavaScript",
        "syntax": [
          "WITH <name> AS PROCEDURE ([ <arg_name> <arg_data_type> ]) [ , ... ] )\n  RETURNS <result_data_type> [ [ NOT ] NULL ]\n  LANGUAGE JAVASCRIPT\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  AS '<procedure_definition>'\n  [ , <cte_nameN> [ ( <cte_column_list> ) ] AS ( SELECT ...  ) ]\nCALL <name> ( [ [ <arg_name> => ] <arg> , ... ] )\n  [ INTO :<snowflake_scripting_variable> ]"
        ]
      },
      {
        "heading": "Python",
        "description": "\nFor in-line procedures, use the following syntax:\nFor a procedure in which the code is in a file on a stage, use the following syntax:",
        "syntax": [
          "WITH <name> AS PROCEDURE ( [ <arg_name> <arg_data_type> ] [ , ... ] )\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  PACKAGES = ( 'snowflake-snowpark-python[==<version>]'[, '<package_name>[==<version>]' ... ])\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<function_name>'\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ , <cte_nameN> [ ( <cte_column_list> ) ] AS ( SELECT ...  ) ]\n  AS '<procedure_definition>'\nCALL <name> ( [ [ <arg_name> => ] <arg> , ... ] )\n  [ INTO :<snowflake_scripting_variable> ]",
          "WITH <name> AS PROCEDURE ( [ <arg_name> <arg_data_type> ] [ , ... ] )\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  PACKAGES = ( 'snowflake-snowpark-python[==<version>]'[, '<package_name>[==<version>]' ... ])\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<module_file_name>.<function_name>'\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ , <cte_nameN> [ ( <cte_column_list> ) ] AS ( SELECT ...  ) ]\nCALL <name> ( [ [ <arg_name> => ] <arg> , ... ] )\n  [ INTO :<snowflake_scripting_variable> ]"
        ]
      },
      {
        "heading": "Snowflake Scripting",
        "syntax": [
          "WITH <name> AS PROCEDURE ([ <arg_name> <arg_data_type> ]) [ , ... ] )\n  RETURNS { <result_data_type> | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE SQL\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  AS '<procedure_definition>'\n  [ , <cte_nameN> [ ( <cte_column_list> ) ] AS ( SELECT ...  ) ]\nCALL <name> ( [ [ <arg_name> => ] <arg> , ... ] )\n  [ INTO :<snowflake_scripting_variable> ]"
        ]
      },
      {
        "heading": "Required parameters"
      },
      {
        "heading": "All languages",
        "description": "\nFor RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the\nSnowflake data types of the columns in the returned table, specify the column names and\ntypes:\nWITH get_top_sales() AS PROCEDURE\n  RETURNS TABLE (sales_date DATE, quantity NUMBER)\n  ...\nCALL get_top_sales();\n\nCopy\nOtherwise (e.g. if you are determining the column types during run time), you can omit the column names and types:\nWITH get_top_sales() AS PROCEDURE\n  ...\n  RETURNS TABLE ()\nCALL get_top_sales();\n\nCopy\n\nNote\nCurrently, in the RETURNS TABLE(...) clause, you cant specify GEOGRAPHY as a column type. This\napplies whether you are creating a stored or anonymous procedure.\nCREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE(g GEOGRAPHY)\n  ...\n\nCopy\nWITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE(g GEOGRAPHY)\n  ...\nCALL test_return_geography_table_1();\n\nCopy\nIf you attempt to specify GEOGRAPHY as a column type, calling the stored procedure results in the error:\nStored procedure execution error: data type of returned table does not match expected returned table type\n\nCopy\nTo work around this issue, you can omit the column arguments and types in RETURNS TABLE().\nCREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE()\n  ...\n\nCopy\nWITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE()\n  ...\nCALL test_return_geography_table_1();\n\nCopy\n\nRETURNS TABLE(...) is supported only in when the handler is written in the following languages:\n\nJava\nPython\nScala\nSnowflake Scripting\nFor procedures in JavaScript, if you are writing a string that contains newlines, you can use\nbackquotes (also called backticks) around the string.\nThe following example of a JavaScript procedure uses $$ and backquotes because the body of the procedure\ncontains single quotes and double quotes:\nWITH proc3 AS PROCEDURE ()\n  RETURNS VARCHAR\n  LANGUAGE javascript\n  AS\n  $$\n  var rs = snowflake.execute( { sqlText:\n      `INSERT INTO table1 (\"column 1\")\n          SELECT 'value 1' AS \"column 1\" ;`\n      } );\n  return 'Done.';\n  $$\nCALL proc3();\n\nCopy",
        "syntax": [
          "WITH get_top_sales() AS PROCEDURE\n  RETURNS TABLE (sales_date DATE, quantity NUMBER)\n  ...\nCALL get_top_sales();",
          "WITH get_top_sales() AS PROCEDURE\n  ...\n  RETURNS TABLE ()\nCALL get_top_sales();",
          "CREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE(g GEOGRAPHY)\n  ...",
          "WITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE(g GEOGRAPHY)\n  ...\nCALL test_return_geography_table_1();",
          "Stored procedure execution error: data type of returned table does not match expected returned table type",
          "CREATE OR REPLACE PROCEDURE test_return_geography_table_1()\n  RETURNS TABLE()\n  ...",
          "WITH test_return_geography_table_1() AS PROCEDURE\n  RETURNS TABLE()\n  ...\nCALL test_return_geography_table_1();",
          "WITH proc3 AS PROCEDURE ()\n  RETURNS VARCHAR\n  LANGUAGE javascript\n  AS\n  $$\n  var rs = snowflake.execute( { sqlText:\n      `INSERT INTO table1 (\"column 1\")\n          SELECT 'value 1' AS \"column 1\" ;`\n      } );\n  return 'Done.';\n  $$\nCALL proc3();"
        ],
        "definitions": [
          {
            "term": "WITH name AS PROCEDURE ( [ arg_name arg_data_type ] [ , ... ] )",
            "definition": "Specifies the identifier (name) and any input arguments for the procedure. For the identifier: The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. My object). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements. For the input arguments: For arg_name, specify the name of the input argument. For arg_data_type, use the Snowflake data type that corresponds to the handler language that you are using. For Java procedures, see SQL-Java Data Type Mappings. For JavaScript procedures, see\nSQL and JavaScript data type mapping. For Python procedures, see\nSQL-Python Data Type Mappings. For Scala procedures, see SQL-Scala Data Type Mappings. For Snowflake Scripting, a SQL data type. Note For procedures you write in Java, Python, or Scala (which use Snowpark APIs), omit the argument for the Snowpark\nSession object. The Session argument is not a formal parameter that you specify. When you execute this command, Snowflake automatically\ncreates a Session object and passes it to the handler function for your procedure."
          },
          {
            "term": "RETURNS result_data_type [ [ NOT ] NULL ]",
            "definition": "Specifies the type of the result returned by the procedure. Use NOT NULL to specify that the procedure must return only non-null values; the default is NULL, meaning that the procedure\ncan return NULL. For result_data_type, use the Snowflake data type that corresponds to the type of the language that you are using. For Java procedures, see SQL-Java Data Type Mappings. For JavaScript procedures, see\nSQL and JavaScript data type mapping. For Python procedures, see\nSQL-Python Data Type Mappings. For Scala procedures, see SQL-Scala Data Type Mappings. For Snowflake Scripting, a SQL data type. Note Procedures you write in Java or Scala must have a return value. In Python, when a procedure returns no value, it is considered to be\nreturning None. Note that regardless of handler language, the WITH clause for this command must include a RETURNS clause that defines a return type,\neven if the procedure does not explicitly return anything. For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the\nSnowflake data types of the columns in the returned table, specify the column names and\ntypes: Otherwise (e.g. if you are determining the column types during run time), you can omit the column names and types: Note Currently, in the RETURNS TABLE(...) clause, you cant specify GEOGRAPHY as a column type. This\napplies whether you are creating a stored or anonymous procedure. If you attempt to specify GEOGRAPHY as a column type, calling the stored procedure results in the error: To work around this issue, you can omit the column arguments and types in RETURNS TABLE(). RETURNS TABLE(...) is supported only in when the handler is written in the following languages: Java Python Scala Snowflake Scripting As a practical matter, outside of a Snowflake Scripting block,\nthe returned value cannot be used because the call cannot be part of an expression."
          },
          {
            "term": "LANGUAGE language",
            "definition": "Specifies the language of the procedures handler code. Currently, the supported values for language include: JAVA (for Java) JAVASCRIPT (for JavaScript) PYTHON (for Python) SCALA (for Scala) SQL (for Snowflake Scripting)"
          },
          {
            "term": "AS procedure_definition",
            "definition": "Defines the code executed by the procedure. The definition can consist of any valid code. Note the following: For procedures for which the code is not in-line, omit the AS clause. This includes procedures whose\nhandlers are on a stage. Instead, use the IMPORTS clause to specify the location of the file containing the code for the procedure. For\ndetails, see: Writing stored procedures with SQL and Python Writing Java handlers for stored procedures created with SQL Writing Scala handlers for stored procedures created with SQL You must use string literal delimiters (' or $$) around\nprocedure definition, even in Snowflake Scripting. For procedures in JavaScript, if you are writing a string that contains newlines, you can use\nbackquotes (also called backticks) around the string. The following example of a JavaScript procedure uses $$ and backquotes because the body of the procedure\ncontains single quotes and double quotes: Snowflake does not validate the handler code. However, invalid handler code will result in errors when you execute the command. For more details about stored procedures, see Working with stored procedures."
          },
          {
            "term": "CALL name ( [ [ arg_name => ] arg , ... ] )",
            "definition": "Specifies the identifier (name) for the procedure to call and any input arguments. You can either specify the input arguments by name (arg_name => arg) or by position (arg). Note the following: You must either specify all arguments by name or by position. You cannot specify some of the arguments by name and other\narguments by position. When specifying an argument by name, you cannot use double quotes around the argument name. If two functions or two procedures have the same name but different argument types, you can use the argument names to specify\nwhich function or procedure to execute, if the argument names are different. Refer to\nOverloading procedures and functions."
          }
        ]
      },
      {
        "heading": "Java, Python, or Scala",
        "description": "\nJava\nSpecify the package name and version number using the following form:\ndomain:package_name:version\n\nCopy\nTo specify the latest version, specify latest for version.\nFor example, to include a package from the latest Snowpark library in Snowflake, use the following:\nPACKAGES = ('com.snowflake:snowpark:latest')\n\nCopy\nWhen specifying a package from the Snowpark library, you must specify version 1.3.0 or later.\nPython\nSnowflake includes a large number of packages available through Anaconda; for more information, see\nUsing third-party packages.\nSpecify the package name and version number using the following form:\npackage_name[==version]\n\nCopy\nTo specify the latest version, omit the version number.\nFor example, to include the spacy package version 2.3.5 (along with the latest version of the required Snowpark package), use the\nfollowing:\nPACKAGES = ('snowflake-snowpark-python', 'spacy==2.3.5')\n\nCopy\nWhen specifying a package from the Snowpark library, you must specify version 0.4.0 or later. Omit the version number to use the\nlatest version available in Snowflake.\nScala\nSpecify the package name and version number using the following form:\ndomain:package_name:version\n\nCopy\nTo specify the latest version, specify latest for version.\nFor example, to include a package from the latest Snowpark library in Snowflake, use the following:\nPACKAGES = ('com.snowflake:snowpark:latest')\n\nCopy\nSnowflake supports using Snowpark version 0.9.0 or later in a Scala procedure. Note, however, that these versions have limitations.\nFor example, versions prior to 1.1.0 do not support the use of transactions in a procedure.\nPython\nUse the name of the procedures function or method. This can differ depending on whether the code is in-line or\nreferenced at a stage.\n\nWhen the code is in-line, you can specify just the function name, as in the following example:\nWITH myproc AS PROCEDURE()\n  ...\n  HANDLER = 'run'\n  AS\n  $$\n  def run(session):\n    ...\n  $$\nCALL myproc();\n\nCopy\n\nWhen the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name>.\nWITH myproc AS PROCEDURE()\n  ...\n  IMPORTS = ('@mystage/my_py_file.py')\n  HANDLER = 'my_py_file.run'\nCALL myproc();\n\nCopy\nWhen the code is in-line, you can specify just the function name, as in the following example:\nWITH myproc AS PROCEDURE()\n  ...\n  HANDLER = 'run'\n  AS\n  $$\n  def run(session):\n    ...\n  $$\nCALL myproc();\n\nCopy\nWhen the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name>.\nWITH myproc AS PROCEDURE()\n  ...\n  IMPORTS = ('@mystage/my_py_file.py')\n  HANDLER = 'my_py_file.run'\nCALL myproc();\n\nCopy\nJava and Scala\nUse the fully-qualified name of the method or function for the procedure. This is typically in the\nfollowing form:\ncom.my_company.my_package.MyClass.myMethod\n\nCopy\nwhere:\ncom.my_company.my_package\n\nCopy\ncorresponds to the package containing the object or class:\npackage com.my_company.my_package;\n\nCopy\nWhen the code is in-line, you can specify just the function name, as in the following example:\nWITH myproc AS PROCEDURE()\n  ...\n  HANDLER = 'run'\n  AS\n  $$\n  def run(session):\n    ...\n  $$\nCALL myproc();\n\nCopy\nWhen the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name>.\nWITH myproc AS PROCEDURE()\n  ...\n  IMPORTS = ('@mystage/my_py_file.py')\n  HANDLER = 'my_py_file.run'\nCALL myproc();\n\nCopy",
        "syntax": [
          "SELECT * FROM information_schema.packages WHERE language = '<language>';",
          "domain:package_name:version",
          "PACKAGES = ('com.snowflake:snowpark:latest')",
          "package_name[==version]",
          "PACKAGES = ('snowflake-snowpark-python', 'spacy==2.3.5')",
          "domain:package_name:version",
          "PACKAGES = ('com.snowflake:snowpark:latest')",
          "WITH myproc AS PROCEDURE()\n  ...\n  HANDLER = 'run'\n  AS\n  $$\n  def run(session):\n    ...\n  $$\nCALL myproc();",
          "WITH myproc AS PROCEDURE()\n  ...\n  IMPORTS = ('@mystage/my_py_file.py')\n  HANDLER = 'my_py_file.run'\nCALL myproc();",
          "com.my_company.my_package.MyClass.myMethod",
          "com.my_company.my_package",
          "package com.my_company.my_package;"
        ],
        "definitions": [
          {
            "term": "RUNTIME_VERSION = 'language_runtime_version'",
            "definition": "The language runtime version to use. Currently, the supported versions are: Java: 11 Python: 3.9 3.10 3.11 3.12 Scala: 2.12"
          },
          {
            "term": "PACKAGES = ( 'snowpark_package_name' [, 'package_name' ...] )",
            "definition": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler codes\nexecution environment. The Snowpark package is required for procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API. By default, the environment in which Snowflake runs procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. For the list of supported packages and versions for a given language, query the\nINFORMATION_SCHEMA.PACKAGES view, specifying the language. For example: where language is java, python, or scala. The syntax for referring to a package in the PACKAGES clause varies by the packages language, as described below. Java Specify the package name and version number using the following form: To specify the latest version, specify latest for version. For example, to include a package from the latest Snowpark library in Snowflake, use the following: When specifying a package from the Snowpark library, you must specify version 1.3.0 or later. Python Snowflake includes a large number of packages available through Anaconda; for more information, see\nUsing third-party packages. Specify the package name and version number using the following form: To specify the latest version, omit the version number. For example, to include the spacy package version 2.3.5 (along with the latest version of the required Snowpark package), use the\nfollowing: When specifying a package from the Snowpark library, you must specify version 0.4.0 or later. Omit the version number to use the\nlatest version available in Snowflake. Scala Specify the package name and version number using the following form: To specify the latest version, specify latest for version. For example, to include a package from the latest Snowpark library in Snowflake, use the following: Snowflake supports using Snowpark version 0.9.0 or later in a Scala procedure. Note, however, that these versions have limitations.\nFor example, versions prior to 1.1.0 do not support the use of transactions in a procedure."
          },
          {
            "term": "HANDLER = 'fully_qualified_method_name'",
            "definition": "Python Use the name of the procedures function or method. This can differ depending on whether the code is in-line or\nreferenced at a stage. When the code is in-line, you can specify just the function name, as in the following example: When the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name>. Java and Scala Use the fully-qualified name of the method or function for the procedure. This is typically in the\nfollowing form: where: corresponds to the package containing the object or class:"
          }
        ]
      },
      {
        "heading": "Optional parameters"
      },
      {
        "heading": "All languages",
        "definitions": [
          {
            "term": "CALLED ON NULL INPUT or . RETURNS NULL ON NULL INPUT | STRICT",
            "definition": "Specifies the behavior of the procedure when called with null inputs. In contrast to system-defined functions, which\nalways return null when any input is null, procedures can handle null inputs, returning non-null values even when an\ninput is null: CALLED ON NULL INPUT will always call the procedure with null inputs. It is up to the procedure to handle such\nvalues appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT) will not call the procedure if any input is null,\nso the statements inside the procedure will not be executed. Instead, a null value will always be returned. Note that\nthe procedure might still return null for non-null inputs. Default: CALLED ON NULL INPUT"
          },
          {
            "term": "INTO :snowflake_scripting_variable",
            "definition": "Sets the specified Snowflake Scripting variable to the return value of\nthe stored procedure."
          }
        ]
      },
      {
        "heading": "Java, Python, or Scala",
        "definitions": [
          {
            "term": "IMPORTS = ( 'stage_path_and_file_name_to_read' [, 'stage_path_and_file_name_to_read' ...] )",
            "definition": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour procedure depends on: If you are writing an in-line procedure, you can omit this clause, unless your code depends on classes defined outside\nthe procedure or resource files. Java or Scala: If you are writing a procedure whose handler will be compiled code, you must also include a path to the JAR file\ncontaining the procedures handler. Python: If your procedures code will be on a stage, you must also include a path to the module file your code is in. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different\nstages."
          }
        ]
      },
      {
        "heading": "Usage notes"
      },
      {
        "heading": "General usage",
        "description": "\nProcedures are not atomic; if one statement in a procedure fails, the other statements in the\nprocedure are not necessarily rolled back. For information about procedures and transactions, see\nTransaction management.\nA procedure can return only a single value, such as a string (for example, a success/failure indicator)\nor a number (for example, an error code). If you need to return more extensive information, you can return a\nVARCHAR that contains values separated by a delimiter (such as a comma), or a semi-structured data type, such\nas VARIANT.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nProcedures are not atomic; if one statement in a procedure fails, the other statements in the\nprocedure are not necessarily rolled back. For information about procedures and transactions, see\nTransaction management.\nA procedure can return only a single value, such as a string (for example, a success/failure indicator)\nor a number (for example, an error code). If you need to return more extensive information, you can return a\nVARCHAR that contains values separated by a delimiter (such as a comma), or a semi-structured data type, such\nas VARIANT.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "Syntax",
        "description": "\nSimilar to when a WITH clause is used with a SELECT statement, a WITH clause used with CALL supports\nspecifying multiple CTEs separated by commas, in addition to the procedure definition. However, it is not possible to pass tabular\nvalues produced by a WITH clause to the CALL clause.\nIt is, however, possible to specify a simple variable whose value is assigned in the WITH clause.\nThe CALL clause must occur last in the syntax.\nSimilar to when a WITH clause is used with a SELECT statement, a WITH clause used with CALL supports\nspecifying multiple CTEs separated by commas, in addition to the procedure definition. However, it is not possible to pass tabular\nvalues produced by a WITH clause to the CALL clause.\nIt is, however, possible to specify a simple variable whose value is assigned in the WITH clause.\nThe CALL clause must occur last in the syntax."
      },
      {
        "heading": "Privileges",
        "description": "\nCreating and calling a procedure with this command does not require a role with CREATE PROCEDURE schema privileges.\nThe procedures handler code will be able to perform only actions permitted for the role assigned to the person who ran this command.\nCreating and calling a procedure with this command does not require a role with CREATE PROCEDURE schema privileges.\nThe procedures handler code will be able to perform only actions permitted for the role assigned to the person who ran this command."
      },
      {
        "heading": "Language-specific",
        "description": "\nFor Java procedures, see the known limitations.\nFor Python procedures, see the known limitations.\nFor Scala procedures, see the known limitations.\nFor Java procedures, see the known limitations.\nFor Python procedures, see the known limitations.\nFor Scala procedures, see the known limitations."
      },
      {
        "heading": "Examples",
        "description": "\nThe following example creates and calls a procedure, specifying the arguments by position:\nThe following example creates and calls a procedure, specifying the arguments by name:\nFor additional examples, refer to the following topics:\nFor examples of Java procedures, see Writing Java handlers for stored procedures created with SQL.\nFor examples of Python procedures, see Writing stored procedures with SQL and Python.\nFor examples of Scala procedures, see Writing Scala handlers for stored procedures created with SQL.\nFor examples of Snowflake Scripting stored procedures, see Writing stored procedures in Snowflake Scripting.\nFor examples of Java procedures, see Writing Java handlers for stored procedures created with SQL.\nFor examples of Python procedures, see Writing stored procedures with SQL and Python.\nFor examples of Scala procedures, see Writing Scala handlers for stored procedures created with SQL.\nFor examples of Snowflake Scripting stored procedures, see Writing stored procedures in Snowflake Scripting.\nFor procedure examples, see Working with stored procedures.\nOn this page\nSyntax\nRequired parameters\nOptional parameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "WITH copy_to_table AS PROCEDURE (fromTable STRING, toTable STRING, count INT)\n  RETURNS STRING\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '2.12'\n  PACKAGES = ('com.snowflake:snowpark:latest')\n  HANDLER = 'DataCopy.copyBetweenTables'\n  AS\n  $$\n    object DataCopy\n    {\n      def copyBetweenTables(session: com.snowflake.snowpark.Session, fromTable: String, toTable: String, count: Int): String =\n      {\n        session.table(fromTable).limit(count).write.saveAsTable(toTable)\n        return \"Success\"\n      }\n    }\n  $$\n  CALL copy_to_table('table_a', 'table_b', 5);",
          "WITH copy_to_table AS PROCEDURE (fromTable STRING, toTable STRING, count INT)\n  RETURNS STRING\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '2.12'\n  PACKAGES = ('com.snowflake:snowpark:latest')\n  HANDLER = 'DataCopy.copyBetweenTables'\n  AS\n  $$\n    object DataCopy\n    {\n      def copyBetweenTables(session: com.snowflake.snowpark.Session, fromTable: String, toTable: String, count: Int): String =\n      {\n        session.table(fromTable).limit(count).write.saveAsTable(toTable)\n        return \"Success\"\n      }\n      }\n    }\n  $$\n  CALL copy_to_table(\n    toTable => 'table_b',\n    count => 5,\n    fromTable => 'table_a');"
        ]
      }
    ]
  },
  {
    "function": "DESCRIBE PROCEDURE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-procedure",
    "details": [
      {
        "heading": "DESCRIBE PROCEDURE",
        "description": "\nDescribes the specified stored procedure, including the stored procedures signature (i.e. arguments), return value, language, and\nbody (i.e. definition).",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DROP PROCEDURE , ALTER PROCEDURE , CREATE PROCEDURE , SHOW PROCEDURES, SHOW USER PROCEDURES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] PROCEDURE <procedure_name> ( [ <arg_data_type> [ , <arg_data_type_2> ... ] ] )"
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo describe a stored procedure, you must specify the name and the argument data type(s), if any, for the stored procedure. The\narguments are required because stored procedures support name overloading (i.e. two stored procedures in the same schema can have\nthe same name as long as their argument data types are different).\nThe body property in the output displays the code for the stored procedure.\nTo describe a stored procedure, you must specify the name and the argument data type(s), if any, for the stored procedure. The\narguments are required because stored procedures support name overloading (i.e. two stored procedures in the same schema can have\nthe same name as long as their argument data types are different).\nThe body property in the output displays the code for the stored procedure.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nThis example shows how to describe a stored procedure that has no parameters:\nThis example shows how to describe a stored procedure that has a parameter:\nOn this page\nSyntax\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESC PROCEDURE my_pi();\n+---------------+----------------------+\n| property      | value                |\n|---------------+----------------------|\n| signature     | ()                   |\n| returns       | FLOAT                |\n| language      | JAVASCRIPT           |\n| null handling | CALLED ON NULL INPUT |\n| volatility    | VOLATILE             |\n| execute as    | CALLER               |\n| body          |                      |\n|               |   return 3.1415926;  |\n|               |                      |\n+---------------+----------------------+",
          "DESC PROCEDURE area_of_circle(FLOAT);\n+---------------+------------------------------------------------------------------+\n| property      | value                                                            |\n|---------------+------------------------------------------------------------------|\n| signature     | (RADIUS FLOAT)                                                   |\n| returns       | FLOAT                                                            |\n| language      | JAVASCRIPT                                                       |\n| null handling | CALLED ON NULL INPUT                                             |\n| volatility    | VOLATILE                                                         |\n| execute as    | OWNER                                                            |\n| body          |                                                                  |\n|               |   var stmt = snowflake.createStatement(                          |\n|               |       {sqlText: \"SELECT pi() * POW($RADIUS, 2)\", binds:[RADIUS]} |\n|               |       );                                                         |\n|               |   var rs = stmt.execute();                                       |\n|               |   rs.next()                                                      |\n|               |   var output = rs.getColumnValue(1);                             |\n|               |   return output;                                                 |\n|               |                                                                  |\n+---------------+------------------------------------------------------------------+"
        ]
      }
    ]
  },
  {
    "function": "DROP PROCEDURE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-procedure",
    "details": [
      {
        "heading": "DROP PROCEDURE",
        "description": "\nRemoves the specified stored procedure from the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE PROCEDURE , ALTER PROCEDURE , SHOW PROCEDURES , DESCRIBE PROCEDURE, SHOW USER PROCEDURES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP PROCEDURE [ IF EXISTS ] <procedure_name> ( [ <arg_data_type> , ... ] )"
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nAll Languages\nFor each argument defined for the procedure, the data type for the argument must be specified. This is required because overloading of\nprocedure names is supported and the data type(s) for the argument(s) are required to identify the procedure.\nFor each argument defined for the procedure, the data type for the argument must be specified. This is required because overloading of\nprocedure names is supported and the data type(s) for the argument(s) are required to identify the procedure.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nJava, Python, and Scala\nFor procedures that store code in a file (such as a .jar file or .py file) in a stage, the DROP PROCEDURE command does not remove\nthe file. Different procedures can use different functions/methods in the same file, so the file should not be removed\nwhile any procedure refers to it. Snowflake does not store a count of the number of references to each staged file and\ndoes not remove that staged file when there are no remaining references.\nFor procedures that store code in a file (such as a .jar file or .py file) in a stage, the DROP PROCEDURE command does not remove\nthe file. Different procedures can use different functions/methods in the same file, so the file should not be removed\nwhile any procedure refers to it. Snowflake does not store a count of the number of references to each staged file and\ndoes not remove that staged file when there are no remaining references."
      },
      {
        "heading": "Examples",
        "description": "\nOn this page\nSyntax\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP PROCEDURE add_accounting_user(varchar);\n\n-------------------------------------------+\n             status                        |\n-------------------------------------------+\n ADD_ACCOUNTING_USER successfully dropped. |\n-------------------------------------------+"
        ]
      }
    ]
  },
  {
    "function": "SHOW PROCEDURES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-procedures",
    "details": [
      {
        "heading": "SHOW PROCEDURES",
        "description": "\nLists all stored procedures that you have privileges to access, including built-in and user-defined procedures.\nFor a command that lists only user-defined procedures, see SHOW USER PROCEDURES.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER PROCEDURE ,  CREATE PROCEDURE , DROP PROCEDURE , DESCRIBE PROCEDURE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW PROCEDURES [ LIKE '<pattern>' ]\n  [ IN\n    {\n      ACCOUNT                                         |\n\n      CLASS <class_name>                              |\n\n      DATABASE                                        |\n      DATABASE <database_name>                        |\n\n      SCHEMA                                          |\n      SCHEMA <schema_name>                            |\n      <schema_name>\n\n      APPLICATION <application_name>                  |\n      APPLICATION PACKAGE <application_package_name>  |\n    }\n  ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nIf you specify CLASS, the command only returns the following columns:\n| name | min_num_arguments | max_num_arguments | arguments | descriptions | language |\nIf you specify CLASS, the command only returns the following columns:",
        "syntax": [
          "| name | min_num_arguments | max_num_arguments | arguments | descriptions | language |"
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Timestamp at which the stored procedure was created."
              ],
              [
                "name",
                "Name of the stored procedure."
              ],
              [
                "schema_name",
                "Name of the schema in which the stored procedure exists."
              ],
              [
                "is_builtin",
                "Y if the stored procedure is built-in (rather than user-defined); N otherwise."
              ],
              [
                "is_aggregate",
                "Not applicable currently."
              ],
              [
                "is_ansi",
                "Y if the stored procedure is defined in the ANSI standard; N otherwise."
              ],
              [
                "min_num_arguments",
                "Minimum number of arguments."
              ],
              [
                "max_num_arguments",
                "Maximum number of arguments."
              ],
              [
                "arguments",
                "Data types of the arguments and of the return types. Optional arguments are displayed with the DEFAULT keyword. For Snowflake Scripting stored procedures, OUT is displayed for output arguments."
              ],
              [
                "description",
                "Description of the stored procedure."
              ],
              [
                "catalog_name",
                "Name of the database in which the stored procedure exists."
              ],
              [
                "is_table_function",
                "Y if the stored procedure returns tabular data; N otherwise."
              ],
              [
                "valid_for_clustering",
                "Not applicable currently."
              ],
              [
                "is_secure",
                "Y if the stored procedure is a secure stored procedure; N otherwise."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nShow all procedures:\nThis example shows how to use SHOW PROCEDURE on a stored procedure that has a parameter. This also shows how to limit the list of\nprocedures to those that match the specified regular expression.\nThe output columns are similar to the output columns for SHOW FUNCTIONS and\nSHOW USER FUNCTIONS. For stored procedures, some of these columns are not currently meaningful\n(e.g. is_aggregate, valid_for_clustering), but are reserved for future use.\nOn this page\nSyntax\nParameters\nUsage notes\nOutput\nExamples\nRelated content\nStored procedures overview\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW PROCEDURES;",
          "SHOW PROCEDURES LIKE 'area_of_%';\n+-------------------------------+----------------+--------------------+------------+--------------+---------+-------------------+-------------------+------------------------------------+------------------------+-----------------------+-------------------+----------------------+-----------+\n| created_on                    | name           | schema_name        | is_builtin | is_aggregate | is_ansi | min_num_arguments | max_num_arguments | arguments                          | description            | catalog_name          | is_table_function | valid_for_clustering | is_secure |\n|-------------------------------+----------------+--------------------+------------+--------------+---------+-------------------+-------------------+------------------------------------+------------------------+-----------------------+-------------------+----------------------+-----------|\n| 1967-06-23 00:00:00.123 -0700 | AREA_OF_CIRCLE | TEMPORARY_DOC_TEST | N          | N            | N       |                 1 |                 1 | AREA_OF_CIRCLE(FLOAT) RETURN FLOAT | user-defined procedure | TEMPORARY_DOC_TEST_DB | N                 | N                    | N         |\n+-------------------------------+----------------+--------------------+------------+--------------+---------+-------------------+-------------------+------------------------------------+------------------------+-----------------------+-------------------+----------------------+-----------+"
        ]
      }
    ]
  },
  {
    "function": "SHOW USER PROCEDURES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-user-procedures",
    "details": [
      {
        "heading": "SHOW USER PROCEDURES",
        "description": "\nLists all user-defined procedures for which you have access privileges. Use this command to list the user-defined procedures for a specified\ndatabase or schema (or the current database/schema for the session), application, or for your entire account.\nFor a command that lists all procedures, including both built-in and user-defind procedures, see SHOW PROCEDURES.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "SHOW PROCEDURES, PROCEDURES view (Information Schema),\nPROCEDURES view (Account Usage), SHOW USER PROCEDURES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW USER PROCEDURES [ LIKE '<pattern>' ]\n  [ IN\n    {\n      ACCOUNT                                         |\n\n      DATABASE                                        |\n      DATABASE <database_name>                        |\n\n      SCHEMA                                          |\n      SCHEMA <schema_name>                            |\n      <schema_name>\n\n      APPLICATION <application_name>                  |\n      APPLICATION PACKAGE <application_package_name>  |\n    }\n  ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Timestamp at which the procedure was created."
              ],
              [
                "name",
                "Name of the procedure."
              ],
              [
                "schema_name",
                "Name of the schema in which the procedure exists."
              ],
              [
                "is_builtin",
                "Y if the procedure is built in; N otherwise (always N for user-created procedures)."
              ],
              [
                "is_aggregate",
                "Not applicable currently."
              ],
              [
                "is_ansi",
                "Not applicable currently."
              ],
              [
                "min_num_arguments",
                "Minimum number of arguments to the procedure."
              ],
              [
                "max_num_arguments",
                "Maximum number of arguments to the procedure."
              ],
              [
                "arguments",
                "Data types of the arguments and return value. For Snowflake Scripting stored procedures, OUT is displayed for output arguments."
              ],
              [
                "description",
                "Description of the procedure."
              ],
              [
                "catalog_name",
                "Name of the database in which the procedure exists."
              ],
              [
                "is_table_function",
                "Y if the procedure returns a table; N otherwise."
              ],
              [
                "valid_for_clustering",
                "Y if the procedure can be used in a CLUSTER BY expression; N otherwise."
              ],
              [
                "is_secure",
                "Y if the procedure is a secure procedure; N otherwise."
              ],
              [
                "secrets",
                "Map of secret values specified by the procedures SECRETS parameter, where map keys are secret variable names and map values are secret object names."
              ],
              [
                "external_access_integrations",
                "Names of external access integrations specified by the procedures EXTERNAL_ACCESS_INTEGRATION parameter."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nShow procedures that you have privileges to view in the current schema whose names begin with GET_:\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nRelated content\nStored procedures overview\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW USER PROCEDURES LIKE 'GET_%' IN SCHEMA;",
          "-------------------------------+-----------------+-------------+------------+--------------+---------+-------------------+-------------------+---------------------------------------+------------------------+--------------+-------------------+----------------------+-----------+---------+------------------------------+\n          created_on           | name            | schema_name | is_builtin | is_aggregate | is_ansi | min_num_arguments | max_num_arguments | arguments                             | description            | catalog_name | is_table_function | valid_for_clustering | is_secure | secrets | external_access_integrations |\n-------------------------------+-----------------+-------------+------------+--------------+---------+-------------------+-------------------+---------------------------------------+------------------------+--------------+-------------------+----------------------+-----------+---------+------------------------------+\n 2023-01-27 15:01:13.862 -0800 | GET_FILE        | PUBLIC      | N          | N            | N       | 1                 | 1                 | GET_FILE(VARCHAR) RETURN VARCHAR      | user-defined procedure | BOOKS_DB     | N                 | N                    | N         |         |                              |\n 2023-03-23 10:38:10.423 -0700 | GET_NUM_RESULTS | PUBLIC      | N          | N            | N       | 1                 | 1                 | GET_NUM_RESULTS(VARCHAR) RETURN FLOAT | user-defined procedure | BOOKS_DB     | N                 | N                    | N         |         |                              |\n 2023-03-23 09:47:55.840 -0700 | GET_RESULTS     | PUBLIC      | N          | N            | N       | 1                 | 1                 | GET_RESULTS(VARCHAR) RETURN TABLE ()  | user-defined procedure | BOOKS_DB     | Y                 | N                    | N         |         |                              |\n-------------------------------+-----------------+-------------+------------+--------------+---------+-------------------+-------------------+---------------------------------------+------------------------+--------------+-------------------+----------------------+-----------+---------+------------------------------+"
        ]
      }
    ]
  },
  {
    "function": "EXECUTE IMMEDIATE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/execute-immediate",
    "details": [
      {
        "heading": "EXECUTE IMMEDIATE",
        "description": "\nExecutes a string that contains a SQL statement or a\nSnowflake Scripting statement.\nYou can use EXECUTE IMMEDIATE to do the following:\nIn a Snowflake Scripting block, execute dynamic SQL, where parts of the SQL statement arent known\nuntil runtime. For examples, see Executing dynamic SQL in a Snowflake Scripting block.\nSet a session variable to a SQL statement, and reference the session variable to run the SQL statement.\nFor an example, see Setting a session variable to a statement and executing it.\nIf you are using SnowSQL or the Classic Console, run a Snowflake Scripting anonymous block.\nFor an example, see Running an anonymous block in SnowSQL or the Classic Console.\nIn a Snowflake Scripting block, execute dynamic SQL, where parts of the SQL statement arent known\nuntil runtime. For examples, see Executing dynamic SQL in a Snowflake Scripting block.\nSet a session variable to a SQL statement, and reference the session variable to run the SQL statement.\nFor an example, see Setting a session variable to a statement and executing it.\nIf you are using SnowSQL or the Classic Console, run a Snowflake Scripting anonymous block.\nFor an example, see Running an anonymous block in SnowSQL or the Classic Console."
      },
      {
        "heading": "Syntax",
        "syntax": [
          "EXECUTE IMMEDIATE '<string_literal>'\n    [ USING ( <bind_variable> [ , <bind_variable> ... ] ) ]\n\nEXECUTE IMMEDIATE <variable>\n    [ USING ( <bind_variable> [ , <bind_variable> ... ] ) ]\n\nEXECUTE IMMEDIATE $<session_variable>\n    [ USING ( <bind_variable> [ , <bind_variable> ... ] ) ]"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "'string_literal' or . variable or . session_variable",
            "definition": "A string literal, Snowflake Scripting variable, or\nsession variable that contains a statement. A statement can be any of the following: A single SQL statement A stored procedure call A control-flow statement (for example, looping or\nbranching statement) A block If you use a session variable, the length of the statement must not exceed the\nmaximum size of a session variable (256 bytes)."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "USING ( bind_variable [ , bind_variable ... ] )",
            "definition": "Specifies one or more bind variables that hold values to be used in the cursors query definition (for example,\nin a WHERE clause)."
          }
        ]
      },
      {
        "heading": "Returns",
        "description": "\nEXECUTE IMMEDIATE returns the result of the executed statement. For example, if the string or variable contained a SELECT\nstatement, the result set of the SELECT statement is returned."
      },
      {
        "heading": "Usage notes",
        "description": "\nThe string_literal, variable, or session_variable must contain only one statement.\n(A block is considered one statement, even if the body of the block\ncontains multiple statements.)\nA session_variable must be preceded by a dollar sign ($).\nA local variable must not be preceded by a dollar sign ($).\nThe string_literal, variable, or session_variable must contain only one statement.\n(A block is considered one statement, even if the body of the block\ncontains multiple statements.)\nA session_variable must be preceded by a dollar sign ($).\nA local variable must not be preceded by a dollar sign ($)."
      },
      {
        "heading": "Examples",
        "description": "\nThe following are examples that use the EXECUTE IMMEDIATE command."
      },
      {
        "heading": "Executing dynamic SQL in a Snowflake Scripting block",
        "description": "\nThe following examples execute dynamic SQL in a Snowflake Scripting block.\nThis example executes statements that are defined in two local variables in a\nSnowflake Scripting stored procedure.\nThis example also demonstrates that EXECUTE IMMEDIATE works not only with a string literal, but also\nwith an expression that evaluates to a string (VARCHAR).\nNote: If you use Snowflake CLI, SnowSQL, the Classic Console, or the\nexecute_stream or execute_string method in Python Connector\ncode, use this example instead (see Using Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector):\nCall the stored procedure:\nThis example uses EXECUTE IMMEDIATE to execute a SELECT statement that contains bind variables\nin the USING parameter in a Snowflake Scripting stored procedure. First create the table and insert\nthe data:\nCreate the stored procedure:\nNote: If you use Snowflake CLI, SnowSQL, the Classic Console, or the\nexecute_stream or execute_string method in Python Connector\ncode, use this example instead (see Using Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector):\nCall the stored procedure:",
        "syntax": [
          "CREATE PROCEDURE execute_immediate_local_variable()\nRETURNS VARCHAR\nAS\nDECLARE\n  v1 VARCHAR DEFAULT 'CREATE TABLE temporary1 (i INTEGER)';\n  v2 VARCHAR DEFAULT 'INSERT INTO temporary1 (i) VALUES (76)';\n  result INTEGER DEFAULT 0;\nBEGIN\n  EXECUTE IMMEDIATE v1;\n  EXECUTE IMMEDIATE v2  ||  ',(80)'  ||  ',(84)';\n  result := (SELECT SUM(i) FROM temporary1);\n  RETURN result::VARCHAR;\nEND;",
          "CREATE PROCEDURE execute_immediate_local_variable()\nRETURNS VARCHAR\nAS\n$$\nDECLARE\n  v1 VARCHAR DEFAULT 'CREATE TABLE temporary1 (i INTEGER)';\n  v2 VARCHAR DEFAULT 'INSERT INTO temporary1 (i) VALUES (76)';\n  result INTEGER DEFAULT 0;\nBEGIN\n  EXECUTE IMMEDIATE v1;\n  EXECUTE IMMEDIATE v2  ||  ',(80)'  ||  ',(84)';\n  result := (SELECT SUM(i) FROM temporary1);\n  RETURN result::VARCHAR;\nEND;\n$$;",
          "CALL execute_immediate_local_variable();",
          "+----------------------------------+\n| EXECUTE_IMMEDIATE_LOCAL_VARIABLE |\n|----------------------------------|\n| 240                              |\n+----------------------------------+",
          "CREATE OR REPLACE TABLE invoices (id INTEGER, price NUMBER(12, 2));\n\nINSERT INTO invoices (id, price) VALUES\n  (1, 11.11),\n  (2, 22.22);",
          "CREATE OR REPLACE PROCEDURE min_max_invoices_sp(\n    minimum_price NUMBER(12,2),\n    maximum_price NUMBER(12,2))\n  RETURNS TABLE (id INTEGER, price NUMBER(12, 2))\n  LANGUAGE SQL\nAS\nDECLARE\n  rs RESULTSET;\n  query VARCHAR DEFAULT 'SELECT * FROM invoices WHERE price > ? AND price < ?';\nBEGIN\n  rs := (EXECUTE IMMEDIATE :query USING (minimum_price, maximum_price));\n  RETURN TABLE(rs);\nEND;",
          "CREATE OR REPLACE PROCEDURE min_max_invoices_sp(\n    minimum_price NUMBER(12,2),\n    maximum_price NUMBER(12,2))\n  RETURNS TABLE (id INTEGER, price NUMBER(12, 2))\n  LANGUAGE SQL\nAS\n$$\nDECLARE\n  rs RESULTSET;\n  query VARCHAR DEFAULT 'SELECT * FROM invoices WHERE price > ? AND price < ?';\nBEGIN\n  rs := (EXECUTE IMMEDIATE :query USING (minimum_price, maximum_price));\n  RETURN TABLE(rs);\nEND;\n$$\n;",
          "CALL min_max_invoices_sp(20, 30);",
          "+----+-------+\n| ID | PRICE |\n|----+-------|\n|  2 | 22.22 |\n+----+-------+"
        ]
      },
      {
        "heading": "Setting a session variable to a statement and executing it",
        "description": "\nThis example executes a statement defined in a session variable:",
        "syntax": [
          "SET stmt =\n$$\n    SELECT PI();\n$$\n;",
          "EXECUTE IMMEDIATE $stmt;",
          "+-------------+\n|        PI() |\n|-------------|\n| 3.141592654 |\n+-------------+"
        ]
      },
      {
        "heading": "Running an anonymous block in SnowSQL or the Classic Console",
        "description": "\nWhen you run a Snowflake Scripting anonymous block\nin SnowSQL or the Classic Console, you must specify the block as\na string literal (delimited by single quotes or double dollar signs), and you must pass the\nblock to the EXECUTE IMMEDIATE command. For more information, see\nUsing Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector.\nThis example runs an anonymous block passed to the EXECUTE IMMEDIATE command:\nOn this page\nSyntax\nRequired parameters\nOptional parameters\nReturns\nUsage notes\nExamples\nExecuting dynamic SQL in a Snowflake Scripting block\nExecuting statements that contain variables\nExecuting a statement that contains bind variables\nSetting a session variable to a statement and executing it\nRunning an anonymous block in SnowSQL or the Classic Console\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "EXECUTE IMMEDIATE $$\nDECLARE\n  radius_of_circle FLOAT;\n  area_of_circle FLOAT;\nBEGIN\n  radius_of_circle := 3;\n  area_of_circle := PI() * radius_of_circle * radius_of_circle;\n  RETURN area_of_circle;\nEND;\n$$\n;",
          "+-----------------+\n| anonymous block |\n|-----------------|\n|    28.274333882 |\n+-----------------+"
        ]
      }
    ]
  },
  {
    "function": "EXECUTE IMMEDIATE FROM",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/execute-immediate-from",
    "details": [
      {
        "heading": "EXECUTE IMMEDIATE FROM",
        "description": "\nEXECUTE IMMEDIATE FROM executes the SQL statements specified in a file in a stage. The file can contain\nSQL statements or Snowflake Scripting blocks. The statements must be syntactically\ncorrect SQL statements.\nYou can use the EXECUTE IMMEDIATE FROM command to execute the statements in a file from any Snowflake session.\nThis feature provides a mechanism to control the deployment and management of your Snowflake objects and code. For example, you can execute\na stored script to create a standard Snowflake environment for all your accounts. The configuration script might include statements\nthat create users, roles, databases, and schemas for every new account."
      },
      {
        "heading": "Jinja2 templating",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nEXECUTE IMMEDIATE FROM can also execute a template file using the Jinja2 templating language.\nA template can contain variables and expressions, enabling the use of loops, conditionals, variable substitution, macros, and more.\nTemplates can also include other templates and can import macros defined in other files located on a stage.\nFor more information about the templating language, see the Jinja2 documentation.\nThe template file to be executed must be:\nA syntactically valid Jinja2 template.\nLocated in a stage or Git repository clone.\nAble to render syntactically valid SQL statements.\nA syntactically valid Jinja2 template.\nLocated in a stage or Git repository clone.\nAble to render syntactically valid SQL statements.\nTemplating enables more flexible control structures and parameterization using environment variables. For example, you can use\na template to dynamically choose the deployment target of the objects defined in the script. To use a template to render a\nSQL script, use the templating directive or add a\nUSING clause with at least one template variable."
      },
      {
        "heading": "Templating directive",
        "description": "\nYou can use either one of the two templating directives.\nThe recommended directive uses valid SQL syntax:\nOptionally, you can use the alternative directive:\nNote\nOnly a byte order mark and up to 10 whitespace characters (newlines, tabs, spaces) may be placed in front of the directive.\nAny characters that come after the directive on the same line will be ignored.",
        "syntax": [
          "--!jinja",
          "#!jinja"
        ]
      },
      {
        "heading": "Using content from staged files in a template",
        "description": "\nA template can load other staged files either directly through the\nSnowflakeFile API\nor through Jinja2s include,\nimport, and\ninheritance features.\nFiles can be referenced by absolute paths:\nInclude, import, and extends also support relative paths while the SnowflakeFile API supports scoped Snowflake file URLs:",
        "syntax": [
          "{% include \"@my_stage/path/to/my_template\" %}\n{% import \"@my_stage/path/to/my_template\" as my_template %}\n{% extends \"@my_stage/path/to/my_template\" %}\n{{ SnowflakeFile.open(\"@my_stage/path/to/my_template\", 'r', require_scoped_url = False).read() }}",
          "{% include \"my_template\" %}\n{% import \"../my_template\" as my_template %}\n{% extends \"/path/to/my_template\" %}"
        ],
        "definitions": [
          {
            "term": "See also:",
            "definition": "EXECUTE IMMEDIATE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:",
        "syntax": [
          "EXECUTE IMMEDIATE\n  FROM { absoluteFilePath | relativeFilePath }\n  [ USING ( <key> => <value> [ , <key> => <value> [ , ... ] ]  )  ]\n  [ DRY_RUN = { TRUE | FALSE } ]",
          "absoluteFilePath ::=\n   @[ <namespace>. ]<stage_name>/<path>/<filename>",
          "relativeFilePath ::=\n  '[ / | ./ | ../ ]<path>/<filename>'"
        ]
      },
      {
        "heading": "Required parameters"
      },
      {
        "heading": "Absolute file path (absoluteFilePath)",
        "definitions": [
          {
            "term": "namespace",
            "definition": "Database and/or schema in which the internal or external stage resides, in the form of database_name.schema_name\nor schema_name. The namespace is optional if a database and schema are currently in use for the user session; otherwise,\nit is required."
          },
          {
            "term": "stage_name",
            "definition": "Name of the internal or external stage."
          },
          {
            "term": "path",
            "definition": "Case-sensitive path to the file in the stage."
          },
          {
            "term": "filename",
            "definition": "Name of the file to execute. It must contain syntactically correct and valid SQL statements. Each statement must be\nseparated by a semicolon."
          }
        ]
      },
      {
        "heading": "Relative file path (relativeFilePath)",
        "definitions": [
          {
            "term": "path",
            "definition": "Case-sensitive relative path to the file in the stage. Relative paths support established conventions such as a leading /\nto indicate the root of a stages file system, ./ to refer to the current directory (the directory the parent file is\nlocated in) and ../ to refer to the parent directory. For more information, see Usage notes."
          },
          {
            "term": "filename",
            "definition": "Name of the file to execute. It must contain syntactically correct and valid SQL statements. Each statement must be\nseparated by a semicolon."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "USING ( <key> => <value> [ , <key> => <value> [ , ... ] ] )",
            "definition": "Preview Feature  Open Available to all accounts. Allows you to pass one or more key-value pairs that can be used to parameterize template expansion. The key-value pairs\nmust form a comma-separated list. When the USING clause is present, the file is first rendered as a Jinja2 template\nbefore being executed as a SQL script. Where: key is the name of the template variable. The template variable name can optionally be enclosed in double quotes\n(\"). value is the value to assign to the variable in the template. String values must be enclosed in ' or\n$$. For an example, see Templating usage notes."
          },
          {
            "term": "DRY_RUN = { TRUE | FALSE }",
            "definition": "Preview Feature  Open Available to all accounts. Specifies whether to preview the rendered file without executing it as a SQL script. TRUE returns the rendered file contents without executing the SQL statements. FALSE renders SQL statements from the template and executes those statements. Default: FALSE"
          }
        ]
      },
      {
        "heading": "Returns",
        "description": "\nEXECUTE IMMEDIATE FROM returns:\nThe result of the last statement in the file if all statements are successfully executed.\nThe error message, if any statement in the file failed.\nIf there is an error in any statement in the file, the EXECUTE IMMEDIATE FROM command fails and returns the error message\nof the failed statement.\n\nNote\nIf the EXECUTE IMMEDIATE FROM command fails and returns an error message, any statements in the file prior to the failed statement\nhave successfully completed.\nThe result of the last statement in the file if all statements are successfully executed.\nThe error message, if any statement in the file failed.\nIf there is an error in any statement in the file, the EXECUTE IMMEDIATE FROM command fails and returns the error message\nof the failed statement.\nNote\nIf the EXECUTE IMMEDIATE FROM command fails and returns an error message, any statements in the file prior to the failed statement\nhave successfully completed."
      },
      {
        "heading": "Access control requirements",
        "description": "\nThe role used to execute the EXECUTE IMMEDIATE FROM command must have the\nUSAGE (external stage) or READ (internal stage) privilege on the stage where the file is located.\nThe role used to execute the EXECUTE IMMEDIATE FROM command must have the\nUSAGE (external stage) or READ (internal stage) privilege on the stage where the file is located.\nThe role used to execute the file can only execute the statements in the file for which it has privileges.\nFor example, if there is a CREATE TABLE statement in the file, the role must have the\nnecessary privileges to create a table in the account or the statement fails.\nThe role used to execute the file can only execute the statements in the file for which it has privileges.\nFor example, if there is a CREATE TABLE statement in the file, the role must have the\nnecessary privileges to create a table in the account or the statement fails.\nThe USAGE privilege on the parent database and schema are required to perform operations on any object in a schema.\nFor instructions on creating a custom role with a specified set of privileges, see Creating custom roles.\nFor general information about roles and privilege grants for performing SQL actions on\nsecurable objects, see Overview of Access Control."
      },
      {
        "heading": "Usage notes",
        "description": "\nThe SQL statements in a file to be executed can include EXECUTE IMMEDIATE FROM statements:\n\nNested EXECUTE IMMEDIATE FROM statements can use relative file paths.\nRelative paths are evaluated in respect to the stage and file path of the parent file. If the relative file path starts with\n/, the path starts at the root directory of the stage containing the parent file.\nFor an example, see Examples.\n\nRelative file paths must be enclosed in single quotes (') or $$.\nThe maximum execution depth for nested files is 5.\nNested EXECUTE IMMEDIATE FROM statements can use relative file paths.\nRelative paths are evaluated in respect to the stage and file path of the parent file. If the relative file path starts with\n/, the path starts at the root directory of the stage containing the parent file.\nFor an example, see Examples.\nRelative file paths must be enclosed in single quotes (') or $$.\nThe maximum execution depth for nested files is 5.\nAbsolute file paths can optionally be enclosed in single quotes (') or $$.\nThe file to be executed cannot be larger than 10MB in size.\nThe file to be executed must be encoded in UTF-8.\nThe file to be executed must be uncompressed. If you use the PUT command to upload a file to an internal\nstage, you must explicitly set the AUTO_COMPRESS parameter to FALSE.\nFor example, upload my_file.sql to my_stage:\nPUT file://~/sql/scripts/my_file.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\n\nCopy\nThe execution of all files in a directory is not supported. For example, EXECUTE IMMEDIATE FROM @stage_name/scripts/\nresults in an error.\nThe SQL statements in a file to be executed can include EXECUTE IMMEDIATE FROM statements:\nNested EXECUTE IMMEDIATE FROM statements can use relative file paths.\nRelative paths are evaluated in respect to the stage and file path of the parent file. If the relative file path starts with\n/, the path starts at the root directory of the stage containing the parent file.\nFor an example, see Examples.\nRelative file paths must be enclosed in single quotes (') or $$.\nThe maximum execution depth for nested files is 5.\nNested EXECUTE IMMEDIATE FROM statements can use relative file paths.\nRelative paths are evaluated in respect to the stage and file path of the parent file. If the relative file path starts with\n/, the path starts at the root directory of the stage containing the parent file.\nFor an example, see Examples.\nRelative file paths must be enclosed in single quotes (') or $$.\nThe maximum execution depth for nested files is 5.\nAbsolute file paths can optionally be enclosed in single quotes (') or $$.\nThe file to be executed cannot be larger than 10MB in size.\nThe file to be executed must be encoded in UTF-8.\nThe file to be executed must be uncompressed. If you use the PUT command to upload a file to an internal\nstage, you must explicitly set the AUTO_COMPRESS parameter to FALSE.\nFor example, upload my_file.sql to my_stage:\nThe execution of all files in a directory is not supported. For example, EXECUTE IMMEDIATE FROM @stage_name/scripts/\nresults in an error.",
        "syntax": [
          "PUT file://~/sql/scripts/my_file.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;"
        ]
      },
      {
        "heading": "Templating usage notes",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nVariable names in templates are case-sensitive.\nThe template variable name can be optionally enclosed in double quotes. Enclosing the variable name can be useful if any\nreserved keywords are used as variable names.\nThe following parameter types are supported in the USING clause:\n\nString. Must be enclosed by ' or $$. For example, USING (a => 'a', b => $$b$$).\nNumber (decimal and integer). For example, USING (a => 1, b => -1.23).\nBoolean. For example, USING (a => TRUE, b => FALSE).\nNULL. For example, USING (a => NULL).\n\nNote\nThe Jinja2 templating engine interprets a NULL value as the Python NoneType type.\n\n\nSession variables. For example, USING (a => $var). Only session variables holding\nvalues of supported data types are allowed.\nBind variables. For example, USING (a => :var). Only bind variables\nholding values of supported data types are allowed. You can use bind variables to pass stored procedure arguments to a template.\nString. Must be enclosed by ' or $$. For example, USING (a => 'a', b => $$b$$).\nNumber (decimal and integer). For example, USING (a => 1, b => -1.23).\nBoolean. For example, USING (a => TRUE, b => FALSE).\nNULL. For example, USING (a => NULL).\n\nNote\nThe Jinja2 templating engine interprets a NULL value as the Python NoneType type.\nSession variables. For example, USING (a => $var). Only session variables holding\nvalues of supported data types are allowed.\nBind variables. For example, USING (a => :var). Only bind variables\nholding values of supported data types are allowed. You can use bind variables to pass stored procedure arguments to a template.\nFiles in Snowflake Git repositories or in Snowflake Native Apps cannot be accessed from the template.\nThe maximum result size for template rendering is 100,000 bytes.\nTemplates are rendered using the Jinja2 version 3.1.4 templating engine.\nVariable names in templates are case-sensitive.\nThe template variable name can be optionally enclosed in double quotes. Enclosing the variable name can be useful if any\nreserved keywords are used as variable names.\nThe following parameter types are supported in the USING clause:\nString. Must be enclosed by ' or $$. For example, USING (a => 'a', b => $$b$$).\nNumber (decimal and integer). For example, USING (a => 1, b => -1.23).\nBoolean. For example, USING (a => TRUE, b => FALSE).\nNULL. For example, USING (a => NULL).\n\nNote\nThe Jinja2 templating engine interprets a NULL value as the Python NoneType type.\nSession variables. For example, USING (a => $var). Only session variables holding\nvalues of supported data types are allowed.\nBind variables. For example, USING (a => :var). Only bind variables\nholding values of supported data types are allowed. You can use bind variables to pass stored procedure arguments to a template.\nString. Must be enclosed by ' or $$. For example, USING (a => 'a', b => $$b$$).\nNumber (decimal and integer). For example, USING (a => 1, b => -1.23).\nBoolean. For example, USING (a => TRUE, b => FALSE).\nNULL. For example, USING (a => NULL).\nNote\nThe Jinja2 templating engine interprets a NULL value as the Python NoneType type.\nSession variables. For example, USING (a => $var). Only session variables holding\nvalues of supported data types are allowed.\nBind variables. For example, USING (a => :var). Only bind variables\nholding values of supported data types are allowed. You can use bind variables to pass stored procedure arguments to a template.\nFiles in Snowflake Git repositories or in Snowflake Native Apps cannot be accessed from the template.\nThe maximum result size for template rendering is 100,000 bytes.\nTemplates are rendered using the Jinja2 version 3.1.4 templating engine."
      },
      {
        "heading": "Troubleshooting EXECUTE IMMEDIATE FROM errors",
        "description": "\nThis section contains some common errors that result from an EXECUTE IMMEDIATE FROM statement and how you can resolve them.\nFile errors\nStage errors\nAccess control errors\nTemplating errors\nFile errors\nStage errors\nAccess control errors\nTemplating errors"
      },
      {
        "heading": "File errors",
        "description": "\nError\nCause\nThere are multiple causes for this error:\nThe file does not exist.\nThe file name is the root of a directory. For example @stage_name/scripts/.\nThe file does not exist.\nThe file name is the root of a directory. For example @stage_name/scripts/.\nSolution\nVerify the name of the file and confirm the file exists. Executing all the files in a directory is not supported.\nError\nCause\nThe statement was executed using a relative file path outside of a file execution.\nSolution\nA relative file path can only be used in EXECUTE IMMEDIATE FROM statements in a file. Use the\nabsolute file path for the file. For more information, see\nUsage notes.\nError\nCause\nThe file contains SQL syntax errors.\nSolution\nFix the syntax errors in the file and reupload the file to the stage.",
        "syntax": [
          "001501 (02000): File '<directory_name>' not found in stage '<stage_name>'.",
          "001503 (42601): Relative file references like '<filename.sql>' cannot be used in top-level EXECUTE IMMEDIATE calls.",
          "001003 (42000): SQL compilation error: syntax error line <n> at position <m> unexpected '<string>'."
        ]
      },
      {
        "heading": "Stage errors",
        "description": "\nError\nCause\nThe stage does not exist or you do not have access to the stage.\nSolution\nVerify the name of the stage and confirm the stage exists.\nExecute the statement using a role that has the required privileges to access the stage. For more information, see\nAccess control requirements.\nVerify the name of the stage and confirm the stage exists.\nExecute the statement using a role that has the required privileges to access the stage. For more information, see\nAccess control requirements.",
        "syntax": [
          "002003 (02000): SQL compilation error: Stage '<stage_name>' does not exist or not authorized."
        ]
      },
      {
        "heading": "Access control errors",
        "description": "\nError\nCause\nThe role used to execute the statement does not have the privileges required to execute some or all of the statements\nin the file.\nSolution\nUse a role that has the appropriate privileges to execute the statements in the file. For more information, see\nAccess control requirements.\nSee also: Stage errors.",
        "syntax": [
          "003001 (42501): Uncaught exception of type 'STATEMENT_ERROR' in file <file_name> on line <n> at position <m>:\nSQL access control error: Insufficient privileges to operate on schema '<schema_name>'"
        ]
      },
      {
        "heading": "Templating errors",
        "description": "\nError\nCause\nThe file contains templating constructs (for example, {{ table_name }}) but is not rendered using the templating engine.\nIf the template is not rendered, the lines of text in the template are executed as SQL statements. The templating constructs\nin the file are likely to result in SQL syntax errors.\nSolution\nAdd a templating directive or re-execute the statement with the\nUSING clause and specify at least one template variable.\nError\nCause\nIf any variables used in the template are left unspecified in the USING clause, an error occurs.\nSolution\nVerify the names and number of variables in the template and update the USING clause to include values for all template variables.\nError\nCause\nThe value for the variable key is an unsupported type.\nSolution\nVerify that you are using a supported parameter type for the template variable value. For more information, see the\nTemplating usage notes.\nError\nCause\nThe size of the rendered template exceeds the current limit.\nSolution\nSplit your templated file into multiple smaller templates and add a new script to execute them sequentially,\nwhile passing down template variables to the nested scripts.",
        "syntax": [
          "001003 (42000): SQL compilation error:\nsyntax error line [n] at position [m] unexpected '{'.",
          "000005 (XX000): Python Interpreter Error:\njinja2.exceptions.UndefinedError: '<key>' is undefined\nin template processing",
          "001510 (42601): Unable to use value of template variable '<key>'",
          "001518 (42601): Size of expanded template exceeds limit of 100,000 bytes."
        ]
      },
      {
        "heading": "Examples"
      },
      {
        "heading": "Basic example",
        "description": "\nThis example executes the file create-inventory.sql located in stage my_stage.\nCreate a file named create-inventory.sql with the following statements:\nCREATE OR REPLACE TABLE my_inventory(\n  sku VARCHAR,\n  price NUMBER\n);\n\nEXECUTE IMMEDIATE FROM './insert-inventory.sql';\n\nSELECT sku, price\n  FROM my_inventory\n  ORDER BY price DESC;\n\nCopy\nCreate a file named insert-inventory.sql with the following statements:\nINSERT INTO my_inventory\n  VALUES ('XYZ12345', 10.00),\n         ('XYZ81974', 50.00),\n         ('XYZ34985', 30.00),\n         ('XYZ15324', 15.00);\n\nCopy\nCreate an internal stage my_stage:\nCREATE STAGE my_stage;\n\nCopy\nUpload both local files to the stage using the PUT command:\nPUT file://~/sql/scripts/create-inventory.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\n\nPUT file://~/sql/scripts/insert-inventory.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\n\nCopy\nExecute the create-inventory.sql script located in my_stage:\nEXECUTE IMMEDIATE FROM @my_stage/scripts/create-inventory.sql;\n\nCopy\nReturns:\n+----------+-------+\n| SKU      | PRICE |\n|----------+-------|\n| XYZ81974 |    50 |\n| XYZ34985 |    30 |\n| XYZ15324 |    15 |\n| XYZ12345 |    10 |\n+----------+-------+\nCreate a file named create-inventory.sql with the following statements:\nCreate a file named insert-inventory.sql with the following statements:\nCreate an internal stage my_stage:\nUpload both local files to the stage using the PUT command:\nExecute the create-inventory.sql script located in my_stage:\nReturns:",
        "syntax": [
          "CREATE OR REPLACE TABLE my_inventory(\n  sku VARCHAR,\n  price NUMBER\n);\n\nEXECUTE IMMEDIATE FROM './insert-inventory.sql';\n\nSELECT sku, price\n  FROM my_inventory\n  ORDER BY price DESC;",
          "INSERT INTO my_inventory\n  VALUES ('XYZ12345', 10.00),\n         ('XYZ81974', 50.00),\n         ('XYZ34985', 30.00),\n         ('XYZ15324', 15.00);",
          "CREATE STAGE my_stage;",
          "PUT file://~/sql/scripts/create-inventory.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\n\nPUT file://~/sql/scripts/insert-inventory.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;",
          "EXECUTE IMMEDIATE FROM @my_stage/scripts/create-inventory.sql;",
          "+----------+-------+\n| SKU      | PRICE |\n|----------+-------|\n| XYZ81974 |    50 |\n| XYZ34985 |    30 |\n| XYZ15324 |    15 |\n| XYZ12345 |    10 |\n+----------+-------+"
        ]
      },
      {
        "heading": "A simple template example",
        "description": "\nCreate a template file setup.sql with two variables and the templating directive:\n--!jinja\n\nCREATE SCHEMA {{env}};\n\nCREATE TABLE RAW (COL OBJECT)\n    DATA_RETENTION_TIME_IN_DAYS = {{retention_time}};\n\nCopy\nCreate a stage  optional if you already have a stage to which you can upload files.\nFor example, create an internal stage in Snowflake:\nCREATE STAGE my_stage;\n\nCopy\nUpload the file to your stage.\nFor example, use the PUT command from your local environment to upload file setup.sql\nto stage my_stage:\nPUT file://path/to/setup.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\n\nCopy\nExecute the file setup.sql:\nEXECUTE IMMEDIATE FROM @my_stage/scripts/setup.sql\n    USING (env=>'dev', retention_time=>0);\n\nCopy\nCreate a template file setup.sql with two variables and the templating directive:\nCreate a stage  optional if you already have a stage to which you can upload files.\nFor example, create an internal stage in Snowflake:\nUpload the file to your stage.\nFor example, use the PUT command from your local environment to upload file setup.sql\nto stage my_stage:\nExecute the file setup.sql:",
        "syntax": [
          "--!jinja\n\nCREATE SCHEMA {{env}};\n\nCREATE TABLE RAW (COL OBJECT)\n    DATA_RETENTION_TIME_IN_DAYS = {{retention_time}};",
          "CREATE STAGE my_stage;",
          "PUT file://path/to/setup.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;",
          "EXECUTE IMMEDIATE FROM @my_stage/scripts/setup.sql\n    USING (env=>'dev', retention_time=>0);"
        ]
      },
      {
        "heading": "A template example with macros, conditionals, loops, and imports",
        "description": "\nCreate a template file containing a macro definition.\nFor example, create a file macros.jinja in your local environment:\n{%- macro get_environments(deployment_type) -%}\n  {%- if deployment_type == 'prod' -%}\n    {{ \"prod1,prod2\" }}\n  {%- else -%}\n    {{ \"dev,qa,staging\" }}\n  {%- endif -%}\n{%- endmacro -%}\n\nCopy\nCreate a template file and add the templating directive (--!jinja2) to the top of the file.\nAfter the templating directive, add an import statement to import the macro defined in the\nfile that you created in the previous step.\nFor example, create a file setup-env.sql in your local environment:\n--!jinja2\n{% from \"macros.jinja\" import get_environments %}\n\n{%- set environments = get_environments(DEPLOYMENT_TYPE).split(\",\") -%}\n\n{%- for environment in environments -%}\n  CREATE DATABASE {{ environment }}_db;\n  USE DATABASE {{ environment }}_db;\n  CREATE TABLE {{ environment }}_orders (\n    id NUMBER,\n    item VARCHAR,\n    quantity NUMBER);\n  CREATE TABLE {{ environment }}_customers (\n    id NUMBER,\n    name VARCHAR);\n{% endfor %}\n\nCopy\nCreate a stage  optional if you already have a stage to which you can upload files.\nFor example, create an internal stage in Snowflake:\nCREATE STAGE my_stage;\n\nCopy\nUpload the file to your stage.\nFor example, use the PUT command from your local environment to upload the files\nsetup-env.sql and macros.jinja to the stage my_stage:\nPUT file://path/to/setup-env.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\nPUT file://path/to/macros.jinja @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\n\nCopy\nPreview the SQL statements rendered by the template to check for any problems with your Jinja2 code:\nEXECUTE IMMEDIATE FROM @my_stage/scripts/setup-env.sql\n  USING (DEPLOYMENT_TYPE => 'prod') DRY_RUN = TRUE;\n\nCopy\nReturns:\n+----------------------------------+\n| rendered file contents           |\n|----------------------------------|\n| --!jinja2                        |\n| CREATE DATABASE prod1_db;        |\n|   USE DATABASE prod1_db;         |\n|   CREATE TABLE prod1_orders (    |\n|     id NUMBER,                   |\n|     item VARCHAR,                |\n|     quantity NUMBER);            |\n|   CREATE TABLE prod1_customers ( |\n|     id NUMBER,                   |\n|     name VARCHAR);               |\n| CREATE DATABASE prod2_db;        |\n|   USE DATABASE prod2_db;         |\n|   CREATE TABLE prod2_orders (    |\n|     id NUMBER,                   |\n|     item VARCHAR,                |\n|     quantity NUMBER);            |\n|   CREATE TABLE prod2_customers ( |\n|     id NUMBER,                   |\n|     name VARCHAR);               |\n|                                  |\n+----------------------------------+\nExecute the file setup-env.sql:\nEXECUTE IMMEDIATE FROM @my_stage/scripts/setup-env.sql\n  USING (DEPLOYMENT_TYPE => 'prod');\n\nCopy\nCreate a template file containing a macro definition.\nFor example, create a file macros.jinja in your local environment:\nCreate a template file and add the templating directive (--!jinja2) to the top of the file.\nAfter the templating directive, add an import statement to import the macro defined in the\nfile that you created in the previous step.\nFor example, create a file setup-env.sql in your local environment:\nCreate a stage  optional if you already have a stage to which you can upload files.\nFor example, create an internal stage in Snowflake:\nUpload the file to your stage.\nFor example, use the PUT command from your local environment to upload the files\nsetup-env.sql and macros.jinja to the stage my_stage:\nPreview the SQL statements rendered by the template to check for any problems with your Jinja2 code:\nReturns:\nExecute the file setup-env.sql:\nOn this page\nJinja2 templating\nSyntax\nRequired parameters\nOptional parameters\nReturns\nAccess control requirements\nUsage notes\nTemplating usage notes\nTroubleshooting EXECUTE IMMEDIATE FROM errors\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "{%- macro get_environments(deployment_type) -%}\n  {%- if deployment_type == 'prod' -%}\n    {{ \"prod1,prod2\" }}\n  {%- else -%}\n    {{ \"dev,qa,staging\" }}\n  {%- endif -%}\n{%- endmacro -%}",
          "--!jinja2\n{% from \"macros.jinja\" import get_environments %}\n\n{%- set environments = get_environments(DEPLOYMENT_TYPE).split(\",\") -%}\n\n{%- for environment in environments -%}\n  CREATE DATABASE {{ environment }}_db;\n  USE DATABASE {{ environment }}_db;\n  CREATE TABLE {{ environment }}_orders (\n    id NUMBER,\n    item VARCHAR,\n    quantity NUMBER);\n  CREATE TABLE {{ environment }}_customers (\n    id NUMBER,\n    name VARCHAR);\n{% endfor %}",
          "CREATE STAGE my_stage;",
          "PUT file://path/to/setup-env.sql @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;\nPUT file://path/to/macros.jinja @my_stage/scripts/\n  AUTO_COMPRESS=FALSE;",
          "EXECUTE IMMEDIATE FROM @my_stage/scripts/setup-env.sql\n  USING (DEPLOYMENT_TYPE => 'prod') DRY_RUN = TRUE;",
          "+----------------------------------+\n| rendered file contents           |\n|----------------------------------|\n| --!jinja2                        |\n| CREATE DATABASE prod1_db;        |\n|   USE DATABASE prod1_db;         |\n|   CREATE TABLE prod1_orders (    |\n|     id NUMBER,                   |\n|     item VARCHAR,                |\n|     quantity NUMBER);            |\n|   CREATE TABLE prod1_customers ( |\n|     id NUMBER,                   |\n|     name VARCHAR);               |\n| CREATE DATABASE prod2_db;        |\n|   USE DATABASE prod2_db;         |\n|   CREATE TABLE prod2_orders (    |\n|     id NUMBER,                   |\n|     item VARCHAR,                |\n|     quantity NUMBER);            |\n|   CREATE TABLE prod2_customers ( |\n|     id NUMBER,                   |\n|     name VARCHAR);               |\n|                                  |\n+----------------------------------+",
          "EXECUTE IMMEDIATE FROM @my_stage/scripts/setup-env.sql\n  USING (DEPLOYMENT_TYPE => 'prod');"
        ]
      }
    ]
  }
]