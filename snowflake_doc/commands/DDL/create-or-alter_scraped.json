[
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-table",
    "title": "DESCRIBE TABLE",
    "description": "Describes either the columns in a table or the set of stage properties for the table (current values and default values).",
    "syntax": "{ DESCRIBE | DESC } TABLE <name> [ TYPE =  { COLUMNS | STAGE } ]",
    "examples": [
        {
            "code": "CREATE OR REPLACE TABLE desc_example(\n  c1 INT PRIMARY KEY,\n  c2 INT,\n  c3 INT UNIQUE,\n  c4 VARCHAR(30) DEFAULT 'Not applicable' COMMENT 'This column is rarely populated',\n  c5 VARCHAR(100));"
        },
        {
            "code": "DESCRIBE TABLE desc_example;"
        },
        {
            "code": "+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+-------------+----------------+-------------------------+\n| name | type         | kind   | null? | default          | primary key | unique key | check | expression | comment                         | policy name | privacy domain | schema evolution record |\n|------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+-------------+----------------+-------------------------|\n| C1   | NUMBER(38,0) | COLUMN | N     | NULL             | Y           | N          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n| C2   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n| C3   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | Y          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n| C4   | VARCHAR(30)  | COLUMN | Y     | 'Not applicable' | N           | N          | NULL  | NULL       | This column is rarely populated | NULL        | NULL           | NULL                    |\n| C5   | VARCHAR(100) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+-------------+----------------+-------------------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE desc_example(\n  c1 INT PRIMARY KEY,\n  c2 INT,\n  c3 INT UNIQUE,\n  c4 VARCHAR(30) DEFAULT 'Not applicable' COMMENT 'This column is rarely populated',\n  c5 VARCHAR(100) WITH MASKING POLICY email_mask);"
        },
        {
            "code": "+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+---------------------------------+----------------+-------------------------+\n| name | type         | kind   | null? | default          | primary key | unique key | check | expression | comment                         | policy name                     | privacy domain | schema evolution record |\n|------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+---------------------------------+----------------|-------------------------|\n| C1   | NUMBER(38,0) | COLUMN | N     | NULL             | Y           | N          | NULL  | NULL       | NULL                            | NULL                            | NULL           | NULL                    |\n| C2   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | NULL                            | NULL           | NULL                    |\n| C3   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | Y          | NULL  | NULL       | NULL                            | NULL                            | NULL           | NULL                    |\n| C4   | VARCHAR(30)  | COLUMN | Y     | 'Not applicable' | N           | N          | NULL  | NULL       | This column is rarely populated | NULL                            | NULL           | NULL                    |\n| C5   | VARCHAR(100) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | HT_SENSORS.HT_SCHEMA.EMAIL_MASK | NULL           | NULL                    |\n+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+---------------------------------+----------------+-------------------------+"
        },
        {
            "code": "DESCRIBE TABLE desc_example TYPE = STAGE;"
        },
        {
            "code": "+--------------------+--------------------------------+---------------+-----------------+------------------+\n| parent_property    | property                       | property_type | property_value  | property_default |\n|--------------------+--------------------------------+---------------+-----------------+------------------|\n| STAGE_FILE_FORMAT  | TYPE                           | String        | CSV             | CSV              |\n| STAGE_FILE_FORMAT  | RECORD_DELIMITER               | String        | \\n              | \\n               |\n| STAGE_FILE_FORMAT  | FIELD_DELIMITER                | String        | ,               | ,                |\n| STAGE_FILE_FORMAT  | FILE_EXTENSION                 | String        |                 |                  |\n| STAGE_FILE_FORMAT  | SKIP_HEADER                    | Integer       | 0               | 0                |\n..."
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the table to describe. If the identifier contains spaces or special characters, the entire string must\nbe enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "TYPE   =   COLUMNS   |   STAGE",
            "description": "Specifies whether to display the columns for the table or the set of stage properties for the table (current values and default values). Default: TYPE = COLUMNS"
        }
    ],
    "usage_notes": "This command does not show the object parameters for a table. Instead, use SHOW PARAMETERS IN TABLE.\nDESCRIBE TABLE and DESCRIBE VIEW are interchangeable. Both commands return details for the specified table or view; however,\nTYPE = STAGE does not apply for views because views do not have stage properties.\nIf schema evolution is enabled on the table, the output contains a SchemaEvolutionRecord column. This column was introduced with the 2023_08 Bundle (Generally Enabled). For more information, see Table schema evolution.\nThe output includes a policy name column to indicate the masking policy set on the column.\nIf a masking policy is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nThe output includes a privacy domain column to indicate the privacy domain\nset on the column.\nIf a privacy domain is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create",
    "title": "CREATE",
    "description": "Creates a new object of the specified type."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-tables",
    "title": "SHOW TABLES",
    "description": "Lists the tables for which you have access privileges, including dropped tables that are still within the Time Travel retention period\nand, therefore, can be undropped. The command can be used to list tables for the current/specified database or schema, or across your\nentire account.",
    "syntax": "SHOW [ TERSE ] TABLES [ HISTORY ] [ LIKE '<pattern>' ]\n                                  [ IN\n                                        {\n                                          ACCOUNT                                         |\n\n                                          DATABASE                                        |\n                                          DATABASE <database_name>                        |\n\n                                          SCHEMA                                          |\n                                          SCHEMA <schema_name>                            |\n                                          <schema_name>\n\n                                          APPLICATION <application_name>                  |\n                                          APPLICATION PACKAGE <application_package_name>  |\n                                        }\n                                  ]\n                                  [ STARTS WITH '<name_string>' ]\n                                  [ LIMIT <rows> [ FROM '<name_string>' ] ]",
    "examples": [
        {
            "code": "SHOW TERSE TABLES IN tpch_sf1 STARTS WITH 'LINE';"
        },
        {
            "code": "+-------------------------------+----------+-------+-----------------------+-------------+\n| created_on                    | name     | kind  | database_name         | schema_name |\n|-------------------------------+----------+-------+-----------------------+-------------|\n| 2016-07-08 13:41:59.960 -0700 | LINEITEM | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n+-------------------------------+----------+-------+-----------------------+-------------+"
        },
        {
            "code": "SHOW TERSE TABLES LIKE '%PART%' IN tpch_sf1;"
        },
        {
            "code": "+-------------------------------+-----------+-------+-----------------------+-------------+\n| created_on                    | name      | kind  | database_name         | schema_name |\n|-------------------------------+-----------+-------+-----------------------+-------------|\n| 2016-07-08 13:41:59.960 -0700 | JPART     | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | JPARTSUPP | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | PART      | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | PARTSUPP  | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n+-------------------------------+-----------+-------+-----------------------+-------------+"
        },
        {
            "code": "SHOW TERSE TABLES IN tpch_sf1 LIMIT 3 FROM 'J';"
        },
        {
            "code": "+-------------------------------+-----------+-------+-----------------------+-------------+\n| created_on                    | name      | kind  | database_name         | schema_name |\n|-------------------------------+-----------+-------+-----------------------+-------------|\n| 2016-07-08 13:41:59.960 -0700 | JCUSTOMER | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | JLINEITEM | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | JNATION   | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n+-------------------------------+-----------+-------+-----------------------+-------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE test_show_tables_history(c1 NUMBER);\n\nDROP TABLE test_show_tables_history;"
        },
        {
            "code": "SHOW TABLES HISTORY LIKE 'test_show_tables_history';"
        }
    ],
    "parameters": [
        {
            "name": "TERSE",
            "description": "Optionally returns only a subset of the output columns: created_on name kind The kind column value is always TABLE. database_name schema_name Default: No value (all columns are included in the output)"
        },
        {
            "name": "HISTORY",
            "description": "Optionally includes dropped tables that have not yet been purged (i.e. they are still within their respective Time Travel retention\nperiods). If multiple versions of a dropped table exist, the output displays a row for each version. The output also includes an\nadditional dropped_on column, which displays: Date and timestamp (for dropped tables). NULL (for active tables). Default: No value (dropped tables are not included in the output)"
        },
        {
            "name": "LIKE   ' pattern '",
            "description": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters ( % and _ ). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
        },
        {
            "name": "[   IN   ...   ]",
            "description": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database ( db_name ). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in schema1 and table t1 in schema2 , and they are both in scope of the database context you’ve specified (that is, the database\nyou’ve selected is the parent of schema1 and schema2 ), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema ( schema_name ). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema ). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
        },
        {
            "name": "ACCOUNT",
            "description": "Returns records for the entire account."
        },
        {
            "name": "DATABASE ,  .   DATABASE   db_name",
            "description": "Returns records for the current database in use or for a specified database ( db_name ). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in schema1 and table t1 in schema2 , and they are both in scope of the database context you’ve specified (that is, the database\nyou’ve selected is the parent of schema1 and schema2 ), then SHOW TABLES only displays one of the t1 tables."
        },
        {
            "name": "SCHEMA ,  .   SCHEMA   schema_name",
            "description": "Returns records for the current schema in use or a specified schema ( schema_name ). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema ). If no database is in use, specifying SCHEMA has no effect on the output."
        },
        {
            "name": "APPLICATION   application_name ,  .   APPLICATION   PACKAGE   application_package_name",
            "description": "Returns records for the named Snowflake Native App or application package."
        },
        {
            "name": "STARTS   WITH   ' name_string '",
            "description": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive . For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
        },
        {
            "name": "LIMIT   rows   [   FROM   ' name_string '   ]",
            "description": "Optionally limits the maximum number of rows returned, while also enabling “pagination” of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM ' name_string ' subclause effectively serves as a “cursor” for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive . The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM ' name_string ' and STARTS WITH ' name_string ' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM ' name_string ' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH ' name_string ' . For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
        }
    ],
    "usage_notes": "If an account (or database or schema) has a large number of tables, then searching the entire account (or table or schema)\ncan consume a significant amount of compute resources.\nIn the output, results are sorted by database name, schema name, and then table name. This means results for a database\ncan contain tables from multiple schemas and might break pagination. In order for pagination to work as expected, you\nmust execute the SHOW TABLES command for a single schema. You can use the IN SCHEMA schema_name parameter to\nthe SHOW TABLES command. Alternatively, you can use the schema in the current context by executing a USE SCHEMA command\nbefore executing a SHOW TABLES command.\nThe command doesn’t require a running warehouse to execute.\nThe command only returns objects for which the current user’s current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe value for LIMIT rows can’t exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-table-column",
    "title": "ALTER TABLE … ALTER COLUMN",
    "description": "This topic describes how to modify one or more column properties for a table using an ALTER COLUMN clause in a\nALTER TABLE statement.",
    "syntax": "ALTER TABLE <name> { ALTER | MODIFY } [ ( ]\n                                              [ COLUMN ] <col1_name> DROP DEFAULT\n                                            , [ COLUMN ] <col1_name> SET DEFAULT <seq_name>.NEXTVAL\n                                            , [ COLUMN ] <col1_name> { [ SET ] NOT NULL | DROP NOT NULL }\n                                            , [ COLUMN ] <col1_name> [ [ SET DATA ] TYPE ] <type>\n                                            , [ COLUMN ] <col1_name> COMMENT '<string>'\n                                            , [ COLUMN ] <col1_name> UNSET COMMENT\n                                          [ , [ COLUMN ] <col2_name> ... ]\n                                          [ , ... ]\n                                      [ ) ]\n\nALTER TABLE <name> { ALTER | MODIFY } [ COLUMN ] dataGovnPolicyTagAction",
    "examples": [
        {
            "code": "CREATE OR REPLACE TABLE t1 (\n   c1 NUMBER NOT NULL,\n   c2 NUMBER DEFAULT 3,\n   c3 NUMBER DEFAULT seq1.nextval,\n   c4 VARCHAR(20) DEFAULT 'abcde',\n   c5 STRING);\n\nDESC TABLE t1;\n\n+------+-------------------+--------+-------+-------------------------+-------------+------------+-------+------------+---------+\n| name | type              | kind   | null? | default                 | primary key | unique key | check | expression | comment |\n|------+-------------------+--------+-------+-------------------------+-------------+------------+-------+------------+---------|\n| C1   | NUMBER(38,0)      | COLUMN | N     | NULL                    | N           | N          | NULL  | NULL       | NULL    |\n| C2   | NUMBER(38,0)      | COLUMN | Y     | 3                       | N           | N          | NULL  | NULL       | NULL    |\n| C3   | NUMBER(38,0)      | COLUMN | Y     | DB1.PUBLIC.SEQ1.NEXTVAL | N           | N          | NULL  | NULL       | NULL    |\n| C4   | VARCHAR(20)       | COLUMN | Y     | 'abcde'                 | N           | N          | NULL  | NULL       | NULL    |\n| C5   | VARCHAR(16777216) | COLUMN | Y     | NULL                    | N           | N          | NULL  | NULL       | NULL    |\n+------+-------------------+--------+-------+-------------------------+-------------+------------+-------+------------+---------+"
        },
        {
            "code": "ALTER TABLE t1 ALTER COLUMN c1 DROP NOT NULL;\n\nALTER TABLE t1 MODIFY c2 DROP DEFAULT, c3 SET DEFAULT seq5.nextval ;\n\nALTER TABLE t1 ALTER c4 SET DATA TYPE VARCHAR(50), COLUMN c4 DROP DEFAULT;\n\nALTER TABLE t1 ALTER c5 COMMENT '50 character column';\n\nDESC TABLE t1;\n\n+------+-------------------+--------+-------+-------------------------+-------------+------------+-------+------------+---------------------+\n| name | type              | kind   | null? | default                 | primary key | unique key | check | expression | comment             |\n|------+-------------------+--------+-------+-------------------------+-------------+------------+-------+------------+---------------------|\n| C1   | NUMBER(38,0)      | COLUMN | Y     | NULL                    | N           | N          | NULL  | NULL       | NULL                |\n| C2   | NUMBER(38,0)      | COLUMN | Y     | NULL                    | N           | N          | NULL  | NULL       | NULL                |\n| C3   | NUMBER(38,0)      | COLUMN | Y     | DB1.PUBLIC.SEQ5.NEXTVAL | N           | N          | NULL  | NULL       | NULL                |\n| C4   | VARCHAR(50)       | COLUMN | Y     | NULL                    | N           | N          | NULL  | NULL       | NULL                |\n| C5   | VARCHAR(16777216) | COLUMN | Y     | NULL                    | N           | N          | NULL  | NULL       | 50 character column |\n+------+-------------------+--------+-------+-------------------------+-------------+------------+-------+------------+---------------------+"
        },
        {
            "code": "ALTER TABLE t1 ALTER (\n   c1 DROP NOT NULL,\n   c5 COMMENT '50 character column',\n   c4 TYPE VARCHAR(50),\n   c2 DROP DEFAULT,\n   COLUMN c4 DROP DEFAULT,\n   COLUMN c3 SET DEFAULT seq5.nextval\n  );"
        },
        {
            "code": "-- single column\n\nALTER TABLE empl_info MODIFY COLUMN empl_id SET MASKING POLICY mask_empl_id;\n\n-- multiple columns\n\nALTER TABLE empl_info MODIFY\n    COLUMN empl_id SET MASKING POLICY mask_empl_id\n  , COLUMN empl_dob SET MASKING POLICY mask_empl_dob\n;"
        },
        {
            "code": "-- single column\n\nALTER TABLE empl_info modify column empl_id unset masking policy;\n\n-- multiple columns\n\nALTER TABLE empl_info MODIFY\n    COLUMN empl_id UNSET MASKING POLICY\n  , COLUMN empl_dob UNSET MASKING POLICY\n;"
        }
    ],
    "usage_notes": "A single ALTER TABLE statement can be used to modify multiple columns in a table. Each change is specified as a clause consisting of the column\nand column property to modify, separated by commas:\nUse either the ALTER or MODIFY keyword to initiate the list of clauses (i.e. columns/properties to modify) in the statement.\nParentheses can be used for grouping the clauses, but are not required.\nThe COLUMN keyword can be specified in each clause, but is not required.\nThe clauses can be specified in any order.\nWhen setting a column to NOT NULL, if the column contains NULL values, an error is returned and no changes are applied to the column.\nColumns that use semi-structured data types (ARRAY, OBJECT, and VARIANT) cannot be set to NOT NULL, except when the table is empty. Setting these columns to NOT NULL when the table contains rows is not supported and results in an error.\nTo change the default sequence for a column, the column must already have a default sequence. You cannot use the command\nALTER TABLE ... SET DEFAULT <seq_name> to add a sequence to a column that does not already have a sequence.\nIf you alter a table to add a column with a DEFAULT value, then you cannot drop the default value for that column.\nFor example, in the following sequence of statements, the last ALTER TABLE ... ALTER COLUMN statement causes an error:\nThis restriction prevents inconsistency between values in rows inserted before the column was added and\nrows inserted after the column was added. If the default were dropped, then the column would contain:\nA NULL value for rows inserted before the column was added.\nThe default value for rows inserted after the column was added.\nDropping the default column value from any clone of the table is also prohibited.\nWhen setting the TYPE for a column, the specified type (i.e. type) must be\nNUMBER or a text data type (VARCHAR,\nSTRING, TEXT, etc.).\nFor the NUMBER data type, TYPE can be used to:\nIncrease the precision of the specified number column.\nDecrease the precision of the specified number column if the new precision is sufficient to hold\nall data values currently in the column.\nFor text data types, TYPE can be used only to increase the length of the column.\nIf the precision of a column is decreased below the maximum precision of any column data retained in Time Travel, you cannot restore the\ntable without first increasing the precision.\nFor masking policies:\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nRegarding metadata (for example, the COMMENT field):\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter",
    "title": "ALTER",
    "description": "Modifies the metadata of an account-level or database object, or the parameters for a session."
},
{
    "url": "https://docs.snowflake.com/user-guide/contacting-support",
    "title": "Contacting Snowflake Support",
    "description": "To submit a case to Snowflake Support, you can either:"
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-database.html#label-create-or-alter-database-syntax",
    "title": "CREATE DATABASE",
    "description": "Creates a new database in the system.",
    "syntax": "CREATE [ OR REPLACE ] [ TRANSIENT ] DATABASE [ IF NOT EXISTS ] <name>\n    [ CLONE <source_schema>\n        [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n        [ IGNORE TABLES WITH INSUFFICIENT DATA RETENTION ]\n        [ IGNORE HYBRID TABLES ] ]\n    [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n    [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n    [ EXTERNAL_VOLUME = <external_volume_name> ]\n    [ CATALOG = <catalog_integration_name> ]\n    [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n    [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n    [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n    [ COMMENT = '<string_literal>' ]\n    [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>' ]\n    [ CATALOG_SYNC_NAMESPACE_MODE = { NEST | FLATTEN } ]\n    [ CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER = '<string_literal>' ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n    [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nCREATE DATABASE <name> FROM LISTING '<listing_global_name>'\n\nCREATE DATABASE <name> FROM SHARE <provider_account>.<share_name>\n\nCREATE DATABASE <name>\n    AS REPLICA OF <account_identifier>.<primary_db_name>\n    [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]",
    "examples": [
        {
            "code": "CREATE DATABASE mytestdb;\n\nCREATE DATABASE mytestdb2 DATA_RETENTION_TIME_IN_DAYS = 10;\n\nSHOW DATABASES LIKE 'my%';\n\n+---------------------------------+------------+------------+------------+--------+----------+---------+---------+----------------+\n| created_on                      | name       | is_default | is_current | origin | owner    | comment | options | retention_time |\n|---------------------------------+------------+------------+------------+--------+----------+---------+---------+----------------|\n| Tue, 17 Mar 2016 16:57:04 -0700 | MYTESTDB   | N          | N          |        | PUBLIC   |         |         | 1              |\n| Tue, 17 Mar 2016 17:06:32 -0700 | MYTESTDB2  | N          | N          |        | PUBLIC   |         |         | 10             |\n+---------------------------------+------------+------------+------------+--------+----------+---------+---------+----------------+"
        },
        {
            "code": "CREATE TRANSIENT DATABASE mytransientdb;\n\nSHOW DATABASES LIKE 'my%';\n\n+---------------------------------+---------------+------------+------------+--------+----------+---------+-----------+----------------+\n| created_on                      | name          | is_default | is_current | origin | owner    | comment | options   | retention_time |\n|---------------------------------+---------------+------------+------------+--------+----------+---------+-----------+----------------|\n| Tue, 17 Mar 2016 16:57:04 -0700 | MYTESTDB      | N          | N          |        | PUBLIC   |         |           | 1              |\n| Tue, 17 Mar 2016 17:06:32 -0700 | MYTESTDB2     | N          | N          |        | PUBLIC   |         |           | 10             |\n| Tue, 17 Mar 2015 17:07:51 -0700 | MYTRANSIENTDB | N          | N          |        | PUBLIC   |         | TRANSIENT | 1              |\n+---------------------------------+---------------+------------+------------+--------+----------+---------+-----------+----------------+"
        },
        {
            "code": "CREATE DATABASE snow_sales FROM SHARE ab67890.sales_s;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the database; must be unique for your account. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements . Important As a best practice for Database Replication and Failover , we recommend giving each\nsecondary database the same name as its primary database. This practice supports referencing fully-qualified objects\n(i.e. '<db>.<schema>.<object>' ) by other objects in the same database, such as querying a fully-qualified table name in a view. If a secondary database has a different name from the primary database, then these object references would break in the secondary database."
        },
        {
            "name": "provider_account . share_name",
            "description": "Specifies the identifier of the share from which to create the database. As documented, the name of the\nshare must be fully-qualified with the name of the account providing the share."
        },
        {
            "name": "AS   REPLICA   OF   account_identifier . primary_db_name",
            "description": "Specifies the identifier for a primary database from which to create a replica (i.e. a secondary database). If the identifier contains spaces,\nspecial characters, or mixed-case characters, the entire string must be enclosed in double quotes. Requires the account identifier and name of the primary database. Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes . Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database. Note As a best practice for Database Replication and Failover, we recommend setting the optional parameter DATA_RETENTION_TIME_IN_DAYS to the same value on the secondary database as on the\nprimary database."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes ."
        },
        {
            "name": "primary_db_name",
            "description": "Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database."
        },
        {
            "name": "' listing_global_name '",
            "description": "Specifies the global name of the listing from which to create the database, which must meet the following requirements: Can’t be a paid listing. Listing terms, if not of type OFFLINE , must have been accepted using Snowsight. Listing data products must be available locally in the current region. Whether a listing is available in the local region can be determined by viewing the is_ready_for_import column\nof DESCRIBE AVAILABLE LISTING ."
        },
        {
            "name": "provider_account . share_name",
            "description": "Specifies the identifier of the share from which to create the database. As documented, the name of the\nshare must be fully-qualified with the name of the account providing the share."
        },
        {
            "name": "AS   REPLICA   OF   account_identifier . primary_db_name",
            "description": "Specifies the identifier for a primary database from which to create a replica (i.e. a secondary database). If the identifier contains spaces,\nspecial characters, or mixed-case characters, the entire string must be enclosed in double quotes. Requires the account identifier and name of the primary database. Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes . Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database. Note As a best practice for Database Replication and Failover, we recommend setting the optional parameter DATA_RETENTION_TIME_IN_DAYS to the same value on the secondary database as on the\nprimary database."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes ."
        },
        {
            "name": "primary_db_name",
            "description": "Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database."
        },
        {
            "name": "' listing_global_name '",
            "description": "Specifies the global name of the listing from which to create the database, which must meet the following requirements: Can’t be a paid listing. Listing terms, if not of type OFFLINE , must have been accepted using Snowsight. Listing data products must be available locally in the current region. Whether a listing is available in the local region can be determined by viewing the is_ready_for_import column\nof DESCRIBE AVAILABLE LISTING ."
        },
        {
            "name": "TRANSIENT",
            "description": "Specifies a database as transient. Transient databases do not have a Fail-safe period so they do not incur additional storage costs once\nthey leave Time Travel; however, this means they are also not protected by Fail-safe in the event of a data loss. For more information, see Understanding and viewing Fail-safe . In addition, by definition, all schemas (and consequently all tables) created in a transient database are transient. For more information about\ntransient tables, see CREATE TABLE . Default: No value (i.e. database is permanent)"
        },
        {
            "name": "CLONE   source_db",
            "description": "Specifies to create a clone of the specified source database. For more details about cloning a database, see CREATE <object> … CLONE ."
        },
        {
            "name": "AT   |   BEFORE   (   TIMESTAMP   =>   timestamp   |   OFFSET   =>   time_difference   |   STATEMENT   =>   id   )",
            "description": "When cloning a database, the AT | BEFORE clause specifies to use Time Travel to clone the database at or\nbefore a specific point in the past. If the specified Time Travel time is at or before the point in time when the database was created,\nthe cloning operation fails with an error."
        },
        {
            "name": "IGNORE   TABLES   WITH   INSUFFICIENT   DATA   RETENTION",
            "description": "Ignore tables that no longer have historical data available in Time Travel to clone. If the time in the past specified in the\nAT | BEFORE clause is beyond the data retention period for any child table in a database or schema, skip the cloning operation\nfor the child table. For more information, see Child Objects and Data Retention Time ."
        },
        {
            "name": "IGNORE   HYBRID   TABLES",
            "description": "Ignore hybrid tables, which will not be cloned. Use this option to clone a database that contains hybrid tables.\nThe cloned database includes other objects but skips hybrid tables. If you don’t use this option and your database contains one or more hybrid tables, the command ignores hybrid tables silently. However, the error handling for databases that contain hybrid tables will change in an upcoming release; therefore, you may want to add this parameter to your commands preemptively."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the\ndefault Time Travel retention time for all schemas created in the database. For more details, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent databases 0 or 1 for transient databases Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the account level) Note A value of 0 effectively disables Time Travel for the database."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the\ndatabase to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "EXTERNAL_VOLUME   =   external_volume_name",
            "description": "Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME ."
        },
        {
            "name": "CATALOG   =   catalog_integration_name",
            "description": "Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG ."
        },
        {
            "name": "REPLACE_INVALID_CHARACTERS   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for all schemas and tables added to the database. The\ndefault can be overridden at the schema and individual table level. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "STORAGE_SERIALIZATION_POLICY   =   {   COMPATIBLE   |   OPTIMIZED   }",
            "description": "Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the database. Default: No value"
        },
        {
            "name": "CATALOG_SYNC   =   ' snowflake_open_catalog_integration_name '",
            "description": "Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the database with an external catalog in your Snowflake Open Catalog\naccount. For more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value"
        },
        {
            "name": "CATALOG_SYNC_NAMESPACE_MODE   =   {   NEST   |   FLATTEN   }",
            "description": "Specifies the catalog sync namespace mode for Snowflake-managed Iceberg tables in the database that you sync with\nSnowflake Open Catalog. This property specifies whether Snowflake syncs the table to Open Catalog with one or two parent namespaces. It\nonly applies if you’re setting the CATALOG_SYNC parameter. After you create the database, you can’t alter this property. NEST : Snowflake syncs two parent namespaces with the table. For example, suppose you have a db2.public.table1 Iceberg table registered in Snowflake. You want to sync this table, along with its\ntwo parent namespaces, to the catalog2 external catalog in Open Catalog. To sync the table with its two parent namespaces, use the\ndefault for CATALOG_SYNC_NAMESPACE_MODE ( NEST ). If you don’t specify the CATALOG_SYNC_NAMESPACE_MODE property, the default for\nthis property is applied, which is NEST . Because you’re using the default for CATALOG_SYNC_NAMESPACE_MODE , you don’t need to specify CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER . As a result, Snowflake syncs the table to Open Catalog with the following fully qualified\nname: catalog2.db2.public.table1 . FLATTEN : Snowflake syncs one parent namespace with the table, which contains the delimiter you set by using the CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER property. Important If your third-party query engine can only query tables located up to the second namespace level in a catalog, you must set the CATALOG_SYNC_NAMESPACE_MODE property to FLATTEN . Otherwise, Snowflake will sync Snowflake-managed Iceberg tables to the\nthird namespace level in Open Catalog and you can’t query the table. For example, suppose that you have a db1.public.table1 Iceberg table registered in Snowflake. You want to sync this table and one parent\nnamespace named db1-public with the catalog1 external catalog in Open Catalog, so that the table is located at the second namespace level in Open Catalog. To sync the table with the db1-public parent namespace, set CATALOG_SYNC_NAMESPACE_MODE to FLATTEN and specify a hyphen ( - ) as the value\nfor CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER . As a result, Snowflake syncs this table to Open Catalog with the following\nfully-qualified name: catalog1.db1-public.table1 . Default: NEST"
        },
        {
            "name": "CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER   =   ' string_literal '",
            "description": "Specifies a delimiter, which Snowflake inserts in the flattened namespace that results when Snowflake syncs a Snowflake-managed Iceberg\ntable to Snowflake Open Catalog with one parent namespace. This delimiter property only applies when you set the CATALOG_SYNC_NAMESPACE_MODE property to FLATTEN . Snowflake inserts this delimiter to avoid conflicts that could\narise from flattening parent namespaces for different tables. After you create the database, you can’t alter this property. For example, suppose you want to sync the customer.data.table1 and custom.erdata.table1 Snowflake-managed Iceberg tables to the catalog1 external catalog in Open Catalog. By setting the CATALOG_SYNC_NAMESPACE_MODE property set to FLATTEN and specifying a hyphen ( - ) for the\ndelimiter, Snowflake syncs these tables with Open Catalog with the following fully qualified names: catalog1.customer-data.table1 catalog1.custom-erdata.table1 If you set the CATALOG_SYNC_NAMESPACE_MODE property to FLATTEN , a non-empty delimiter value is required. However, if you set the CATALOG_SYNC_NAMESPACE_MODE property to NEST , this delimiter property doesn’t apply and the configured value will be ignored. Valid characters: 0-9 , A-Z , a-z , _ , $ , -"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-tag.html#label-create-or-alter-tag-syntax",
    "title": "CREATE TAG",
    "description": "Creates a new tag or replaces an existing tag in the system.",
    "syntax": "CREATE [ OR REPLACE ] TAG [ IF NOT EXISTS ] <name>\n    [ ALLOWED_VALUES '<val_1>' [ , '<val_2>' [ , ... ] ] ]\n    [ PROPAGATE = { ON_DEPENDENCY_AND_DATA_MOVEMENT | ON_DEPENDENCY | ON_DATA_MOVEMENT }\n      [ ON_CONFLICT = { '<string>' | ALLOWED_VALUES_SEQUENCE } ] ]\n    [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE TAG cost_center COMMENT = 'cost_center tag';"
        },
        {
            "code": "CREATE OR ALTER TAG cost_center ALLOWED_VALUES 'finance', 'engineering', 'sales';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the tag. Assign the tag string value on an object using either a CREATE <object> statement or an ALTER <object> statement. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. “My object”). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "ALLOWED_VALUES   ' val_1 '   [   ,   ' val_2 '   [   ,   ...   ]   ]",
            "description": "Specifies a comma-separated list of the possible string values that can be assigned to the tag when the tag is set on an object using the corresponding CREATE <object> or ALTER <object> command. Must come before all other parameters to work. The maximum number of tag values in this list is 300. If a tag is configured to automatically propagate to target objects, the order of values in the allowed list can affect how conflicts are\nresolved. For more information, see Tag propagation conflicts . Default: NULL (all string values are allowed, including an empty string value (that is, ' ' ))."
        },
        {
            "name": "PROPAGATE   =   {   ON_DEPENDENCY_AND_DATA_MOVEMENT   |   ON_DEPENDENCY   |   ON_DATA_MOVEMENT   }",
            "description": "Enterprise Edition Feature This parameter requires Enterprise Edition or higher. To inquire about upgrading, please contact Snowflake Support . Specifies that the tag will be automatically propagated from source objects to target\nobjects. You can configure the tag to propagate when there is an object dependency , data movement , or both. Propagates the tag when there is an object dependency or data movement. Propagates the tag for object dependencies, but not for data movement. Propagates the tag when there is data movement, but not for object dependencies."
        },
        {
            "name": "ON_CONFLICT   =   {   ' string '   |   ALLOWED_VALUES_SEQUENCE   }",
            "description": "Enterprise Edition Feature This parameter requires Enterprise Edition or higher. To inquire about upgrading, please contact Snowflake Support . Specifies what happens when there is a conflict between the values of propagated tags . If you don’t set this parameter and there is a conflict, the value of the tag is set to the string CONFLICT . Possible values are: When there is a conflict, the value of the tag is set to the specified string. The order of the values in the ALLOWED_VALUES property of the tag determines which value is used when there is a conflict. For example,\nsuppose you created a tag with the following statement: If there is a conflict, then the value of my_tag will be blue because it comes before red in the allowed values list. Default: Set the value of the tag to CONFLICT ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the tag. Default: No value"
        },
        {
            "name": "ON_DEPENDENCY_AND_DATA_MOVEMENT",
            "description": "Propagates the tag when there is an object dependency or data movement."
        },
        {
            "name": "ON_DEPENDENCY",
            "description": "Propagates the tag for object dependencies, but not for data movement."
        },
        {
            "name": "ON_DATA_MOVEMENT",
            "description": "Propagates the tag when there is data movement, but not for object dependencies."
        },
        {
            "name": "' string '",
            "description": "When there is a conflict, the value of the tag is set to the specified string."
        },
        {
            "name": "ALLOWED_VALUES_SEQUENCE",
            "description": "The order of the values in the ALLOWED_VALUES property of the tag determines which value is used when there is a conflict. For example,\nsuppose you created a tag with the following statement: If there is a conflict, then the value of my_tag will be blue because it comes before red in the allowed values list."
        }
    ],
    "usage_notes": "Snowflake limits the number of tags in an account to 10,000.\nFor more information about how tags can be associated with Snowflake objects, see Introduction to object tagging.\nFor more information about tag DDL authorization, see required privileges.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-warehouse.html#label-create-or-alter-warehouse-syntax",
    "title": "CREATE WAREHOUSE",
    "description": "Creates a new virtual warehouse in the system.",
    "syntax": "CREATE [ OR REPLACE ] WAREHOUSE [ IF NOT EXISTS ] <name>\n       [ [ WITH ] objectProperties ]\n       [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n       [ objectParams ]\n\nobjectProperties ::=\n  WAREHOUSE_TYPE = { STANDARD | 'SNOWPARK-OPTIMIZED' }\n  WAREHOUSE_SIZE = { XSMALL | SMALL | MEDIUM | LARGE | XLARGE | XXLARGE | XXXLARGE | X4LARGE | X5LARGE | X6LARGE }\n  RESOURCE_CONSTRAINT = { STANDARD_GEN_1 | STANDARD_GEN_2 | MEMORY_1X | MEMORY_1X_x86 | MEMORY_16X | MEMORY_16X_x86 | MEMORY_64X | MEMORY_64X_x86 }\n  MAX_CLUSTER_COUNT = <num>\n  MIN_CLUSTER_COUNT = <num>\n  SCALING_POLICY = { STANDARD | ECONOMY }\n  AUTO_SUSPEND = { <num> | NULL }\n  AUTO_RESUME = { TRUE | FALSE }\n  INITIALLY_SUSPENDED = { TRUE | FALSE }\n  RESOURCE_MONITOR = <monitor_name>\n  COMMENT = '<string_literal>'\n  ENABLE_QUERY_ACCELERATION = { TRUE | FALSE }\n  QUERY_ACCELERATION_MAX_SCALE_FACTOR = <num>\n\nobjectParams ::=\n  MAX_CONCURRENCY_LEVEL = <num>\n  STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = <num>\n  STATEMENT_TIMEOUT_IN_SECONDS = <num>",
    "examples": [
        {
            "code": "CREATE OR REPLACE WAREHOUSE my_wh WITH WAREHOUSE_SIZE='X-LARGE';"
        },
        {
            "code": "CREATE OR REPLACE WAREHOUSE my_wh WAREHOUSE_SIZE=LARGE INITIALLY_SUSPENDED=TRUE;"
        },
        {
            "code": "CREATE WAREHOUSE so_warehouse WITH\n  WAREHOUSE_TYPE = 'SNOWPARK-OPTIMIZED'\n  WAREHOUSE_SIZE = xlarge\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_x86';"
        },
        {
            "code": "CREATE OR ALTER WAREHOUSE so_warehouse\n  WAREHOUSE_TYPE = 'SNOWPARK_OPTIMIZED'\n  WAREHOUSE_SIZE = 'X-LARGE'\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_X86'\n  AUTO_RESUME = TRUE\n  COMMENT = 'Snowpark warehouse for ingestion';"
        },
        {
            "code": "CREATE OR ALTER WAREHOUSE so_warehouse\n  WAREHOUSE_TYPE = 'SNOWPARK-OPTIMIZED'\n  WAREHOUSE_SIZE = 'X-LARGE'\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_X86'\n  AUTO_RESUME = FALSE\n  COMMENT = 'Snowpark warehouse for ingestion (disabled for auto-resume)';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the virtual warehouse; must be unique for your account. In addition, the identifier must start with an alphabetic character and can’t contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "MAX_CONCURRENCY_LEVEL   =   num",
            "description": "Object parameter that specifies the concurrency level for SQL statements (i.e. queries and DML) executed by a warehouse cluster. For a detailed description of this parameter, see MAX_CONCURRENCY_LEVEL ."
        },
        {
            "name": "STATEMENT_QUEUED_TIMEOUT_IN_SECONDS   =   num",
            "description": "Object parameter that specifies the time, in seconds, a SQL statement (query, DDL, DML, etc.) can be queued on a warehouse before it is\ncanceled by the system. For a detailed description of this parameter, see STATEMENT_QUEUED_TIMEOUT_IN_SECONDS ."
        },
        {
            "name": "STATEMENT_TIMEOUT_IN_SECONDS   =   num",
            "description": "Object parameter that specifies the time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For a detailed description of this parameter, see STATEMENT_TIMEOUT_IN_SECONDS ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-view.html#label-create-or-alter-view-syntax",
    "title": "CREATE VIEW",
    "description": "Creates a new view in the current/specified schema, based on a query of one or more existing tables (or any other valid query expression).",
    "syntax": "CREATE [ OR REPLACE ] [ SECURE ] [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE } ] [ RECURSIVE ] VIEW [ IF NOT EXISTS ] <name>\n  [ ( <column_list> ) ]\n  [ <col1> [ WITH ] MASKING POLICY <policy_name> [ USING ( <col1> , <cond_col1> , ... ) ]\n           [ WITH ] PROJECTION POLICY <policy_name>\n           [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ , <col2> [ ... ] ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] JOIN POLICY <policy_name> [ ALLOWED JOIN KEYS ( <col_name> [ , ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  AS <select_statement>",
    "examples": [
        {
            "code": "CREATE VIEW myview COMMENT='Test view' AS SELECT col1, col2 FROM mytable;\n\nSHOW VIEWS;\n\n+---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------+\n| created_on                      | name              | reserved | database_name | schema_name | owner    | comment   | text                                                                     |\n|---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------|\n| Thu, 19 Jan 2017 15:00:37 -0800 | MYVIEW            |          | MYTEST1       | PUBLIC      | SYSADMIN | Test view | CREATE VIEW myview COMMENT='Test view' AS SELECT col1, col2 FROM mytable |\n+---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------+"
        },
        {
            "code": "CREATE OR REPLACE SECURE VIEW myview COMMENT='Test secure view' AS SELECT col1, col2 FROM mytable;\n\nSELECT is_secure FROM information_schema.views WHERE table_name = 'MYVIEW';"
        },
        {
            "code": "CREATE OR REPLACE TABLE employees (title VARCHAR, employee_ID INTEGER, manager_ID INTEGER);"
        },
        {
            "code": "INSERT INTO employees (title, employee_ID, manager_ID) VALUES\n    ('President', 1, NULL),  -- The President has no manager.\n        ('Vice President Engineering', 10, 1),\n            ('Programmer', 100, 10),\n            ('QA Engineer', 101, 10),\n        ('Vice President HR', 20, 1),\n            ('Health Insurance Analyst', 200, 20);"
        },
        {
            "code": "CREATE VIEW employee_hierarchy (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n   WITH RECURSIVE employee_hierarchy_cte (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n      -- Start at the top of the hierarchy ...\n      SELECT title, employee_ID, manager_ID, NULL AS \"MGR_EMP_ID (SHOULD BE SAME)\", 'President' AS \"MGR TITLE\"\n        FROM employees\n        WHERE title = 'President'\n      UNION ALL\n      -- ... and work our way down one level at a time.\n      SELECT employees.title, \n             employees.employee_ID, \n             employees.manager_ID, \n             employee_hierarchy_cte.employee_id AS \"MGR_EMP_ID (SHOULD BE SAME)\", \n             employee_hierarchy_cte.title AS \"MGR TITLE\"\n        FROM employees INNER JOIN employee_hierarchy_cte\n       WHERE employee_hierarchy_cte.employee_ID = employees.manager_ID\n   )\n   SELECT * \n      FROM employee_hierarchy_cte\n);"
        },
        {
            "code": "SELECT * \n    FROM employee_hierarchy \n    ORDER BY employee_ID;\n+----------------------------+-------------+------------+-----------------------------+----------------------------+\n| TITLE                      | EMPLOYEE_ID | MANAGER_ID | MGR_EMP_ID (SHOULD BE SAME) | MGR TITLE                  |\n|----------------------------+-------------+------------+-----------------------------+----------------------------|\n| President                  |           1 |       NULL |                        NULL | President                  |\n| Vice President Engineering |          10 |          1 |                           1 | President                  |\n| Vice President HR          |          20 |          1 |                           1 | President                  |\n| Programmer                 |         100 |         10 |                          10 | Vice President Engineering |\n| QA Engineer                |         101 |         10 |                          10 | Vice President Engineering |\n| Health Insurance Analyst   |         200 |         20 |                          20 | Vice President HR          |\n+----------------------------+-------------+------------+-----------------------------+----------------------------+"
        },
        {
            "code": "CREATE RECURSIVE VIEW employee_hierarchy_02 (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n      -- Start at the top of the hierarchy ...\n      SELECT title, employee_ID, manager_ID, NULL AS \"MGR_EMP_ID (SHOULD BE SAME)\", 'President' AS \"MGR TITLE\"\n        FROM employees\n        WHERE title = 'President'\n      UNION ALL\n      -- ... and work our way down one level at a time.\n      SELECT employees.title, \n             employees.employee_ID, \n             employees.manager_ID, \n             employee_hierarchy_02.employee_id AS \"MGR_EMP_ID (SHOULD BE SAME)\", \n             employee_hierarchy_02.title AS \"MGR TITLE\"\n        FROM employees INNER JOIN employee_hierarchy_02\n        WHERE employee_hierarchy_02.employee_ID = employees.manager_ID\n);"
        },
        {
            "code": "SELECT * \n    FROM employee_hierarchy_02 \n    ORDER BY employee_ID;\n+----------------------------+-------------+------------+-----------------------------+----------------------------+\n| TITLE                      | EMPLOYEE_ID | MANAGER_ID | MGR_EMP_ID (SHOULD BE SAME) | MGR TITLE                  |\n|----------------------------+-------------+------------+-----------------------------+----------------------------|\n| President                  |           1 |       NULL |                        NULL | President                  |\n| Vice President Engineering |          10 |          1 |                           1 | President                  |\n| Vice President HR          |          20 |          1 |                           1 | President                  |\n| Programmer                 |         100 |         10 |                          10 | Vice President Engineering |\n| QA Engineer                |         101 |         10 |                          10 | Vice President Engineering |\n| Health Insurance Analyst   |         200 |         20 |                          20 | Vice President HR          |\n+----------------------------+-------------+------------+-----------------------------+----------------------------+"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(a INT);"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one)\n  AS SELECT a FROM my_table;"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one)\n  COMMENT = 'fff'\n  CHANGE_TRACKING = true\n  AS SELECT a FROM my_table;"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one COMMENT 'bar')\n  COMMENT = 'foo'\n  AS SELECT a FROM my_table;"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one COMMENT 'bar')\n  CHANGE_TRACKING = true\n  AS SELECT a FROM my_table;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the view; must be unique for the schema in which the view is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "select_statement",
            "description": "Specifies the query used to create the view. Can be on one or more source tables or any other valid SELECT statement. This\nquery serves as the text/definition for the view and is displayed in the SHOW VIEWS output and the VIEWS Information Schema view."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the view is secure. For more information about secure views, see Working with Secure Views . Default: No value (view is not secure)"
        },
        {
            "name": "{   [   {   LOCAL   |   GLOBAL   }   ]   TEMP   |   TEMPORARY   |   VOLATILE   }",
            "description": "Specifies that the view persists only for the duration of the session that you created it in. A\ntemporary view and all its contents are dropped at the end of the session. The synonyms and abbreviations for TEMPORARY (e.g. GLOBAL TEMPORARY ) are provided for compatibility with other databases\n(e.g. to prevent errors when migrating CREATE VIEW statements). Views created with any of these keywords appear and behave identically to\na view created with the TEMPORARY keyword. Default: No value. If a view is not declared as TEMPORARY , the view is permanent. If you want to avoid unexpected conflicts, avoid naming temporary views after views that already exist in the schema. If you created a temporary view with the same name as another view in the schema, all queries and operations used on the view only affect\nthe temporary view in the session, until you drop the temporary view. If you drop the view, you drop the temporary view, and not the view\nthat already exists in the schema."
        },
        {
            "name": "RECURSIVE",
            "description": "Specifies that the view can refer to itself using recursive syntax without necessarily using a CTE (common table\nexpression). For more information about recursive views in general, and the RECURSIVE keyword in particular,\nsee Recursive Views (Non-materialized Views Only) and the recursive view examples below. Default: No value (view is not recursive, or is recursive only by using a CTE)"
        },
        {
            "name": "column_list",
            "description": "If you want to change the name of a column or add a comment to a column in the new view,\ninclude a column list that specifies the column names and (if needed) comments about\nthe columns. (You do not need to specify the data types of the columns.) If any of the columns in the view are based on expressions (not just simple column names), then you must supply\na column name for each column in the view. For example, the column names are required in the following case: You can specify an optional comment for each column. For example: Comments are particularly helpful when column names are cryptic. To view comments, use DESCRIBE VIEW ."
        },
        {
            "name": "MASKING   POLICY   =   policy_name",
            "description": "Specifies the masking policy to set on a column."
        },
        {
            "name": "USING   (   col_name   ,   cond_col_1   ...   )",
            "description": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal masking policy ."
        },
        {
            "name": "PROJECTION   POLICY   policy_name",
            "description": "Specifies the projection policy to set on a column."
        },
        {
            "name": "CHANGE_TRACKING   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to enable change tracking on the view. TRUE enables change tracking on the view. This setting adds a pair of hidden columns to the source table and begins\nstoring change tracking metadata in the columns. These columns consume a small amount of storage. The change-tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the view."
        },
        {
            "name": "COPY   GRANTS",
            "description": "Retains the access permissions from the original view when a new view is created using the OR REPLACE clause. The parameter copies all privileges, except OWNERSHIP, from the existing view to the new view. The new view does not inherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE VIEW statement owns\nthe new view. If the parameter is not included in the CREATE VIEW statement, then the new view does not inherit any explicit access\nprivileges granted on the original view but does inherit any future grants defined for the object type in the schema. Note that the operation to copy grants occurs atomically with the CREATE VIEW statement (i.e. within the same transaction). Default: No value (grants are not copied)"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the view. Default: No value"
        },
        {
            "name": "ROW   ACCESS   POLICY   policy_name   ON   (   col_name   [   ,   col_name   ...   ]   )",
            "description": "Specifies the row access policy to set on a view."
        },
        {
            "name": "AGGREGATION   POLICY   policy_name   [   ENTITY   KEY   (   col_name   [   ,   col_name   ...   ]   )   ]",
            "description": "Specifies the aggregation policy to set on a view. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the view. For more information, see Implementing entity-level privacy with aggregation policies ."
        },
        {
            "name": "JOIN   POLICY   policy_name   [   ALLOWED   JOIN   KEYS   (   col_name   [   ,   ...   ]   )   ]",
            "description": "Specifies the join policy to set on a view. Use the optional ALLOWED JOIN KEYS parameter to define which columns are allowed to be used as joining columns when\nthis policy is in effect. For more information, see Join policies . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/parameters",
    "title": "Parameters",
    "description": "Snowflake provides parameters that let you control the behavior of your account, individual user sessions, and objects. All the parameters have default values, which can be set and then overridden at\ndifferent levels depending on the parameter type (Account, Session, or Object).",
    "examples": [
        {
            "code": "SHOW PARAMETERS;"
        },
        {
            "code": "SHOW PARAMETERS IN DATABASE mydb;\n\nSHOW PARAMETERS IN WAREHOUSE mywh;"
        },
        {
            "code": "SHOW PARAMETERS IN ACCOUNT;"
        },
        {
            "code": "SHOW PARAMETERS LIKE '%time%';"
        },
        {
            "code": "SHOW PARAMETERS LIKE 'time%' IN ACCOUNT;"
        },
        {
            "code": "ALTER ACCOUNT SET CORTEX_MODELS_ALLOWLIST = 'All';"
        },
        {
            "code": "ALTER ACCOUNT SET CORTEX_MODELS_ALLOWLIST = 'mistral-large2,llama3.1-70b';"
        },
        {
            "code": "ALTER ACCOUNT SET CORTEX_MODELS_ALLOWLIST = 'None';"
        },
        {
            "code": "CREATE TABLE test(c1 INTEGER, c2 STRING, c3 STRING COLLATE 'en-cs');\n\nCREATE TABLE test(c1 INTEGER, c2 STRING COLLATE 'en-ci', c3 STRING COLLATE 'en-cs');"
        },
        {
            "code": "# __________ minute (0-59)\n# | ________ hour (0-23)\n# | | ______ day of month (1-31, or L)\n# | | | ____ month (1-12, JAN-DEC)\n# | | | | __ day of week (0-6, SUN-SAT, or L)\n# | | | | |\n# | | | | |\n  * * * * *"
        },
        {
            "code": "{\n  \"certificate\": \"\",\n  \"issuer\": \"\",\n  \"ssoUrl\": \"\",\n  \"type\"  : \"\",\n  \"label\" : \"\"\n}"
        },
        {
            "code": "Shared view consumer simulation requires that the executing role owns the view."
        },
        {
            "code": "alter session set TIMESTAMP_DAY_IS_ALWAYS_24H = true;\n\n-- With DST beginning on 2018-03-11 at 2 AM, America/Los_Angeles time zone\nselect dateadd(day, 1, '2018-03-10 09:00:00'::TIMESTAMP_LTZ), dateadd(day, 1, '2018-11-03 09:00:00'::TIMESTAMP_LTZ);\n\n+-------------------------------------------------------+-------------------------------------------------------+\n| DATEADD(DAY, 1, '2018-03-10 09:00:00'::TIMESTAMP_LTZ) | DATEADD(DAY, 1, '2018-11-03 09:00:00'::TIMESTAMP_LTZ) |\n|-------------------------------------------------------+-------------------------------------------------------|\n| 2018-03-11 10:00:00.000 -0700                         | 2018-11-04 08:00:00.000 -0800                         |\n+-------------------------------------------------------+-------------------------------------------------------+\n\nalter session set TIMESTAMP_DAY_IS_ALWAYS_24H = false;\n\nselect dateadd(day, 1, '2018-03-10 09:00:00'::TIMESTAMP_LTZ), dateadd(day, 1, '2018-11-03 09:00:00'::TIMESTAMP_LTZ);\n\n+-------------------------------------------------------+-------------------------------------------------------+\n| DATEADD(DAY, 1, '2018-03-10 09:00:00'::TIMESTAMP_LTZ) | DATEADD(DAY, 1, '2018-11-03 09:00:00'::TIMESTAMP_LTZ) |\n|-------------------------------------------------------+-------------------------------------------------------|\n| 2018-03-11 09:00:00.000 -0700                         | 2018-11-04 09:00:00.000 -0800                         |\n+-------------------------------------------------------+-------------------------------------------------------+"
        }
    ],
    "parameters": [
        {
            "name": "Account :",
            "description": "Account administrators can use the ALTER ACCOUNT command to set session parameters for the account. The values set for the account default to individual users and\ntheir sessions."
        },
        {
            "name": "User :",
            "description": "Administrators with the appropriate privileges (typically SECURITYADMIN role) can use the ALTER USER command to override session parameters for individual users. The values set for a user default to\nany sessions started by the user. In addition, users can override default sessions parameters for themselves using ALTER USER ."
        },
        {
            "name": "Session :",
            "description": "Users can use the ALTER SESSION to explicitly set session parameters within their sessions."
        },
        {
            "name": "Account :",
            "description": "Account administrators can use the ALTER ACCOUNT command to set object parameters for the account. The values set for the account default to the objects created in\nthe account."
        },
        {
            "name": "Object :",
            "description": "Users with the appropriate privileges can use the corresponding CREATE <object> or ALTER <object> commands to override object parameters for an individual\nobject."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service\ninterruption)."
        },
        {
            "name": "Values :",
            "description": "TRUE : In-progress queries are aborted 5 minutes after connectivity is lost. FALSE : In-progress queries are completed."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Sets the profiler to use for the session when profiling Python handler code ."
        },
        {
            "name": "Values :",
            "description": "'LINE' : To have the profile focus on line use activity. 'MEMORY' : To have the profile focus on memory use activity."
        },
        {
            "name": "Default :",
            "description": "None."
        },
        {
            "name": "Type :",
            "description": "Account — Can only be set for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether an MFA token can be saved in the client-side operating system keystore to promote continuous, secure connectivity without users needing to respond to an MFA prompt at the start of each connection attempt to Snowflake. For details and the list of supported Snowflake-provided clients, see Using MFA token caching to minimize the number of prompts during authentication — optional ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Stores an MFA token in the client-side operating system keystore to enable the client application to use the MFA token whenever a new connection is established. While true, users are not prompted to respond to additional MFA prompts. FALSE : Does not store an MFA token. Users must respond to an MFA prompt whenever the client application establishes a new connection with Snowflake."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether a connection token can be saved in the client-side operating system keystore to promote continuous, secure connectivity without users needing to enter login credentials at the start of each connection attempt to Snowflake. For details and the list of supported Snowflake-provided clients, see Using connection caching to minimize the number of prompts for authentication — Optional ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Stores a connection token in the client-side operating system keystore to enable the client application to perform browser-based SSO without prompting users to authenticate whenever a new connection is established. FALSE : Does not store a connection token. Users are prompted to authenticate whenever the client application establishes a new connection with Snowflake. SSO to Snowflake is still possible if this parameter is set to false."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Object (for Snowflake Scripting stored procedures)"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Controls whether Snowflake Scripting log messages and trace events are ingested automatically into the event table . To set this parameter, run the ALTER PROCEDURE command."
        },
        {
            "name": "Values :",
            "description": "LOGGING : Automatically adds the following additional logging information to the event table when a\nprocedure is executed: BEGIN/END of a Snowflake Scripting block. BEGIN/END of a child job request. This information is added to the event table only if the effective LOG_LEVEL is set\nto TRACE for the stored procedure. TRACING : Automatically adds the following additional trace information to the event table when a\nstored procedure is executed: Exception catching. Information about child job execution. Child job statistics. Stored procedure statistics, including execution time and input values. This information is added to the event table only if the effective TRACE_LEVEL is set\nto ALWAYS or ON_EVENT for the stored procedure. ALL : Automatically adds both the logging information added for the LOGGING value\nand the trace information added for the TRACING value. OFF : Does not automatically add logging information or trace information to the event table."
        },
        {
            "name": "Default :",
            "description": "OFF"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether autocommit is enabled for the session. Autocommit determines whether a DML statement, when executed without an active transaction, is automatically committed after the\nstatement successfully completes. For more information, see Transactions ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Autocommit is enabled. FALSE : Autocommit is disabled, meaning DML statements must be explicitly committed or rolled back."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "N/A"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "For Snowflake internal use only. View-only parameter that indicates whether API support for autocommit is enabled for your account. If the value is TRUE , you can enable or disable\nautocommit through the APIs for the following drivers/connectors: JDBC driver ODBC driver Snowflake Connector for Python"
        },
        {
            "name": "Type :",
            "description": "Object (for databases and schemas) — Can be set for Account » Database » Schema"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies a prefix for Snowflake to use in the write path for Snowflake-managed Apache Iceberg™ tables.\nFor more information, see data and metadata directories for Iceberg tables ."
        },
        {
            "name": "Values :",
            "description": "Any valid string prefix that complies with the storage naming conventions of your cloud provider."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "The format of VARCHAR values passed as input to VARCHAR-to-BINARY conversion functions. For more information, see Binary input and output ."
        },
        {
            "name": "Values :",
            "description": "HEX , BASE64 , or UTF8 / UTF-8"
        },
        {
            "name": "Default :",
            "description": "HEX"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "The format for VARCHAR values returned as output by BINARY-to-VARCHAR conversion functions. For more information, see Binary input and output ."
        },
        {
            "name": "Values :",
            "description": "HEX or BASE64"
        },
        {
            "name": "Default :",
            "description": "HEX"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and Apache Iceberg™ tables) — Can be set for Account » Database » Schema » Iceberg table"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the catalog for Apache Iceberg™ tables.\nFor more information, see the Iceberg table documentation ."
        },
        {
            "name": "Values :",
            "description": "SNOWFLAKE or any valid catalog integration identifier."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and Iceberg tables) — Can be set for Account » Database » Schema » Iceberg Table"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the name of your catalog integration for Snowflake Open Catalog .\nSnowflake syncs tables that use the specified catalog integration with your Snowflake Open Catalog account. For more information, see Sync a Snowflake-managed table with Snowflake Open Catalog ."
        },
        {
            "name": "Values :",
            "description": "The name of any existing catalog integration for Open Catalog."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Clients :",
            "description": "JDBC"
        },
        {
            "name": "Description :",
            "description": "Enables users to log the data values bound to PreparedStatements . To see the values, you must not only set this session-level parameter to TRUE , but also set the\nconnection parameter named TRACING to either INFO or ALL . Set TRACING to ALL to see all debugging information and all binding information. Set TRACING to INFO to see the binding parameter values and less other debug information. Caution If you bind confidential information, such as medical diagnoses or passwords, that information is\nlogged. Snowflake recommends making sure that the log file is secure, or only using test data, when you set\nthis parameter to TRUE ."
        },
        {
            "name": "Values :",
            "description": "TRUE or FALSE ."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Clients :",
            "description": "Any"
        },
        {
            "name": "Description :",
            "description": "Specifies the AES encryption key size, in bits, used by Snowflake to encrypt/decrypt files stored on internal stages (for loading/unloading data) when you use the SNOWFLAKE_FULL encryption type."
        },
        {
            "name": "Values :",
            "description": "128 or 256"
        },
        {
            "name": "Default :",
            "description": "128"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Clients :",
            "description": "JDBC, ODBC"
        },
        {
            "name": "Description :",
            "description": "Parameter that specifies the maximum amount of memory the JDBC driver or ODBC driver should use for the result set from queries (in MB). For the JDBC driver: To simplify JVM memory management, the parameter sets a global maximum memory usage limit for all queries. CLIENT_RESULT_CHUNK_SIZE specifies the maximum size of each set (or chunk ) of query results to download (in MB).\nThe driver might require additional memory to process a chunk; if so, it will adjust memory usage during runtime to process\nat least one thread/query. Verify that CLIENT_MEMORY_LIMIT is set significantly higher than CLIENT_RESULT_CHUNK_SIZE to\nensure sufficient memory is available. For the ODBC driver: This parameter is supported in version 2.22.0 and higher. CLIENT_RESULT_CHUNK_SIZE is not supported."
        },
        {
            "name": "Values :",
            "description": "Any valid number of megabytes."
        },
        {
            "name": "Default :",
            "description": "1536 (effectively 1.5 GB) Most users should not need to set this parameter. If this parameter is not set by the user, the driver starts\nwith the default specified above. In addition, the JDBC driver actively manages its memory conservatively to avoid using up all available memory."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Clients :",
            "description": "JDBC, ODBC"
        },
        {
            "name": "Description :",
            "description": "For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all\ndatabases/schemas to the current database/schema. The narrower search typically returns fewer rows and executes\nmore quickly. For example, the getTables() JDBC method accepts a database name and schema name as arguments, and returns the\nnames of the tables in the database and schema. If the database and schema arguments are null , then by default, the\nmethod searches all databases and all schemas in the account. Setting CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX to TRUE narrows the search to the current database and schema specified by the connection context . In essence, setting this parameter to TRUE creates the following precedence for database and schema: Values passed as arguments to the functions/methods. Values specified in the connection context (if any). Default (all databases and all schemas). For more details, see the information below. This parameter applies to the following: JDBC driver methods (for the DatabaseMetaData class): getColumns getCrossReference getExportedKeys getForeignKeys getFunctions getImportedKeys getPrimaryKeys getSchemas getTables ODBC driver functions: SQLTables SQLColumns SQLPrimaryKeys SQLForeignKeys SQLGetFunctions SQLProcedures"
        },
        {
            "name": "Values :",
            "description": "TRUE : If the database and schema arguments are null , then the driver retrieves metadata for only\nthe database and schema specified by the connection context . The interaction is described in more detail in the table below. FALSE : If the database and schema arguments are null , then the driver retrieves\nmetadata for all databases and schemas in the account."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Additional Notes :",
            "description": "The connection context refers to the current database and schema for the session, which can be set using\nany of the following options: Specify the default namespace for the user who connects to Snowflake (and initiates the session). This can be\nset for the user through the CREATE USER or ALTER USER command, but must be set before the user connects. Specify the database and schema when connecting to Snowflake through the driver. Issue a USE DATABASE or USE SCHEMA command within the session. If the database or schema was specified by more than one of these, then the most recent one applies. When CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX is set to TRUE : database argument schema argument Database used Schema used Non-null Non-null Argument Argument Non-null Null Argument All schemas Null Non-null Connection context Argument Null Null Connection context Session context"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Clients :",
            "description": "JDBC"
        },
        {
            "name": "Description :",
            "description": "This parameter applies to only the methods affected by CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX . This parameter applies only when both of the following conditions are met: CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX is FALSE or unset. No database or schema is passed to the relevant ODBC function or JDBC method. For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all\ndatabases to the current database. The narrower search typically returns fewer rows and executes\nmore quickly. For more details, see the information below."
        },
        {
            "name": "Values :",
            "description": "TRUE : The driver searches all schemas in the connection context’s database. (For more details about the connection context , see the documentation for CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX .) FALSE : The driver searches all schemas in all databases."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Additional Notes :",
            "description": ""
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Clients :",
            "description": "JDBC, ODBC, Python, .NET"
        },
        {
            "name": "Description :",
            "description": "Parameter that specifies the number of threads used by the client to pre-fetch large result sets. The driver will attempt to honor the parameter value, but defines the\nminimum and maximum values (depending on your system’s resources) to improve performance."
        },
        {
            "name": "Values :",
            "description": "1 to 10"
        },
        {
            "name": "Default :",
            "description": "4 Most users should not need to set this parameter. If this parameter is not set by the user, the driver starts\nwith the default specified above, but also actively manages its thread count conservatively to avoid using up all\navailable memory."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Clients :",
            "description": "JDBC, Node.js, SQL API, Go"
        },
        {
            "name": "Description :",
            "description": "Parameter that specifies the maximum size of each set (or chunk ) of query results to download (in MB). The JDBC driver downloads query results in chunks. Also see CLIENT_MEMORY_LIMIT ."
        },
        {
            "name": "Values :",
            "description": "16 to 160"
        },
        {
            "name": "Default :",
            "description": "160 Most users should not need to set this parameter. If this parameter is not set by the user, the driver starts\nwith the default specified above, but also actively manages its memory conservatively to avoid using up all\navailable memory."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Clients :",
            "description": "JDBC"
        },
        {
            "name": "Description :",
            "description": "Parameter that indicates whether to match column name case-insensitively in ResultSet.get* methods in JDBC."
        },
        {
            "name": "Values :",
            "description": "TRUE : matches column names case-insensitively. FALSE : matches column names case-sensitively."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Clients :",
            "description": ".NET, Golang, JDBC, Node.js, ODBC, Python,"
        },
        {
            "name": "Description :",
            "description": "Parameter that indicates whether to force a user to log in again after a period of inactivity in the session."
        },
        {
            "name": "Values :",
            "description": "TRUE : Snowflake keeps the session active indefinitely as long as the connection is active , even if there is no activity from the user. FALSE : The user must log in again after four hours of inactivity."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Clients :",
            "description": "SnowSQL, JDBC, Python, Node.js"
        },
        {
            "name": "Description :",
            "description": "Number of seconds in-between client attempts to update the token for the session."
        },
        {
            "name": "Values :",
            "description": "900 to 3600"
        },
        {
            "name": "Default :",
            "description": "3600"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Clients :",
            "description": "Any"
        },
        {
            "name": "Description :",
            "description": "Specifies the TIMESTAMP_* variation to use when binding timestamp variables for JDBC or ODBC applications that use the bind API to load data."
        },
        {
            "name": "Values :",
            "description": "TIMESTAMP_LTZ or TIMESTAMP_NTZ"
        },
        {
            "name": "Default :",
            "description": "TIMESTAMP_LTZ"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the models that users in the account can access. Use this parameter to allowlist models for all users in the account. If you need to provide specific users with access beyond what you’ve specified in the allowlist, use role-based access control instead. For more information, see Model allowlist ."
        },
        {
            "name": "Values :",
            "description": "'All' : Provides access to all models, including fine-tuned models. Example: ' model1,model2,... ' : Provides access to the models specified in a comma-separated list. Example: 'None' : Prevents access to any model. Example:"
        },
        {
            "name": "Default :",
            "description": "'None'"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the regions where an inference request may be processed in case the request cannot be processed in the region\nwhere request is originally placed. Specifying DISABLED disables cross-region inferencing. For examples and details,\nsee Cross-region inference ."
        },
        {
            "name": "Values :",
            "description": "This parameter can be set to one of the following: DISABLED ANY_REGION Comma-separated list including one or more of the following values: AWS_APJ AWS_EU AWS_US AZURE_EU AZURE_US Value Behavior DISABLED Inference requests will only be handled in the region where the request is placed. ANY_REGION Inference requests may be routed to any region that supports cross-region inference (listed in this table) and that has availability,\nincluding the region where the request is placed. AWS_APJ Inference requests will be handled in either: The region where the request is placed. AWS ap-northeast-1 AWS_EU Inference requests will be handled in either: The region where the request is placed. AWS Europe (Frankfurt) eu-central-1 AWS Europe (Stockholm) eu-north-1 AWS Europe (Ireland) eu-west-1 AWS Europe (Paris) eu-west-3 AWS_US Inference requests will be handled in either: The region where the request is placed. Any of the following AWS cloud regions physically located within the United States: AWS US East (N. Virginia) us-east-1 AWS US East (Ohio) us-east-2 AWS US West (Oregon) us-west-2 AZURE_EU Inference requests will be handled in either: The region where the request is placed. Azure Europe (Netherlands) westeurope AZURE_US Inference requests will be handled in either: The region where the request is placed. Azure US (Virginia) eastus2"
        },
        {
            "name": "Default :",
            "description": "DISABLED"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the format for TIMESTAMP values in CSV files downloaded from Snowsight. If this parameter is not set, TIMESTAMP_LTZ_OUTPUT_FORMAT will be used for TIMESTAMP_LTZ values, TIMESTAMP_TZ_OUTPUT_FORMAT will be used for TIMESTAMP_TZ and TIMESTAMP_NTZ_OUTPUT_FORMAT for TIMESTAMP_NTZ values. For more information, see Date and time input and output formats or Download your query results ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported timestamp format."
        },
        {
            "name": "Default :",
            "description": "No value."
        },
        {
            "name": "Type :",
            "description": "Object (for tables)"
        },
        {
            "name": "Data type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the schedule to run the data metric functions associated to the table."
        },
        {
            "name": "Values :",
            "description": "The schedule can be based on a defined number of minutes, a cron expression, or a DML event on the table that does not involve\nreclustering. For details, see: Data metric function actions (dataMetricFunctionAction) . Schedule the DMF to run ."
        },
        {
            "name": "Default :",
            "description": "No value."
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tables) — Can be set for Account » Database » Schema » Table"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Number of days for which Snowflake retains historical data for performing Time Travel actions (SELECT, CLONE, UNDROP) on the object. A value of 0 effectively disables\nTime Travel for the specified database, schema, or table. For more information, see Understanding & using Time Travel ."
        },
        {
            "name": "Values :",
            "description": "0 or 1 (for Standard Edition ) 0 to 90 (for Enterprise Edition or higher )"
        },
        {
            "name": "Default :",
            "description": "1"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the input format for the DATE data type. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported date format or AUTO ( AUTO specifies that Snowflake attempts to automatically detect the format of dates stored in the system during the session)"
        },
        {
            "name": "Default :",
            "description": "AUTO"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the display format for the DATE data type. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported date format"
        },
        {
            "name": "Default :",
            "description": "YYYY-MM-DD"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tables) — Can be set for Account » Database » Schema » Table"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Sets the default collation used for the following DDL operations: CREATE TABLE ALTER TABLE … ADD COLUMN Setting this parameter forces all subsequently-created columns in the affected objects (table, schema, database, or account) to have\nthe specified collation as the default, unless the collation for the column is explicitly defined in the DDL. For example, if DEFAULT_DDL_COLLATION = 'en-ci' , then the following two statements are equivalent: Note This parameter isn’t supported for dynamic tables , hybrid tables , and Apache Iceberg™ tables ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported collation specification ."
        },
        {
            "name": "Default :",
            "description": "Empty string"
        },
        {
            "name": "Type :",
            "description": "Object (for databases and schemas) — Can be set for Account » Database » Schema"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Sets the preferred CPU compute pool used for Notebooks on CPU Container Runtime ."
        },
        {
            "name": "Values :",
            "description": "Name of a compute pool in your account."
        },
        {
            "name": "Default :",
            "description": "SYSTEM_COMPUTE_POOL_CPU (see Compute pools for Notebooks )."
        },
        {
            "name": "Type :",
            "description": "Object (for databases and schemas) — Can be set for Account » Database » Schema"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Sets the preferred GPU compute pool used for Notebooks on GPU Container Runtime ."
        },
        {
            "name": "Values :",
            "description": "Name of a compute pool in your account."
        },
        {
            "name": "Default :",
            "description": "SYSTEM_COMPUTE_POOL_GPU (see Compute pools for Notebooks )."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the default ordering of NULL values in a result set."
        },
        {
            "name": "Values :",
            "description": "FIRST : NULL values are lower than any non-NULL values. LAST : NULL values are higher than any non-NULL values."
        },
        {
            "name": "Default :",
            "description": "LAST"
        },
        {
            "name": "Type :",
            "description": "Object (for databases and schemas) — Can be set for Account » Database » Schema"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the name of the default warehouse to use when creating a notebook. For more information, see ALTER ACCOUNT , ALTER DATABASE , and ALTER SCHEMA ."
        },
        {
            "name": "Values :",
            "description": "The name of any existing warehouse."
        },
        {
            "name": "Default :",
            "description": "SYSTEM$STREAMLIT_NOTEBOOK_WH"
        },
        {
            "name": "Type :",
            "description": "Object (for users) — Can be set for Account > User"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Controls whether users in an account see a button to download data in Snowsight or the Classic Console, such as a table\nreturned from running a query in a worksheet. If the button to download is hidden in Snowsight or the Classic Console, users can still download or export data using third-party software ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Users in the account don’t see a button to download data in Snowsight or the Classic Console. FALSE : Users in the account see a button to download data in Snowsight or the Classic Console."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Object (for users) — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Controls whether users in an account can grant privileges directly to other users. Disabling user privilege grants (that is, setting DISABLE_USER_PRIVILEGE_GRANTS to TRUE ) does not affect existing grants to users.\nExisting grants to users continue to confer privileges to those users. For more information, see GRANT <privileges> … TO USER ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Users in the account cannot grant privileges to another user. FALSE : Users in the account can grant privileges to another user."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Controls whether events from automatic sensitive data classification are logged in the user event table."
        },
        {
            "name": "Values :",
            "description": "TRUE : Snowflake logs events for automatic sensitive data classification in the user event table. FALSE : Events for automatic sensitive data classification are not logged."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Enables or disables the Listing Cross-cloud auto-fulfillment Egress cost optimizer."
        },
        {
            "name": "Values :",
            "description": "TRUE : Enable the Egress cost optimizer. FALSE : Disable the Egress cost optimizer."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Determines the login flow for users. When enabled, Snowflake prompts users for their username or email address before presenting\nauthentication methods. For details, see Identifier-first login ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Snowflake uses an identifier-first login flow to authenticate users. FALSE : Snowflake presents all possible login options, even if those options don’t apply to a particular user."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether the SYSTEM$GET_PRIVATELINK_CONFIG function returns the private-internal-stages key in the query\nresult. The corresponding value in the query result is used during the configuration process for private connectivity to internal stages."
        },
        {
            "name": "Values :",
            "description": "TRUE : Returns the private-internal-stages key and value in the query result. FALSE : Does not return the private-internal-stages key and value in the query result."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "User — Can be set for Account > User"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether users can create private notebooks (stored in their personal databases). When TRUE, users in the account can\ncreate private notebooks (assuming other necessary privileges are granted)."
        },
        {
            "name": "Values :",
            "description": "TRUE : Enables users to create private notebooks. FALSE : Prevents users from creating private notebooks."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies choice for the image repository to opt out of Tri-Secret Secure and Periodic rekeying ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Opts out Tri-Secret Secure and Periodic Rekeying for Image Repository. FALSE : Disallows the creation of an image repository for Tri-Secret Secure and periodic rekeying accounts. Similarly, disallows\nenabling Tri-Secret Secure and periodic rekeying for accounts that have enabled Image Repository."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies the choice for the Snowpark Container Services block storage volume to opt out of Tri-Secret Secure and Periodic rekeying ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Opts out Tri-Secret Secure and periodic rekeying for Snowpark Container Services block storage volumes. FALSE : Disallows the creation of block storage volumes for Tri-Secret Secure and periodic rekeying accounts. Similarly,\ndisallows enabling Tri-Secret Secure and periodic rekeying for accounts that have enabled block storage volumes."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether Snowflake may capture – in an event table – log messages or trace event data for unhandled exceptions\nin procedure or UDF handler code. For more information, see Capturing messages from unhandled exceptions ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Data about unhandled exceptions is captured as log or trace data if logging and tracing are enabled. FALSE : Data about unhandled exceptions is not captured."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to set the schema for unloaded Parquet files based on the logical column data types (that is, the types in the unload SQL query or source table) or on the\nunloaded column values (that is, the smallest data types and precision that support the values in the output columns of the unload SQL statement or source table)."
        },
        {
            "name": "Values :",
            "description": "TRUE : The schema of unloaded Parquet data files is determined by the column values in the unload SQL query or source table. Snowflake optimizes table columns by setting the smallest precision that accepts all of the values. The unloader follows this pattern when writing values to Parquet files. The data type and precision of an output column are set to the smallest data type and precision that support its values in the unload SQL statement or source table. Accept this setting for better performance and smaller data files. FALSE : The schema is determined by the logical column data types. Set this value for a consistent output file schema."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "User — Can be set for Account » User"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Controls whether query text is redacted if a SQL query fails due to a syntax or parsing error. If FALSE , the content of a\nfailed query is redacted in the views, pages, and functions that provide a query history. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the query\ntext, not the user who executed the query (if those are different users)."
        },
        {
            "name": "Values :",
            "description": "TRUE : Disables the redaction of query text for queries that fail due to a syntax or parsing error. FALSE : Redacts the contents of a query from the views, pages, and functions that provide a query history when a query fails due to a\nsyntax or parsing error."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "User — Can be set for Account » User"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Controls whether error messages related to secure objects are redacted in metadata. For more information,\nsee Secure objects: Redaction of information in error messages . Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_SECURE_OBJECT_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the\nredacted error messages in metadata, not the user who caused the error."
        },
        {
            "name": "Values :",
            "description": "TRUE : Disables the redaction of error messages related to secure objects in metadata. FALSE : Redacts the contents of error messages related to secure objects in metadata."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether a network policy that uses network rules can restrict access to AWS internal stages. This parameter has no effect on network policies that do not use network rules. This account-level parameter affects both account-level and user-level network policies. For details about using network policies and network rules to restrict access to AWS internal stages, including the use of this parameter,\nsee Protecting internal stages on AWS ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Allows network policies that use network rules to restrict access to AWS internal stages. The network rule must\nalso use the appropriate MODE and TYPE to restrict access to the internal stage. FALSE : Network policies never restrict access to internal stages."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to return an error when the MERGE command is used to update or delete a target row that joins multiple source rows and the system cannot\ndetermine the action to perform on the target row."
        },
        {
            "name": "Values :",
            "description": "TRUE : An error is returned that includes values from one of the target rows that caused the error. FALSE : No error is returned and the merge completes successfully, but the results of the merge are nondeterministic."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to return an error when the UPDATE command is used to update a target row that joins multiple source rows and the system cannot determine the\naction to perform on the target row."
        },
        {
            "name": "Values :",
            "description": "TRUE : An error is returned that includes values from one of the target rows that caused the error. FALSE : No error is returned and the update completes, but the results of the update are nondeterministic."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Object — Can be set for Account » Database"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the name of the event table for logging messages from stored procedures and UDFs contained by the object with which\nthe event table is associated. Associating an event table with a database is available in Enterprise Edition or higher ."
        },
        {
            "name": "Values :",
            "description": "Any existing event table created by executing the CREATE EVENT TABLE command."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Determines whether the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles can be used as the primary role when creating a\nSnowflake session based on the access token from the External OAuth authorization server."
        },
        {
            "name": "Values :",
            "description": "TRUE : Adds the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles to the EXTERNAL_OAUTH_BLOCKED_ROLES_LIST property of the\nExternal OAuth security integration, which means these roles cannot be used as the primary role when creating a Snowflake session using\nExternal OAuth authentication. FALSE : Removes the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN from the list of blocked roles defined by the EXTERNAL_OAUTH_BLOCKED_ROLES_LIST property of the External OAuth security integration."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the external volume for Apache Iceberg™ tables. For more information,\nsee the Iceberg table documentation ."
        },
        {
            "name": "Values :",
            "description": "Any valid external volume identifier."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Display format for GEOGRAPHY values . For EWKT and EWKB, the SRID is always 4326 in the output.\nRefer to the note on EWKT and EWKB handling ."
        },
        {
            "name": "Values :",
            "description": "GeoJSON , WKT , WKB , EWKT , or EWKB"
        },
        {
            "name": "Default :",
            "description": "GeoJSON"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Display format for GEOMETRY values ."
        },
        {
            "name": "Values :",
            "description": "GeoJSON , WKT , WKB , EWKT , or EWKB"
        },
        {
            "name": "Default :",
            "description": "GeoJSON"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Number of seconds to wait while trying to acquire row-level locks on a hybrid table, before timing out and aborting the statement."
        },
        {
            "name": "Values :",
            "description": "0 to any integer (no limit). A value of 0 disables lock waiting (that is, the statement must acquire the lock\nimmediately or abort). This value specifies how long the statement will wait for all of the row-level locks it needs to acquire after each\nexecution attempt (1 hour by default). If the statement cannot acquire all of the locks, it can be retried, and the same waiting period is applied."
        },
        {
            "name": "Default :",
            "description": "3600 (1 hour)"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Number."
        },
        {
            "name": "Description :",
            "description": "Sets the maximum estimated size limit for the initial replication of a primary database to a secondary database (in TB). Set this parameter on any account that stores a secondary database. This size limit helps prevent accounts from accidentally incurring large database replication charges. To remove the size limit, set the value to 0.0 . Note that there is currently no default size limit applied to subsequent refreshes of a secondary database."
        },
        {
            "name": "Values :",
            "description": "0.0 and above with a scale of at least 1 (e.g. 20.5 , 32.25 , 33.333 , etc.)."
        },
        {
            "name": "Default :",
            "description": "10.0"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to allow PUT and GET commands access to local file systems."
        },
        {
            "name": "Values :",
            "description": "TRUE : JDBC enables PUT and GET commands. FALSE : JDBC disables PUT and GET commands."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies how JDBC processes columns that have a scale of zero ( 0 )."
        },
        {
            "name": "Values :",
            "description": "TRUE : JDBC processes a column whose scale is zero as BIGINT. FALSE : JDBC processes a column whose scale is zero as DECIMAL."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies how JDBC processes TIMESTAMP_NTZ values. By default, when the JDBC driver fetches a value of type TIMESTAMP_NTZ from Snowflake, it converts the value to\n“wallclock” time using the client JVM timezone. Users who want to keep UTC timezone for the conversion can set this parameter to TRUE . This parameter applies only to the JDBC driver."
        },
        {
            "name": "Values :",
            "description": "TRUE : The driver uses UTC to get the TIMESTAMP_NTZ value in “wallclock” time. FALSE : The driver uses the client JVM’s current timezone to get the TIMESTAMP_NTZ value in “wallclock” time."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether the JDBC Driver uses the time zone of the JVM or the time zone of the session (specified by the TIMEZONE parameter) for the getDate() , getTime() , and getTimestamp() methods of the ResultSet class."
        },
        {
            "name": "Values :",
            "description": "TRUE : The JDBC Driver uses the time zone of the session. FALSE : The JDBC Driver uses the time zone of the JVM."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Specifies the number of blank spaces to indent each new element in JSON output in the session. Also specifies whether to insert newline characters after each element."
        },
        {
            "name": "Values :",
            "description": "0 to 16 (a value of 0 returns compact output by removing all blank spaces and newline characters from the output)"
        },
        {
            "name": "Default :",
            "description": "2"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies how the Snowflake Node.js Driver processes numeric columns that have a scale of zero ( 0 ), for example INTEGER or NUMBER(p, 0)."
        },
        {
            "name": "Values :",
            "description": "TRUE : JavaScript processes a column whose scale is zero as Bigint. FALSE : JavaScript processes a column whose scale is zero as Number."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Sets the time interval used to refresh the application package based data products to other regions."
        },
        {
            "name": "Values :",
            "description": "num MINUTES : A value between 1 and 11520 . Must include the unit MINUTES. USING CRON expr time_zone : Specifies a cron expression and time zone for the refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the Wikipedia topic list of tz database time zones .\nThe cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July, and October. For example, every three months, starting with the fourth\nmonth of the year. The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh listings based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or the day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0 a.m. on the tenth to twentieth day of any month and also on any Tuesday or Thursday outside of those dates."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/ n",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July, and October. For example, every three months, starting with the fourth\nmonth of the year. The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run)."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Number of seconds to wait while trying to lock a resource, before timing out and aborting the statement."
        },
        {
            "name": "Values :",
            "description": "0 to any integer (no limit). A value of 0 disables lock waiting (the statement must acquire the lock\nimmediately or abort). If multiple resources need to be locked by the statement, the timeout applies separately\nto each lock attempt."
        },
        {
            "name": "Default :",
            "description": "43200 (12 hours)"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session Object (for databases, schemas, stored procedures, UDFs, dynamic tables, tasks, services) — Can be set for: Account » Database » Schema » Procedure Account » Database » Schema » Function Account » Database » Schema » Dynamic table Account » Database » Schema » Task Account » Database » Schema » Service"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about log levels, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "Values :",
            "description": "TRACE DEBUG INFO WARN ERROR FATAL OFF"
        },
        {
            "name": "Default :",
            "description": "OFF"
        },
        {
            "name": "Additional Notes :",
            "description": "The following table lists the levels of messages ingested when you set the LOG_LEVEL parameter to a level. LOG_LEVEL Parameter Setting Levels of Messages Ingested TRACE TRACE DEBUG INFO WARN ERROR FATAL DEBUG DEBUG INFO WARN ERROR FATAL INFO INFO WARN ERROR FATAL WARN WARN ERROR FATAL ERROR ERROR FATAL FATAL ERROR (Only for Java UDFs, Java UDTFs, and Java and Scala stored procedures. For more information, see Understanding logging, metrics, and tracing levels ) FATAL If this parameter is set in both the session and the object (or schema, database, or account), the more verbose value is used.\nSee Understanding how Snowflake determines level in effect ."
        },
        {
            "name": "Type :",
            "description": "Object (for warehouses) — Can be set for Account » Warehouse"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Specifies the concurrency level for SQL statements (that is, queries and DML) executed by a warehouse. When the level is reached, the operation performed depends on whether\nthe warehouse is a single-cluster or multi-cluster warehouse: Single-cluster or multi-cluster (in Maximized mode): Statements are queued until already-allocated resources are freed or additional resources are provisioned, which can be accomplished by\nincreasing the size of the warehouse. Multi-cluster (in Auto-scale mode): Additional clusters are started. MAX_CONCURRENCY_LEVEL can be used in conjunction with the STATEMENT_QUEUED_TIMEOUT_IN_SECONDS parameter to ensure a warehouse is never backlogged. In general, it limits the number of statements\nthat can be executed concurrently by a warehouse cluster, but there are exceptions. In the following cases, the actual number of\nstatements executed concurrently by a warehouse might be more or less than the specified level: Smaller, more basic statements: More statements might execute concurrently because small statements generally execute on a subset of the available compute resources in a warehouse. This means they\nonly count as a fraction towards the concurrency level. Larger, more complex statements: Fewer statements might execute concurrently."
        },
        {
            "name": "Default :",
            "description": "8"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tables) — Can be set for Account » Database » Schema » Table"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Maximum number of days Snowflake can extend the data retention period for tables to prevent streams on the tables from becoming stale. By default, if the DATA_RETENTION_TIME_IN_DAYS setting for a source table is less than 14 days, and a stream has not been consumed, Snowflake temporarily extends this period to the stream’s offset, up to a maximum of 14 days, regardless of the Snowflake Edition for your account. The MAX_DATA_EXTENSION_TIME_IN_DAYS parameter enables you to limit this automatic extension period to control storage costs for data retention or for compliance reasons."
        },
        {
            "name": "Values :",
            "description": "0 to 90 (90 days) — a value of 0 disables the automatic extension of the data retention period. To increase the maximum value for tables in your account, contact Snowflake Support ."
        },
        {
            "name": "Default :",
            "description": "14"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session Object (for databases, schemas, stored procedures, and UDFs) — Can be set for Account » Database » Schema » Procedure and Account » Database » Schema » Function"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Controls how metrics data is ingested into the event table. For more information about metric levels, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "Values :",
            "description": "ALL : All metrics data will be recorded in the event table. NONE : No metrics data will be recorded in the event table."
        },
        {
            "name": "Default :",
            "description": "NONE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Integer (Constant)"
        },
        {
            "name": "Clients :",
            "description": "SQL API, JDBC, .NET, ODBC"
        },
        {
            "name": "Description :",
            "description": "Number of statements to execute when using the multi-statement capability."
        },
        {
            "name": "Values :",
            "description": "0 : Variable number of statements. 1 : One statement. More than 1 : When MULTI_STATEMENT_COUNT is set as a session parameter, you can specify the exact number of statements to\nexecute. Negative numbers are not permitted."
        },
        {
            "name": "Default :",
            "description": "1"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Minimum number of days for which Snowflake retains historical data for performing Time Travel actions (SELECT, CLONE, UNDROP)\non an object. If a minimum number of days for data retention is set on an account, the data retention period for an object is determined by\nMAX( DATA_RETENTION_TIME_IN_DAYS , MIN_DATA_RETENTION_TIME_IN_DAYS). For more information, see Understanding & using Time Travel ."
        },
        {
            "name": "Values :",
            "description": "0 or 1 (for Standard Edition ) 0 to 90 (for Enterprise Edition or higher )"
        },
        {
            "name": "Default :",
            "description": "0"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account (can be set by account administrators and security administrators)"
        },
        {
            "name": "Type :",
            "description": "Object (for users) — Can be set for Account » User"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the network policy to enforce for your account. Network policies enable restricting access to your account based on\nusers’ IP address. For more details, see Controlling network traffic with network policies ."
        },
        {
            "name": "Values :",
            "description": "Any existing network policy (created using CREATE NETWORK POLICY )"
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether the ORDER or NOORDER property is set by default when you create a new sequence or add a new table\ncolumn. The ORDER and NOORDER properties determine whether or not the values are generated for the sequence or auto-incremented column\nin increasing or decreasing order ."
        },
        {
            "name": "Values :",
            "description": "TRUE : When you create a new sequence or add a new table column, the NOORDER property is set by default. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). FALSE : When you create a new sequence or add a new table column, the ORDER property is set by default. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. If you set this parameter, the value that you set overrides the value in the 2024_01 behavior change bundle."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies how ODBC processes columns that have a scale of zero ( 0 )."
        },
        {
            "name": "Values :",
            "description": "TRUE : ODBC processes a column whose scale is zero as BIGINT. FALSE : ODBC processes a column whose scale is zero as DECIMAL."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Determines whether the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles can be used as the primary role when creating a\nSnowflake session based on the access token from Snowflake’s authorization server."
        },
        {
            "name": "Values :",
            "description": "TRUE : Adds the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles to the BLOCKED_ROLES_LIST property of the Snowflake OAuth\nsecurity integration, which means these roles cannot be used as the primary role when creating a Snowflake session using Snowflake\nOAuth. FALSE : Removes the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN from the list of blocked roles defined by the BLOCKED_ROLES_LIST property of the Snowflake OAuth security integration."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "This parameter only applies to Enterprise Edition (or higher). It enables/disables re-encryption of table data with new keys on a yearly basis to provide\nadditional levels of data protection. You can enable and disable rekeying at any time. Enabling/disabling rekeying does not result in gaps in your encrypted data: If rekeying is enabled for a period of time and then disabled, all data already tagged for rekeying is rekeyed, but no further data is rekeyed until you re-enable it again. If rekeying is re-enabled, Snowflake automatically rekeys all data that has keys which meet the criteria (that is, keys that are older than one year). For more information about rekeying of encrypted data, see Understanding Encryption Key Management in Snowflake ."
        },
        {
            "name": "Values :",
            "description": "TRUE : Data is rekeyed after one year has passed since the data was last encrypted. Rekeying occurs in the background so no down-time is experienced and the affected data/table is always\navailable. FALSE : Data is not rekeyed."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Object — Can be set for Account » Schema » Pipe"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to pause a running pipe, primarily in preparation for transferring ownership of the pipe to a different role: An account administrator (user with the ACCOUNTADMIN role) can set this parameter at the account level, effectively pausing or resuming all pipes in the account. A user with the MODIFY privilege on a schema can pause or resume all pipes in the schema. The pipe owner can set this parameter for a pipe. Note that setting the parameter at the account or schema level only affects pipes for which the parameter has not already been explicitly set at a lower level\n(e.g. at the pipe level by the pipe owner). This enables the practical use case in which an account administrator can pause all pipes at the account level, while a pipe owner can still have an individual pipe\nrunning."
        },
        {
            "name": "Values :",
            "description": "TRUE : Pauses the pipe. When the parameter is set to this value, the SYSTEM$PIPE_STATUS function shows the executionState as PAUSED . Note that the pipe owner can continue to submit files to a paused pipe; however, the files are not processed until the pipe is resumed. FALSE : Resumes the pipe, but only if ownership of the pipe has not been transferred while it was paused. When the parameter is set to this value, the SYSTEM$PIPE_STATUS function shows the executionState as RUNNING . If ownership of the pipe was transferred to another role after the pipe was paused, this parameter cannot be used to resume the pipe. Instead, use the SYSTEM$PIPE_FORCE_RESUME function to explicitly force the pipe to resume. This enables the new owner to use SYSTEM$PIPE_STATUS to evaluate the pipe status (e.g. determine how many files are waiting to be loaded)\nbefore resuming the pipe."
        },
        {
            "name": "Default :",
            "description": "FALSE (pipes are running by default)"
        },
        {
            "name": "Type :",
            "description": "Object (for users) — Can be set for Account » User"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to prevent ad hoc data unload operations to external cloud storage locations (that is, COPY INTO <location> statements that specify the cloud storage URL and access settings directly in the statement). For an example, see Unloading data from a table directly to files in an external location ."
        },
        {
            "name": "Values :",
            "description": "TRUE : COPY INTO <location> statements must reference either a named internal (Snowflake) or external stage or an internal user or table stage. A named external stage must store the cloud storage URL and access settings in its definition. FALSE : Ad hoc data unload operations to external cloud storage locations are permitted."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "User — Can be set for Account » User"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to prevent data unload operations to internal (Snowflake) stages using COPY INTO <location> statements."
        },
        {
            "name": "Values :",
            "description": "TRUE : Unloading data from Snowflake tables to any internal stage, including user stages, table stages, or named internal stages is prevented. FALSE : Unloading data to internal stages is permitted, limited only by the default restrictions of the stage type: The current user can only unload data to their own user stage. Users can only unload data to table stages when their active role has the OWNERSHIP privilege on the table. Users can only unload data to named internal stages when their active role has the WRITE privilege on the stage."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the fully-qualified name of the stage in which to save a report when profiling Python handler code ."
        },
        {
            "name": "Values :",
            "description": "Fully-qualified name of the stage in which to save the report. Use a temporary stage to store output only for the duration of the session. Use a permanent stage to preserve the profiler output outside of the scope of a session. For more information, see Specify the Snowflake stage where profile output should be written ."
        },
        {
            "name": "Default :",
            "description": "''"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the list of Python modules to include in a report when profiling Python handler code . Use this parameter to specify modules that are contained in staged handlers or that contain dependencies that you want to include\nin the profile."
        },
        {
            "name": "Values :",
            "description": "A comma-separated list of Python module names. For examples, see Including modules with the PYTHON_PROFILER_MODULES parameter and Profiling staged handler code ."
        },
        {
            "name": "Default :",
            "description": "''"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (up to 2000 characters)"
        },
        {
            "name": "Description :",
            "description": "Optional string that can be used to tag queries and other SQL statements executed within a session. The tags are displayed in the output of the QUERY_HISTORY , QUERY_HISTORY_BY_* functions."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session Object — Can be set for Account » Database » Schema » Table"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether letters in double-quoted object identifiers are stored and resolved as uppercase letters. By default,\nSnowflake preserves the case of alphabetic characters when storing and resolving double-quoted identifiers. (see Identifier resolution .) You can use this parameter in situations in which third-party applications always use double quotes around identifiers . Note Changing this parameter from the default value can affect your ability to find objects that were previously created with\ndouble-quoted mixed case identifiers. Refer to Impact of changing the parameter . When set on a table, schema, or database, the setting only affects the evaluation of table names in the bodies of views and\nuser-defined functions (UDFs). If your account uses double-quoted identifiers that should be treated as case-insensitive and you plan to share a view or UDF with an account that treats double-quoted identifiers as case-sensitive , you can set\nthis on the view or UDF that you plan to share. This allows the other account to resolve the table names in the view or UDF\ncorrectly."
        },
        {
            "name": "Values :",
            "description": "TRUE : Letters in double-quoted identifiers are stored and resolved as uppercase letters. FALSE : The case of letters in double-quoted identifiers is preserved. Snowflake resolves and stores the identifiers in the specified case. For more information, see Identifier resolution ."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Object — Can be set for Account » Database » Schema » Iceberg table"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results\nfor Apache Iceberg™ tables that use an external catalog."
        },
        {
            "name": "Values :",
            "description": "TRUE : Snowflake replaces invalid UTF-8 characters with the Unicode replacement character. FALSE : Snowflake leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message if it encounters an invalid UTF-8\ncharacter."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to require a storage integration object as cloud credentials when creating a named external stage (using CREATE STAGE ) to access a private cloud storage location."
        },
        {
            "name": "Values :",
            "description": "TRUE : Creating an external stage to access a private cloud storage location requires referencing a storage integration object as cloud credentials. FALSE : Creating an external stage does not require referencing a storage integration object. Users can instead reference explicit cloud provider credentials, such as secret keys or access tokens, if they have been configured for the storage location."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to require using a named external stage that references a storage integration object as cloud credentials when loading data from or unloading data to a private cloud storage location."
        },
        {
            "name": "Values :",
            "description": "TRUE : Loading data from or unloading data to a private cloud storage location requires using a named external stage that references a storage integration object; specifying a named external stage that references explicit cloud provider credentials, such as secret keys or access tokens, produces a user error. FALSE : Users can load data from or unload data to a private cloud storage location using a named external stage that references explicit cloud provider credentials. If PREVENT_UNLOAD_TO_INLINE_URL is FALSE, then users can specify the explicit cloud provider credentials directly in the COPY statement."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Clients :",
            "description": "SQL API"
        },
        {
            "name": "Description :",
            "description": "Specifies the maximum number of rows returned in a result set."
        },
        {
            "name": "Values :",
            "description": "0 to any number (no limit) — a value of 0 specifies no maximum."
        },
        {
            "name": "Default :",
            "description": "0"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the DNS name of an Amazon S3 interface endpoint. Requests sent to the internal stage of an account via AWS PrivateLink for Amazon S3 use this\nendpoint to connect. For more information, see Accessing Internal stages with dedicated interface endpoints ."
        },
        {
            "name": "Values :",
            "description": "Valid region-scoped DNS Name of an S3 interface endpoint. The standard format begins with an asterisk ( * ) and ends with vpce.amazonaws.com (e.g. *.vpce-sd98fs0d9f8g.s3.us-west-2.vpce.amazonaws.com ). For more details about obtaining this value, refer to AWS configuration . Alternative formats include bucket.vpce-xxxxxxxx.s3.<region>.vpce.amazonaws.com and vpce-xxxxxxxx.s3.<region>.vpce.amazonaws.com ."
        },
        {
            "name": "Default :",
            "description": "Empty string"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "JSON"
        },
        {
            "name": "Description :",
            "description": "Enables federated authentication. This deprecated parameter enables federated authentication. This parameter accepts a JSON\nobject, enclosed in single quotes, with the following fields: Where: Specifies the certificate (generated by the IdP) that verifies communication between the IdP and Snowflake. Indicates the Issuer/EntityID of the IdP. Optional. For information on how to obtain this value in Okta and AD FS, see Migrating to a SAML2 security integration . Specifies the URL endpoint (provided by the IdP) where Snowflake sends the SAML requests. Specifies the type of IdP used for federated authentication ( \"OKTA\" , \"ADFS\" , \"Custom\" ). Specifies the button text for the IdP in the Snowflake login page. The default label is Single Sign On . If you change the default label, the label you specify can only contain alphanumeric\ncharacters (special characters and blank spaces are not currently supported). Note that, if the \"type\" field is \"Okta\" , a value for the label field does not need to be specified because Snowflake displays the Okta logo in the button. For more information, including examples of setting the parameter, see Migrating to a SAML2 security integration ."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "certificate",
            "description": "Specifies the certificate (generated by the IdP) that verifies communication between the IdP and Snowflake."
        },
        {
            "name": "issuer",
            "description": "Indicates the Issuer/EntityID of the IdP. Optional. For information on how to obtain this value in Okta and AD FS, see Migrating to a SAML2 security integration ."
        },
        {
            "name": "ssoUrl",
            "description": "Specifies the URL endpoint (provided by the IdP) where Snowflake sends the SAML requests."
        },
        {
            "name": "type",
            "description": "Specifies the type of IdP used for federated authentication ( \"OKTA\" , \"ADFS\" , \"Custom\" )."
        },
        {
            "name": "label",
            "description": "Specifies the button text for the IdP in the Snowflake login page. The default label is Single Sign On . If you change the default label, the label you specify can only contain alphanumeric\ncharacters (special characters and blank spaces are not currently supported). Note that, if the \"type\" field is \"Okta\" , a value for the label field does not need to be specified because Snowflake displays the Okta logo in the button."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the path to search to resolve unqualified object names in queries. For more information, see Name resolution in queries ."
        },
        {
            "name": "Values :",
            "description": "Comma-separated list of identifiers. An identifier can be a fully or partially qualified schema name."
        },
        {
            "name": "Default :",
            "description": "$current, $public For more information about the default settings, see default search path ."
        },
        {
            "name": "Type :",
            "description": "Object — Can be set for Account » Database » Schema » Task"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the maximum allowed warehouse size for Serverless tasks ."
        },
        {
            "name": "Values :",
            "description": "Any traditional warehouse size : XSMALL , SMALL , MEDIUM , LARGE , XLARGE , X2LARGE . The maximum size is X2LARGE . Also supports the syntax: XXLARGE ."
        },
        {
            "name": "Default :",
            "description": "X2LARGE"
        },
        {
            "name": "Type :",
            "description": "Object — Can be set for Account » Database » Schema » Task"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the maximum allowed warehouse size for Serverless tasks ."
        },
        {
            "name": "Values :",
            "description": "Any traditional warehouse size : XSMALL , SMALL , MEDIUM , LARGE , XLARGE , X2LARGE . The maximum size is X2LARGE . Also supports the syntax: XXLARGE ."
        },
        {
            "name": "Default :",
            "description": "XSMALL"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the name of a consumer account to simulate for testing/validating shared data, particularly shared secure views. When this parameter is set in a session, shared views return rows as if executed in the specified consumer account rather than the provider account. Note Simulations only succeed when the current role is the owner of the view.\nIf the current role does not own the view, simulations fail with the error: For more information, see About Secure Data Sharing and Create and configure shares ."
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Account — Can be set only for Account"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "This deprecated parameter disables preview mode for testing SSO (after enabling federated authentication) before rolling it out to users:"
        },
        {
            "name": "Values :",
            "description": "TRUE : Preview mode is disabled and users will see the button for Snowflake-initiated SSO for your identity provider (as specified in SAML_IDENTITY_PROVIDER ) in the Snowflake main login page. FALSE : Preview mode is enabled and SSO can be tested using the following URL: If your account is in US West: https://<account_identifier>.snowflakecomputing.com/console/login?fedpreview=true If your account is in any other region: https://<account_identifier>.<region_id>.snowflakecomputing.com/console/login?fedpreview=true For more information, see: Migrating to a SAML2 security integration Account identifiers"
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session and Object (for warehouses) Can be set for Account » User » Session; can also be set for individual warehouses"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged. The parameter can be set within the session hierarchy. It can also be set for an individual warehouse to control the runtime for all SQL statements processed by the warehouse. When the parameter is set for both the session and the warehouse, the timeout is the lowest non-zero value of the two parameters. Note When both STATEMENT_QUEUED_TIMEOUT_IN_SECONDS and USER_TASK_TIMEOUT_MS are set, the value of USER_TASK_TIMEOUT_MS takes precedence. When comparing the values of these two parameters, note that\nSTATEMENT_QUEUED_TIMEOUT_IN_SECONDS is set in units of seconds, while USER_TASK_TIMEOUT_MS\nuses units of milliseconds."
        },
        {
            "name": "Values :",
            "description": "0 to any number (no limit) — a value of 0 specifies that no timeout is enforced. A statement will remained queued as long as the queue persists."
        },
        {
            "name": "Default :",
            "description": "0 (no timeout)"
        },
        {
            "name": "Type :",
            "description": "Session and Object (for warehouses) Can be set for Account » User » Session; can also be set for individual warehouses"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Amount of time, in seconds, after which a running SQL statement (query, DDL, DML, and so on) is canceled by the system. The parameter can be set within the session hierarchy. It can also be set for an individual warehouse to control the runtime for all SQL statements processed by the warehouse. When the parameter is\nset for some combination of account, user, session, and warehouse, the lowest non-zero value is enforced. For example: When both USER_TASK_TIMEOUT_MS and STATEMENT_TIMEOUT_IN_SECONDS are set, the timeout is the lowest non-zero value of the two parameters. When comparing the values of these two parameters, note that STATEMENT_TIMEOUT_IN_SECONDS is set in units of seconds, while USER_TASK_TIMEOUT_MS uses units of milliseconds. The parameter setting applies to all of the time taken by the statement, including queue time, locked time, execution time, compilation time, and\nso on. It applies to the overall time taken by the statement, not just the warehouse execution time."
        },
        {
            "name": "Values :",
            "description": "0 to 604800 (7 days) — a value of 0 specifies that the maximum timeout value is enforced."
        },
        {
            "name": "Default :",
            "description": "172800 (2 days)"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and Apache Iceberg™ tables) — Can be set for Account » Database » Schema » Iceberg table"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Specifies the storage serialization policy for Snowflake-managed Apache Iceberg™ tables ."
        },
        {
            "name": "Values :",
            "description": "COMPATIBLE : Snowflake performs encoding and compression that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression that ensures the best table performance within Snowflake."
        },
        {
            "name": "Default :",
            "description": "OPTIMIZED"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "This parameter specifies whether JSON output in a session is compatible with the general standard (as described by http://json.org ). By design, Snowflake allows JSON input that contains non-standard values; however, these non-standard values might result in Snowflake outputting JSON that is incompatible with other platforms and\nlanguages. This parameter, when enabled, ensures that Snowflake outputs valid/compatible JSON."
        },
        {
            "name": "Values :",
            "description": "TRUE : Strict JSON output is enabled, enforcing the following behavior: Missing and undefined values in input mapped to JSON NULL. Non-finite numeric values in input (Infinity, -Infinity, NaN, etc.) mapped to strings with valid JavaScript representations. This enables compatibility with JavaScript and also allows conversion of\nthese values back to numeric values. FALSE : Strict JSON output is not enabled."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tasks) — Can be set for Account » Database » Schema » Task"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Number of consecutive failed task runs after which a standalone task or task graph root task is suspended automatically. Failed task runs include\nruns in which the SQL code in the task body either produces a user error or times out. Task\nruns that are skipped, canceled, or that fail due to a system error are considered indeterminate\nand are not included in the count of failed task runs. When the parameter is set to 0 , the failed task is not automatically suspended. When the parameter is set to a value greater than 0 , the following behavior applies to\nruns of standalone tasks or task graph root tasks: A standalone task is automatically suspended after the specified number of consecutive task\nruns either fail or time out. A root task is automatically suspended after the specified number of times in consecutive runs\nafter any single task in a task graph fails or times out, after all TASK_AUTO_RETRY_ATTEMPTS for that task. For example, if a root task has SUSPEND_TASK_AFTER_NUM_FAILURES set to 3, and\nit has a child task with TASK_AUTO_RETRY_ATTEMPTS set to 3, then after that child task\nfails 9 consecutive times, the root task is suspended. The default value for the parameter is set to 10 , which means that the task is automatically suspended after 10 consecutive failed task runs. When you explicitly set the parameter value at the account, database, or schema level, the\nchange is applied to tasks contained in the modified object during their next scheduled run\n(including any child task in a task graph run in progress). Suspending a standalone task resets its count of failed task runs. Suspending the root task of a task graph resets the count for each\ntask in the task graph."
        },
        {
            "name": "Values :",
            "description": "0 - No upper limit."
        },
        {
            "name": "Default :",
            "description": "10"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tasks) — Can be set for Account » Database » Schema » Task"
        },
        {
            "name": "Data Type :",
            "description": "Integer"
        },
        {
            "name": "Description :",
            "description": "Specifies the number of automatic task graph retry attempts. If any task graphs complete in a FAILED state, Snowflake\ncan automatically retry the task graphs from the last task in the graph that failed. Failed task runs include runs in which the SQL code in\nthe task body either produces a user error or times out. Task runs that are skipped or canceled are considered indeterminate and are not included in the count of failed task runs. The automatic task graph retry is disabled by default. To enable this feature, set TASK_AUTO_RETRY_ATTEMPTS to a value greater than 0 . When you set the parameter value at the account, database, or schema level, the change is applied to tasks contained in the modified object\nduring their next scheduled run."
        },
        {
            "name": "Values :",
            "description": "0 - No upper limit."
        },
        {
            "name": "Default :",
            "description": "0"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether the DATEADD function (and its aliases) always consider a day to be exactly 24 hours for expressions that span multiple days."
        },
        {
            "name": "Values :",
            "description": "TRUE : A day is always exactly 24 hours. FALSE : A day is not always 24 hours."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the input format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported timestamp format or AUTO ( AUTO specifies that Snowflake attempts to automatically detect the format of timestamps stored in the system during the session)"
        },
        {
            "name": "Default :",
            "description": "AUTO"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the display format for the TIMESTAMP_LTZ data type. If CSV_TIMESTAMP_FORMAT is not set, TIMESTAMP_LTZ_OUTPUT_FORMAT is used when downloading CSV files. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported timestamp format"
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the display format for the TIMESTAMP_NTZ data type. If CSV_TIMESTAMP_FORMAT is not set, TIMESTAMP_NTZ_OUTPUT_FORMAT is used when downloading CSV files. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported timestamp format"
        },
        {
            "name": "Default :",
            "description": "YYYY-MM-DD HH24:MI:SS.FF3"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the display format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported timestamp format"
        },
        {
            "name": "Default :",
            "description": "YYYY-MM-DD HH24:MI:SS.FF3 TZHTZM"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the TIMESTAMP_* variation that the TIMESTAMP data type alias maps to."
        },
        {
            "name": "Values :",
            "description": "TIMESTAMP_LTZ , TIMESTAMP_NTZ , or TIMESTAMP_TZ"
        },
        {
            "name": "Default :",
            "description": "TIMESTAMP_NTZ"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the display format for the TIMESTAMP_TZ data type. If CSV_TIMESTAMP_FORMAT is not set, TIMESTAMP_TZ_OUTPUT_FORMAT is used when downloading CSV files. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported timestamp format"
        },
        {
            "name": "Default :",
            "description": "None"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Specifies the time zone for the session."
        },
        {
            "name": "Values :",
            "description": "You can specify a time zone name or a link name from release 2021a of the IANA Time Zone Database (e.g. America/Los_Angeles , Europe/London , UTC , Etc/GMT , etc.)."
        },
        {
            "name": "Default :",
            "description": "America/Los_Angeles"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the input format for the TIME data type. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported time format or AUTO ( AUTO specifies that Snowflake attempts to automatically detect the format of times stored in the system during the session)"
        },
        {
            "name": "Default :",
            "description": "AUTO"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the display format for the TIME data type. For more information, see Date and time input and output formats ."
        },
        {
            "name": "Values :",
            "description": "Any valid, supported time format"
        },
        {
            "name": "Default :",
            "description": "HH24:MI:SS"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session Object (for databases, schemas, stored procedures, and UDFs) — Can be set for Account » Database » Schema » Procedure and Account » Database » Schema » Function"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Controls how trace events are ingested into the event table. For more information about trace levels, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "Values :",
            "description": "ALWAYS : All spans and trace events will be recorded in the event table. ON_EVENT : Trace events will be recorded in the event table only when your stored procedures or UDFs explicitly add events. OFF : No spans or trace events will be recorded in the event table."
        },
        {
            "name": "Default :",
            "description": "OFF"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "BOOLEAN"
        },
        {
            "name": "Description :",
            "description": "Specifies the action to perform when a statement issued within a non-autocommit transaction returns with an error."
        },
        {
            "name": "Values :",
            "description": "TRUE : The non-autocommit transaction is aborted. All statements issued inside that transaction will fail until a commit or rollback statement is executed to close that transaction. FALSE : The non-autocommit transaction is not aborted."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the isolation level for transactions in the user session."
        },
        {
            "name": "Values :",
            "description": "READ COMMITTED (only currently-supported value)"
        },
        {
            "name": "Default :",
            "description": "READ COMMITTED"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Specifies the “century start” year for 2-digit years (that is, the earliest year such dates can represent). This parameter prevents ambiguous dates when importing or converting data with\nthe YY date format component (years represented as 2 digits)."
        },
        {
            "name": "Values :",
            "description": "1900 to 2100 (any value outside of this range returns an error)"
        },
        {
            "name": "Default :",
            "description": "1970"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "String (Constant)"
        },
        {
            "name": "Description :",
            "description": "Determines if an unsupported (non-default) value specified for a constraint property returns an error."
        },
        {
            "name": "Values :",
            "description": "IGNORE : Snowflake does not return an error for unsupported values. FAIL : Snowflake returns an error for unsupported values."
        },
        {
            "name": "Default :",
            "description": "IGNORE"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Description :",
            "description": "Specifies whether to reuse persisted query results, if available, when a matching query is submitted."
        },
        {
            "name": "Values :",
            "description": "TRUE : When a query is submitted, Snowflake checks for matching query results for previously-executed queries and, if a matching result exists, uses the result instead of executing the\nquery. This can help reduce query time because Snowflake retrieves the result directly from the cache. FALSE : Snowflake executes each query when submitted, regardless of whether a matching query result exists."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tasks) — Can be set for Account » Database » Schema » Task"
        },
        {
            "name": "Data Type :",
            "description": "String"
        },
        {
            "name": "Description :",
            "description": "Specifies the size of the compute resources to provision for the first run of the task, before a task history is available for\nSnowflake to determine an ideal size. Once a task has successfully completed a few runs, Snowflake ignores this parameter setting. If the\ntask history is unavailable for a given task, the compute resources revert to this initial size. Note This parameter applies only to serverless tasks . The size is equivalent to the compute resources available when creating a warehouse. If the parameter is omitted, the first runs of the\ntask are executed using a medium-sized ( MEDIUM ) warehouse. You can change the initial size for individual tasks (using ALTER TASK ) after the task is created but before it has run successfully once. Changing the parameter after the first run of this task starts has no effect on the\ncompute resources for current or future task runs. Note that suspending and resuming a task does not remove the task history used to size the compute resources. The task history is\nonly removed if the task is recreated (using the CREATE OR REPLACE TASK syntax)."
        },
        {
            "name": "Values :",
            "description": "Any traditional warehouse size : XSMALL , SMALL , MEDIUM , LARGE , XLARGE , X2LARGE . The maximum size is X2LARGE . Also supports the syntax: XXLARGE ."
        },
        {
            "name": "Default :",
            "description": "MEDIUM"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tasks) — Can be set for Account » Database » Schema » Task"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Defines how frequently a triggered task can execute in seconds. If data changes occur more often than the specified minimum, changes will be grouped and processed together. The task will run every 12 hours even if this value is set to more than 12 hours."
        },
        {
            "name": "Values :",
            "description": "10 - 604800 (1 week)."
        },
        {
            "name": "Default :",
            "description": "30"
        },
        {
            "name": "Type :",
            "description": "Object (for databases, schemas, and tasks) — Can be set for Account » Database » Schema » Task"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Specifies the time limit on a single run of the task before it times out (in milliseconds). Note Before you increase the time limit for tasks significantly, consider whether the SQL statements in the task definitions could be\noptimized (either by rewriting the statements or using stored procedures) or whether the warehouse size for tasks with user-managed\ncompute resources should be increased. When both STATEMENT_TIMEOUT_IN_SECONDS and USER_TASK_TIMEOUT_MS are set, the timeout is the lowest non-zero value of the two parameters. When both STATEMENT_QUEUED_TIMEOUT_IN_SECONDS and USER_TASK_TIMEOUT_MS are set, the value of USER_TASK_TIMEOUT_MS takes precedence. For more information about USER_TASK_TIMEOUT_MS, see CREATE TASK…USER_TASK_TIMEOUT ."
        },
        {
            "name": "Values :",
            "description": "0 - 604800000 (7 days). A value of 0 specifies that the maximum timeout value is enforced."
        },
        {
            "name": "Default :",
            "description": "3600000 (1 hour)"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Specifies how the weeks in a given year are computed."
        },
        {
            "name": "Values :",
            "description": "0 : The semantics used are equivalent to the ISO semantics, in which a week belongs to a given year if at least 4 days of that week are in that year. 1 : January 1 is included in the first week of the year and December 31 is included in the last week of the year."
        },
        {
            "name": "Default :",
            "description": "0 (ISO-like behavior)"
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Number"
        },
        {
            "name": "Description :",
            "description": "Specifies the first day of the week (used by week-related date functions)."
        },
        {
            "name": "Values :",
            "description": "0 : Legacy Snowflake behavior is used (ISO-like semantics). 1 (Monday) to 7 (Sunday): All the week-related functions use weeks that start on the specified day of the week."
        },
        {
            "name": "Default :",
            "description": "0 (legacy Snowflake behavior)"
        },
        {
            "name": "Account :",
            "description": "Account administrators can use the ALTER ACCOUNT command to set session parameters for the account. The values set for the account default to individual users and\ntheir sessions."
        },
        {
            "name": "User :",
            "description": "Administrators with the appropriate privileges (typically SECURITYADMIN role) can use the ALTER USER command to override session parameters for individual users. The values set for a user default to\nany sessions started by the user. In addition, users can override default sessions parameters for themselves using ALTER USER ."
        },
        {
            "name": "Session :",
            "description": "Users can use the ALTER SESSION to explicitly set session parameters within their sessions."
        },
        {
            "name": "Account :",
            "description": "Account administrators can use the ALTER ACCOUNT command to set object parameters for the account. The values set for the account default to the objects created in\nthe account."
        },
        {
            "name": "Object :",
            "description": "Users with the appropriate privileges can use the corresponding CREATE <object> or ALTER <object> commands to override object parameters for an individual\nobject."
        },
        {
            "name": "Account :",
            "description": "Account administrators can use the ALTER ACCOUNT command to set session parameters for the account. The values set for the account default to individual users and\ntheir sessions."
        },
        {
            "name": "User :",
            "description": "Administrators with the appropriate privileges (typically SECURITYADMIN role) can use the ALTER USER command to override session parameters for individual users. The values set for a user default to\nany sessions started by the user. In addition, users can override default sessions parameters for themselves using ALTER USER ."
        },
        {
            "name": "Session :",
            "description": "Users can use the ALTER SESSION to explicitly set session parameters within their sessions."
        },
        {
            "name": "Account :",
            "description": "Account administrators can use the ALTER ACCOUNT command to set object parameters for the account. The values set for the account default to the objects created in\nthe account."
        },
        {
            "name": "Object :",
            "description": "Users with the appropriate privileges can use the corresponding CREATE <object> or ALTER <object> commands to override object parameters for an individual\nobject."
        },
        {
            "name": "Type :",
            "description": "Session — Can be set for Account » User » Session"
        },
        {
            "name": "Data Type :",
            "description": "Boolean"
        },
        {
            "name": "Clients :",
            "description": "JDBC"
        },
        {
            "name": "Description :",
            "description": "Enables users to log the data values bound to PreparedStatements . To see the values, you must not only set this session-level parameter to TRUE , but also set the\nconnection parameter named TRACING to either INFO or ALL . Set TRACING to ALL to see all debugging information and all binding information. Set TRACING to INFO to see the binding parameter values and less other debug information. Caution If you bind confidential information, such as medical diagnoses or passwords, that information is\nlogged. Snowflake recommends making sure that the log file is secure, or only using test data, when you set\nthis parameter to TRUE ."
        },
        {
            "name": "Values :",
            "description": "TRUE or FALSE ."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-function.html#label-create-or-alter-function-syntax",
    "title": "CREATE FUNCTION",
    "description": "Creates a new UDF (user-defined function). Depending on how you configure it, the function can\nreturn either scalar results or tabular results.",
    "syntax": "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <java_jdk_version> ]\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name_and_version>' [ , ... ] ) ]\n  HANDLER = '<path_to_method>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <java_jdk_version> ]\n  [ COMMENT = '<string_literal>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  HANDLER = '<path_to_method>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVASCRIPT\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ COMMENT = '<string_literal>' ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] [ AGGREGATE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE PYTHON\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  RUNTIME_VERSION = <python_version>\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name>[==<version>]' [ , ... ] ) ]\n  [ ARTIFACT_REPOSITORY = '<repository_name>' ]\n  HANDLER = '<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] [ AGGREGATE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE PYTHON\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  RUNTIME_VERSION = <python_version>\n  [ COMMENT = '<string_literal>' ]\n  [ ARTIFACT_REPOSITORY = '<repository_name>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  [ PACKAGES = ( '<package_name>[==<version>]' [ , ... ] ) ]\n  HANDLER = '<module_file_name>.<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  LANGUAGE SCALA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <scala_version> ]\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name_and_version>' [ , ... ] ) ]\n  HANDLER = '<path_to_method>'\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  LANGUAGE SCALA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <scala_version> ]\n  [ COMMENT = '<string_literal>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  HANDLER = '<path_to_method>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ MEMOIZABLE ]\n  [ COMMENT = '<string_literal>' ]\n  AS '<function_definition>'",
    "examples": [
        {
            "code": "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE JAVA\n  CALLED ON NULL INPUT\n  HANDLER = 'TestFunc.echoVarchar'\n  TARGET_PATH = '@~/testfunc.jar'\n  AS\n  'class TestFunc {\n    public static String echoVarchar(String x) {\n      return x;\n    }\n  }';"
        },
        {
            "code": "create function my_decrement_udf(i numeric(9, 0))\n    returns numeric\n    language java\n    imports = ('@~/my_decrement_udf_package_dir/my_decrement_udf_jar.jar')\n    handler = 'my_decrement_udf_package.my_decrement_udf_class.my_decrement_udf_method'\n    ;"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION js_factorial(d double)\n  RETURNS double\n  LANGUAGE JAVASCRIPT\n  STRICT\n  AS '\n  if (D <= 0) {\n    return 1;\n  } else {\n    var result = 1;\n    for (var i = 2; i <= D; i++) {\n      result = result * i;\n    }\n    return result;\n  }\n  ';"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION py_udf()\n  RETURNS VARIANT\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.10'\n  PACKAGES = ('numpy','pandas','xgboost==1.5.0')\n  HANDLER = 'udf'\nAS $$\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\ndef udf():\n    return [np.__version__, pd.__version__, xgb.__version__]\n$$;"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION dream(i int)\n  RETURNS VARIANT\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.10'\n  HANDLER = 'sleepy.snore'\n  IMPORTS = ('@my_stage/sleepy.py')"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE SCALA\n  RUNTIME_VERSION = 2.12\n  HANDLER='Echo.echoVarchar'\n  AS\n  $$\n  class Echo {\n    def echoVarchar(x : String): String = {\n      return x\n    }\n  }\n  $$;"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE SCALA\n  RUNTIME_VERSION = 2.12\n  IMPORTS = ('@udf_libs/echohandler.jar')\n  HANDLER='Echo.echoVarchar';"
        },
        {
            "code": "CREATE FUNCTION pi_udf()\n  RETURNS FLOAT\n  AS '3.141592654::FLOAT'\n  ;"
        },
        {
            "code": "CREATE FUNCTION simple_table_function ()\n  RETURNS TABLE (x INTEGER, y INTEGER)\n  AS\n  $$\n    SELECT 1, 2\n    UNION ALL\n    SELECT 3, 4\n  $$\n  ;"
        },
        {
            "code": "SELECT * FROM TABLE(simple_table_function());"
        },
        {
            "code": "SELECT * FROM TABLE(simple_table_function());\n+---+---+\n| X | Y |\n|---+---|\n| 1 | 2 |\n| 3 | 4 |\n+---+---+"
        },
        {
            "code": "CREATE FUNCTION multiply1 (a number, b number)\n  RETURNS number\n  COMMENT='multiply two numbers'\n  AS 'a * b';"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION get_countries_for_user ( id NUMBER )\n  RETURNS TABLE (country_code CHAR, country_name VARCHAR)\n  AS 'SELECT DISTINCT c.country_code, c.country_name\n      FROM user_addresses a, countries c\n      WHERE a.user_id = id\n      AND c.country_code = a.country_code';"
        },
        {
            "code": "CREATE OR ALTER FUNCTION multiply(a NUMBER, b NUMBER)\n  RETURNS NUMBER\n  AS 'a * b';"
        },
        {
            "code": "CREATE OR ALTER SECURE FUNCTION multiply(a NUMBER, b NUMBER)\n  RETURNS NUMBER\n  COMMENT = 'Multiply two numbers.'\n  AS 'a * b';"
        }
    ],
    "parameters": [
        {
            "name": "name   (   [   arg_name   arg_data_type   [   DEFAULT   default_value   ]   ]   [   ,   ...   ]   )",
            "description": "Specifies the identifier ( name ), any input arguments, and the default values for any optional arguments for the UDF. For the identifier: The identifier does not need to be unique for the schema in which the function is created because UDFs are identified and resolved by the combination of the name and argument types . The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, “My object”). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements . For the input arguments: For arg_name , specify the name of the input argument. For arg_data_type , use the Snowflake data type that corresponds to the handler language that you are using. For Java handlers , see SQL-Java Data Type Mappings . For JavaScript handlers , see SQL and JavaScript data type mapping . For Python handlers , see SQL-Python Data Type Mappings . For Scala handlers , see SQL-Scala Data Type Mappings . To indicate that an argument is optional, use DEFAULT default_value to specify the default value of the argument.\nFor the default value, you can use a literal or an expression. If you specify any optional arguments, you must place these after the required arguments. If a function has optional arguments, you cannot define additional functions with the same name and different signatures. For details, see Specify optional arguments ."
        },
        {
            "name": "RETURNS   ...",
            "description": "Specifies the results returned by the UDF, which determines the UDF type: result_data_type : Creates a scalar UDF that returns a single value with the specified data type. Note For UDF handlers written in Java, Python, or Scala, the result_data_type must be in the SQL Data Type column of the\nfollowing table corresponding to the handler language: SQL-Java Type Mappings table SQL-Python Type Mappings table SQL-Scala Type Mappings table TABLE ( col_name col_data_type , ... ) : Creates a table UDF that returns tabular results with the specified table column(s)\nand column type(s). Note For Scala UDFs, the TABLE return type is not supported."
        },
        {
            "name": "AS   function_definition",
            "description": "Defines the handler code executed when the UDF is called. The function_definition value must be source code in one of the\nlanguages supported for handlers. The code may be: Java. For more information, see Introduction to Java UDFs . JavaScript. For more information, see Introduction to JavaScript UDFs . Python. For more information, see Introduction to Python UDFs . Scala. For more information, see Introduction to Scala UDFs . A SQL expression. For more information, see Introduction to SQL UDFs . For more details, see General usage notes (in this topic). Note The AS clause is not required when the UDF handler code is referenced on a stage with the IMPORTS clause."
        },
        {
            "name": "LANGUAGE   JAVA",
            "description": "Specifies that the code is in the Java language."
        },
        {
            "name": "RUNTIME_VERSION   =   java_jdk_version",
            "description": "Specifies the Java JDK runtime version to use. The supported versions of Java are: 11.x 17.x If RUNTIME_VERSION is not set, Java JDK 11 is used."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import. A file can be a JAR file or another type of file. If the file is a JAR file, it can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode). Java UDFs can also read non-JAR files. For an example, see Reading a file specified statically in IMPORTS . If you plan to copy a file (JAR file or other file) to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file\nto a stage. External stages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom each file name in the IMPORTS clause, even if the files are in different subdirectories or different stages. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an existing file. For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF. For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files. For Snowflake system packages, such the Snowpark package ,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value. In-line Java In-line Java UDFs require a function definition ."
        },
        {
            "name": "HANDLER   =   handler_name",
            "description": "The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod . If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class."
        },
        {
            "name": "AS   function_definition",
            "description": "In-line Java UDFs require a function definition ."
        },
        {
            "name": "LANGUAGE   JAVASCRIPT",
            "description": "Specifies that the code is in the JavaScript language."
        },
        {
            "name": "LANGUAGE   PYTHON",
            "description": "Specifies that the code is in the Python language."
        },
        {
            "name": "RUNTIME_VERSION   =   python_version",
            "description": "Specifies the Python version to use. The supported versions of Python are: 3.9 3.10 3.11 3.12"
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import. A file can be a .py file or another type of file. Python UDFs can also read non-Python files, such as text files. For an example, see Reading a file . If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a file\nto a stage. External stages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. When the handler code is stored in a stage, you must use the IMPORTS clause to specify the handler code’s location. For an in-line Python UDF, the IMPORTS clause is needed only if the UDF handler needs to access other files, such as\npackages or text files. For packages included on the Snowflake system, such numpy ,\nyou can specify the package with the PACKAGES clause alone, omitting the package’s source as an IMPORTS value."
        },
        {
            "name": "HANDLER   =   handler_name",
            "description": "The name of the handler function or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a function name. If the handler code\nis in-line with the CREATE FUNCTION statement, you can use the function name alone. When the handler code is referenced at a stage, this\nvalue should be qualified with the module name, as in the following form: my_module.my_function . If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class."
        },
        {
            "name": "LANGUAGE   SCALA",
            "description": "Specifies that the code is in the Scala language."
        },
        {
            "name": "RUNTIME_VERSION   =   scala_version",
            "description": "Specifies the Scala runtime version to use. The supported versions of Scala are: 2.12 If RUNTIME_VERSION is not set, Scala 2.12 is used."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import, such as a JAR or other kind of file. The JAR file might contain handler dependency libraries. It can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode). A non-JAR file might a file read by handler code. For an example, see Reading a file specified statically in IMPORTS . If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the PUT command supports\ncopying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file to a stage. External\nstages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different stage subdirectories or different stages. If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom that of any file listed in the IMPORTS clause, even if the files are in different stage subdirectories or different stages. For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF. For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files. For Snowflake system packages, such the Snowpark package ,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value. In-line Scala UDFs with in-line Scala handler code require a function definition ."
        },
        {
            "name": "HANDLER   =   handler_name",
            "description": "The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod ."
        },
        {
            "name": "AS   function_definition",
            "description": "UDFs with in-line Scala handler code require a function definition ."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the function is secure. For more information about secure functions, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   }",
            "description": "Specifies that the function persists only for the duration of the session that you created it in. A\ntemporary function is dropped at the end of the session. Default: No value. If a function is not declared as TEMPORARY , the function is permanent. You cannot create temporary user-defined functions that have the same name as a function that already\nexists in the schema."
        },
        {
            "name": "[   [   NOT   ]   NULL   ]",
            "description": "Specifies whether the function can return NULL values or must return only NON-NULL values. The default is NULL (i.e. the function can\nreturn NULL). Note Currently, the NOT NULL clause is not enforced for SQL UDFs.\nSQL UDFs declared as NOT NULL can return NULL values. Snowflake recommends avoiding NOT NULL for SQL UDFs unless the code in the function is written to ensure that NULL values are never returned."
        },
        {
            "name": "CALLED   ON   NULL   INPUT   or   .   {   RETURNS   NULL   ON   NULL   INPUT   |   STRICT   }",
            "description": "Specifies the behavior of the UDF when called with null inputs. In contrast to system-defined functions, which always return null when any\ninput is null, UDFs can handle null inputs, returning non-null values even when an input is null: CALLED ON NULL INPUT will always call the UDF with null inputs. It is up to the UDF to handle such values appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT ) will not call the UDF if any input is null. Instead, a null value\nwill always be returned for that row. Note that the UDF might still return null for non-null inputs. Note RETURNS NULL ON NULL INPUT ( STRICT ) is not supported for SQL UDFs. SQL UDFs effectively use CALLED ON NULL INPUT . In your SQL UDFs, you must handle null input values. Default: CALLED ON NULL INPUT"
        },
        {
            "name": "{   VOLATILE   |   IMMUTABLE   }",
            "description": "Specifies the behavior of the UDF when returning results: VOLATILE : UDF might return different values for different rows, even for the same input (e.g. due to non-determinism and\nstatefullness). IMMUTABLE : UDF assumes that the function, when called with the same inputs, will always return the same result. This guarantee\nis not checked. Specifying IMMUTABLE for a UDF that returns different values for the same input will result in undefined\nbehavior. Default: VOLATILE Note IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the UDF, which is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output. Default: user-defined function"
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original function when a new function is created using CREATE OR REPLACE FUNCTION. The parameter copies all privileges, except OWNERSHIP, from the existing function to the new function. The new function will\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE FUNCTION\nstatement owns the new function. Note: With data sharing , if the existing function was shared to another account, the replacement function is\nalso shared. The SHOW GRANTS output for the replacement function lists the grantee for the copied privileges as the\nrole that executed the CREATE FUNCTION statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE FUNCTION command (i.e. within the same transaction)."
        },
        {
            "name": "PACKAGES   =   (   ' package_name_and_version '   [   ,   ...   ]   )",
            "description": "The name and version number of Snowflake system packages required as dependencies. The value should be of the form package_name : version_number , where package_name is snowflake_domain : package . Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause. In-line Java Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nfunction’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "AGGREGATE",
            "description": "Specifies that the function is an aggregate function. For more information about user-defined aggregate functions, see Python user-defined aggregate functions . Preview Feature — Open Using Python to write a handler for a user-defined aggregate function (UDAF) is a preview feature that is available to all accounts. Note IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default."
        },
        {
            "name": "PACKAGES   =   (   ' package_name_and_version '   [   ,   ...   ]   )",
            "description": "The name and version number of packages required as dependencies. The value should be of the form package_name == version_number . If you omit the version number, Snowflake will use the latest package available on the\nsystem. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For more information about included packages, see Using third-party packages . Preview Feature — Open Specifying a range of Python package versions is available as a preview feature to all accounts. You can specify package versions by using these version\nspecifiers: == , <= , >= , < ,or > . For example:"
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nfunction’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "MEMOIZABLE",
            "description": "Specifies that the function is memoizable. For details, see Memoizable UDFs ."
        },
        {
            "name": "PACKAGES   =   (   ' package_name_and_version '   [   ,   ...   ]   )",
            "description": "The name and version number of Snowflake system packages required as dependencies. The value should be of the form package_name : version_number , where package_name is snowflake_domain : package . Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause. Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-data-metric-function.html#label-create-or-alter-dmf-function-syntax",
    "title": "CREATE DATA METRIC FUNCTION",
    "description": "Creates a new data metric function (DMF) in the current or specified schema, or replaces an existing data metric function.",
    "syntax": "CREATE [ OR REPLACE ] [ SECURE ] DATA METRIC FUNCTION [ IF NOT EXISTS ] <name>\n  ( <table_arg> TABLE( <col_arg> <data_type> [ , ... ] )\n    [ , <table_arg> TABLE( <col_arg> <data_type> [ , ... ] ) ] )\n  RETURNS NUMBER [ [ NOT ] NULL ]\n  [ LANGUAGE SQL ]\n  [ COMMENT = '<string_literal>' ]\n  AS\n  '<expression>'",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] [ SECURE ] DATA METRIC FUNCTION [ IF NOT EXISTS ] <name>\n  ( <table_arg> TABLE( <col_arg> <data_type> [ , ... ] )\n    [ , <table_arg> TABLE( <col_arg> <data_type> [ , ... ] ) ] )\n  RETURNS NUMBER [ [ NOT ] NULL ]\n  [ LANGUAGE SQL ]\n  [ COMMENT = '<string_literal>' ]\n  AS\n  '<expression>'"
        },
        {
            "code": "CREATE [ OR ALTER ] DATA METRIC FUNCTION ..."
        },
        {
            "code": "CREATE OR REPLACE DATA METRIC FUNCTION governance.dmfs.count_positive_numbers(\n  arg_t TABLE(\n    arg_c1 NUMBER,\n    arg_c2 NUMBER,\n    arg_c3 NUMBER\n  )\n)\nRETURNS NUMBER\nAS\n$$\n  SELECT\n    COUNT(*)\n  FROM arg_t\n  WHERE\n    arg_c1>0\n    AND arg_c2>0\n    AND arg_c3>0\n$$;"
        },
        {
            "code": "CREATE OR REPLACE DATA METRIC FUNCTION governance.dmfs.referential_check(\n  arg_t1 TABLE (arg_c1 INT), arg_t2 TABLE (arg_c2 INT))\nRETURNS NUMBER\nAS\n$$\n  SELECT\n    COUNT(*)\n    FROM arg_t1\n  WHERE\n    arg_c1 NOT IN (SELECT arg_c2 FROM arg_t2)\n$$;"
        },
        {
            "code": "CREATE OR ALTER SECURE DATA METRIC FUNCTION governance.dmfs.count_positive_numbers(\n  arg_t TABLE(\n    arg_c1 NUMBER,\n    arg_c2 NUMBER,\n    arg_c3 NUMBER\n  )\n)\nRETURNS NUMBER\nCOMMENT = \"count positive numbers\"\nAS\n$$\n  SELECT\n    COUNT(*)\n  FROM arg_t\n  WHERE\n    arg_c1>0\n    AND arg_c2>0\n    AND arg_c3>0\n$$;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the DMF; must be unique for your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "(   table_arg   TABLE(   col_arg   data_type   [   ,   ...   ]   )   [   ,   table_arg   TABLE(   col_arg   data_type   [   ,   ...   ]   )   ]   )",
            "description": "The signature for the DMF, which is used as input for the expression. You must specify: An argument name for each table ( table_arg ). For each table, an argument name for at least one column, along with its data type ( col_arg data_type ). You can optionally specify arguments for additional columns and their data types. The columns must be in the same table and cannot\nreference a different table."
        },
        {
            "name": "RETURNS   NUMBER",
            "description": "The data type of the output of the function. The data type can only be NUMBER."
        },
        {
            "name": "AS   expression",
            "description": "SQL expression that determines the output of the function. The expression must be deterministic and return a scalar value. The expression\ncan reference other table objects, such as by using a WITH clause or a WHERE clause. The delimiters around the expression can be either single quotes or a pair of dollar signs. Using $$ as the delimiter makes\nit easier to write expressions that contain single quotes. If the delimiter for the expression is the single quote character, then any single quotes within expression (for example, string literals) must be escaped by single quotes. The expression does not support the following: Using nondeterministic functions (for example, CURRENT_TIME ). Referencing an object that depends on a UDF or UDTF. Returning a nonscalar output."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the data metric function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "LANGUAGE   SQL",
            "description": "Specifies the language used to write the expression. SQL is the only supported language."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "A comment for the DMF."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-schema.html#label-create-or-alter-schema-syntax",
    "title": "CREATE SCHEMA",
    "description": "Creates a new schema in the current database.",
    "syntax": "CREATE [ OR REPLACE ] [ TRANSIENT ] SCHEMA [ IF NOT EXISTS ] <name>\n  [ CLONE <source_schema>\n      [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n      [ IGNORE TABLES WITH INSUFFICIENT DATA RETENTION ]\n      [ IGNORE HYBRID TABLES ] ]\n  [ WITH MANAGED ACCESS ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ EXTERNAL_VOLUME = <external_volume_name> ]\n  [ CATALOG = <catalog_integration_name> ]\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n  [ CLASSIFICATION_PROFILE = '<classification_profile>' ]\n  [ COMMENT = '<string_literal>' ]\n  [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]",
    "examples": [
        {
            "code": "CREATE SCHEMA myschema;\n\nSHOW SCHEMAS;\n\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+---------+----------------+\n| created_on                    | name               | is_default | is_current | database_name | owner        | comment                                                   | options | retention_time |\n|-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+---------+----------------|\n| 2018-12-10 09:34:02.127 -0800 | INFORMATION_SCHEMA | N          | N          | MYDB          |              | Views describing the contents of schemas in this database |         | 1              |\n| 2018-12-10 09:33:56.793 -0800 | MYSCHEMA           | N          | Y          | MYDB          | PUBLIC       |                                                           |         | 1              |\n| 2018-11-26 06:08:24.263 -0800 | PUBLIC             | N          | N          | MYDB          | PUBLIC       |                                                           |         | 1              |\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+---------+----------------+"
        },
        {
            "code": "CREATE TRANSIENT SCHEMA tschema;\n\nSHOW SCHEMAS;\n\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+-----------+----------------+\n| created_on                    | name               | is_default | is_current | database_name | owner        | comment                                                   | options   | retention_time |\n|-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+-----------+----------------|\n| 2018-12-10 09:34:02.127 -0800 | INFORMATION_SCHEMA | N          | N          | MYDB          |              | Views describing the contents of schemas in this database |           | 1              |\n| 2018-12-10 09:33:56.793 -0800 | MYSCHEMA           | N          | Y          | MYDB          | PUBLIC       |                                                           |           | 1              |\n| 2018-11-26 06:08:24.263 -0800 | PUBLIC             | N          | N          | MYDB          | PUBLIC       |                                                           |           | 1              |\n| 2018-12-10 09:35:32.326 -0800 | TSCHEMA            | N          | Y          | MYDB          | PUBLIC       |                                                           | TRANSIENT | 1              |\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+-----------+----------------+"
        },
        {
            "code": "CREATE SCHEMA mschema WITH MANAGED ACCESS;\n\nSHOW SCHEMAS;\n\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+----------------+----------------+\n| created_on                    | name               | is_default | is_current | database_name | owner        | comment                                                   | options        | retention_time |\n|-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+----------------+----------------|\n| 2018-12-10 09:34:02.127 -0800 | INFORMATION_SCHEMA | N          | N          | MYDB          |              | Views describing the contents of schemas in this database |                | 1              |\n| 2018-12-10 09:36:47.738 -0800 | MSCHEMA            | N          | Y          | MYDB          | ROLE1        |                                                           | MANAGED ACCESS | 1              |\n| 2018-12-10 09:33:56.793 -0800 | MYSCHEMA           | N          | Y          | MYDB          | PUBLIC       |                                                           |                | 1              |\n| 2018-11-26 06:08:24.263 -0800 | PUBLIC             | N          | N          | MYDB          | PUBLIC       |                                                           |                | 1              |\n| 2018-12-10 09:35:32.326 -0800 | TSCHEMA            | N          | Y          | MYDB          | PUBLIC       |                                                           | TRANSIENT      | 1              |\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+----------------+----------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the schema; must be unique for the database in which the schema is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TRANSIENT",
            "description": "Specifies a schema as transient. Transient schemas do not have a Fail-safe period so they do not incur additional storage costs once\nthey leave Time Travel; however, this means they are also not protected by Fail-safe in the event of a data loss. For more information,\nsee Understanding and viewing Fail-safe . In addition, by definition, all tables created in a transient schema are transient. For more information about transient tables, see CREATE TABLE . Default: No value (i.e. schema is permanent)"
        },
        {
            "name": "CLONE   source_schema",
            "description": "Specifies to create a clone of the specified source schema. For more details about cloning a schema, see CREATE <object> … CLONE ."
        },
        {
            "name": "AT   |   BEFORE   (   TIMESTAMP   =>   timestamp   |   OFFSET   =>   time_difference   |   STATEMENT   =>   id   )",
            "description": "When cloning a schema, the AT | BEFORE clause specifies to use Time Travel to clone the schema at or\nbefore a specific point in the past."
        },
        {
            "name": "IGNORE   TABLES   WITH   INSUFFICIENT   DATA   RETENTION",
            "description": "Ignore tables that no longer have historical data available in Time Travel to clone. If the time in the past specified in the\nAT | BEFORE clause is beyond the data retention period for any child table in a database or schema, skip the cloning operation\nfor the child table. For more information, see Child Objects and Data Retention Time ."
        },
        {
            "name": "IGNORE   HYBRID   TABLES",
            "description": "Ignore hybrid tables, which will not be cloned. Use this option to clone a schema that contains hybrid tables.\nThe cloned schema includes other objects but skips hybrid tables. If you don’t use this option and your schema contains one or more hybrid tables, the command ignores hybrid tables silently. However, the error handling for schemas that contain hybrid tables will change in an upcoming release; therefore, you may want to add this parameter to your commands preemptively."
        },
        {
            "name": "WITH   MANAGED   ACCESS",
            "description": "Specifies a managed schema. Managed access schemas centralize privilege management with the schema owner. In regular schemas, the owner of an object (i.e. the role that has the OWNERSHIP privilege on the object) can grant further privileges\non their objects to other roles. In managed schemas, the schema owner manages all privilege grants, including future grants , on objects in the schema. Object owners retain the OWNERSHIP\nprivileges on the objects; however, only the schema owner can manage privilege grants on the objects."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the schema, as well as specifying the\ndefault Time Travel retention time for all tables created in the schema. For more details, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . For more information about table-level retention time, see CREATE TABLE and Understanding & using Time Travel . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent schemas 0 or 1 for transient schemas Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the database or account level) Note A value of 0 effectively disables Time Travel for the schema."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in\nthe schema to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "EXTERNAL_VOLUME   =   external_volume_name",
            "description": "Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME ."
        },
        {
            "name": "CATALOG   =   catalog_integration_name",
            "description": "Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG ."
        },
        {
            "name": "REPLACE_INVALID_CHARACTERS   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for all tables added to the schema. The default\ncan be overridden at the individual table level. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "STORAGE_SERIALIZATION_POLICY   =   {   COMPATIBLE   |   OPTIMIZED   }",
            "description": "Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED"
        },
        {
            "name": "CLASSIFICATION_PROFILE   =   ' classification_profile '",
            "description": "Associates the schema with a classification profile so that sensitive data in the schema is automatically classified ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the schema. Default: No value"
        },
        {
            "name": "CATALOG_SYNC   =   ' snowflake_open_catalog_integration_name '",
            "description": "Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the schema with an external catalog in your Snowflake Open Catalog account.\nFor more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-application-role.html#label-create-or-alter-app-role-syntax",
    "title": "CREATE APPLICATION ROLE",
    "description": "Creates a new application role or replaces an existing application role. Use application roles to\nenable access control security for objects within an application object.",
    "syntax": "CREATE [ OR REPLACE ] APPLICATION ROLE [ IF NOT EXISTS ] <name>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE APPLICATION ROLE app_role\n  COMMENT = 'Application role for the Hello Snowflake application.';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the application role. This value must be unique within the application object\nin which the role is created. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. If the identifier is not fully qualified, in the form of application_name . application_role_name , the command creates the\napplication role in the current application for the session. For more details, see Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the application role. Default: No value"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-task.html#label-create-or-alter-task-syntax",
    "title": "CREATE TASK",
    "description": "Creates a new task in the current/specified schema or replaces an existing task.",
    "syntax": "CREATE [ OR REPLACE ] TASK [ IF NOT EXISTS ] <name>\n    [ WITH TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n    [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n    [ { WAREHOUSE = <string> }\n      | { USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = <string> } ]\n    [ SCHEDULE = { '<num> { HOURS | MINUTES | SECONDS }'\n      | 'USING CRON <expr> <time_zone>' } ]\n    [ CONFIG = <configuration_string> ]\n    [ ALLOW_OVERLAPPING_EXECUTION = TRUE | FALSE ]\n    [ <session_parameter> = <value>\n      [ , <session_parameter> = <value> ... ] ]\n    [ USER_TASK_TIMEOUT_MS = <num> ]\n    [ SUSPEND_TASK_AFTER_NUM_FAILURES = <num> ]\n    [ ERROR_INTEGRATION = <integration_name> ]\n    [ SUCCESS_INTEGRATION = <integration_name> ]\n    [ LOG_LEVEL = '<log_level>' ]\n    [ COMMENT = '<string_literal>' ]\n    [ FINALIZE = <string> ]\n    [ TASK_AUTO_RETRY_ATTEMPTS = <num> ]\n    [ USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS = <num> ]\n    [ TARGET_COMPLETION_INTERVAL = '<num> { HOURS | MINUTES | SECONDS }' ]\n    [ SERVERLESS_TASK_MIN_STATEMENT_SIZE = '{ XSMALL | SMALL\n      | MEDIUM | LARGE | XLARGE | XXLARGE }' ]\n    [ SERVERLESS_TASK_MAX_STATEMENT_SIZE = '{ XSMALL | SMALL\n      | MEDIUM | LARGE | XLARGE | XXLARGE }' ]\n  [ AFTER <string> [ , <string> , ... ] ]\n  [ WHEN <boolean_expr> ]\n  AS\n    <sql>",
    "examples": [
        {
            "code": "CREATE TASK t1\n  SCHEDULE = 'USING CRON 0 9-17 * * SUN America/Los_Angeles'\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  AS\n    SELECT CURRENT_TIMESTAMP;"
        },
        {
            "code": "CREATE TASK mytask_hour\n  WAREHOUSE = mywh\n  SCHEDULE = 'USING CRON 0 9-17 * * SUN America/Los_Angeles'\n  AS\n    SELECT CURRENT_TIMESTAMP;"
        },
        {
            "code": "CREATE TASK t1\n  SCHEDULE = '60 MINUTES'\n  TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD HH24'\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  AS\n    INSERT INTO mytable(ts) VALUES(CURRENT_TIMESTAMP);"
        },
        {
            "code": "CREATE TASK mytask_minute\n  WAREHOUSE = mywh\n  SCHEDULE = '5 MINUTES'\n  AS\n    INSERT INTO mytable(ts) VALUES(CURRENT_TIMESTAMP);"
        },
        {
            "code": "CREATE TASK mytask1\n  WAREHOUSE = mywh\n  SCHEDULE = '5 MINUTES'\n  WHEN\n    SYSTEM$STREAM_HAS_DATA('MYSTREAM')\n  AS\n    INSERT INTO mytable1(id,name) SELECT id, name FROM mystream WHERE METADATA$ACTION = 'INSERT';"
        },
        {
            "code": "-- Create task5 and specify task2, task3, task4 as predecessors tasks.\n-- The new task is a serverless task that inserts the current timestamp into a table column.\nCREATE TASK task5\n  AFTER task2, task3, task4\nAS\n  INSERT INTO t1(ts) VALUES(CURRENT_TIMESTAMP);"
        },
        {
            "code": "-- Create a stored procedure that unloads data from a table\n-- The COPY statement in the stored procedure unloads data to files in a path identified by epoch time (using the Date.now() method)\nCREATE OR REPLACE PROCEDURE my_unload_sp()\n  returns string not null\n  language javascript\n  AS\n    $$\n      var my_sql_command = \"\"\n      var my_sql_command = my_sql_command.concat(\"copy into @mystage\",\"/\",Date.now(),\"/\",\" from mytable overwrite=true;\");\n      var statement1 = snowflake.createStatement( {sqlText: my_sql_command} );\n      var result_set1 = statement1.execute();\n    return my_sql_command; // Statement returned for info/debug purposes\n    $$;\n\n-- Create a task that calls the stored procedure every hour\nCREATE TASK my_copy_task\n  WAREHOUSE = mywh\n  SCHEDULE = '60 MINUTES'\n  AS\n    CALL my_unload_sp();"
        },
        {
            "code": "!set sql_delimiter=/\nCREATE OR REPLACE TASK test_logging\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  SCHEDULE = 'USING CRON  0 * * * * America/Los_Angeles'\n  AS\n    BEGIN\n      ALTER SESSION SET TIMESTAMP_OUTPUT_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';\n      SELECT CURRENT_TIMESTAMP;\n    END;/\n!set sql_delimiter=';'"
        },
        {
            "code": "CREATE TASK t1\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  SCHEDULE = '15 SECONDS'\n  AS\n    EXECUTE IMMEDIATE\n    $$\n    DECLARE\n      radius_of_circle float;\n      area_of_circle float;\n    BEGIN\n      radius_of_circle := 3;\n      area_of_circle := pi() * radius_of_circle * radius_of_circle;\n      return area_of_circle;\n    END;\n    $$;"
        },
        {
            "code": "CREATE OR REPLACE TASK root_task_with_config\n  WAREHOUSE=mywarehouse\n  SCHEDULE='10 m'\n  CONFIG=$${\"output_dir\": \"/temp/test_directory/\", \"learning_rate\": 0.1}$$\n  AS\n    BEGIN\n      LET OUTPUT_DIR STRING := SYSTEM$GET_TASK_GRAPH_CONFIG('output_dir')::string;\n      LET LEARNING_RATE DECIMAL := SYSTEM$GET_TASK_GRAPH_CONFIG('learning_rate')::DECIMAL;\n    ...\n    END;"
        },
        {
            "code": "CREATE TASK finalize_task\n  WAREHOUSE = my_warehouse\n  FINALIZE = my_root_task\n  AS\n    CALL SYSTEM$SEND_EMAIL(\n      'my_email_int',\n      'first.last@example.com, first2.last2@example.com',\n      'Email Alert: Task A has finished.',\n      'Task A has successfully finished.\\nStart Time: 10:10:32\\nEnd Time: 12:15:45\\nTotal Records Processed: 115678'\n    );"
        },
        {
            "code": "CREATE TASK triggeredTask  WAREHOUSE = my_warehouse\n  WHEN system$stream_has_data('my_stream')\n  AS\n    INSERT INTO my_downstream_table\n    SELECT * FROM my_stream;\n\nALTER TASK triggeredTask RESUME;"
        },
        {
            "code": "CREATE OR ALTER TASK my_task\n  WAREHOUSE = my_warehouse\n  SCHEDULE = '60 MINUTES'\n  AS\n    SELECT PI();"
        },
        {
            "code": "CREATE OR ALTER TASK my_task\n  WAREHOUSE = regress\n  AFTER my_other_task\n  AS\n    SELECT 2 * PI();"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier for the task; must be unique for the schema in which the task is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes, such as \"My object\" . Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "sql",
            "description": "Any one of the following: Single SQL statement Call to a stored procedure Procedural logic using Snowflake Scripting The SQL code is executed when the task runs. Verify that the {sql} executes as expected before using it in a task."
        },
        {
            "name": "WAREHOUSE   =   string",
            "description": "Specifies the virtual warehouse that provides compute resources for task runs. Omit this parameter to use serverless compute resources for runs of this task. Snowflake automatically resizes and scales serverless\ncompute resources as required for each workload. When a schedule is specified for a task, Snowflake adjusts the resource size to\ncomplete future runs of the task within the specified time frame. To specify the initial warehouse size for the task, set the USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = string parameter."
        },
        {
            "name": "USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE   =   string",
            "description": "Applied only to serverless tasks. Specifies the size of the compute resources to provision for the first run of the task, before a task history is available for\nSnowflake to determine an ideal size. Once a task has successfully completed a few runs, Snowflake ignores this parameter setting. Note that if the task history is unavailable for a given task, the compute resources revert to this initial size. Note If a WAREHOUSE = string parameter value is specified, then setting this parameter produces a user error. The size is equivalent to the compute resources available when creating a warehouse (using CREATE WAREHOUSE ), such as SMALL , MEDIUM , or LARGE . The largest size supported by the parameter\nis XXLARGE . If the parameter is omitted, the first runs of the task are executed using a medium-sized ( MEDIUM ) warehouse. You can change the initial size (using ALTER TASK ) after the task is created but before it has run successfully once. Changing the parameter after the first run of this task starts has no effect on the\ncompute resources for current or future task runs. Note that suspending and resuming a task doesn’t remove the task history used to size the compute resources. The task history is\nonly removed if the task is recreated (using the CREATE OR REPLACE TASK syntax). For more information about this parameter, see USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE ."
        },
        {
            "name": "SCHEDULE   =   ...",
            "description": "Specifies the schedule for periodically running the task: Note For Triggered tasks , a schedule is not required. For other tasks, a schedule must be defined for a standalone task or the root task in a task graph ;\notherwise, the task only runs if manually executed using EXECUTE TASK . A schedule cannot be specified for child tasks in a task graph. Specifies a cron expression and time zone for periodically running the task. Supports a subset of standard cron utility syntax. ' expr ' : The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the task is scheduled for April, July, and October, which is every 3 months, starting with the\n4th month of the year. The same schedule is maintained in subsequent years. That is, the task is not scheduled to run in\nJanuary (3 months after the October run). Timing examples: SCHEDULE Value Description * * * * * UTC Every minute. UTC time zone. 0/5 * * * * UTC Every five minutes, starting at the top of the hour. UTC time zone. 5 * * * * UTC The 5th minute of every hour. UTC time zone. 30 3 * * * UTC Every night at 3:30 a.m. UTC time zone. 0 6,18 * * * UTC Twice daily, at 6:00 a.m. and 6:00 p.m.UTC time zone. 0 3 * * MON-FRI UTC Weekdays at 3:00 a.m. UTC time zone. 0 0 1 * * UTC At midnight on the first day of every month. UTC time zone. 0 0 L * * UTC At midnight on the last day of every month. UTC time zone. Note The cron expression defines all valid run times for the task. Snowflake attempts to run a task based on this schedule;\nhowever, any valid run time is skipped if a previous run hasn’t completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the task is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a task at midnight on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. To set a task to run in a shorter interval, use the SCHEDULE = ' <num> SECONDS' parameter instead. For example, SCHEDULE = '10 SECONDS' runs the task every 10 seconds. If a task is resumed during the minute defined in its cron expression,\nthe first scheduled run of the task is the next occurrence of the instance of the cron expression. For example, if task\nscheduled to run daily at midnight ( USING CRON 0 0 * * * ) is resumed at midnight plus 5 seconds ( 00:00:05 ), the\nfirst task run is scheduled for the following midnight. ' time_zone ' : The cron time zone for the task. The time zone is specified as a string literal. For a list of time zones, see the list of tz database time zones (in Wikipedia). Example: SCHEDULE Value Description 0 3 * * * America/Los_Angeles Every night at 3:00 a.m., Pacific Standard Time / Pacific Daylight Time (PST/PDT) time zone Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value for the account (or setting the value at the user or session level) does not change the time zone for the task. For time zones that observe daylight saving time, tasks scheduled during daylight saving time transitions can have unexpected behaviors. Examples: During the change from daylight saving time to standard time, a task scheduled to start at 1:00 a.m. in the America/Los_Angeles time zone ( 0 1 * * * America/Los_Angeles ) would run twice: at 1:00 a.m., and then again when 1:59:59 a.m. shifts to 1:00:00 a.m. local time. During the change from standard time to daylight saving time, a task scheduled to start at 2:00 a.m. in the America/Los_Angeles time zone ( 0 2 * * * America/Los_Angeles ) would not run because the local time shifts from 1:59:59 a.m. to 3:00:00 a.m. To avoid unexpected task executions due to daylight saving time, consider the following: Don’t schedule tasks to start between 1:00 a.m. and 2:59 a.m. Manually adjust the cron expression for tasks scheduled between 1 a.m. and 3 a.m. twice each year to compensate for the time change. Use a time format that does not apply daylight saving time, such as UTC. Do not change the time zone for the task. Specifies an interval of wait time between runs of the task. Snowflake sets the base interval time when the task is resumed ( ALTER TASK … RESUME ) or when a different interval is set ( ALTER TASK … SET SCHEDULE ). For example, if an INTERVAL value of 10 MINUTES is set and the task is enabled at 9:03 a.m., then the task runs at 9:13 a.m., 9:23 a.m., and\nso on. Snowflake ensures that a task won’t run before the set interval; however, Snowflake can’t guarantee task runs at precisely the specified interval. Values: { 10 - 691200 } SECONDS , { 1 - 11520 } MINUTES , or { 1-192 } HOURS (That is, from 10 seconds to the equivalent of 8 days). Accepts positive integers only. Also supports the notations: HOUR, MINUTE, SECOND, and H, M, S."
        },
        {
            "name": "CONFIG   =   configuration_string",
            "description": "Specifies a string representation of key value pairs that can be accessed by all tasks in the task graph. Must be in JSON format. For more information about getting the configuration string for the task that is currently running, see SYSTEM$GET_TASK_GRAPH_CONFIG . Note This parameter can only be set on a root task. The setting applies to all tasks in the task graph. The parameter can be set on standalone tasks but doesn’t affect the task behavior. Snowflake ensures only one instance of a\nstandalone task is running at a given time."
        },
        {
            "name": "ALLOW_OVERLAPPING_EXECUTION   =   TRUE   |   FALSE",
            "description": "Specifies whether to allow multiple instances of the task graph to run concurrently. Note This parameter can only be set on a root task. The setting applies to all tasks in the task graph. The parameter can be set on standalone tasks but doesn’t affect the task behavior. Snowflake ensures only one instance of a\nstandalone task is running at a given time. TRUE If the next scheduled run of the root task occurs while the current run of any child task is still in operation, another\ninstance of the task graph begins. If a root task is still running when the next scheduled run time occurs, then that scheduled time is\nskipped. FALSE The next run of a root task is scheduled only after all child tasks in the task graph have finished running. This means\nthat if the cumulative time required to run all tasks in the task graph exceeds the explicit scheduled time set in the definition of\nthe root task, at least one run of the Task Graph is skipped. Default: FALSE"
        },
        {
            "name": "session_parameter   =   value   [   ,   session_parameter   =   value   ...   ]",
            "description": "Specifies a comma-separated list of session parameters to set for the session when the task runs. A task supports all session\nparameters. For the complete list, see Session parameters ."
        },
        {
            "name": "'USING   CRON   expr   time_zone '",
            "description": "Specifies a cron expression and time zone for periodically running the task. Supports a subset of standard cron utility syntax."
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/ n",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the task is scheduled for April, July, and October, which is every 3 months, starting with the\n4th month of the year. The same schedule is maintained in subsequent years. That is, the task is not scheduled to run in\nJanuary (3 months after the October run)."
        },
        {
            "name": "' num   {   HOURS   |   MINUTES   |   SECONDS   }'",
            "description": "Specifies an interval of wait time between runs of the task. Snowflake sets the base interval time when the task is resumed ( ALTER TASK … RESUME ) or when a different interval is set ( ALTER TASK … SET SCHEDULE ). For example, if an INTERVAL value of 10 MINUTES is set and the task is enabled at 9:03 a.m., then the task runs at 9:13 a.m., 9:23 a.m., and\nso on. Snowflake ensures that a task won’t run before the set interval; however, Snowflake can’t guarantee task runs at precisely the specified interval. Values: { 10 - 691200 } SECONDS , { 1 - 11520 } MINUTES , or { 1-192 } HOURS (That is, from 10 seconds to the equivalent of 8 days). Accepts positive integers only. Also supports the notations: HOUR, MINUTE, SECOND, and H, M, S."
        },
        {
            "name": "USER_TASK_TIMEOUT_MS   =   num",
            "description": "Specifies the time limit on a single run of the task before it times out (in milliseconds). Note Before you increase the time limit on a task significantly, consider whether the SQL statement initiated by the task could be\noptimized (either by rewriting the statement or using a stored procedure) or the warehouse size should be increased. When both STATEMENT_TIMEOUT_IN_SECONDS and USER_TASK_TIMEOUT_MS are set, the timeout is the lowest non-zero value of the two parameters. When both STATEMENT_QUEUED_TIMEOUT_IN_SECONDS and USER_TASK_TIMEOUT_MS are set, the value of USER_TASK_TIMEOUT_MS takes precedence. For more information about this parameter, see USER_TASK_TIMEOUT_MS . Values: 0 - 604800000 (7 days). A value of 0 specifies that the maximum timeout value is enforced. Default: 3600000 (1 hour)"
        },
        {
            "name": "SUSPEND_TASK_AFTER_NUM_FAILURES   =   num",
            "description": "Specifies the number of consecutive failed task runs after which the current task is suspended automatically. Failed task runs\ninclude runs in which the SQL code in the task body either produces a user error or times out. Task runs that are skipped,\ncanceled, or that fail due to a system error are considered indeterminate and aren’t included in the count of failed task runs. Set the parameter on a standalone task or the root task in a task graph. When the parameter is set to a value greater than 0 , the\nfollowing behavior applies to runs of the standalone task or task graph: Standalone tasks are automatically suspended after the specified number of consecutive task runs either fail or time out. The root task is automatically suspended after the run of any single task in a task graph fails or times out the specified\nnumber of times in consecutive runs. When the parameter is set to 0 , failed tasks aren’t automatically suspended. The setting applies to tasks that rely on either serverless compute resources or virtual warehouse compute resources. For more information about this parameter, see SUSPEND_TASK_AFTER_NUM_FAILURES . Values: 0 - No upper limit. Default: 10"
        },
        {
            "name": "ERROR_INTEGRATION   =   ' integration_name '",
            "description": "Required only when configuring a task to send error notifications using Amazon Simple Notification Service (SNS), Microsoft Azure Event Grid, or Google Pub/Sub. Specifies the name of the notification integration used to communicate with Amazon SNS, MS Azure Event Grid, or Google Pub/Sub. For more information, see Enabling notifications for tasks ."
        },
        {
            "name": "SUCCESS_INTEGRATION   =   ' integration_name '",
            "description": "Required only when configuring a task to send success notifications using Amazon Simple Notification Service (SNS), Microsoft Azure Event Grid, or Google Pub/Sub. Specifies the name of the notification integration used to communicate with Amazon SNS, MS Azure Event Grid, or Google Pub/Sub. For more information, see Enabling notifications for tasks ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of events for this task that are ingested and made available in\nthe active event table. Events at the specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the task. Default: No value"
        },
        {
            "name": "AFTER   string   [   ,   string   ,   ...   ]",
            "description": "Specifies one or more predecessor tasks for the current task. Use this option to create a task graph or\nadd this task to an existing task graph. A task graph is a series of tasks that starts with a scheduled root task and is linked together\nby dependencies. Note that the structure of a task graph can be defined after all of its component tasks are created. Execute ALTER TASK … ADD AFTER statements to specify the predecessors for each task in the planned task graph. A task runs after all of its predecessor tasks have finished their own runs successfully (after a brief lag). Note The root task should have a defined schedule. Each child task must have one or more defined predecessor tasks, specified\nusing the AFTER parameter, to link the tasks together. A single task is limited to 100 predecessor tasks and 100 child tasks. In addition, a task graph is limited to a maximum of 1000 tasks total (including the root task) in either a resumed or suspended state. Accounts are currently limited to a maximum of 30000 resumed tasks. All tasks in a task graph must have the same task owner. A single role must have the OWNERSHIP privilege on all of the tasks in\nthe task graph. All tasks in a task graph must exist in the same schema. The root task must be suspended before any task is recreated (using the CREATE OR REPLACE TASK syntax) or a child task\nis added (using CREATE TASK … AFTER or ALTER TASK … ADD AFTER) or removed (using ALTER TASK … REMOVE AFTER). If any task in a task graph is cloned, the role that clones the task becomes the owner of the clone by default. If the owner of the original task creates the clone, then the task clone retains the link between the task and the predecessor\ntask. This means the same predecessor task triggers both the original task and the task clone. If another role creates the clone, then the task clone can have a schedule but not a predecessor. Current limitations: Snowflake guarantees that at most one instance of a task with a defined schedule is running at a given time; however, we cannot\nprovide the same guarantee for tasks with a defined predecessor task."
        },
        {
            "name": "WHEN   boolean_expr",
            "description": "Specifies a Boolean SQL expression; multiple conditions joined with AND/OR are supported. When a task is triggered (based on its SCHEDULE or AFTER setting), it validates the conditions of the expression to determine whether to execute. If the\nconditions of the expression are not met, then the task skips the current run. Any tasks that identify this task as a\npredecessor also don’t run. The following are supported in a task WHEN clause: SYSTEM$STREAM_HAS_DATA is supported for evaluation in the SQL expression. This function indicates whether a specified stream contains change tracking data. You can use this function to evaluate whether the specified stream contains\nchange data before starting the current run. If the result is FALSE, then the task doesn’t run. Note SYSTEM$STREAM_HAS_DATA is designed to avoid returning a FALSE value even when the stream contains\nchange data. However, this function isn’t guaranteed to avoid returning a TRUE value when the stream contains no change data. SYSTEM$GET_PREDECESSOR_RETURN_VALUE is supported for evaluation in the SQL expression. This function retrieves the return value for the predecessor task in a task graph.  The return value can be used as part of\na boolean expression.  When using SYSTEM$GET_PREDECESSOR_RETURN_VALUE, you can cast the returned value to\nthe appropriate numeric, string, or boolean type if required. Simple examples include: Note Use of PARSE_JSON in TASK … WHEN expressions isn’t supported as it requires warehouse based compute resources. Boolean operators such as AND, OR, NOT, and others. Simple example that runs whenever data changes in either of two streams: Casts between numeric, string, and boolean types. Comparison operators such as equal, not equal, greater than, less than, and others. Validating the conditions of the WHEN expression does not require compute resources. The validation is instead processed in the cloud\nservices layer. A nominal charge accrues each time a task evaluates its WHEN condition and doesn’t run. The charges accumulate each time\nthe task is triggered until it runs. At that time, the charge is converted to Snowflake credits and added to the compute resource usage\nfor the task run. Generally the compute time to validate the condition is insignificant compared to task execution time. As a best practice, align\nscheduled and actual task runs as closely as possible. Avoid task schedules that don’t align with task runs. For\nexample, if data is inserted into a table with a stream roughly every 24 hours, don’t schedule a task that checks for stream data\nevery minute. The charge to validate the WHEN expression with each run is generally insignificant, but the charges are cumulative. Note that daily consumption of cloud services that falls below the 10% quota of the daily usage of the compute resources accumulates no cloud services charges."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        },
        {
            "name": "FINALIZE   =   string",
            "description": "Specifies the name of a root task that the finalizer task is associated with. Finalizer tasks run after all other tasks in the task graph run to completion. You can define the SQL of a finalizer task to handle notifications and the release and cleanup of resources that a task graph uses. For more information, see Finalizer task . A root task can only have one finalizer task. If you create multiple finalizer tasks for a root task, the task creation will fail. A finalizer task cannot have any child tasks. Any command attempting to make the finalizer task a predecessor will fail. A finalizer task cannot have a schedule. Creating a finalizer task with a schedule will fail. Default: No value"
        },
        {
            "name": "TASK_AUTO_RETRY_ATTEMPTS   =   num",
            "description": "Specifies the number of automatic task graph retry attempts. If any task graphs complete in a FAILED state, Snowflake can automatically\nretry the task graphs from the last task in the graph that failed. The automatic task graph retry is disabled by default. To enable this feature, set TASK_AUTO_RETRY_ATTEMPTS to a value greater than 0 on the root task of a task graph. Note that this parameter must be set to the root task of a task graph. If it’s set to a child task, an error will be returned. Values: 0 - 30 . Default: 0"
        },
        {
            "name": "USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS   =   num",
            "description": "Defines how frequently a task can execute in seconds. If data changes occur more often than the specified minimum, changes will be\ngrouped and processed together. The task will run every 12 hours even if this value is set to more than 12 hours. Values: Minimum 10 , maximum 604800 . Default: 30"
        },
        {
            "name": "TARGET_COMPLETION_INTERVAL   =   ' num   {   HOURS   |   MINUTES   |   SECONDS   }'",
            "description": "Specifies the desired task completion time. This parameter only applies to serverless tasks. This property is only set on a Task. This parameter is required when you create serverless Triggered tasks . Values: { 10 - 86400 } SECONDS , { 1 - 1440 } MINUTES , or { 1-24 } HOURS (That is, from 10 seconds to the equivalent of 1 day). Accepts positive integers only. Also supports the notations: HOUR, MINUTE, SECOND, and H, M, S. Default: Snowflake resizes serverless compute resources to complete before the next scheduled execution time."
        },
        {
            "name": "SERVERLESS_TASK_MIN_STATEMENT_SIZE   =   string",
            "description": "Specifies the minimum allowed warehouse size for the serverless task. This parameter only applies to serverless tasks. This parameter can be specified on the Task, Schema, Database, or Account. Precedence follows the standard parameter hierarchy. Values: Minimum XSMALL , Maximum XXLARGE . Values are consistent with WAREHOUSE_SIZE values . Also supports the notation: X2LARGE. Default: XSMALL Note that if both SERVERLESS_TASK_MIN_STATEMENT_SIZE and USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE are specified, SERVERLESS_TASK_MIN_STATEMENT_SIZE must be equal to or smaller than USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE."
        },
        {
            "name": "SERVERLESS_TASK_MAX_STATEMENT_SIZE   =   string",
            "description": "Specifies the maximum allowed warehouse size for the serverless task. This parameter only applies to serverless tasks. This parameter can be specified on the Task, Schema, Database, or Account. Precedence follows the standard parameter hierarchy. Values: Minimum XSMALL , Maximum XXLARGE . Also supports the notation: X2LARGE. Default: XXLARGE Note that if both SERVERLESS_TASK_MIN_STATEMENT_SIZE and SERVERLESS_TASK_MAX_STATEMENT_SIZE are specified, SERVERLESS_TASK_MIN_STATEMENT_SIZE must be less than or equal to SERVERLESS_TASK_MAX_STATEMENT_SIZE. SERVERLESS_TASK_MAX_STATEMENT_SIZE must be equal to or greater than USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE"
        }
    ],
    "usage_notes": "Tasks run using the task owner’s privileges. For the list of minimum required privileges to run tasks, see\nTask security.\nRun the SQL statement or call the stored procedure, as the task owner role, before you include it in a task definition to ensure\nthe role has the required privileges on objects referenced by the SQL or stored procedure.\nFor serverless tasks:\nServerless compute resources for a task can range from the equivalent of XSMALL to XXLARGE in warehouse sizes. To request a\nsize increase, contact Snowflake Support.\nIndividual tasks in a task graph can use serverless or user-managed compute resources. Using the serverless compute for\nall tasks in the task graph isn’t required.\nIf a task fails with an unexpected error, you can receive a notification about the error.\nFor more information on configuring task error notifications, see Enabling notifications for tasks.\nBy default, a DML statement executed without explicitly starting a transaction is automatically committed on success or rolled back on\nfailure at the end of the statement. This behavior is called autocommit and is controlled with the AUTOCOMMIT parameter.\nThis parameter must be set to TRUE. If the AUTOCOMMIT parameter is set to FALSE at the account level, then set the parameter to\nTRUE for the individual task (using ALTER TASK … SET AUTOCOMMIT = TRUE); otherwise, any DML statement executed by the task fails.\nOnly one task should consume data from a stream. Create multiple streams for the same table to be consumed by more than one task. When a\ntask consumes the data in a stream using a DML statement, the stream advances the offset and change data is no longer available for the\nnext task to consume.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-authentication-policy.html#label-create-or-alter-auth-policy-syntax",
    "title": "CREATE AUTHENTICATION POLICY",
    "description": "Creates a new authentication policy in the current or specified schema or replaces\nan existing authentication policy. You can use authentication policies to define authentication controls and security requirements\nfor accounts or users.",
    "syntax": "CREATE [ OR REPLACE ] AUTHENTICATION POLICY [ IF NOT EXISTS ] <name>\n  [ AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_ENROLLMENT = { REQUIRED | OPTIONAL } ]\n  [ MFA_POLICY= ( ALLOWED_METHODS = ( { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } [ , { 'PASSKEY' | 'TOTP' | 'DUO' } ... ] ) ) ]\n  [ CLIENT_TYPES = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ SECURITY_INTEGRATIONS = ( '<string_literal>' [ , '<string_literal>' , ... ] ) ]\n  [ PAT_POLICY = ( {list_of_properties} ) ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] AUTHENTICATION POLICY [ IF NOT EXISTS ] <name>\n  [ AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_ENROLLMENT = { REQUIRED | OPTIONAL } ]\n  [ MFA_POLICY= ( ALLOWED_METHODS = ( { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } [ , { 'PASSKEY' | 'TOTP' | 'DUO' } ... ] ) ) ]\n  [ CLIENT_TYPES = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ SECURITY_INTEGRATIONS = ( '<string_literal>' [ , '<string_literal>' , ... ] ) ]\n  [ PAT_POLICY = ( {list_of_properties} ) ]\n  [ COMMENT = '<string_literal>' ]"
        },
        {
            "code": "CREATE OR ALTER AUTHENTICATION POLICY <name>\n  [ AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_ENROLLMENT = { REQUIRED | OPTIONAL } ]\n  [ MFA_POLICY= ( ALLOWED_METHODS = ( { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } [ , { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } ... ] ) ) ]\n  [ CLIENT_TYPES = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ SECURITY_INTEGRATIONS = ( '<string_literal>' [ , '<string_literal>' , ... ] ) ]\n  [ PAT_POLICY = ( {list_of_properties} ) ]\n  [ COMMENT = '<string_literal>' ]"
        },
        {
            "code": "PAT_POLICY=(\n  DEFAULT_EXPIRY_IN_DAYS=30\n  MAX_EXPIRY_IN_DAYS=365\n  NETWORK_POLICY_EVALUATION = ENFORCED_NOT_REQUIRED\n);"
        },
        {
            "code": "CREATE AUTHENTICATION POLICY restrict_client_types_policy\n  CLIENT_TYPES = ('SNOWFLAKE_UI')\n  COMMENT = 'Auth policy that only allows access through the web interface';"
        },
        {
            "code": "CREATE OR ALTER AUTHENTICATION POLICY restrict_client_types_policy\n  MFA_ENROLLMENT = REQUIRED\n  MFA_AUTHENTICATION_METHODS = ('PASSWORD', 'SAML')\n  CLIENT_TYPES = ('SNOWFLAKE_UI', 'SNOWFLAKE_CLI');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the authentication policy. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "AUTHENTICATION_METHODS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "Caution Restricting by authentication method can have unintended consequences, such as blocking driver connections or third-party integrations. A list of authentication methods that are allowed during login. This parameter accepts one or more of the following values: Allow all authentication methods. Allows SAML2 security integrations . If SAML is\npresent, an SSO login option appears. If SAML is not present, an SSO login option does not appear. Allows users to authenticate using username and password. Allows External OAuth . Allows Key pair authentication . Allows users to authenticate with a programmatic access token . Default: ALL ."
        },
        {
            "name": "ALL",
            "description": "Allow all authentication methods."
        },
        {
            "name": "SAML",
            "description": "Allows SAML2 security integrations . If SAML is\npresent, an SSO login option appears. If SAML is not present, an SSO login option does not appear."
        },
        {
            "name": "PASSWORD",
            "description": "Allows users to authenticate using username and password."
        },
        {
            "name": "OAUTH",
            "description": "Allows External OAuth ."
        },
        {
            "name": "KEYPAIR",
            "description": "Allows Key pair authentication ."
        },
        {
            "name": "PROGRAMMATIC_ACCESS_TOKEN",
            "description": "Allows users to authenticate with a programmatic access token ."
        },
        {
            "name": "MFA_AUTHENTICATION_METHODS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "A list of authentication methods that enforce multi-factor authentication (MFA) during login. Authentication methods not listed in this\nparameter do not prompt for multi-factor authentication. The following authentication methods support MFA: SAML PASSWORD This parameter accepts one or more of the following values: Prompts users for MFA, if they are enrolled in MFA, when authenticating with SAML2 security integrations . Prompts users for MFA, if they are enrolled in MFA, when authenticating with a username and password. Default: ('PASSWORD') ."
        },
        {
            "name": "MFA_ENROLLMENT   =   {   REQUIRED   |   OPTIONAL   }",
            "description": "Determines whether a user must enroll in multi-factor authentication. Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI , because Snowsight is the only place users can enroll in multi-factor authentication (MFA) . Users can choose whether to enroll in MFA. Default: REQUIRED ."
        },
        {
            "name": "MFA_POLICY=   (   ALLOWED_METHODS   =   (   {   'ALL'   |   'PASSKEY'   |   'TOTP'   |   'DUO'   }   [   ,   {   'PASSKEY'   |   'TOTP'   |   'DUO'   }   ...   ]   )   )",
            "description": "Specifies the multi-factor authentication (MFA) methods that users can use as a second factor of authentication. You can specify more than one method. Users can use a passkey, an authenticator app, or Duo as their second factor of authentication. Users can use a passkey as their second factor of authentication. Users can use an authenticator app as their second factor of authentication. Users can use Duo as their second factor of authentication. Default: ALL ."
        },
        {
            "name": "CLIENT_TYPES   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "A list of clients that can authenticate with Snowflake. If a client tries to connect, and the client is not one of the valid CLIENT_TYPES values listed below, then the login attempt fails. If you set MFA_ENROLLMENT to REQUIRED , then you must include SNOWFLAKE_UI in the CLIENT_TYPES list to allow\nusers to enroll in MFA. If you want to exclude SNOWFLAKE_UI from the CLIENT_TYPES list, then you must set MFA_ENROLLMENT to OPTIONAL . The CLIENT_TYPES property of an authentication policy is a best effort method to block user logins based on specific clients. It should not be used as the sole control to establish a security boundary. This parameter accepts one or more of the following values: Allow all clients to authenticate. Snowsight or Classic Console , the Snowflake web interfaces. Caution If SNOWFLAKE_UI is not included in the CLIENT_TYPES list while MFA_ENROLLMENT is set to REQUIRED , or MFA_ENROLLMENT is unspecified, MFA enrollment doesn’t work. Drivers allow access to Snowflake from applications written in supported languages . For example, the Go , JDBC , .NET drivers, and Snowpipe Streaming . Caution If DRIVERS is not included in the CLIENT_TYPES list, automated ingestion may stop working. A command-line client for connecting to Snowflake and for managing developer-centric workloads and SQL operations. A command-line client for connecting to Snowflake. Default: ALL ."
        },
        {
            "name": "SECURITY_INTEGRATIONS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "A list of security integrations the authentication policy is associated with. This parameter has no effect when SAML or OAUTH are not in the AUTHENTICATION_METHODS list. All values in the SECURITY_INTEGRATIONS list must be compatible with the values in the AUTHENTICATION_METHODS list. For\nexample, if SECURITY_INTEGRATIONS contains a SAML security integration, and AUTHENTICATION_METHODS contains OAUTH , then you cannot create the authentication policy. Allow all security integrations. Default: ALL ."
        },
        {
            "name": "SAML",
            "description": "Prompts users for MFA, if they are enrolled in MFA, when authenticating with SAML2 security integrations ."
        },
        {
            "name": "PASSWORD",
            "description": "Prompts users for MFA, if they are enrolled in MFA, when authenticating with a username and password."
        },
        {
            "name": "REQUIRED",
            "description": "Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI , because Snowsight is the only place users can enroll in multi-factor authentication (MFA) ."
        },
        {
            "name": "OPTIONAL",
            "description": "Users can choose whether to enroll in MFA."
        },
        {
            "name": "ALL",
            "description": "Users can use a passkey, an authenticator app, or Duo as their second factor of authentication."
        },
        {
            "name": "PASSKEY",
            "description": "Users can use a passkey as their second factor of authentication."
        },
        {
            "name": "TOTP",
            "description": "Users can use an authenticator app as their second factor of authentication."
        },
        {
            "name": "DUO",
            "description": "Users can use Duo as their second factor of authentication."
        },
        {
            "name": "ALL",
            "description": "Allow all clients to authenticate."
        },
        {
            "name": "SNOWFLAKE_UI",
            "description": "Snowsight or Classic Console , the Snowflake web interfaces. Caution If SNOWFLAKE_UI is not included in the CLIENT_TYPES list while MFA_ENROLLMENT is set to REQUIRED , or MFA_ENROLLMENT is unspecified, MFA enrollment doesn’t work."
        },
        {
            "name": "DRIVERS",
            "description": "Drivers allow access to Snowflake from applications written in supported languages . For example, the Go , JDBC , .NET drivers, and Snowpipe Streaming . Caution If DRIVERS is not included in the CLIENT_TYPES list, automated ingestion may stop working."
        },
        {
            "name": "SNOWFLAKE_CLI",
            "description": "A command-line client for connecting to Snowflake and for managing developer-centric workloads and SQL operations."
        },
        {
            "name": "SNOWSQL",
            "description": "A command-line client for connecting to Snowflake."
        },
        {
            "name": "ALL",
            "description": "Allow all security integrations."
        },
        {
            "name": "PAT_POLICY   =   (   list_of_properties   )",
            "description": "Specifies the policies for programmatic access tokens . Set this to a\nspace-delimited list of one or more of the following properties and values: Specifies the default expiration time (in days) for a programmatic access token. You can specify a value from 1 to the\nmaximum time (which you can specify by setting MAX_EXPIRY_IN_DAYS). The default expiration time is 15 days. For more information, see Setting the default expiration time . Specifies the maximum number of days that can be set for the expiration time for a programmatic access token. You can specify\na value from 1 to 365. The default maximum expiration time is 365 days. Note If there are existing programmatic access tokens with expiration times that exceed the new maximum expiration time, attempts to\nauthenticate with those tokens will fail. For example, suppose that you generate a programmatic access token named my_token with the expiration time of 7 days. If you\nlater change the maximum expiration time for all tokens to 2 days, authenticating with my_token will fail because the\nexpiration time of the token exceeds the new maximum expiration time. For more information, see Setting the maximum expiration time . Specifies how network policy requirements are handled for programmatic access tokens. By default, a user must be subject to a network policy with one or more network rules to generate or use programmatic access tokens: Service users (with TYPE=SERVICE) must be subject to a network policy to generate and use programmatic access tokens. Human users (with TYPE=PERSON) must be subject to a network policy to use programmatic access tokens. To override this behavior, set this property to one of the following values: The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication. For example:"
        },
        {
            "name": "DEFAULT_EXPIRY_IN_DAYS   =   number_of_days",
            "description": "Specifies the default expiration time (in days) for a programmatic access token. You can specify a value from 1 to the\nmaximum time (which you can specify by setting MAX_EXPIRY_IN_DAYS). The default expiration time is 15 days. For more information, see Setting the default expiration time ."
        },
        {
            "name": "MAX_EXPIRY_IN_DAYS   =   number_of_days",
            "description": "Specifies the maximum number of days that can be set for the expiration time for a programmatic access token. You can specify\na value from 1 to 365. The default maximum expiration time is 365 days. Note If there are existing programmatic access tokens with expiration times that exceed the new maximum expiration time, attempts to\nauthenticate with those tokens will fail. For example, suppose that you generate a programmatic access token named my_token with the expiration time of 7 days. If you\nlater change the maximum expiration time for all tokens to 2 days, authenticating with my_token will fail because the\nexpiration time of the token exceeds the new maximum expiration time. For more information, see Setting the maximum expiration time ."
        },
        {
            "name": "NETWORK_POLICY_EVALUATION   =   {   ENFORCED_REQUIRED   |   ENFORCED_NOT_REQUIRED   |   NOT_ENFORCED   }",
            "description": "Specifies how network policy requirements are handled for programmatic access tokens. By default, a user must be subject to a network policy with one or more network rules to generate or use programmatic access tokens: Service users (with TYPE=SERVICE) must be subject to a network policy to generate and use programmatic access tokens. Human users (with TYPE=PERSON) must be subject to a network policy to use programmatic access tokens. To override this behavior, set this property to one of the following values: The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication."
        },
        {
            "name": "ENFORCED_REQUIRED  (default behavior)",
            "description": "The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication."
        },
        {
            "name": "ENFORCED_NOT_REQUIRED",
            "description": "The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication."
        },
        {
            "name": "NOT_ENFORCED",
            "description": "The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a description of the policy."
        }
    ],
    "usage_notes": "After creating an authentication policy, you must use the ALTER ACCOUNT or\nALTER USER command to set it on an account or user before Snowflake enforces the policy.\nIf you want to update an existing authentication policy and need to see the definition of the policy, run the\nDESCRIBE AUTHENTICATION POLICY command or GET_DDL function.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-database-role.html#label-create-or-alter-db-role-syntax",
    "title": "CREATE DATABASE ROLE",
    "description": "Create a new database role or replace an existing database role in the system.",
    "syntax": "CREATE [ OR REPLACE ] DATABASE ROLE [ IF NOT EXISTS ] <name>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE DATABASE ROLE d1.dr1;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the database role; must be unique in the database in which the role is created. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. If the identifier is not fully qualified in the form of db_name . database_role_name , the command creates the database role\nin the current database for the session. For more details, see Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the database role. Default: No value"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-stage.html#label-create-or-alter-stage-syntax",
    "title": "CREATE STAGE",
    "description": "Creates a new named internal or external stage to use for loading data from files into Snowflake tables and unloading data from\ntables into files:",
    "syntax": "-- Internal stage\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] STAGE [ IF NOT EXISTS ] <internal_stage_name>\n    internalStageParams\n    directoryTableParams\n  [ FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM } [ formatTypeOptions ] } ) ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n\n-- External stage\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] STAGE [ IF NOT EXISTS ] <external_stage_name>\n    externalStageParams\n    directoryTableParams\n  [ FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM } [ formatTypeOptions ] } ) ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n\ninternalStageParams ::=\n  [ ENCRYPTION = (   TYPE = 'SNOWFLAKE_FULL'\n                   | TYPE = 'SNOWFLAKE_SSE' ) ]\n\nexternalStageParams (for Amazon S3) ::=\n  URL = '<protocol>://<bucket>[/<path>/]'\n  [ AWS_ACCESS_POINT_ARN = '<string>' ]\n  [ { STORAGE_INTEGRATION = <integration_name> } | { CREDENTIALS = ( {  { AWS_KEY_ID = '<string>' AWS_SECRET_KEY = '<string>' [ AWS_TOKEN = '<string>' ] } | AWS_ROLE = '<string>'  } ) } ]\n  [ ENCRYPTION = ( [ TYPE = 'AWS_CSE' ] MASTER_KEY = '<string>'\n                   | TYPE = 'AWS_SSE_S3'\n                   | TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = '<string>' ]\n                   | TYPE = 'NONE' ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\nexternalStageParams (for Google Cloud Storage) ::=\n  URL = 'gcs://<bucket>[/<path>/]'\n  [ STORAGE_INTEGRATION = <integration_name> ]\n  [ ENCRYPTION = (   TYPE = 'GCS_SSE_KMS' [ KMS_KEY_ID = '<string>' ]\n                   | TYPE = 'NONE' ) ]\n\nexternalStageParams (for Microsoft Azure) ::=\n  URL = 'azure://<account>.blob.core.windows.net/<container>[/<path>/]'\n  [ { STORAGE_INTEGRATION = <integration_name> } | { CREDENTIALS = ( [ AZURE_SAS_TOKEN = '<string>' ] ) } ]\n  [ ENCRYPTION = (   TYPE = 'AZURE_CSE' MASTER_KEY = '<string>'\n                   | TYPE = 'NONE' ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\nexternalStageParams (for Amazon S3-compatible Storage) ::=\n  URL = 's3compat://{bucket}[/{path}/]'\n  ENDPOINT = '<s3_api_compatible_endpoint>'\n  [ { CREDENTIALS = ( AWS_KEY_ID = '<string>' AWS_SECRET_KEY = '<string>' ) } ]\n\ndirectoryTableParams (for internal stages) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ AUTO_REFRESH = { TRUE | FALSE } ] ) ]\n\ndirectoryTableParams (for Amazon S3) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n                  [ AUTO_REFRESH = { TRUE | FALSE } ] ) ]\n\ndirectoryTableParams (for Google Cloud Storage) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ AUTO_REFRESH = { TRUE | FALSE } ]\n                  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n                  [ NOTIFICATION_INTEGRATION = '<notification_integration_name>' ] ) ]\n\ndirectoryTableParams (for Microsoft Azure) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n                  [ AUTO_REFRESH = { TRUE | FALSE } ]\n                  [ NOTIFICATION_INTEGRATION = '<notification_integration_name>' ] ) ]\n\nformatTypeOptions ::=\n-- If TYPE = CSV\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     FILE_EXTENSION = '<string>'\n     PARSE_HEADER = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     ESCAPE = '<character>' | NONE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n     ENCODING = '<string>' | UTF8\n-- If TYPE = JSON\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     TRIM_SPACE = TRUE | FALSE\n     MULTI_LINE = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     FILE_EXTENSION = '<string>'\n     ENABLE_OCTAL = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n-- If TYPE = AVRO\n     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = ORC\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = PARQUET\n     COMPRESSION = AUTO | LZO | SNAPPY | NONE\n     SNAPPY_COMPRESSION = TRUE | FALSE\n     BINARY_AS_TEXT = TRUE | FALSE\n     USE_LOGICAL_TYPE = TRUE | FALSE\n     TRIM_SPACE = TRUE | FALSE\n     USE_VECTORIZED_SCANNER = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = XML\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     PRESERVE_SPACE = TRUE | FALSE\n     STRIP_OUTER_ELEMENT = TRUE | FALSE\n     DISABLE_AUTO_CONVERT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE",
    "examples": [
        {
            "code": "CREATE STAGE my_int_stage\n  ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');"
        },
        {
            "code": "CREATE TEMPORARY STAGE my_temp_int_stage;"
        },
        {
            "code": "CREATE TEMPORARY STAGE my_int_stage\n  FILE_FORMAT = my_csv_format;"
        },
        {
            "code": "CREATE STAGE mystage\n  DIRECTORY = (ENABLE = TRUE)\n  FILE_FORMAT = myformat;"
        },
        {
            "code": "CREATE STAGE my_ext_stage\n  URL='s3://load/files/'\n  STORAGE_INTEGRATION = myint;"
        },
        {
            "code": "CREATE STAGE my_ext_stage1\n  URL='s3://load/files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z');"
        },
        {
            "code": "CREATE STAGE my_ext_stage2\n  URL='s3://load/encrypted_files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z')\n  ENCRYPTION=(MASTER_KEY = 'eSx...');"
        },
        {
            "code": "CREATE STAGE my_ext_stage3\n  URL='s3://load/encrypted_files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z')\n  ENCRYPTION=(TYPE='AWS_SSE_KMS' KMS_KEY_ID = 'aws/key');"
        },
        {
            "code": "CREATE STAGE my_ext_stage3\n  URL='s3://load/encrypted_files/'\n  CREDENTIALS=(AWS_ROLE='arn:aws:iam::001234567890:role/mysnowflakerole')\n  ENCRYPTION=(TYPE='AWS_SSE_KMS' KMS_KEY_ID = 'aws/key');"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='s3://load/files/'\n  STORAGE_INTEGRATION = my_storage_int\n  DIRECTORY = (\n    ENABLE = true\n    AUTO_REFRESH = true\n  );"
        },
        {
            "code": "CREATE STAGE my_ext_stage\n  URL='gcs://load/files/'\n  STORAGE_INTEGRATION = myint;"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='gcs://load/files/'\n  STORAGE_INTEGRATION = my_storage_int\n  DIRECTORY = (\n    ENABLE = true\n    AUTO_REFRESH = true\n    NOTIFICATION_INTEGRATION = 'MY_NOTIFICATION_INT'\n  );"
        },
        {
            "code": "CREATE STAGE my_ext_stage2\n  URL='gcs://load/encrypted_files/'\n  STORAGE_INTEGRATION = my_storage_int\n  ENCRYPTION=(TYPE = 'GCS_SSE_KMS' KMS_KEY_ID = '{a1b2c3});"
        },
        {
            "code": "CREATE STAGE my_ext_stage\n  URL='azure://myaccount.blob.core.windows.net/load/files/'\n  STORAGE_INTEGRATION = myint;"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='azure://myaccount.blob.core.windows.net/mycontainer/files/'\n  CREDENTIALS=(AZURE_SAS_TOKEN='?sv=2016-05-31&ss=b&srt=sco&sp=rwdl&se=2018-06-27T10:05:50Z&st=2017-06-27T02:05:50Z&spr=https,http&sig=bgqQwoXwxzuD2GJfagRg7VOS8hzNr3QLT7rhS8OFRLQ%3D')\n  ENCRYPTION=(TYPE='AZURE_CSE' MASTER_KEY = 'kPx...');"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='azure://myaccount.blob.core.windows.net/load/files/'\n  STORAGE_INTEGRATION = my_storage_int\n  DIRECTORY = (\n    ENABLE = true\n    AUTO_REFRESH = true\n    NOTIFICATION_INTEGRATION = 'MY_NOTIFICATION_INT'\n  );"
        },
        {
            "code": "CREATE OR ALTER STAGE my_int_stage\n  COMMENT='my_comment'\n  ;"
        },
        {
            "code": "CREATE OR ALTER STAGE my_int_stage\n  DIRECTORY=(ENABLE=true);"
        },
        {
            "code": "CREATE OR ALTER STAGE my_ext_stage\n  URL='s3://load/files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z');"
        },
        {
            "code": "CREATE OR ALTER STAGE my_ext_stage\n  URL='s3://load/files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z')\n  DIRECTORY=(ENABLE=true);"
        }
    ],
    "parameters": [
        {
            "name": "internal_stage_name   or   .   external_stage_name",
            "description": "Specifies the identifier for the stage; must be unique for the schema in which the stage is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   }",
            "description": "Specifies that the stage created is temporary and will be dropped at the end of the session in which it was created. Note: When a temporary external stage is dropped, only the stage itself is dropped; the data files are not removed. When a temporary internal stage is dropped, all of the files in the stage are purged from Snowflake, regardless of their load status.\nThis prevents files in temporary internal stages from using data storage and, consequently, accruing storage charges. However, this also\nmeans that the staged files cannot be recovered through Snowflake once the stage is dropped. Tip If you plan to create and use temporary internal stages, you should maintain copies of your data files outside of Snowflake."
        },
        {
            "name": "FILE_FORMAT   =   (   FORMAT_NAME   =   ' file_format_name '   )   or   .   FILE_FORMAT   =   (   TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   |   CUSTOM   [   ...   ]   )",
            "description": "Specifies the file format for the stage, which can be either: Specifies an existing named file format to use for the stage. The named file format determines the format type (CSV, JSON, etc.), as\nwell as any other format options, for the data files loaded using this stage. For more details, see CREATE FILE FORMAT . Specifies the type of files for the stage: Loading data from a stage (using COPY INTO <table> ) accommodates all of the supported format types. Unloading data into a stage (using COPY INTO <location> ) accommodates CSV , JSON , or PARQUET . If a file format type is specified, additional format-specific options can be specified. For more details, see Format type options (formatTypeOptions) (in this topic). The CUSTOM format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option. Default: TYPE = CSV"
        },
        {
            "name": "FORMAT_NAME   =   ' file_format_name '",
            "description": "Specifies an existing named file format to use for the stage. The named file format determines the format type (CSV, JSON, etc.), as\nwell as any other format options, for the data files loaded using this stage. For more details, see CREATE FILE FORMAT ."
        },
        {
            "name": "TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   |   CUSTOM   [   ...   ]",
            "description": "Specifies the type of files for the stage: Loading data from a stage (using COPY INTO <table> ) accommodates all of the supported format types. Unloading data into a stage (using COPY INTO <location> ) accommodates CSV , JSON , or PARQUET . If a file format type is specified, additional format-specific options can be specified. For more details, see Format type options (formatTypeOptions) (in this topic). The CUSTOM format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the stage. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "[   ENCRYPTION   =   (   TYPE   =   'SNOWFLAKE_FULL'   |   TYPE   =   'SNOWFLAKE_SSE'   )   ]",
            "description": "Specifies the type of encryption supported for all files stored on the stage. You cannot change the encryption type after you create the stage. Specifies the encryption type used. Important If you require Tri-Secret Secure for security compliance, use the SNOWFLAKE_FULL encryption type for internal stages. SNOWFLAKE_SSE does not support Tri-Secret Secure. Possible values are: SNOWFLAKE_FULL : Client-side and server-side encryption. The files are encrypted by a client when it uploads them to the internal stage\nusing PUT . Snowflake uses a 128-bit encryption key by default. You can configure a 256-bit key by setting the CLIENT_ENCRYPTION_KEY_SIZE parameter. All files are also automatically encrypted using AES-256 strong encryption on the server side. SNOWFLAKE_SSE : Server-side encryption only. The files are encrypted when they arrive on the stage by the cloud service\nwhere your Snowflake account is hosted. Specify server-side encryption if you plan to query pre-signed URLs for your staged files. For more information, see Types of URLs available to access files . Default: SNOWFLAKE_FULL"
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Important If you require Tri-Secret Secure for security compliance, use the SNOWFLAKE_FULL encryption type for internal stages. SNOWFLAKE_SSE does not support Tri-Secret Secure. Possible values are: SNOWFLAKE_FULL : Client-side and server-side encryption. The files are encrypted by a client when it uploads them to the internal stage\nusing PUT . Snowflake uses a 128-bit encryption key by default. You can configure a 256-bit key by setting the CLIENT_ENCRYPTION_KEY_SIZE parameter. All files are also automatically encrypted using AES-256 strong encryption on the server side. SNOWFLAKE_SSE : Server-side encryption only. The files are encrypted when they arrive on the stage by the cloud service\nwhere your Snowflake account is hosted. Specify server-side encryption if you plan to query pre-signed URLs for your staged files. For more information, see Types of URLs available to access files ."
        },
        {
            "name": "URL   =   ' cloud_specific_url '",
            "description": "If this parameter is omitted, Snowflake creates an internal stage Important Enclose the URL in single quotes ( '' ) in order for Snowflake to identify the string. If the quotes are omitted, any credentials\nyou supply may be displayed in plain text in the history. We strongly recommend verifying the syntax of the CREATE STAGE statement\nbefore you execute it. When you create a stage in the Snowflake web interface, the interface automatically encloses field values in quotation characters,\nas needed. Append a forward slash ( / ) to the URL to filter to the specified folder path. If the forward slash is omitted, all files and\nfolders starting with the prefix for the specified path are included. Note that the forward slash is required to access and retrieve unstructured data files in the stage. Amazon S3 Specifies the URL for the external location (existing S3 bucket) used to store data files for loading/unloading, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . Accessing cloud storage in a government region using a storage integration is limited to Snowflake\naccounts hosted in the same government region. Similarly, if you need to access cloud storage in a region in China, you can use a storage integration only from a Snowflake\naccount hosted in the same region in China. In these cases, use the CREDENTIALS parameter in the CREATE STAGE command (rather than using a storage\nintegration) to provide the credentials for authentication. bucket is the name of the S3 bucket or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the AWS_ACCESS_POINT_ARN parameter. path is an optional case-sensitive path for files in the cloud storage location (files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage URL . Google Cloud Storage Specifies the URL for the external location (existing GCS bucket) used to store data files for loading/unloading, where: bucket is the name of the GCS bucket. path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Microsoft Azure Specifies the URL for the external location (existing Azure container) used to store data files for loading, where: account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all\nsupported types of Azure blob storage accounts, including Data Lake Storage Gen2. Note that currently, accessing Azure blob storage in government regions using a storage\nintegration is limited to Snowflake accounts hosted on Azure in the same government region. Accessing your blob storage from an\naccount hosted outside of the government region using direct credentials is supported. container is the name of the Azure container (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Default: No value (an internal stage is created)"
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name   or   .   CREDENTIALS   =   (   cloud_specific_credentials   )",
            "description": "Required only if the storage location is private/protected; not required for public buckets/containers Amazon S3 Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing S3 storage in government regions using a storage integration is limited to Snowflake accounts hosted on AWS in\nthe same government region. Accessing your S3 storage from an account hosted outside of the government region using direct\ncredentials is supported. Specifies the security credentials for connecting to AWS and accessing the private/protected S3 bucket where the files to\nload/unload are staged. For more information, see Configuring secure access to Amazon S3 . The credentials you specify depend on whether you associated the Snowflake access permissions for the bucket with an AWS IAM\n(Identity & Access Management) user or role: IAM user: IAM credentials are required. Temporary (aka “scoped”) credentials are generated by AWS Security Token Service\n(STS) and consist of three components: AWS_KEY_ID AWS_SECRET_KEY AWS_TOKEN All three are required to access a private/protected bucket. After a designated period of time, temporary credentials\nexpire and can no longer be used. You must then generate a new set of valid temporary credentials. Important The COPY command also allows permanent (aka “long-term”) credentials to be used; however, for security reasons, Snowflake does not recommend using them. If you must use permanent credentials, Snowflake recommends periodically generating new\npermanent credentials for external stages. IAM role: Omit the security credentials and access keys and, instead, identify the role using AWS_ROLE and specify\nthe AWS role ARN (Amazon Resource Name). Google Cloud Storage Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Microsoft Azure Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing Azure blob storage in government regions using a storage integration is limited to Snowflake accounts hosted on Azure in the\nsame government region. Accessing your blob storage from an account hosted outside\nof the government region using direct credentials is supported. Specifies the SAS (shared access signature) token for connecting to Azure and accessing the private/protected container\nwhere the files containing loaded data are staged. Credentials are generated by Azure. Default: No value (no credentials are provided for the external stage)"
        },
        {
            "name": "URL   =   ' protocol :// bucket [/ path /]'",
            "description": "Specifies the URL for the external location (existing S3 bucket) used to store data files for loading/unloading, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . Accessing cloud storage in a government region using a storage integration is limited to Snowflake\naccounts hosted in the same government region. Similarly, if you need to access cloud storage in a region in China, you can use a storage integration only from a Snowflake\naccount hosted in the same region in China. In these cases, use the CREDENTIALS parameter in the CREATE STAGE command (rather than using a storage\nintegration) to provide the credentials for authentication. bucket is the name of the S3 bucket or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the AWS_ACCESS_POINT_ARN parameter. path is an optional case-sensitive path for files in the cloud storage location (files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "AWS_ACCESS_POINT_ARN   =   ' string '",
            "description": "Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage URL ."
        },
        {
            "name": "URL   =   'gcs:// bucket [/ path /]'",
            "description": "Specifies the URL for the external location (existing GCS bucket) used to store data files for loading/unloading, where: bucket is the name of the GCS bucket. path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "URL   =   'azure:// account .blob.core.windows.net/ container [/ path /]'",
            "description": "Specifies the URL for the external location (existing Azure container) used to store data files for loading, where: account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all\nsupported types of Azure blob storage accounts, including Data Lake Storage Gen2. Note that currently, accessing Azure blob storage in government regions using a storage\nintegration is limited to Snowflake accounts hosted on Azure in the same government region. Accessing your blob storage from an\naccount hosted outside of the government region using direct credentials is supported. container is the name of the Azure container (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing S3 storage in government regions using a storage integration is limited to Snowflake accounts hosted on AWS in\nthe same government region. Accessing your S3 storage from an account hosted outside of the government region using direct\ncredentials is supported."
        },
        {
            "name": "CREDENTIALS   =   (   AWS_KEY_ID   =   ' string '   AWS_SECRET_KEY   =   ' string '   [   AWS_TOKEN   =   ' string '   ]   )   or   .   CREDENTIALS   =   (   AWS_ROLE   =   ' string '   )",
            "description": "Specifies the security credentials for connecting to AWS and accessing the private/protected S3 bucket where the files to\nload/unload are staged. For more information, see Configuring secure access to Amazon S3 . The credentials you specify depend on whether you associated the Snowflake access permissions for the bucket with an AWS IAM\n(Identity & Access Management) user or role: IAM user: IAM credentials are required. Temporary (aka “scoped”) credentials are generated by AWS Security Token Service\n(STS) and consist of three components: AWS_KEY_ID AWS_SECRET_KEY AWS_TOKEN All three are required to access a private/protected bucket. After a designated period of time, temporary credentials\nexpire and can no longer be used. You must then generate a new set of valid temporary credentials. Important The COPY command also allows permanent (aka “long-term”) credentials to be used; however, for security reasons, Snowflake does not recommend using them. If you must use permanent credentials, Snowflake recommends periodically generating new\npermanent credentials for external stages. IAM role: Omit the security credentials and access keys and, instead, identify the role using AWS_ROLE and specify\nthe AWS role ARN (Amazon Resource Name)."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION ."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing Azure blob storage in government regions using a storage integration is limited to Snowflake accounts hosted on Azure in the\nsame government region. Accessing your blob storage from an account hosted outside\nof the government region using direct credentials is supported."
        },
        {
            "name": "CREDENTIALS   =   (   AZURE_SAS_TOKEN   =   ' string '   )",
            "description": "Specifies the SAS (shared access signature) token for connecting to Azure and accessing the private/protected container\nwhere the files containing loaded data are staged. Credentials are generated by Azure."
        },
        {
            "name": "ENCRYPTION   =   (   cloud_specific_encryption   )",
            "description": "Required when loading from encrypted files or unloading into encrypted files. Not required if storage location and files are unencrypted. Modifies the encryption settings used to decrypt encrypted files in the storage location and extract data. Modifies the encryption settings used to encrypt files unloaded to the storage location. Amazon S3 ENCRYPTION = ( [ TYPE = 'AWS_CSE' ] MASTER_KEY = ' string ' | TYPE = 'AWS_SSE_S3' | TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = ' string ' ] | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: AWS_CSE : Client-side encryption (requires a MASTER_KEY value). Currently, the client-side master key you provide can only be a symmetric key. When a MASTER_KEY value is provided, Snowflake assumes TYPE = AWS_CSE (when a MASTER_KEY value is\nprovided, TYPE is not required). AWS_SSE_S3 : Server-side encryption that requires no additional encryption settings. AWS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information about the encryption types, see the AWS documentation for client-side encryption or server-side encryption . NONE : No encryption. Specifies the client-side master key used to encrypt the files in the bucket. The master key must be a 128-bit or 256-bit key\nin Base64-encoded form. Optionally specifies the ID for the AWS KMS-managed key used to encrypt files unloaded into the bucket. If no value\nis provided, your default KMS key ID is used to encrypt files on unload. Note that this value is ignored for data loading. Default: NONE Google Cloud Storage ENCRYPTION = ( TYPE = 'GCS_SSE_KMS' [ KMS_KEY_ID = ' string ' ] | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information, see the Google Cloud documentation: https://cloud.google.com/storage/docs/encryption/customer-managed-keys https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys NONE : No encryption. Optionally specifies the ID for the Cloud KMS-managed key that is used to encrypt files unloaded into the bucket. If\nno value is provided, your default KMS key ID set on the bucket is used to encrypt files on unload. Note that this value is ignored for data loading. The load operation should succeed if the service account has sufficient\npermissions to decrypt data in the bucket. Default: NONE Microsoft Azure ENCRYPTION = ( TYPE = 'AZURE_CSE' MASTER_KEY = ' string ' | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: AZURE_CSE : Client-side encryption (requires a MASTER_KEY value). For information, see the Client-side encryption information in the Microsoft Azure documentation. NONE : No encryption. Specifies the client-side master key used to encrypt or decrypt files. The master key must be a 128-bit or 256-bit key in\nBase64-encoded form. Default: NONE"
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use private connectivity for an external stage to harden your\nsecurity posture. If the external stage uses a storage integration, and that integration is configured for private connectivity, set this parameter to\nFALSE. For information about using this parameter, see one of the following: AWS private connectivity to external stages . Azure private connectivity for external stages and Snowpipe automation ."
        },
        {
            "name": "Data loading :",
            "description": "Modifies the encryption settings used to decrypt encrypted files in the storage location and extract data."
        },
        {
            "name": "Data unloading :",
            "description": "Modifies the encryption settings used to encrypt files unloaded to the storage location."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: AWS_CSE : Client-side encryption (requires a MASTER_KEY value). Currently, the client-side master key you provide can only be a symmetric key. When a MASTER_KEY value is provided, Snowflake assumes TYPE = AWS_CSE (when a MASTER_KEY value is\nprovided, TYPE is not required). AWS_SSE_S3 : Server-side encryption that requires no additional encryption settings. AWS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information about the encryption types, see the AWS documentation for client-side encryption or server-side encryption . NONE : No encryption."
        },
        {
            "name": "MASTER_KEY   =   ' string '  (applies to  AWS_CSE  encryption only)",
            "description": "Specifies the client-side master key used to encrypt the files in the bucket. The master key must be a 128-bit or 256-bit key\nin Base64-encoded form."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  AWS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the AWS KMS-managed key used to encrypt files unloaded into the bucket. If no value\nis provided, your default KMS key ID is used to encrypt files on unload. Note that this value is ignored for data loading."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information, see the Google Cloud documentation: https://cloud.google.com/storage/docs/encryption/customer-managed-keys https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys NONE : No encryption."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  GCS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the Cloud KMS-managed key that is used to encrypt files unloaded into the bucket. If\nno value is provided, your default KMS key ID set on the bucket is used to encrypt files on unload. Note that this value is ignored for data loading. The load operation should succeed if the service account has sufficient\npermissions to decrypt data in the bucket."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: AZURE_CSE : Client-side encryption (requires a MASTER_KEY value). For information, see the Client-side encryption information in the Microsoft Azure documentation. NONE : No encryption."
        },
        {
            "name": "MASTER_KEY   =   ' string '  (applies to AZURE_CSE encryption only)",
            "description": "Specifies the client-side master key used to encrypt or decrypt files. The master key must be a 128-bit or 256-bit key in\nBase64-encoded form."
        },
        {
            "name": "URL   =   's3compat:// bucket [/ path /]'",
            "description": "Specifies the URL for the external location (existing bucket accessed using an S3-compatible API endpoint) used to store data files, where: bucket is the name of the bucket. path is an optional case-sensitive path (or prefix in S3 terminology) for files in the cloud storage location (i.e. files with names that begin with a common string)."
        },
        {
            "name": "ENDPOINT   =   ' s3_api_compatible_endpoint '",
            "description": "Fully-qualified domain that points to the S3-compatible API endpoint."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to enable a directory table on the internal named stage. Default: FALSE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should automatically refresh the directory table metadata when new or updated data files are available on the internal named stage . Snowflake automatically refreshes the directory table metadata. Snowflake does not automatically refresh the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path. Default: FALSE"
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to add a directory table to the stage. When the value is TRUE, a directory table is created with the stage. Default: FALSE"
        },
        {
            "name": "REFRESH_ON_CREATE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically refresh the directory table metadata once, immediately after the stage is\ncreated. Refreshing the directory table metadata synchronizes the metadata with the current list of data files\nin the specified stage path. This action is required for the metadata to register any existing data\nfiles in the named stage specified in the URL = setting. Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH. Default: TRUE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the directory table metadata when new or updated data files are available in the named external stage specified in the URL value. Snowflake enables triggering automatic refreshes of the directory table metadata. Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path. Default: FALSE"
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH."
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to add a directory table to the stage. When the value is TRUE, a directory table is created with the stage. Default: FALSE"
        },
        {
            "name": "REFRESH_ON_CREATE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically refresh the directory table metadata once, immediately after the stage is\ncreated. Refreshing the directory table metadata synchronizes the metadata with the current list of data files\nin the specified stage path. This action is required for the metadata to register any existing data\nfiles in the named stage specified in the URL = setting. Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH. Default: TRUE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the directory table metadata when new or updated data files are available in the named external stage specified in the [ WITH ] LOCATION = setting. Snowflake enables triggering automatic refreshes of the directory table metadata. Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "NOTIFICATION_INTEGRATION   =   ' notification_integration_name '",
            "description": "Specifies the name of the notification integration used to automatically refresh the directory table metadata using GCS Pub/Sub\nnotifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party cloud\nmessage queuing services."
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH."
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to add a directory table to the stage. When the value is TRUE, a directory table is created with the stage. Default: FALSE"
        },
        {
            "name": "REFRESH_ON_CREATE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically refresh the directory table metadata once, immediately after the stage is\ncreated. Refreshing the directory table metadata synchronizes the metadata with the current list of data files\nin the specified stage path. This action is required for the metadata to register any existing data\nfiles in the named stage specified in the URL = setting. Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH. Default: TRUE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the directory table metadata when new or updated data files are available in the named external stage specified in the [ WITH ] LOCATION = setting. Snowflake enables triggering automatic refreshes of the directory table metadata. Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path. Default: FALSE"
        },
        {
            "name": "NOTIFICATION_INTEGRATION   =   ' notification_integration_name '",
            "description": "Specifies the name of the notification integration used to automatically refresh the directory table metadata using Azure Event Grid\nnotifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party cloud\nmessage queuing services."
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH."
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-procedure.html#label-create-or-alter-procedure-syntax",
    "title": "CREATE PROCEDURE",
    "description": "Creates a new stored procedure.",
    "syntax": "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '<java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '<java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type> [ NOT NULL ]\n  LANGUAGE JAVASCRIPT\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  [ ARTIFACT_REPOSITORY = `<repository_name>` ]\n  [ PACKAGES = ( '<package_name>' [ , ... ] ) ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  [ ARTIFACT_REPOSITORY = `<repository_name>` ]\n  [ PACKAGES = ( '<package_name>' [ , ... ] ) ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<module_file_name>.<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n\nCREATE [ OR REPLACE ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '<scala_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '<scala_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n\nCREATE [ OR REPLACE ] PROCEDURE <name> (\n    [ <arg_name> [ { IN | INPUT | OUT | OUTPUT } ] <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  [ NOT NULL ]\n  LANGUAGE SQL\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS <procedure_definition>",
    "examples": [
        {
            "code": "CREATE OR REPLACE PROCEDURE sp_pi()\n    RETURNS FLOAT NOT NULL\n    LANGUAGE JAVASCRIPT\n    AS\n    $$\n    return 3.1415926;\n    $$\n    ;"
        },
        {
            "code": "CREATE OR REPLACE PROCEDURE stproc1(FLOAT_PARAM1 FLOAT)\n    RETURNS STRING\n    LANGUAGE JAVASCRIPT\n    STRICT\n    EXECUTE AS OWNER\n    AS\n    $$\n    var sql_command = \n     \"INSERT INTO stproc_test_table1 (num_col1) VALUES (\" + FLOAT_PARAM1 + \")\";\n    try {\n        snowflake.execute (\n            {sqlText: sql_command}\n            );\n        return \"Succeeded.\";   // Return a success/error indicator.\n        }\n    catch (err)  {\n        return \"Failed: \" + err;   // Return a success/error indicator.\n        }\n    $$\n    ;"
        },
        {
            "code": "CREATE OR REPLACE PROCEDURE my_proc(from_table STRING, to_table STRING, count INT)\n  RETURNS STRING\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.9'\n  PACKAGES = ('snowflake-snowpark-python')\n  HANDLER = 'run'\nAS\n$$\ndef run(session, from_table, to_table, count):\n  session.table(from_table).limit(count).write.save_as_table(to_table)\n  return \"SUCCESS\"\n$$;"
        },
        {
            "code": "CREATE OR REPLACE PROCEDURE my_proc(fromTable STRING, toTable STRING, count INT)\n  RETURNS STRING\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '11'\n  PACKAGES = ('com.snowflake:snowpark:latest')\n  IMPORTS = ('@mystage/myjar.jar')\n  HANDLER = 'MyClass.myMethod';"
        }
    ],
    "parameters": [
        {
            "name": "name   (   [   arg_name   [   {   IN   |   INPUT   |   OUT   |   OUTPUT   }   ]   arg_data_type   .   [   DEFAULT   {default_value}   ]   ]   [   ,   ...   ]   )",
            "description": "Specifies the identifier ( name ), any arguments, and the default values for any optional arguments for the\nstored procedure. For the identifier: The identifier does not need to be unique for the schema in which the procedure is created because stored procedures are identified and resolved by the combination of the name and argument types . The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. “My object”). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements . For the arguments: For arg_name , specify the name of the argument. For { IN | INPUT | OUT | OUTPUT } , specify the type of the argument (input or output). The type specification is only valid\nfor a Snowflake Scripting stored procedure. For more information, see Using arguments passed to a stored procedure . For arg_data_type , use the Snowflake data type that corresponds to the language that you are using. For Java stored procedures , see SQL-Java Data Type Mappings . For JavaScript stored procedures , see SQL and JavaScript data type mapping . For Python stored procedures , see SQL-Python Data Type Mappings . For Scala stored procedures , see SQL-Scala Data Type Mappings . For Snowflake Scripting, a SQL data type . Note For stored procedures you write in Java, Python, or Scala (which use Snowpark APIs), omit the argument for the Snowpark Session object. The Session argument is not a formal parameter that you specify in CREATE PROCEDURE or CALL. When you call your\nstored procedure, Snowflake automatically creates a Session object and passes it to the handler function for your\nstored procedure. To indicate that an argument is optional, use DEFAULT default_value to specify the default value of the argument.\nFor the default value, you can use a literal or an expression. If you specify any optional arguments, you must place these after the required arguments. If a procedure has optional arguments, you cannot define additional procedures with the same name and different signatures. For details, see Specify optional arguments ."
        },
        {
            "name": "RETURNS   {   result_data_type   [   [   NOT   ]   NULL   ]   |   TABLE   (   [   col_name   col_data_type   [   ,   ...   ]   ]   )   }",
            "description": "Specifies the type of the result returned by the stored procedure. For result_data_type , use the Snowflake data type that corresponds to the type of the language that you are using. For Java stored procedures , see SQL-Java Data Type Mappings . For JavaScript stored procedures , see SQL and JavaScript data type mapping . For Python stored procedures , see SQL-Python Data Type Mappings . For Scala stored procedures , see SQL-Scala Data Type Mappings . For Snowflake Scripting, a SQL data type . Note Stored procedures you write in Snowpark (Java or Scala) must have a return value. In Snowpark (Python), when a stored procedure\nreturns no value, it is considered to be returning None . Note that every CREATE PROCEDURE statement must include a RETURNS\nclause that defines a return type, even if the procedure does not explicitly return anything. For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ) , if you know the Snowflake data types of the columns in the returned table, specify the column names and\ntypes: Otherwise (for example, if you are determining the column types during run time), you can omit the column names and types: Note Currently, in the RETURNS TABLE(...) clause, you can’t specify GEOGRAPHY as a column type. This\napplies whether you are creating a stored or anonymous procedure. If you attempt to specify GEOGRAPHY as a column type, calling the stored procedure results in the error: To work around this issue, you can omit the column arguments and types in RETURNS TABLE() . RETURNS TABLE(…) is supported only when the handler is written in the following languages: Java Python Scala Snowflake Scripting As a practical matter, outside of a Snowflake Scripting block , the returned value cannot be used because the call cannot be part of an expression ."
        },
        {
            "name": "LANGUAGE   language",
            "description": "Specifies the language of the stored procedure code. Note that this is optional for stored procedures written with Snowflake Scripting . Currently, the supported values for language include: JAVA (for Java ) JAVASCRIPT (for JavaScript ) PYTHON (for Python ) SCALA (for Scala ) SQL (for Snowflake Scripting ) Default: SQL ."
        },
        {
            "name": "AS   procedure_definition",
            "description": "Defines the code executed by the stored procedure. The definition can consist of any valid code. Note the following: For stored procedures for which the code is not in-line, omit the AS clause. This includes stored procedures with staged handlers. Instead, use the IMPORTS clause to specify the location of the file containing the code for the stored procedure. For\ndetails, see: Writing stored procedures with SQL and Python Writing Java handlers for stored procedures created with SQL Writing Scala handlers for stored procedures created with SQL For more information on in-line and staged handlers, see Keeping handler code in-line or on a stage . You must use string literal delimiters ( ' or $$ ) around procedure definition if: You are using a language other than Snowflake Scripting. You are creating a Snowflake Scripting procedure in SnowSQL or the Classic Console. See Using Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector . For stored procedures in JavaScript, if you are writing a string that contains newlines, you can use\nbackquotes (also called “backticks”) around the string. The following example of a JavaScript stored procedure uses $$ and backquotes because the body of the stored procedure\ncontains single quotes and double quotes: Snowflake does not completely validate the code when you execute the CREATE PROCEDURE command. For example, for Snowpark (Scala) stored procedures, the number and types of arguments are validated, but the body of\nthe function is not validated. If the number or types do not match (e.g. if the Snowflake data type NUMBER is used when the\nargument is a non-numeric type), executing the CREATE PROCEDURE command causes an error. If the code is not valid, the CREATE PROCEDURE command will succeed, and errors will be returned when the stored procedure is\ncalled. For more details about stored procedures, see Working with stored procedures ."
        },
        {
            "name": "RUNTIME_VERSION   =   ' language_runtime_version '",
            "description": "The language runtime version to use. Currently, the supported versions are: 11"
        },
        {
            "name": "PACKAGES   =   (   ' snowpark_package_name '   [,   ' package_name '   ...]   )",
            "description": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler code’s\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API . By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Java, query the INFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: To specify the package name and version number use the following form: To specify the latest version, specify latest for version . For example, to include a package from the latest Snowpark library in Snowflake, use the following: When specifying a package from the Snowpark library, you must specify version 1.3.0 or later."
        },
        {
            "name": "HANDLER   =   ' fully_qualified_method_name '",
            "description": "Use the fully qualified name of the method or function for the stored procedure. This is typically in the\nfollowing form: where: corresponds to the package containing the object or class:"
        },
        {
            "name": "RUNTIME_VERSION   =   ' language_runtime_version '",
            "description": "The language runtime version to use. Currently, the supported versions are: 3.9 3.10 3.11 3.12"
        },
        {
            "name": "PACKAGES   =   (   ' snowpark_package_name '   [,   ' package_name '   ...]   )",
            "description": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler code’s\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API . By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Python, query the INFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: Snowflake includes a large number of packages available through Anaconda; for more information, see Using third-party packages . To specify the package name and version number use the following form: To specify the latest version, omit the version number. For example, to include the spacy package version 2.3.5 (along with the latest version of the required Snowpark package), use the\nfollowing: When specifying a package from the Snowpark library, you must specify version 0.4.0 or later. Omit the version number to use the\nlatest version available in Snowflake. Preview Feature — Open Specifying a range of Python package versions is available as a preview feature to all accounts. You can specify package versions by using these version\nspecifiers: == , <= , >= , < ,or > . For example:"
        },
        {
            "name": "HANDLER   =   ' fully_qualified_method_name '",
            "description": "Use the name of the stored procedure’s function or method. This can differ depending on whether the code is in-line or\nreferenced at a stage. When the code is in-line, you can specify just the function name, as in the following example: When the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name> ."
        },
        {
            "name": "RUNTIME_VERSION   =   ' language_runtime_version '",
            "description": "The language runtime version to use. Currently, the supported versions are: 2.12"
        },
        {
            "name": "PACKAGES   =   (   ' snowpark_package_name '   [,   ' package_name '   ...]   )",
            "description": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler code’s\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API . By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Scala, query the INFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: To specify the package name and version number use the following form: To specify the latest version, specify latest for version . For example, to include a package from the latest Snowpark library in Snowflake, use the following: Snowflake supports using Snowpark version 0.9.0 or later in a Scala stored procedure. Note, however, that these versions have\nlimitations. For example, versions prior to 1.1.0 do not support the use of transactions in a stored procedure."
        },
        {
            "name": "HANDLER   =   ' fully_qualified_method_name '",
            "description": "Use the fully qualified name of the method or function for the stored procedure. This is typically in the following form: where: corresponds to the package containing the object or class:"
        },
        {
            "name": "SECURE",
            "description": "Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   }",
            "description": "Specifies that the procedure persists for only the duration of the session in which you created it.\nA temporary procedure is dropped at the end of the session. Default: No value. If a procedure is not declared as TEMPORARY , it is permanent. You cannot create temporary procedures that have the same name as\na procedure that already exists in the schema. Note that creating a temporary procedure does not require the CREATE PROCEDURE privilege on the schema in which the object is created. For more information about creating temporary procedures, see Temporary procedures ."
        },
        {
            "name": "[   [   NOT   ]   NULL   ]",
            "description": "Specifies whether the stored procedure can return NULL values or must return only NON-NULL values. The default is NULL (i.e. the stored procedure can return NULL)."
        },
        {
            "name": "CALLED   ON   NULL   INPUT   or   .   {   RETURNS   NULL   ON   NULL   INPUT   |   STRICT   }",
            "description": "Specifies the behavior of the stored procedure when called with null inputs. In contrast to system-defined functions, which\nalways return null when any input is null, stored procedures can handle null inputs, returning non-null values even when an\ninput is null: CALLED ON NULL INPUT will always call the stored procedure with null inputs. It is up to the procedure to handle such\nvalues appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT ) will not call the stored procedure if any input is null,\nso the statements inside the stored procedure will not be executed. Instead, a null value will always be returned. Note that\nthe procedure might still return null for non-null inputs. Default: CALLED ON NULL INPUT"
        },
        {
            "name": "VOLATILE   |   IMMUTABLE",
            "description": "Deprecated Attention These keywords are deprecated for stored procedures. These keywords are not intended to apply to stored procedures. In a\nfuture release, these keywords will be removed from the documentation."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the stored procedure, which is displayed in the DESCRIPTION column in the SHOW PROCEDURES output. Default: stored procedure"
        },
        {
            "name": "EXECUTE   AS   OWNER   or   .   EXECUTE   AS   CALLER   or   .   EXECUTE   AS   RESTRICTED   CALLER",
            "description": "Preview Feature — Open Restricted caller’s rights ( EXECUTE AS RESTRICTED CALLER ) is a preview feature available to all accounts. Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with\nthe privileges of the caller (a “caller’s rights” stored procedure): If you execute CREATE PROCEDURE … EXECUTE AS OWNER, then the procedure will execute as an owner’s rights procedure. If you execute the statement CREATE PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a\ncaller’s rights procedure. If you execute the statement CREATE PROCEDURE … EXECUTE AS RESTRICTED CALLER, then in the future the procedure will execute as a\ncaller’s rights procedure, but might not be able to run with all of the caller’s privileges. For more information, see Restricted caller’s rights . If EXECUTE AS ... isn’t specified, the procedure runs as an owner’s rights stored procedure. Owner’s rights stored\nprocedures have less access to the caller’s environment (for example, the caller’s session variables), and Snowflake defaults to this\nhigher level of privacy and security. For more information, see Understanding caller’s rights and owner’s rights stored procedures . Default: OWNER"
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original procedure when a new procedure is created using CREATE OR REPLACE PROCEDURE. The parameter copies all privileges, except OWNERSHIP, from the existing procedure to the new procedure. The new procedure will\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE PROCEDURE\nstatement owns the new procedure. Note: The SHOW GRANTS output for the replacement procedure lists the grantee for the copied privileges as the\nrole that executed the CREATE PROCEDURE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE PROCEDURE command (i.e. within the same transaction)."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [,   ' stage_path_and_file_name_to_read '   ...]   )",
            "description": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the\nstored procedure’s handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the procedure_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. If you specify both the IMPORTS and TARGET_PATH clauses, the file name in the TARGET_PATH clause must\nbe different from each file name in the IMPORTS clause, even if the files are in different subdirectories or different\nstages. The generated JAR file remains until you explicitly delete it, even if you drop the procedure. When you drop the procedure you should\nseparately remove the JAR file because the JAR is no longer needed to support the procedure. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this procedure to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nprocedure’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE PROCEDURE command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [,   ' stage_path_and_file_name_to_read '   ...]   )",
            "description": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If your stored procedure’s code will be on a stage, you must also include a path to the module file your code is in. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nprocedure’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE PROCEDURE command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [,   ' stage_path_and_file_name_to_read '   ...]   )",
            "description": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the\nstored procedure’s handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the procedure_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. If you specify both the IMPORTS and TARGET_PATH clauses, the file name in the TARGET_PATH clause must\nbe different from each file name in the IMPORTS clause, even if the files are in different subdirectories or different\nstages. The generated JAR file remains until you explicitly delete it, even if you drop the procedure. When you drop the procedure you should\nseparately remove the JAR file because the JAR is no longer needed to support the procedure. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this procedure to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-file-format.html#label-create-or-alter-file-format-syntax",
    "title": "CREATE FILE FORMAT",
    "description": "Creates a named file format that describes a set of staged data to access or load into Snowflake tables.",
    "syntax": "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY | VOLATILE } ] FILE FORMAT [ IF NOT EXISTS ] <name>\n  [ TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM} [ formatTypeOptions ] ]\n  [ COMMENT = '<string_literal>' ]\n\nformatTypeOptions ::=\n-- If TYPE = CSV\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     FILE_EXTENSION = '<string>'\n     PARSE_HEADER = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     ESCAPE = '<character>' | NONE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n     ENCODING = '<string>' | UTF8\n-- If TYPE = JSON\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     TRIM_SPACE = TRUE | FALSE\n     MULTI_LINE = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     FILE_EXTENSION = '<string>'\n     ENABLE_OCTAL = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n-- If TYPE = AVRO\n     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = ORC\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = PARQUET\n     COMPRESSION = AUTO | LZO | SNAPPY | NONE\n     SNAPPY_COMPRESSION = TRUE | FALSE\n     BINARY_AS_TEXT = TRUE | FALSE\n     USE_LOGICAL_TYPE = TRUE | FALSE\n     TRIM_SPACE = TRUE | FALSE\n     USE_VECTORIZED_SCANNER = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = XML\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     PRESERVE_SPACE = TRUE | FALSE\n     STRIP_OUTER_ELEMENT = TRUE | FALSE\n     DISABLE_AUTO_CONVERT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE",
    "examples": [
        {
            "code": "CREATE OR REPLACE FILE FORMAT my_csv_format\n  TYPE = CSV\n  COMMENT = 'my_file_format';"
        },
        {
            "code": "CREATE OR ALTER FILE FORMAT my_csv_format\n  TYPE = CSV\n  FIELD_DELIMITER = '|'\n  SKIP_HEADER = 1\n  NULL_IF = ('NULL', 'null')\n  EMPTY_FIELD_AS_NULL = true\n  COMPRESSION = gzip;"
        },
        {
            "code": "CREATE OR REPLACE FILE FORMAT my_json_format\n  TYPE = JSON;"
        },
        {
            "code": "CREATE OR REPLACE FILE FORMAT my_parquet_format\n  TYPE = PARQUET\n  USE_VECTORIZED_SCANNER = TRUE\n  USE_LOGICAL_TYPE = TRUE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the file format; must be unique for the schema in which the file format is created. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ), Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   |   VOLATILE   }",
            "description": "Specifies that the file format persists only for the duration of the session that you created it in.\nA temporary file format is dropped at the end of the session. Default: No value. If a file format is not declared as TEMPORARY , the file format is permanent. If you want to avoid unexpected conflicts, avoid naming temporary file formats after file formats that already exist in the schema. If you created a temporary file format with the same name as another file format in the schema, all queries and operations used on the\nfile format only affect the temporary file format in the session, until you drop the temporary file format. If you drop the file format\nusing a DROP FILE FORMAT command, you drop the temporary file format, and not the file format that already exists in the schema."
        },
        {
            "name": "TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   [   ...   ]",
            "description": "Specifies the format of the input files (for data loading) or output files (for data unloading). Depending on the format type, you can\nspecify additional format-specific options. For more information, see Format Type Options (in this topic). Valid values depend on whether the file format is for loading or unloading data: Any flat, delimited plain text file that uses specific characters such as the following: Separators for fields within records (for example, commas). Separators for records (for example, new line characters). Although the name (CSV) suggests comma-separated values, you can use any valid character as a field separator. Any plain text file containing one or more JSON documents (such as objects or arrays). JSON is a semi-structured file format. The\ndocuments can be comma-separated and optionally enclosed in a big array. A single JSON document can span multiple lines. Note When you load data from files into tables, Snowflake supports either NDJSON (newline delimited JSON)\nstandard format or comma-separated JSON format. When you unload table data to files, Snowflake outputs only to NDJSON format. Binary file in AVRO format. Binary file in ORC format. Binary file in PARQUET format. Plain text file containing XML elements. Preview Feature — Open Available to all accounts. This format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option. For more information about CSV, see Usage Notes in this topic. For more information about JSON and the other semi-structured file formats,\nsee Introduction to Loading Semi-structured Data . For more information about CUSTOM type, see Loading unstructured data with Document AI . Default: CSV"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the file format. Default: No value"
        },
        {
            "name": "CSV  (for loading or unloading)",
            "description": "Any flat, delimited plain text file that uses specific characters such as the following: Separators for fields within records (for example, commas). Separators for records (for example, new line characters). Although the name (CSV) suggests comma-separated values, you can use any valid character as a field separator."
        },
        {
            "name": "JSON  (for loading or unloading)",
            "description": "Any plain text file containing one or more JSON documents (such as objects or arrays). JSON is a semi-structured file format. The\ndocuments can be comma-separated and optionally enclosed in a big array. A single JSON document can span multiple lines. Note When you load data from files into tables, Snowflake supports either NDJSON (newline delimited JSON)\nstandard format or comma-separated JSON format. When you unload table data to files, Snowflake outputs only to NDJSON format."
        },
        {
            "name": "AVRO  (for loading only; you can’t unload data to AVRO format)",
            "description": "Binary file in AVRO format."
        },
        {
            "name": "ORC  (for loading only; you can’t unload data to ORC format)",
            "description": "Binary file in ORC format."
        },
        {
            "name": "PARQUET  (for loading or unloading)",
            "description": "Binary file in PARQUET format."
        },
        {
            "name": "XML  (for loading only; you can’t unload data to XML format)",
            "description": "Plain text file containing XML elements."
        },
        {
            "name": "CUSTOM  (for loading unstructured data only)",
            "description": "Preview Feature — Open Available to all accounts. This format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option."
        }
    ],
    "usage_notes": "Caution\nRecreating a file format (using CREATE OR REPLACE FILE FORMAT) breaks the association between the file format and any external table that\nreferences it. This is because an external table links to a file format using a hidden ID rather than the name of the file format.\nBehind the scenes, the CREATE OR REPLACE syntax drops an object and recreates it with a different hidden ID.\nIf you must recreate a file format after it has been linked to one or more external tables, you must recreate each of the external tables\n(using CREATE OR REPLACE EXTERNAL TABLE) to reestablish the association. Call the GET_DDL function to\nretrieve a DDL statement to recreate each of the external tables.\nConflicting file format values in a SQL statement produce an error. A conflict occurs when the same option is specified multiple times\nwith different values (e.g. ...TYPE = 'CSV' ... TYPE = 'JSON'...).\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-function-spcs.html#label-create-or-alter-function-spcs-syntax",
    "title": "CREATE FUNCTION (Snowpark Container Services)",
    "description": "Creates a service function.",
    "syntax": "CREATE [ OR REPLACE ] FUNCTION <name> ( [ <arg_name> <arg_data_type> ] [ , ... ] )\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  SERVICE = <service_name>\n  ENDPOINT = <endpoint_name>\n  [ COMMENT = '<string_literal>' ]\n  [ CONTEXT_HEADERS = ( <context_function_1> [ , <context_function_2> ...] ) ]\n  [ MAX_BATCH_ROWS = <integer> ]\n  [ MAX_BATCH_RETRIES = <integer> ]\n  [ ON_BATCH_FAILURE = { ABORT | RETURN_NULL } ]\n  [ BATCH_TIMEOUT_SECS = <integer> ]\n  AS '<http_path_to_request_handler>'",
    "examples": [
        {
            "code": "CREATE FUNCTION my_echo_udf (InputText VARCHAR)\n  RETURNS VARCHAR\n  SERVICE=echo_service\n  ENDPOINT=echoendpoint\n  AS '/echo';"
        },
        {
            "code": "CREATE OR ALTER FUNCTION my_echo_udf (InputText VARCHAR)\n  RETURNS VARCHAR\n  SERVICE = echo_service\n  ENDPOINT = reverse_echoendpoint\n  CONTEXT_HEADERS = (current_account)\n  MAX_BATCH_ROWS = 100\n  AS '/echo';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier ( name ) and any input arguments for the function. The identifier does not need to be unique for the schema in which the function is created because functions are identified and resolved by the combination of the name and argument types . The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, “My object”). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements ."
        },
        {
            "name": "(   [   arg_name   arg_data_type   ]   [   ,   ...   ]   )",
            "description": "Specifies the arguments/inputs for the service function. These should correspond to the arguments that the\nservice expects. If there are no arguments, then include the parentheses without any argument name(s) and data type(s)."
        },
        {
            "name": "RETURNS   result_data_type",
            "description": "Specifies the data type of the result returned by the function."
        },
        {
            "name": "SERVICE   =   service_name",
            "description": "Specifies the name of the Snowpark Container Services service."
        },
        {
            "name": "ENDPOINT   =   endpoint_name",
            "description": "Specifies the name of the endpoint as defined in the service specification."
        },
        {
            "name": "AS   http_path_to_request_handler",
            "description": "Specifies the HTTP path to the service code that is executed when the function is called."
        },
        {
            "name": "[   [   NOT   ]   NULL   ]",
            "description": "Specifies whether the function can return NULL values or must return only NON-NULL values. The default is NULL (that is, the function can\nreturn NULL)."
        },
        {
            "name": "CALLED   ON   NULL   INPUT   or   .   {   RETURNS   NULL   ON   NULL   INPUT   |   STRICT   }",
            "description": "Specifies the behavior of the function when called with null inputs. In contrast to system-defined functions, which always return null when any\ninput is null, functions can handle null inputs, returning non-null values even when an input is null: CALLED ON NULL INPUT will always call the function with null inputs. It’s up to the function to handle such values appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT ) will not call the function if any input is null. Instead, a null value\nwill always be returned for that row. Note that the function might still return null for non-null inputs. Default: CALLED ON NULL INPUT"
        },
        {
            "name": "{   VOLATILE   |   IMMUTABLE   }",
            "description": "Specifies the behavior of the function when returning results: VOLATILE : function might return different values for different rows, even for the same input (for example, due to non-determinism and\nstatefulness). IMMUTABLE : function assumes that the function, when called with the same inputs, will always return the same result. This guarantee\nis not checked. Specifying IMMUTABLE for a function that returns different values for the same input will result in undefined\nbehavior. Default: VOLATILE"
        },
        {
            "name": "MAX_BATCH_ROWS   =   integer",
            "description": "Specifies the batch size when sending data to a service to increase concurrency"
        },
        {
            "name": "MAX_BATCH_RETRIES   =   integer",
            "description": "Specifies the number of times you want Snowflake to retry a failed batch. Default: 3"
        },
        {
            "name": "ON_BATCH_FAILURE   =   {   ABORT   |   RETURN_NULL   }",
            "description": "Specifies the behavior of the function after Snowflake reaches the maximum number of retries processing the batch. ABORT : Service function aborts execution. Any remaining batches of rows are not processed. RETURN_NULL : Service function returns a NULL for each row in the failed batch and continues processing the remaining batches. If you choose this option, note the following caveats: If these batches depend on each other and one batch fails, this could lead to unexpected results. If your service can return a NULL as a valid response, then it is not possible to differentiate NULL returned by Snowflake due to batch failure and NULL returned by your service. Default: ABORT"
        },
        {
            "name": "BATCH_TIMEOUT_SECS   =   integer",
            "description": "Specifies the maximum duration for processing a single batch of rows, including retries (and polling for async function requests), after which Snowflake should terminate the batch request. Acceptable Values: greater than 0 and less than or equal to 604800 seconds (7 days). Default: 604800 seconds (7 days)"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the function, which is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output. Default: user-defined function"
        },
        {
            "name": "CONTEXT_HEADERS   =   (   context_function_1   [   ,   context_function_2   ...]   )",
            "description": "This binds Snowflake context function results to HTTP headers.\n(For more information about Snowflake context functions, see: Context functions .) Not all context functions are supported in context headers. The following are supported: CURRENT_ACCOUNT() CURRENT_CLIENT() CURRENT_DATABASE() CURRENT_DATE() CURRENT_IP_ADDRESS() CURRENT_REGION() CURRENT_ROLE() CURRENT_SCHEMA() CURRENT_SCHEMAS() CURRENT_SESSION() CURRENT_STATEMENT() CURRENT_TIME() CURRENT_TIMESTAMP() CURRENT_TRANSACTION() CURRENT_USER() CURRENT_VERSION() CURRENT_WAREHOUSE() LAST_QUERY_ID() LAST_TRANSACTION() LOCALTIME() LOCALTIMESTAMP() When function names are listed in the CONTEXT_HEADERS clause, the function names should not be quoted. Snowflake prepends sf-context to the header before it’s written to the HTTP request. Example: In this example, Snowflake writes the header sf-context-current-timestamp into the HTTP request. Context functions can generate characters that are illegal in HTTP header values, including (but not limited to) the following: newline Ä Î ß ë ¬ ± © ® Snowflake replaces each sequence of one or more illegal characters with one space character. (The replacement\nis per sequence, not per character.) For example, suppose that the context function CURRENT_STATEMENT() returns the following: The value sent in sf-context-current-statement is the following: To ensure that your service code can access the original result (with illegal characters) from the context function\neven if illegal characters have been replaced, Snowflake also sends a binary context header that contains the\ncontext function result encoded in base64 . In the example above, the value sent in the base64-encoded header is the result of the following call: The remote service is responsible for decoding the base64 value if needed. Each such base64 header is named according to the following convention: In the example above, the name of the header would be the following: If no context headers are sent, then no base64 context headers are sent. If the rows sent to a service function are split across multiple batches, then all batches contain the same\ncontext headers and the same binary context headers."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-external-function.html#label-create-or-alter-external-function-syntax",
    "title": "CREATE EXTERNAL FUNCTION",
    "description": "Creates a new external function.",
    "syntax": "CREATE [ OR REPLACE ] [ SECURE ] EXTERNAL FUNCTION <name> ( [ <arg_name> <arg_data_type> ] [ , ... ] )\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ COMMENT = '<string_literal>' ]\n  API_INTEGRATION = <api_integration_name>\n  [ HEADERS = ( '<header_1>' = '<value_1>' [ , '<header_2>' = '<value_2>' ... ] ) ]\n  [ CONTEXT_HEADERS = ( <context_function_1> [ , <context_function_2> ...] ) ]\n  [ MAX_BATCH_ROWS = <integer> ]\n  [ COMPRESSION = <compression_type> ]\n  [ REQUEST_TRANSLATOR = <request_translator_udf_name> ]\n  [ RESPONSE_TRANSLATOR = <response_translator_udf_name> ]\n  AS '<url_of_proxy_and_resource>';",
    "examples": [
        {
            "code": "CREATE OR REPLACE EXTERNAL FUNCTION local_echo(string_col VARCHAR)\n  RETURNS VARIANT\n  API_INTEGRATION = demonstration_external_api_integration_01\n  AS 'https://xyz.execute-api.us-west-2.amazonaws.com/prod/remote_echo';"
        },
        {
            "code": "CREATE OR ALTER SECURE EXTERNAL FUNCTION local_echo(string_col VARCHAR)\n  RETURNS VARIANT\n  API_INTEGRATION = demonstration_external_api_integration_01\n  HEADERS = ('header_variable1'='header_value', 'header_variable2'='header_value2')\n  CONTEXT_HEADERS = (current_account)\n  MAX_BATCH_ROWS = 100\n  COMPRESSION = \"GZIP\"\n  AS 'https://xyz.execute-api.us-west-2.amazonaws.com/prod/remote_echo';"
        }
    ],
    "parameters": [
        {
            "name": "name :",
            "description": "Specifies the identifier for the function. The identifier can contain the schema name and database name, as well as the function name. The identifier does not need to be unique for the schema in which the function is created because functions are\nidentified and resolved by their name and argument types. However, the signature (name and argument data types)\nmust be unique within the schema. The name must follow the rules for Snowflake identifiers .\nFor more details, see Identifier requirements . Setting name the same as the remote service name can make the relationship more clear.\nHowever, this is not required."
        },
        {
            "name": "(   [   arg_name   arg_data_type   ]   [   ,   ...   ]   )",
            "description": "Specifies the arguments/inputs for the external function. These should correspond to the arguments that the remote\nservice expects. If there are no arguments, then include the parentheses without any argument name(s) and data type(s)."
        },
        {
            "name": "RETURNS   result_data_type",
            "description": "Specifies the data type returned by the function."
        },
        {
            "name": "API_INTEGRATION   =   api_integration_name",
            "description": "This is the name of the API integration object that should be used to authenticate the call to the proxy service."
        },
        {
            "name": "AS   ' url_of_proxy_and_resource '",
            "description": "This is the invocation URL of the proxy service (e.g. API Gateway or API Management service) and resource through\nwhich Snowflake calls the remote service."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the function is secure. If a function is secure, the URL, the HTTP headers, and the context headers\nare hidden from all users who are not owners of the function."
        },
        {
            "name": "[   [   NOT   ]   NULL   ]",
            "description": "This clause indicates whether the function can return NULL values or must return only NON-NULL values.\nIf NOT NULL is specified, the function must return only non-NULL values. If NULL is specified, the\nfunction can return NULL values. Default: The default is NULL (i.e. the function can return NULL values)."
        },
        {
            "name": "CALLED   ON   NULL   INPUT   or   .   {   RETURNS   NULL   ON   NULL   INPUT   |   STRICT   }",
            "description": "Specifies the behavior of the function when called with null inputs. In contrast to system-defined functions,\nwhich always return null when any input is null, external functions can handle null inputs,\nreturning non-null values even when an input is null: CALLED ON NULL INPUT will always call the function with null inputs. It is up to the function to\nhandle such values appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT ) will not call the function if any input\nis null. Instead, a null value will always be returned for that row. Note that the function might\nstill return null for non-null inputs. Default: CALLED ON NULL INPUT"
        },
        {
            "name": "{   VOLATILE   |   IMMUTABLE   }",
            "description": "Specifies the behavior of the function when returning results: VOLATILE : The function can return different values for different rows, even for the same input (e.g.\ndue to non-determinism and statefulness). IMMUTABLE : The function always returns the same result when called with the same input.\nSnowflake does not check or guarantee this; the remote service must be designed to behave this way.\nSpecifying IMMUTABLE for a function that actually returns different values for the same input will\nresult in undefined behavior. Default: VOLATILE Snowflake recommends that you set this explicitly rather than accept the default. Setting this\nexplicitly reduces the chance of error, and tells users how the function behaves.\n(The SHOW EXTERNAL FUNCTIONS command shows whether a function is volatile or immutable.) For important additional information about VOLATILE vs. IMMUTABLE external functions, see Categorize your function as volatile or immutable ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the function, which is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW EXTERNAL FUNCTIONS output. Default: user-defined function"
        },
        {
            "name": "HEADERS   =   (   ' header_1 '   =   ' value_1 '   [   ,   ' header_2 '   =   ' value_2 '   ...   ]   )",
            "description": "This clause allows users to specify key-value metadata that is sent with every request.\nThe creator of the external function decides what goes into the headers, and the caller does not have any control\nover it. Snowflake prepends all of the specified header names with the prefix “sf-custom-”, and sends them as HTTP\nheaders. The value must be a constant string, not an expression. Here’s an example: This causes Snowflake to add 2 HTTP headers into every HTTPS request: sf-custom-volume-measure and sf-custom-distance-measure , with their corresponding values. The rules for header names are different from the rules for Snowflake database identifiers. Header names can be\ncomposed of most visible standard ASCII characters (decimal 32 - 126) except the following: the space character ( ) , / : ; < > = \" ? @ [ ] \\ { } _ Note specifically that the underscore character is not allowed in header names. The header name and value are delimited by single quotes, so any single quotes inside the header name or value\nmust be escaped with the backslash character. If the backslash character is used as a literal character inside a header value, it must be escaped. In header values, both spaces and tabs are allowed, but header values should not contain more than one whitespace\ncharacter in a row. This restriction applies to combinations of whitespace characters (e.g. a space followed by a\ntab) as well as individual whitespace characters (e.g. two spaces in a row). If the function author marks the function as secure (with CREATE SECURE EXTERNAL FUNCTION... ), then the\nheaders, the context headers, the binary context headers, and the URL are not visible to function users. The sum of the sizes of the header names and header values for an external function must be less than or equal\nto 8 KB."
        },
        {
            "name": "CONTEXT_HEADERS   =   (   context_function_1   [   ,   context_function_2   ...]   )",
            "description": "This is similar to HEADERS, but instead of using constant strings, it binds Snowflake context function results to HTTP headers.\n(For more information about Snowflake context functions, see: Context functions .) Not all context functions are supported in context headers. The following are supported: CURRENT_ACCOUNT() CURRENT_CLIENT() CURRENT_DATABASE() CURRENT_DATE() CURRENT_IP_ADDRESS() CURRENT_REGION() CURRENT_ROLE() CURRENT_SCHEMA() CURRENT_SCHEMAS() CURRENT_SESSION() CURRENT_STATEMENT() CURRENT_TIME() CURRENT_TIMESTAMP() CURRENT_TRANSACTION() CURRENT_USER() CURRENT_VERSION() CURRENT_WAREHOUSE() LAST_QUERY_ID() LAST_TRANSACTION() LOCALTIME() LOCALTIMESTAMP() When function names are listed in the CONTEXT_HEADERS clause, the function names should not be quoted. Snowflake prepends sf-context to the header before it is written to the HTTP request. Example: In this example, Snowflake writes the header sf-context-current-timestamp into the HTTP request. The characters allowed in context header names and values are the same as the characters allowed in custom header names and values . Context functions can generate characters that are illegal in HTTP header values, including (but not limited to): newline Ä Î ß ë ¬ ± © ® Snowflake replaces each sequence of one or more illegal characters with one space character. (The replacement\nis per sequence, not per character.) For example, suppose that the context function CURRENT_STATEMENT() returns: The value sent in sf-context-current-statement is: To ensure that remote services can access the original result (with illegal characters) from the context function\neven if illegal characters have been replaced, Snowflake also sends a binary context header that contains the\ncontext function result encoded in base64 . In the example above, the value sent in the base64 header is the result of calling: The remote service is responsible for decoding the base64 value if needed. Each such base64 header is named according to the following convention: In the example above, the name of the header would be If no context headers are sent, then no base64 context headers are sent. If the rows sent to an external function are split across multiple batches, then all batches contain the same\ncontext headers and the same binary context headers."
        },
        {
            "name": "MAX_BATCH_ROWS   =   integer",
            "description": "This specifies the maximum number of rows in each batch sent to the proxy service. The purpose of this parameter is to limit batch sizes for remote services that have memory constraints or other\nlimitations. This parameter is not a performance tuning parameter. This parameter specifies a maximum\nsize, not a recommended size. If you do not specify MAX_BATCH_ROWS, Snowflake estimates the optimal batch size and uses that. Snowflake recommends leaving this parameter unset unless the remote service requires a limit."
        },
        {
            "name": "COMPRESSION   =   compression_type",
            "description": "If this clause is specified, the JSON payload is compressed when sent from Snowflake to the proxy service, and when\nsent back from the proxy service to Snowflake. Valid values are: NONE . GZIP . DEFLATE . AUTO . On AWS, AUTO is equivalent to GZIP . On Azure, AUTO is equivalent to NONE . On GCP, AUTO is equivalent to NONE . The Amazon API Gateway automatically compresses/decompresses requests. For more information about\nAmazon API Gateway compression and decompression, see: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-gzip-compression-decompression.html For information about compression and decompression for other cloud platform proxy services, see the documentation\nfor those cloud platforms. Default: The default is AUTO ."
        },
        {
            "name": "REQUEST_TRANSLATOR   =   request_translator_udf_name",
            "description": "This specifies the name of the request translator function. For more information, see Using request and response translators with data for a remote service ."
        },
        {
            "name": "RESPONSE_TRANSLATOR   =   response_translator_udf_name",
            "description": "This specifies the name of the response translator function. For more information, see Using request and response translators with data for a remote service ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-role.html#label-create-or-alter-role-syntax",
    "title": "CREATE ROLE",
    "description": "Create a new role or replace an existing role in the system.",
    "syntax": "CREATE [ OR REPLACE ] ROLE [ IF NOT EXISTS ] <name>\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]",
    "examples": [
        {
            "code": "CREATE ROLE myrole;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the role; must be unique for your account. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the role. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-table.html#label-create-or-alter-table-syntax",
    "title": "CREATE TABLE",
    "description": "Creates a new table in the current/specified schema, replaces an existing table, or alters an existing table. A table can have multiple\ncolumns, with each column definition consisting of a name, data type, and optionally whether the column:",
    "syntax": "CREATE [ OR REPLACE ]\n    [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE | TRANSIENT } ]\n  TABLE [ IF NOT EXISTS ] <table_name>\n\n  (\n    -- Column definition\n    <col_name> <col_type>\n      [ inlineConstraint ]\n      [ NOT NULL ]\n      [ COLLATE '<collation_specification>' ]\n      [\n        {\n          DEFAULT <expr>\n          | { AUTOINCREMENT | IDENTITY }\n            [\n              {\n                ( <start_num> , <step_num> )\n                | START <num> INCREMENT <num>\n              }\n            ]\n            [ { ORDER | NOORDER } ]\n        }\n      ]\n      [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n      [ [ WITH ] PROJECTION POLICY <policy_name> ]\n      [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n    -- Out-of-line constraints\n    [ , outoflineConstraint [ ... ] ]\n  )\n\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE } ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] JOIN POLICY <policy_name> [ ALLOWED JOIN KEYS ( <col_name> [ , ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\ninlineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE\n    | PRIMARY KEY\n    | [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ]\n  }\n  [ <constraint_properties> ]\n\noutoflineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n      REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n  }\n  [ <constraint_properties> ]\n  [ COMMENT '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE TABLE mytable (amount NUMBER);\n\n+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table MYTABLE successfully created. |\n+-------------------------------------+\n\nINSERT INTO mytable VALUES(1);\n\nSHOW TABLES like 'mytable';\n\n+---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:32:28 -0700 | MYTABLE | TESTDB        | PUBLIC      | TABLE |         |            |    1 |  1024 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n\nDESC TABLE mytable;\n\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name   | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| AMOUNT | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+"
        },
        {
            "code": "CREATE TABLE example (col1 NUMBER COMMENT 'a column comment') COMMENT='a table comment';\n\n+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table EXAMPLE successfully created. |\n+-------------------------------------+\n\nSHOW TABLES LIKE 'example';\n\n+---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment         | cluster_by | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:35:59 -0700 | EXAMPLE | TESTDB        | PUBLIC      | TABLE | a table comment |            |    0 |     0 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------+\n\nDESC TABLE example;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment          |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------|\n| COL1 | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | a column comment |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------+"
        },
        {
            "code": "CREATE TABLE mytable_copy (b) AS SELECT * FROM mytable;\n\nDESC TABLE mytable_copy;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| B    | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nCREATE TABLE mytable_copy2 AS SELECT b+1 AS c FROM mytable_copy;\n\nDESC TABLE mytable_copy2;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| C    | NUMBER(39,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nSELECT * FROM mytable_copy2;\n\n+---+\n| C |\n|---|\n| 2 |\n+---+"
        },
        {
            "code": "CREATE TABLE testtable_summary (name, summary_amount) AS SELECT name, amount1 + amount2 FROM testtable;"
        },
        {
            "code": "CREATE OR REPLACE TABLE parquet_col (\n  custKey NUMBER DEFAULT NULL,\n  orderDate DATE DEFAULT NULL,\n  orderStatus VARCHAR(100) DEFAULT NULL,\n  price VARCHAR(255)\n)\nAS SELECT\n  $1:o_custkey::number,\n  $1:o_orderdate::date,\n  $1:o_orderstatus::text,\n  $1:o_totalprice::text\nFROM @my_stage;\n\n+-----------------------------------------+\n| status                                  |\n|-----------------------------------------|\n| Table PARQUET_COL successfully created. |\n+-----------------------------------------+\n\nDESC TABLE parquet_col;\n\n+-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name        | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| CUSTKEY     | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| ORDERDATE   | DATE         | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| ORDERSTATUS | VARCHAR(100) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| PRICE       | VARCHAR(255) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+"
        },
        {
            "code": "CREATE TABLE mytable (amount NUMBER);\n\nINSERT INTO mytable VALUES(1);\n\nSELECT * FROM mytable;\n\n+--------+\n| AMOUNT |\n|--------|\n|      1 |\n+--------+\n\nCREATE TABLE mytable_2 LIKE mytable;\n\nDESC TABLE mytable_2;\n\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name   | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| AMOUNT | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nSELECT * FROM mytable_2;\n\n+--------+\n| AMOUNT |\n|--------|\n+--------+"
        },
        {
            "code": "CREATE TABLE mytable (date TIMESTAMP_NTZ, id NUMBER, content VARIANT) CLUSTER BY (date, id);\n\nSHOW TABLES LIKE 'mytable';\n\n+---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment | cluster_by       | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:20:41 -0700 | MYTABLE | TESTDB        | PUBLIC      | TABLE |         | LINEAR(DATE, ID) |    0 |     0 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE collation_demo (\n  uncollated_phrase VARCHAR, \n  utf8_phrase VARCHAR COLLATE 'utf8',\n  english_phrase VARCHAR COLLATE 'en',\n  spanish_phrase VARCHAR COLLATE 'es');\n\nINSERT INTO collation_demo (\n      uncollated_phrase, \n      utf8_phrase, \n      english_phrase, \n      spanish_phrase) \n   VALUES (\n     'pinata', \n     'pinata', \n     'pinata', \n     'piñata');"
        },
        {
            "code": "CREATE TABLE mytable\n  USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    WITHIN GROUP (ORDER BY order_id)\n      FROM TABLE(\n        INFER_SCHEMA(\n          LOCATION=>'@mystage',\n          FILE_FORMAT=>'my_parquet_format'\n        )\n      ));"
        },
        {
            "code": "CREATE TEMPORARY TABLE demo_temporary (i INTEGER);\nCREATE TEMP TABLE demo_temp (i INTEGER);"
        },
        {
            "code": "CREATE LOCAL TEMPORARY TABLE demo_local_temporary (i INTEGER);\nCREATE LOCAL TEMP TABLE demo_local_temp (i INTEGER);\n\nCREATE GLOBAL TEMPORARY TABLE demo_global_temporary (i INTEGER);\nCREATE GLOBAL TEMP TABLE demo_global_temp (i INTEGER);\n\nCREATE VOLATILE TABLE demo_volatile (i INTEGER);"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(a INT);"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    b VARCHAR(200)\n  )\n  DATA_RETENTION_TIME_IN_DAYS = 5\n  DEFAULT_DDL_COLLATION = 'de';"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    c VARCHAR(200)\n  )\n  DEFAULT_DDL_COLLATION = 'de';"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    b INT\n  );"
        },
        {
            "code": "INSERT INTO my_table VALUES (1, 2), (2, 3);\n\nSELECT * FROM my_table;"
        },
        {
            "code": "+---+---+\n| A | B |\n|---+---|\n| 1 | 2 |\n| 2 | 3 |\n+---+---+"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    c INT\n  );"
        },
        {
            "code": "SELECT * FROM my_table;"
        },
        {
            "code": "+---+------+\n| A | C    |\n|---+------|\n| 1 | NULL |\n| 2 | NULL |\n+---+------+"
        },
        {
            "code": "CREATE TABLE t(a INT);"
        },
        {
            "code": "CREATE OR ALTER TABLE t(a INT PRIMARY KEY);"
        },
        {
            "code": "DESC TABLE t;"
        },
        {
            "code": "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name | privacy domain |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------|\n| A    | NUMBER(38,0) | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | NULL    | NULL        | NULL           |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE t(a INT);"
        },
        {
            "code": "INSERT INTO t VALUES (null);"
        },
        {
            "code": "CREATE OR ALTER TABLE t(a INT PRIMARY KEY);"
        },
        {
            "code": "001471 (42601): SQL compilation error:\nColumn 'A' contains null values. Not null constraint cannot be added."
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the table; must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "col_name",
            "description": "Specifies the column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. For more details, see Identifier requirements and Reserved & limited keywords . Note In addition to the standard reserved keywords, the following keywords cannot be used as column identifiers because they are\nreserved for ANSI-standard context functions: CURRENT_DATE CURRENT_ROLE CURRENT_TIME CURRENT_TIMESTAMP CURRENT_USER For the list of reserved keywords, see Reserved & limited keywords ."
        },
        {
            "name": "col_type",
            "description": "Specifies the data type for the column. For details about the data types that can be specified for table columns, see SQL data types reference ."
        },
        {
            "name": "query",
            "description": "Required for CTAS and USING TEMPLATE. For CTAS, specifies the SELECT statement that populates the table. This query must be\nspecified last in the CTAS statement, regardless of the other parameters that you include. For CREATE TABLE … USING TEMPLATE, specifies the subquery that calls the INFER_SCHEMA function and\nformats the output as an array. Alternatively, USING TEMPLATE accepts the INFER_SCHEMA output as a string\nliteral or variable."
        },
        {
            "name": "source_table",
            "description": "Required for LIKE and CLONE. For CREATE TABLE … LIKE, specifies the table from which properties and column definitions are copied. For CREATE TABLE … CLONE, specifies the table to use as the source for the clone."
        },
        {
            "name": "{   [   {   LOCAL   |   GLOBAL   }   ]   TEMP   [   READ   ONLY]   |   .   TEMPORARY   [   READ   ONLY]   |   .   VOLATILE   |   .   TRANSIENT   }",
            "description": "Specifies that the table persists only for the duration of the session that you created it in. A\ntemporary table and all its contents are dropped at the end of the session. The synonyms and abbreviations for TEMPORARY (e.g. GLOBAL TEMPORARY ) are provided for compatibility with other databases\n(e.g. to prevent errors when migrating CREATE TABLE statements). Tables created with any of these keywords appear and behave identically\nto a table created with the TEMPORARY keyword. Default: No value. If a table is not declared as TEMPORARY or TRANSIENT , the table is permanent. If you want to avoid unexpected conflicts, avoid naming temporary tables after tables that already exist in the schema. If you created a temporary table with the same name as another table in the schema, all queries and operations used on the table only\naffect the temporary table in the session, until you drop the temporary table. If you drop the table, you drop the temporary table, and\nnot the table that already exists in the schema. For information about temporary or transient tables, and how they can affect storage and cost, refer to the following resources: Working with Temporary and Transient Tables Storage costs for Time Travel and Fail-safe Specifies that the table is read-only. READ ONLY is valid only for a temporary table that is being created with the CREATE TABLE … CLONE variant of the CREATE TABLE command. A read-only table does not allow DML operations and only allows the following subset of DDL operations: ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } COMMENT ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } MASKING POLICY ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } TAG ALTER TABLE … RENAME COLUMN … TO ALTER TABLE … RENAME TO ALTER TABLE … { SET | UNSET } COMMENT ALTER TABLE … { SET | UNSET } TAG COMMENT DESCRIBE DROP SHOW UNDROP Read-only tables have a METADATA$ROW_POSITION column. This metadata column assigns a row number to each row in\nthe table that is continuous and starts from 0. The row number assigned to each row remains unchanged until the\nread-only table is dropped."
        },
        {
            "name": "TRANSIENT",
            "description": "Specifies that the table is transient. Like a permanent table, a transient table exists until explicitly dropped and is visible to any\nuser with the appropriate privileges. However, transient tables have a lower level of data protection than permanent tables, meaning\nthat data in a transient table might be lost in the event of a system failure. As such, transient tables should only be used for data\nthat can be recreated externally to Snowflake. Default: No value. If a table is not declared as TRANSIENT or TEMPORARY , the table is permanent. Note Transient tables have some storage considerations. For more information about these and other considerations when deciding whether to create temporary or transient tables, see Working with Temporary and Transient Tables and Storage costs for Time Travel and Fail-safe ."
        },
        {
            "name": "CONSTRAINT   ...",
            "description": "Defines an inline or out-of-line constraint for the specified column(s) in the table. For syntax details, see CREATE | ALTER TABLE … CONSTRAINT . For more information about constraints, see Constraints ."
        },
        {
            "name": "COLLATE   ' collation_specification '",
            "description": "Specifies the collation to use for column operations such as string comparison. This option applies only to text columns\n(VARCHAR, STRING, TEXT, etc.). For more details, see Collation specifications ."
        },
        {
            "name": "DEFAULT   ...   or   .   AUTOINCREMENT   ...",
            "description": "Specifies whether a default value is automatically inserted in the column if a value is not explicitly specified via an INSERT\nor CREATE TABLE AS SELECT statement: Column default value is defined by the specified expression which can be any of the following: Constant value. Sequence reference ( seq_name .NEXTVAL ). Simple expression that returns a scalar value. The simple expression can include a SQL UDF (user-defined function) if the UDF is not a secure UDF . Note If a default expression refers to a SQL UDF, then the function is replaced by its\ndefinition at table creation time. If the user-defined function is redefined in the future, this does not\nupdate the column’s default expression. The simple expression cannot contain references to: Subqueries. Aggregates. Window functions. Secure UDFs. UDFs written in languages other than SQL (e.g. Java, JavaScript). External functions. When you specify AUTOINCREMENT or IDENTITY, the default value for the column starts with a specified number and each\nsuccessive value automatically increments by the specified amount. AUTOINCREMENT and IDENTITY are synonymous and can be used only for columns with numeric data types, such as NUMBER, INT,\nFLOAT. Caution Snowflake uses a sequence to generate the values for an auto-incremented column. Sequences have limitations;\nsee Sequence Semantics . The default value for both the start value and the step/increment value is 1 . Note Manually inserting values into an AUTOINCREMENT or IDENTITY column can result in duplicate values. If you manually insert the\nvalue 5 into an AUTOINCREMENT or IDENTITY column, a subsequently inserted row might use the same value 5 as the\ndefault value for the column. Use ORDER or NOORDER to specify whether or not the values are generated for the auto-incremented column in increasing or decreasing order . ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). If you do not specify ORDER or NOORDER, the NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is\nset. Note DEFAULT and AUTOINCREMENT are mutually exclusive; only one can be specified for a column."
        },
        {
            "name": "MASKING   POLICY   =   policy_name",
            "description": "Specifies the masking policy to set on a column. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "PROJECTION   POLICY   policy_name",
            "description": "Specifies the projection policy to set on a column. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "COMMENT   ' string_literal '",
            "description": "Specifies a comment for the column. (Note that comments can be specified at the column level or the table level. The syntax for each is slightly different.)"
        },
        {
            "name": "USING   (   col_name   ,   cond_col_1   ...   )",
            "description": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal masking policy ."
        },
        {
            "name": "CLUSTER   BY   (   expr   [   ,   expr   ,   ...   ]   )",
            "description": "Specifies one or more columns or column expressions in the table as the clustering key. For more details, see Clustering Keys & Clustered Tables . Default: No value (no clustering key is defined for the table) Important Clustering keys are not intended or recommended for all tables; they typically benefit very large (i.e. multi-terabyte)\ntables. Before you specify a clustering key for a table, you should understand micro-partitions. For more information, see Understanding Snowflake Table Structures ."
        },
        {
            "name": "ENABLE_SCHEMA_EVOLUTION   =   {   TRUE   |   FALSE   }",
            "description": "Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support . The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER , ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false."
        },
        {
            "name": "READ   ONLY",
            "description": "Specifies that the table is read-only. READ ONLY is valid only for a temporary table that is being created with the CREATE TABLE … CLONE variant of the CREATE TABLE command. A read-only table does not allow DML operations and only allows the following subset of DDL operations: ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } COMMENT ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } MASKING POLICY ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } TAG ALTER TABLE … RENAME COLUMN … TO ALTER TABLE … RENAME TO ALTER TABLE … { SET | UNSET } COMMENT ALTER TABLE … { SET | UNSET } TAG COMMENT DESCRIBE DROP SHOW UNDROP Read-only tables have a METADATA$ROW_POSITION column. This metadata column assigns a row number to each row in\nthe table that is continuous and starts from 0. The row number assigned to each row remains unchanged until the\nread-only table is dropped."
        },
        {
            "name": "DEFAULT   expr",
            "description": "Column default value is defined by the specified expression which can be any of the following: Constant value. Sequence reference ( seq_name .NEXTVAL ). Simple expression that returns a scalar value. The simple expression can include a SQL UDF (user-defined function) if the UDF is not a secure UDF . Note If a default expression refers to a SQL UDF, then the function is replaced by its\ndefinition at table creation time. If the user-defined function is redefined in the future, this does not\nupdate the column’s default expression. The simple expression cannot contain references to: Subqueries. Aggregates. Window functions. Secure UDFs. UDFs written in languages other than SQL (e.g. Java, JavaScript). External functions."
        },
        {
            "name": "{   AUTOINCREMENT   |   IDENTITY   }   .   [   {   (   start_num   ,   step_num   )   |   START   num   INCREMENT   num   }   ]   .   [   {   ORDER   |   NOORDER   }   ]",
            "description": "When you specify AUTOINCREMENT or IDENTITY, the default value for the column starts with a specified number and each\nsuccessive value automatically increments by the specified amount. AUTOINCREMENT and IDENTITY are synonymous and can be used only for columns with numeric data types, such as NUMBER, INT,\nFLOAT. Caution Snowflake uses a sequence to generate the values for an auto-incremented column. Sequences have limitations;\nsee Sequence Semantics . The default value for both the start value and the step/increment value is 1 . Note Manually inserting values into an AUTOINCREMENT or IDENTITY column can result in duplicate values. If you manually insert the\nvalue 5 into an AUTOINCREMENT or IDENTITY column, a subsequently inserted row might use the same value 5 as the\ndefault value for the column. Use ORDER or NOORDER to specify whether or not the values are generated for the auto-incremented column in increasing or decreasing order . ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). If you do not specify ORDER or NOORDER, the NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is\nset."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the retention period for the table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical\ndata in the table. For more details, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "CHANGE_TRACKING   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to enable change tracking on the table. TRUE enables change tracking on the table. This setting adds a pair of hidden columns to the source table and begins\nstoring change-tracking metadata in the columns. These columns consume a small amount of storage. The change-tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the table. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for the columns in the table, including columns\nadded to the table in the future. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original table when a new table is created using any of the following\nCREATE TABLE variants: CREATE OR REPLACE TABLE CREATE TABLE … LIKE CREATE TABLE … CLONE The parameter copies all privileges, except OWNERSHIP, from the existing table to the new table. The new table does not inherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE TABLE statement\nowns the new table. If the parameter is not included in the CREATE TABLE statement, then the new table does not inherit any explicit access\nprivileges granted on the original table, but does inherit any future grants defined for the object type in the schema. Note: With data sharing : If the existing table was shared to another account, the replacement table is also shared. If the existing table was shared with your account as a data consumer, and access was further granted to other roles in\nthe account (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement\ntable. The SHOW GRANTS output for the replacement table lists the grantee for the copied privileges as the\nrole that executed the CREATE TABLE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE TABLE command (i.e. within the same transaction). This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the table. Default: No value (Note that comments can be specified at the column level, constraint level, or table level. The syntax for each is slightly different.)"
        },
        {
            "name": "ROW   ACCESS   POLICY   policy_name   ON   (   col_name   [   ,   col_name   ...   ]   )",
            "description": "Specifies the row access policy to set on a table. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "AGGREGATION   POLICY   policy_name   [   ENTITY   KEY   (   col_name   [   ,   col_name   ...   ]   )   ]",
            "description": "Specifies an aggregation policy to set on a table. You can apply one or more aggregation\npolicies on a table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see Implementing entity-level privacy with aggregation policies . You can specify one or more entity keys for an aggregation policy. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "JOIN   POLICY   policy_name   [   ALLOWED   JOIN   KEYS   (   col_name   [   ,   ...   ]   )   ]",
            "description": "Specifies the join policy to set on a table. Use the optional ALLOWED JOIN KEYS parameter to define which columns are allowed to be used as joining columns when\nthis policy is in effect. For more information, see Join policies . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ],
    "usage_notes": "A schema cannot contain tables and/or views with the same name. When creating a table:\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then creating a new table\nwith the same name; however, the dropped table is not permanently removed from the system. Instead, it is retained in\nTime Travel. This is important to note because dropped tables in Time Travel can be recovered, but they also contribute to data\nstorage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data. Any stream on the table becomes stale. In\naddition, any stream on a view that has this table as an underlying table, becomes stale. A stale stream is unreadable.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nSimilar to reserved keywords, ANSI-reserved function names\n(CURRENT_DATE, CURRENT_TIMESTAMP, etc.) cannot be used as column\nnames.\nCREATE OR ALTER TABLE:\nFor more information, see CREATE OR ALTER TABLE usage notes.\nCREATE TABLE … CLONE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is suspended\nfor the new table – even if Automatic Clustering was not suspended for the source table.\nCREATE TABLE … CHANGE_TRACKING = TRUE:\nWhen change tracking is enabled, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nCREATE TABLE … LIKE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is not\nsuspended for the new table – even if Automatic Clustering was suspended for the source table.\nCREATE TABLE … AS SELECT (CTAS):\nIf the aliases for the column names in the SELECT list are valid columns, then the column definitions\nare not required in the CTAS statement; if omitted, the column names and types are inferred from the underlying query:\nAlternatively, the names can be explicitly specified using the following syntax:\nThe number of column names specified must match the number of SELECT list items in the query; the\ntypes of the columns are inferred from the types produced by the query.\nWhen clustering keys are specified in a CTAS statement:\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table – even if Automatic Clustering is suspended for the\nsource table.\nIf you want the table to be created with rows in a specific order, then use an ORDER BY sub-clause in the SELECT clause of the\nCTAS. Specifying CLUSTER BY does not cluster the data at the time that the table is created; instead, CLUSTER BY relies on\nautomatic clustering to recluster the data over time.\nThe ORDER BY sub-clause in a CREATE TABLE statement does not affect the order of the rows returned by future SELECT statements\non that table. To specify the order of rows in future SELECT statements, use an ORDER BY sub-clause in those statements.\nInside a transaction, any DDL statement (including CREATE TEMPORARY/TRANSIENT TABLE) commits\nthe transaction before executing the DDL statement itself. The DDL statement then runs in its own transaction. The\nnext statement after the DDL statement starts a new transaction. Therefore, you can’t create, use, and drop a\ntemporary or transient table within a single transaction. If you want to use a temporary or transient table inside a\ntransaction, then create the table before the transaction, and drop the table after the transaction.\nRecreating a table (using the optional OR REPLACE keyword) drops its history, which makes any stream on the table stale.\nA stale stream is unreadable.\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen creating a table with a masking policy on one or more table columns, or a row access policy added to the table, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the table\nprotected by a row access policy.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
}
]