[
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-projection-policy",
    "title": "CREATE PROJECTION POLICY",
    "description": "Creates a new projection policy in the current/specified schema or replaces an existing\nprojection policy.",
    "syntax": "CREATE [ OR REPLACE ] PROJECTION POLICY [ IF NOT EXISTS ] <name>\n  AS () RETURNS PROJECTION_CONSTRAINT -> <body>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE OR REPLACE PROJECTION POLICY do_not_project AS ()\n  RETURNS PROJECTION_CONSTRAINT ->\n  PROJECTION_CONSTRAINT(ALLOW => false);"
        },
        {
            "code": "CREATE OR REPLACE PROJECTION POLICY project_analyst_only AS ()\n  RETURNS PROJECTION_CONSTRAINT ->\n    CASE\n      WHEN CURRENT_ROLE() = 'ANALYST'\n        THEN PROJECTION_CONSTRAINT(ALLOW => true)\n      ELSE PROJECTION_CONSTRAINT(ALLOW => false, ENFORCEMENT => 'NULLIFY')\n    END;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the projection policy; must be unique for your schema. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "body",
            "description": "SQL expression that determines whether to project a column. The expression can contain CASE and other logic statements, but must call the PROJECTION_CONSTRAINT function: ALLOW ( boolean ) - TRUE allows the column to be projected. FALSE prevents the column from being projected, with the behavior\nspecified by ENFORCEMENT. FALSE affects only columns that appear in the final results table. ENFORCEMENT ( string, optional ) - If ALLOW=FALSE, specifies what should happen if a query includes a protected column.\nSupported values: FAIL - The query will fail if a protected column is included in the outermost query. NULLIFY - All rows in the protected column return the value NULL. Default: FAIL"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the projection policy."
        }
    ],
    "usage_notes": "If you want to update an existing projection policy and need to see the current definition of the policy, run the\nDESCRIBE PROJECTION POLICY command or GET_DDL function.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-external-access-integration",
    "title": "CREATE EXTERNAL ACCESS INTEGRATION",
    "description": "Creates an external access integration for access\nto external network locations from a UDF or procedure handler.",
    "syntax": "CREATE [ OR REPLACE ] EXTERNAL ACCESS INTEGRATION <name>\n  ALLOWED_NETWORK_RULES = ( <rule_name_1> [, <rule_name_2>, ... ] )\n  [ ALLOWED_API_AUTHENTICATION_INTEGRATIONS = ( { <integration_name_1> [, <integration_name_2>, ... ] | none } ) ]\n  [ ALLOWED_AUTHENTICATION_SECRETS = ( { <secret_name_1> [, <secret_name_2>, ... ] | all | none } ) ]\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE OR REPLACE SECRET oauth_token\n  TYPE = OAUTH2\n  API_AUTHENTICATION = google_translate_oauth\n  OAUTH_REFRESH_TOKEN = 'my-refresh-token';"
        },
        {
            "code": "USE ROLE USERADMIN;\nCREATE OR REPLACE ROLE developer;"
        },
        {
            "code": "USE ROLE SECURITYADMIN;\nGRANT READ ON SECRET oauth_token TO ROLE developer;"
        },
        {
            "code": "USE ROLE SYSADMIN;\nCREATE OR REPLACE NETWORK RULE google_apis_network_rule\n  MODE = EGRESS\n  TYPE = HOST_PORT\n  VALUE_LIST = ('translation.googleapis.com');"
        },
        {
            "code": "USE ROLE ACCOUNTADMIN;\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION google_apis_access_integration\n  ALLOWED_NETWORK_RULES = (google_apis_network_rule)\n  ALLOWED_AUTHENTICATION_SECRETS = (oauth_token)\n  ENABLED = true;"
        },
        {
            "code": "GRANT USAGE ON INTEGRATION google_apis_access_integration TO ROLE developer;"
        },
        {
            "code": "USE ROLE developer;\n\nCREATE OR REPLACE FUNCTION google_translate_python(sentence STRING, language STRING)\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = 3.10\nHANDLER = 'get_translation'\nEXTERNAL_ACCESS_INTEGRATIONS = (google_apis_access_integration)\nPACKAGES = ('snowflake-snowpark-python','requests')\nSECRETS = ('cred' = oauth_token )\nAS\n$$\nimport _snowflake\nimport requests\nimport json\nsession = requests.Session()\ndef get_translation(sentence, language):\n  token = _snowflake.get_oauth_access_token('cred')\n  url = \"https://translation.googleapis.com/language/translate/v2\"\n  data = {'q': sentence,'target': language}\n  response = session.post(url, json = data, headers = {\"Authorization\": \"Bearer \" + token})\n  return response.json()['data']['translations'][0]['translatedText']\n$$;"
        },
        {
            "code": "GRANT USAGE ON FUNCTION google_translate_python(string, string) TO ROLE user;"
        },
        {
            "code": "USE ROLE user;\nSELECT google_translate_python('Happy Thursday!', 'zh-CN');"
        },
        {
            "code": "-------------------------------------------------------\n| GOOGLE_TRANSLATE_PYTHON('HAPPY THURSDAY!', 'ZH-CN') |\n-------------------------------------------------------\n| 快乐星期四！                                          |\n-------------------------------------------------------"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the external access integration. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "ALLOWED_NETWORK_RULES   =   ( rule_name   [   ,   rule_name   ...   ])",
            "description": "Specifies the allowed network rules . Only egress rules may be specified."
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this integration is enabled or disabled. If the integration is disabled, any handler code that relies\non it will be unable to reach the external network location. The value is case-insensitive. The default is TRUE ."
        },
        {
            "name": "ALLOWED_API_AUTHENTICATION_INTEGRATIONS   =   (   integration_name_1   [,   integration_name_2 ,   ...   ]   |   none   )",
            "description": "Specifies the security integrations whose OAuth authorization server issued the secret used by the UDF or procedure. The security\nintegration must be the type used for external API integration . This parameter’s value must be one of the following: One or more Snowflake security integration names to allow any of the listed integrations. none to allow no integrations. Security integrations specified by this parameter – as well as secrets specified by the ALLOWED_AUTHENTICATION_SECRETS parameter – are\nways to allow secrets for use in a UDF or procedure that uses this external access integration. For more information, see Usage notes . For reference information about security integrations, refer to CREATE SECURITY INTEGRATION (External API Authentication) ."
        },
        {
            "name": "ALLOWED_AUTHENTICATION_SECRETS   =   (   secret_name   [,   secret_name   ...   ]   |   all   |   none   )",
            "description": "Specifies the secrets that UDF or procedure handler code can use when accessing the external network locations referenced in allowed\nnetwork rules. This parameter’s value must be one of the following: One or more Snowflake secret names to allow any of the listed secrets. all to allow any secret. none to allow no secrets. The ALLOWED_API_AUTHENTICATION_INTEGRATIONS parameter can also specify allowed secrets. For more information, see Usage notes . For reference information about secrets, refer to CREATE SECRET ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the external access integration. Default: No value"
        }
    ],
    "usage_notes": "You can allow secrets for use by a UDF or procedure by using two external access integration parameters, as described below.\nWith the ALLOWED_AUTHENTICATION_SECRETS parameter. You can specify secrets as parameter values or set the parameter’s value to\nall, allowing handler code to use any secret.\nWith the ALLOWED_API_AUTHENTICATION_INTEGRATIONS parameter. A secret is allowed for use when\nthe secret itself specifies a security integration whose name is also specified by this parameter. The secret specifies the security\nintegration with its API_AUTHENTICATION parameter. In other words, when both the secret and the external access integration specify\nthe security integration, the secret is allowed for use in functions and procedures that specify the external access integration.\nNote that these two alternatives function independently of one another. A secret is allowed if either (or both) of the parameters allows\nit, regardless of the value specified for the other parameter. For example, setting one of the parameters to none does not\nprevent a secret specified by the other parameter from being used in handler code.\nWhile you can specify network rules using a hostname, Snowflake enforces the rules at the IP level of granularity. Snowflake will not\ninspect your application’s traffic, so it is your responsibility to ensure that the external location’s host has the authentic\nservice and that it is not possible to connect to other services on the same host. Whenever possible, you should use secure protocols\nsuch as HTTPS and TLS when communicating with internet endpoints.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-provisioned-throughput",
    "title": "CREATE PROVISIONED THROUGHPUT",
    "description": "Creates a new Provisioned Throughput resource or replaces an existing one.",
    "syntax": "CREATE [ OR REPLACE ] PROVISIONED THROUGHPUT <name>\n    CLOUD_PROVIDER = '<cloud_provider>'\n    MODEL = '<model_name>'\n    PTUS = <num_ptus>\n    TERM_START = '<start_date>'\n    TERM_END = '<end_date>';",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] PROVISIONED THROUGHPUT <name>\n    CLOUD_PROVIDER = '<cloud_provider>'\n    MODEL = '<model_name>'\n    PTUS = <num_ptus>\n    TERM_START = '<start_date>'\n    TERM_END = '<end_date>';"
        },
        {
            "code": "CREATE PROVISIONED THROUGHPUT my_pt\n    CLOUD_PROVIDER = 'aws'\n    MODEL = 'llama3.1-8B'\n    PTUS = 64\n    TERM_START = '2025-04-15'\n    TERM_END = '2025-05-15';"
        },
        {
            "code": "CREATE OR REPLACE PROVISIONED THROUGHPUT my_pt\n    CLOUD_PROVIDER = 'aws'\n    MODEL = 'llama3.1-8B'\n    PTUS = 128\n    TERM_START = '2025-06-01'\n    TERM_END = '2025-07-01';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e., name) for the provisioned throughput resource; must be unique for the schema in which the resource is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "CLOUD_PROVIDER   =   ' cloud_provider '",
            "description": "Specifies the cloud provider where the provisioned throughput will be allocated. Supported values are aws and azure ."
        },
        {
            "name": "MODEL   =   ' model_name '",
            "description": "Specifies the model for which the provisioned throughput is being reserved. Supported models include: Mistral Large 2 Llama 3.1-405B Llama 3.1-70B Llama 3.1-8B Snowflake-Llama3.3-70B Snowflake-Llama3.3-405B"
        },
        {
            "name": "PTUS   =   num_ptus",
            "description": "Specifies the number of provisioned throughput units (PTUs) to allocate. The value must meet the minimum and incremental PTU requirements for the specified model."
        },
        {
            "name": "TERM_START   =   ' start_date '",
            "description": "Specifies the start date of the provisioned throughput term in the format YYYY-MM-DD ."
        },
        {
            "name": "TERM_END   =   ' end_date '",
            "description": "Specifies the end date of the provisioned throughput term in the format YYYY-MM-DD ."
        }
    ],
    "usage_notes": "Attention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nProvisioned Throughput is subject to minimum and incremental PTU requirements. Ensure that your PTU request meets these requirements for the specified model.\nThe term for provisioned throughput starts and ends at 8:00 a.m. PT on the specified dates.\nProvisioned Throughput does not renew automatically. To reserve throughput for another term, create a new provisioned throughput resource.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-api-integration",
    "title": "CREATE API INTEGRATION",
    "description": "Creates a new API integration object in the account or replaces an existing API integration.",
    "syntax": "CREATE [ OR REPLACE ] API INTEGRATION [ IF NOT EXISTS ] <integration_name>\n  API_PROVIDER = { aws_api_gateway | aws_private_api_gateway | aws_gov_api_gateway | aws_gov_private_api_gateway }\n  API_AWS_ROLE_ARN = '<iam_role>'\n  [ API_KEY = '<api_key>' ]\n  API_ALLOWED_PREFIXES = ('<...>')\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]\n  ;\n\nCREATE [ OR REPLACE ] API INTEGRATION [ IF NOT EXISTS ] <integration_name>\n  API_PROVIDER = azure_api_management\n  AZURE_TENANT_ID = '<tenant_id>'\n  AZURE_AD_APPLICATION_ID = '<azure_application_id>'\n  [ API_KEY = '<api_key>' ]\n  API_ALLOWED_PREFIXES = ( '<...>' )\n  [ API_BLOCKED_PREFIXES = ( '<...>' ) ]\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]\n  ;\n\nCREATE [ OR REPLACE ] API INTEGRATION [ IF NOT EXISTS ] <integration_name>\n  API_PROVIDER = google_api_gateway\n  GOOGLE_AUDIENCE = '<google_audience_claim>'\n  API_ALLOWED_PREFIXES = ( '<...>' )\n  [ API_BLOCKED_PREFIXES = ( '<...>' ) ]\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]\n  ;\n\nCREATE [ OR REPLACE ] API INTEGRATION [ IF NOT EXISTS ] <integration_name>\n  API_PROVIDER = git_https_api\n  API_ALLOWED_PREFIXES = ('<...>')\n  [ API_BLOCKED_PREFIXES = ('<...>') ]\n  [ ALLOWED_AUTHENTICATION_SECRETS = ( { <secret_name> [, <secret_name>, ... ] | all | none } ) ]\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]\n  ;\n\nCREATE [ OR REPLACE ] API INTEGRATION [ IF NOT EXISTS ] <integration_name>\n  API_PROVIDER = git_https_api\n  API_ALLOWED_PREFIXES = ('https://github.com/<...>')\n  [ API_BLOCKED_PREFIXES = ('<...>') ]\n  API_USER_AUTHENTICATION = (\n    TYPE = snowflake_github_app\n  )\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]\n  ;\n\nCREATE [ OR REPLACE ] API INTEGRATION [ IF NOT EXISTS ] <integration_name>\n  API_PROVIDER = git_https_api\n  API_ALLOWED_PREFIXES = ('<...>')\n  [ API_BLOCKED_PREFIXES = ('<...>') ]\n  [ ALLOWED_AUTHENTICATION_SECRETS = ( { <secret_name> [, <secret_name>, ... ] | all | none } ) ]\n  USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE }\n  [ TLS_TRUSTED_CERTIFICATES = ( { <secret_name> [, <secret_name>, ... ] } ) ]\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]\n  ;",
    "examples": [
        {
            "code": "CREATE OR REPLACE API INTEGRATION demonstration_external_api_integration_01\n  API_PROVIDER = aws_api_gateway\n  API_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/my_cloud_account_role'\n  API_ALLOWED_PREFIXES = ('https://xyz.execute-api.us-west-2.amazonaws.com/production')\n  ENABLED = TRUE;\n\nCREATE OR REPLACE EXTERNAL FUNCTION local_echo(string_col VARCHAR)\n  RETURNS VARIANT\n  API_INTEGRATION = demonstration_external_api_integration_01\n  AS 'https://xyz.execute-api.us-west-2.amazonaws.com/production/remote_echo';"
        }
    ],
    "parameters": [
        {
            "name": "integration_name",
            "description": "Specifies the name of the API integration. This name follows the rules for Object identifiers .\nThe name must be unique among API integrations in your account."
        },
        {
            "name": "API_PROVIDER   =   {   aws_api_gateway   |   aws_private_api_gateway   |   aws_gov_api_gateway   |   aws_gov_private_api_gateway   }",
            "description": "Specifies the HTTPS proxy service type. Valid values are: aws_api_gateway : for Amazon API Gateway using regional endpoints. aws_private_api_gateway : for Amazon API Gateway using private endpoints. aws_gov_api_gateway : for Amazon API Gateway using U.S. government GovCloud endpoints. aws_gov_private_api_gateway : for Amazon API Gateway using U.S. government GovCloud endpoints that are also private\nendpoints."
        },
        {
            "name": "API_AWS_ROLE_ARN   =   iam_role",
            "description": "For Amazon AWS, this is the ARN (Amazon resource name) of a cloud platform role."
        },
        {
            "name": "API_ALLOWED_PREFIXES   =   (...)",
            "description": "Explicitly limits external functions that use the integration to reference one or more HTTPS proxy\nservice endpoints (such as Amazon API Gateway) and resources within those proxies. Supports a comma-separated\nlist of URLs, which are treated as prefixes (for details, see below). Each URL in API_ALLOWED_PREFIXES = (...) is treated as a prefix. For example, if you specify: https://xyz.amazonaws.com/production/ that means all resources under https://xyz.amazonaws.com/production/ are allowed. For example the following is allowed: https://xyz.amazonaws.com/production/ml1 To maximize security, you should restrict allowed locations as narrowly as practical."
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this API integration is enabled or disabled. If the API integration is disabled, any external function that relies\non it will not work. The value is case-insensitive. The default is TRUE ."
        },
        {
            "name": "integration_name",
            "description": "Specifies the name of the API integration. This name follows the rules for Object identifiers .\nThe name should be unique among API integrations in your account."
        },
        {
            "name": "API_PROVIDER   =   azure_api_management",
            "description": "Specifies that this integration is used with Azure API Management services. Do not use quotation marks around azure_api_management ."
        },
        {
            "name": "AZURE_TENANT_ID   =   tenant_id",
            "description": "Specifies the ID for your Office 365 tenant that all Azure API Management instances belong to. An API integration\ncan authenticate to only one tenant, and so the allowed and blocked locations must refer to API Management\ninstances that all belong to this tenant. To find your tenant ID, sign in to the Azure portal and select Azure Active Directory » Properties .\nThe tenant ID is displayed in the Tenant ID field."
        },
        {
            "name": "AZURE_AD_APPLICATION_ID   =   azure_application_id",
            "description": "The “Application (client) id” of the Azure AD (Active Directory) app for your remote service.\nIf you followed the instructions in Creating external functions on Microsoft Azure ,\nthen this is the Azure Function App AD Application ID that you recorded in the worksheet in those instructions."
        },
        {
            "name": "API_ALLOWED_PREFIXES   =   (...)",
            "description": "Explicitly limits external functions that use the integration to reference one or more HTTPS proxy\nservice endpoints (such as Azure API Management services) and resources within those proxies. Supports a comma-separated\nlist of URLs, which are treated as prefixes (for details, see below). Each URL in API_ALLOWED_PREFIXES = (...) is treated as a prefix. For example, if you specify: https://my-external-function-demo.azure-api.net/my-function-app-name that means all resources under https://my-external-function-demo.azure-api.net/my-function-app-name are allowed. For example the following is allowed: https://my-external-function-demo.azure-api.net/my-function-app-name/my-http-trigger-function To maximize security, you should restrict allowed locations as narrowly as practical."
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this API integration is enabled or disabled. If the API integration is disabled, any external function that relies\non it will not work. The value is case-insensitive. The default is TRUE ."
        },
        {
            "name": "integration_name",
            "description": "Specifies the name of the API integration. This name follows the rules for Object identifiers .\nThe name should be unique among API integrations in your account."
        },
        {
            "name": "API_PROVIDER   =   google_api_gateway",
            "description": "Specifies that this integration is used with Google Cloud. The only valid value for this purpose is google_api_gateway .\nThe value must not be in quotation marks."
        },
        {
            "name": "GOOGLE_AUDIENCE   =   google_audience",
            "description": "This is used as the audience claim when generating the JWT (JSON Web Token) to authenticate to the Google API Gateway.\nFor more information about authenticating with Google, please see the Google service account authentication documentation."
        },
        {
            "name": "API_ALLOWED_PREFIXES   =   (...)",
            "description": "Explicitly limits external functions that use the integration to reference one or more HTTPS proxy\nservice endpoints (such as Google Cloud API Gateways) and resources within those proxies. Supports a comma-separated\nlist of URLs, which are treated as prefixes (for details, see below). Each URL in API_ALLOWED_PREFIXES = (...) is treated as a prefix. For example, if you specify: https://my-external-function-demo.uc.gateway.dev/x that means all resources under https://my-external-function-demo.uc.gateway.dev/x are allowed. For example the following is allowed: https://my-external-function-demo.uc.gateway.dev/x/y To maximize security, you should restrict allowed locations as narrowly as practical."
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this API integration is enabled or disabled. If the API integration is disabled, any external function that relies\non it will not work. The value is case-insensitive. The default is TRUE ."
        },
        {
            "name": "integration_name",
            "description": "Specifies the name of the API integration. This name follows the rules for Object identifiers .\nThe name must be unique among API integrations in your account."
        },
        {
            "name": "API_PROVIDER   =   git_https_api",
            "description": "Specifies that this integration is used with CREATE GIT REPOSITORY to create an integration with a remote Git repository . The only valid value for this purpose is git_https_api . The value must not be in quotation marks."
        },
        {
            "name": "API_ALLOWED_PREFIXES   =   (...)",
            "description": "Explicitly limits requests that use the integration to reference one or more HTTPS endpoints and resources beneath those\nendpoints. Supports a comma-separated list of URLs, which are treated as prefixes. Note When you’re authenticating using OAuth, you must specify https://github.com in the base URL of the allowed prefix.\nAuthenticating to a Git repository using OAuth is supported only from Workspaces and when the repository is hosted on github.com. In most cases, Snowflake supports any HTTPS Git repository URL. For example, you can specify a custom URL to a corporate Git server\nwithin your own domain. https://example.com/my-repo Each URL in API_ALLOWED_PREFIXES = (...) is treated as a prefix. For example, you can specify the following: https://github.com/my-account With this prefix, all resources under that URL are allowed. For example, the following is allowed: https://github.com/my-account/myproject To maximize security, you should restrict allowed locations as narrowly as practical."
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this API integration is enabled or disabled. If the API integration is disabled, the Git repository will not be accessible. The value is case-insensitive. The default is TRUE ."
        },
        {
            "name": "API_KEY   =   api_key",
            "description": "The API key (also called a “subscription key”)."
        },
        {
            "name": "API_BLOCKED_PREFIXES   =   (...)",
            "description": "Lists the endpoints and resources in the HTTPS proxy service that are not allowed to be called from Snowflake. The possible values for locations follow the same rules as for API_ALLOWED_PREFIXES above. API_BLOCKED_PREFIXES takes precedence over API_ALLOWED_PREFIXES. If a prefix matches both, then it is blocked.\nIn other words, Snowflake allows all values that match API_ALLOWED_PREFIXES except values that also\nmatch API_BLOCKED_PREFIXES. If a value is outside API_ALLOWED_PREFIXES, you do not need to explicitly block it."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "A description of the integration."
        },
        {
            "name": "ALLOWED_AUTHENTICATION_SECRETS   =   (   secret_name   [,   secret_name   ...   ]   |   all   |   none   )",
            "description": "Specifies the secrets that UDF or procedure handler code can use when accessing the Git repository at the API_ALLOWED_PREFIXES value. You\nspecify a secret from this list when specifying Git credentials with the GIT_CREDENTIALS parameter . This parameter’s value must be one of the following: One or more fully-qualified Snowflake secret names to allow any of the listed secrets. (Default) all to allow any secret. none to allow no secrets. The ALLOWED_API_AUTHENTICATION_INTEGRATIONS parameter can also specify allowed secrets. For more information, see Usage notes . For reference information about secrets, refer to CREATE SECRET ."
        },
        {
            "name": "API_USER_AUTHENTICATION   =   (   TYPE   =   snowflake_github_app   )",
            "description": "Specifies security integration settings for an OAuth 2.0 flow. The value snowflake_github_app specifies OAuth 2.0 as the\nauthentication type for Snowflake to use to connect to the repository. Note Using OAuth to work with a Git repository is supported only from Workspaces and only when the repository is hosted at github.com.\nFor more information, see Create a Git workspace ."
        },
        {
            "name": "TLS_TRUSTED_CERTIFICATES   =   (   {secret_name}   [,   {secret_name}   ...   ]   )",
            "description": "Specifies secrets containing self-signed certificates to be used when authenticating with a Git repository server over private link. This parameter is\nneeded only when the certificate is self-signed, rather than signed by a certificate authority. This parameter’s value must be one or more fully qualified Snowflake secret names. The secrets must be of type generic string whose SECRET_STRING value is Base64-encoded certificate data."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this API integration will be used only to configure access to a remote Git repository over an outbound private link connection through private connectivity . This parameter must be set to FALSE (the default) for public Git servers. The default is FALSE ."
        }
    ],
    "usage_notes": "Only Snowflake roles with OWNERSHIP or USAGE privileges on the API integration can use the API integration directly\n(for example, by creating an external function that specifies that API integration).\nAn API integration object is tied to a specific cloud platform account and role within that account, but not\nto a specific HTTPS proxy URL. You can create more than one instance of an HTTPS proxy service in a cloud provider\naccount, and you can use the same API integration to authenticate to multiple proxy services in that account.\nYour Snowflake account can have multiple API integration objects, for example, for different cloud platform accounts.\nMultiple external functions can use the same API integration object, and thus the same HTTPS proxy service.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-authentication-policy",
    "title": "CREATE AUTHENTICATION POLICY",
    "description": "Creates a new authentication policy in the current or specified schema or replaces\nan existing authentication policy. You can use authentication policies to define authentication controls and security requirements\nfor accounts or users.",
    "syntax": "CREATE [ OR REPLACE ] AUTHENTICATION POLICY [ IF NOT EXISTS ] <name>\n  [ AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_ENROLLMENT = { REQUIRED | OPTIONAL } ]\n  [ MFA_POLICY= ( ALLOWED_METHODS = ( { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } [ , { 'PASSKEY' | 'TOTP' | 'DUO' } ... ] ) ) ]\n  [ CLIENT_TYPES = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ SECURITY_INTEGRATIONS = ( '<string_literal>' [ , '<string_literal>' , ... ] ) ]\n  [ PAT_POLICY = ( {list_of_properties} ) ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] AUTHENTICATION POLICY [ IF NOT EXISTS ] <name>\n  [ AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_ENROLLMENT = { REQUIRED | OPTIONAL } ]\n  [ MFA_POLICY= ( ALLOWED_METHODS = ( { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } [ , { 'PASSKEY' | 'TOTP' | 'DUO' } ... ] ) ) ]\n  [ CLIENT_TYPES = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ SECURITY_INTEGRATIONS = ( '<string_literal>' [ , '<string_literal>' , ... ] ) ]\n  [ PAT_POLICY = ( {list_of_properties} ) ]\n  [ COMMENT = '<string_literal>' ]"
        },
        {
            "code": "CREATE OR ALTER AUTHENTICATION POLICY <name>\n  [ AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_ENROLLMENT = { REQUIRED | OPTIONAL } ]\n  [ MFA_POLICY= ( ALLOWED_METHODS = ( { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } [ , { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } ... ] ) ) ]\n  [ CLIENT_TYPES = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ SECURITY_INTEGRATIONS = ( '<string_literal>' [ , '<string_literal>' , ... ] ) ]\n  [ PAT_POLICY = ( {list_of_properties} ) ]\n  [ COMMENT = '<string_literal>' ]"
        },
        {
            "code": "PAT_POLICY=(\n  DEFAULT_EXPIRY_IN_DAYS=30\n  MAX_EXPIRY_IN_DAYS=365\n  NETWORK_POLICY_EVALUATION = ENFORCED_NOT_REQUIRED\n);"
        },
        {
            "code": "CREATE AUTHENTICATION POLICY restrict_client_types_policy\n  CLIENT_TYPES = ('SNOWFLAKE_UI')\n  COMMENT = 'Auth policy that only allows access through the web interface';"
        },
        {
            "code": "CREATE OR ALTER AUTHENTICATION POLICY restrict_client_types_policy\n  MFA_ENROLLMENT = REQUIRED\n  MFA_AUTHENTICATION_METHODS = ('PASSWORD', 'SAML')\n  CLIENT_TYPES = ('SNOWFLAKE_UI', 'SNOWFLAKE_CLI');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the authentication policy. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "AUTHENTICATION_METHODS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "Caution Restricting by authentication method can have unintended consequences, such as blocking driver connections or third-party integrations. A list of authentication methods that are allowed during login. This parameter accepts one or more of the following values: Allow all authentication methods. Allows SAML2 security integrations . If SAML is\npresent, an SSO login option appears. If SAML is not present, an SSO login option does not appear. Allows users to authenticate using username and password. Allows External OAuth . Allows Key pair authentication . Allows users to authenticate with a programmatic access token . Default: ALL ."
        },
        {
            "name": "ALL",
            "description": "Allow all authentication methods."
        },
        {
            "name": "SAML",
            "description": "Allows SAML2 security integrations . If SAML is\npresent, an SSO login option appears. If SAML is not present, an SSO login option does not appear."
        },
        {
            "name": "PASSWORD",
            "description": "Allows users to authenticate using username and password."
        },
        {
            "name": "OAUTH",
            "description": "Allows External OAuth ."
        },
        {
            "name": "KEYPAIR",
            "description": "Allows Key pair authentication ."
        },
        {
            "name": "PROGRAMMATIC_ACCESS_TOKEN",
            "description": "Allows users to authenticate with a programmatic access token ."
        },
        {
            "name": "MFA_AUTHENTICATION_METHODS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "A list of authentication methods that enforce multi-factor authentication (MFA) during login. Authentication methods not listed in this\nparameter do not prompt for multi-factor authentication. The following authentication methods support MFA: SAML PASSWORD This parameter accepts one or more of the following values: Prompts users for MFA, if they are enrolled in MFA, when authenticating with SAML2 security integrations . Prompts users for MFA, if they are enrolled in MFA, when authenticating with a username and password. Default: ('PASSWORD') ."
        },
        {
            "name": "MFA_ENROLLMENT   =   {   REQUIRED   |   OPTIONAL   }",
            "description": "Determines whether a user must enroll in multi-factor authentication. Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI , because Snowsight is the only place users can enroll in multi-factor authentication (MFA) . Users can choose whether to enroll in MFA. Default: REQUIRED ."
        },
        {
            "name": "MFA_POLICY=   (   ALLOWED_METHODS   =   (   {   'ALL'   |   'PASSKEY'   |   'TOTP'   |   'DUO'   }   [   ,   {   'PASSKEY'   |   'TOTP'   |   'DUO'   }   ...   ]   )   )",
            "description": "Specifies the multi-factor authentication (MFA) methods that users can use as a second factor of authentication. You can specify more than one method. Users can use a passkey, an authenticator app, or Duo as their second factor of authentication. Users can use a passkey as their second factor of authentication. Users can use an authenticator app as their second factor of authentication. Users can use Duo as their second factor of authentication. Default: ALL ."
        },
        {
            "name": "CLIENT_TYPES   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "A list of clients that can authenticate with Snowflake. If a client tries to connect, and the client is not one of the valid CLIENT_TYPES values listed below, then the login attempt fails. If you set MFA_ENROLLMENT to REQUIRED , then you must include SNOWFLAKE_UI in the CLIENT_TYPES list to allow\nusers to enroll in MFA. If you want to exclude SNOWFLAKE_UI from the CLIENT_TYPES list, then you must set MFA_ENROLLMENT to OPTIONAL . The CLIENT_TYPES property of an authentication policy is a best effort method to block user logins based on specific clients. It should not be used as the sole control to establish a security boundary. This parameter accepts one or more of the following values: Allow all clients to authenticate. Snowsight or Classic Console , the Snowflake web interfaces. Caution If SNOWFLAKE_UI is not included in the CLIENT_TYPES list while MFA_ENROLLMENT is set to REQUIRED , or MFA_ENROLLMENT is unspecified, MFA enrollment doesn’t work. Drivers allow access to Snowflake from applications written in supported languages . For example, the Go , JDBC , .NET drivers, and Snowpipe Streaming . Caution If DRIVERS is not included in the CLIENT_TYPES list, automated ingestion may stop working. A command-line client for connecting to Snowflake and for managing developer-centric workloads and SQL operations. A command-line client for connecting to Snowflake. Default: ALL ."
        },
        {
            "name": "SECURITY_INTEGRATIONS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "A list of security integrations the authentication policy is associated with. This parameter has no effect when SAML or OAUTH are not in the AUTHENTICATION_METHODS list. All values in the SECURITY_INTEGRATIONS list must be compatible with the values in the AUTHENTICATION_METHODS list. For\nexample, if SECURITY_INTEGRATIONS contains a SAML security integration, and AUTHENTICATION_METHODS contains OAUTH , then you cannot create the authentication policy. Allow all security integrations. Default: ALL ."
        },
        {
            "name": "SAML",
            "description": "Prompts users for MFA, if they are enrolled in MFA, when authenticating with SAML2 security integrations ."
        },
        {
            "name": "PASSWORD",
            "description": "Prompts users for MFA, if they are enrolled in MFA, when authenticating with a username and password."
        },
        {
            "name": "REQUIRED",
            "description": "Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI , because Snowsight is the only place users can enroll in multi-factor authentication (MFA) ."
        },
        {
            "name": "OPTIONAL",
            "description": "Users can choose whether to enroll in MFA."
        },
        {
            "name": "ALL",
            "description": "Users can use a passkey, an authenticator app, or Duo as their second factor of authentication."
        },
        {
            "name": "PASSKEY",
            "description": "Users can use a passkey as their second factor of authentication."
        },
        {
            "name": "TOTP",
            "description": "Users can use an authenticator app as their second factor of authentication."
        },
        {
            "name": "DUO",
            "description": "Users can use Duo as their second factor of authentication."
        },
        {
            "name": "ALL",
            "description": "Allow all clients to authenticate."
        },
        {
            "name": "SNOWFLAKE_UI",
            "description": "Snowsight or Classic Console , the Snowflake web interfaces. Caution If SNOWFLAKE_UI is not included in the CLIENT_TYPES list while MFA_ENROLLMENT is set to REQUIRED , or MFA_ENROLLMENT is unspecified, MFA enrollment doesn’t work."
        },
        {
            "name": "DRIVERS",
            "description": "Drivers allow access to Snowflake from applications written in supported languages . For example, the Go , JDBC , .NET drivers, and Snowpipe Streaming . Caution If DRIVERS is not included in the CLIENT_TYPES list, automated ingestion may stop working."
        },
        {
            "name": "SNOWFLAKE_CLI",
            "description": "A command-line client for connecting to Snowflake and for managing developer-centric workloads and SQL operations."
        },
        {
            "name": "SNOWSQL",
            "description": "A command-line client for connecting to Snowflake."
        },
        {
            "name": "ALL",
            "description": "Allow all security integrations."
        },
        {
            "name": "PAT_POLICY   =   (   list_of_properties   )",
            "description": "Specifies the policies for programmatic access tokens . Set this to a\nspace-delimited list of one or more of the following properties and values: Specifies the default expiration time (in days) for a programmatic access token. You can specify a value from 1 to the\nmaximum time (which you can specify by setting MAX_EXPIRY_IN_DAYS). The default expiration time is 15 days. For more information, see Setting the default expiration time . Specifies the maximum number of days that can be set for the expiration time for a programmatic access token. You can specify\na value from 1 to 365. The default maximum expiration time is 365 days. Note If there are existing programmatic access tokens with expiration times that exceed the new maximum expiration time, attempts to\nauthenticate with those tokens will fail. For example, suppose that you generate a programmatic access token named my_token with the expiration time of 7 days. If you\nlater change the maximum expiration time for all tokens to 2 days, authenticating with my_token will fail because the\nexpiration time of the token exceeds the new maximum expiration time. For more information, see Setting the maximum expiration time . Specifies how network policy requirements are handled for programmatic access tokens. By default, a user must be subject to a network policy with one or more network rules to generate or use programmatic access tokens: Service users (with TYPE=SERVICE) must be subject to a network policy to generate and use programmatic access tokens. Human users (with TYPE=PERSON) must be subject to a network policy to use programmatic access tokens. To override this behavior, set this property to one of the following values: The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication. For example:"
        },
        {
            "name": "DEFAULT_EXPIRY_IN_DAYS   =   number_of_days",
            "description": "Specifies the default expiration time (in days) for a programmatic access token. You can specify a value from 1 to the\nmaximum time (which you can specify by setting MAX_EXPIRY_IN_DAYS). The default expiration time is 15 days. For more information, see Setting the default expiration time ."
        },
        {
            "name": "MAX_EXPIRY_IN_DAYS   =   number_of_days",
            "description": "Specifies the maximum number of days that can be set for the expiration time for a programmatic access token. You can specify\na value from 1 to 365. The default maximum expiration time is 365 days. Note If there are existing programmatic access tokens with expiration times that exceed the new maximum expiration time, attempts to\nauthenticate with those tokens will fail. For example, suppose that you generate a programmatic access token named my_token with the expiration time of 7 days. If you\nlater change the maximum expiration time for all tokens to 2 days, authenticating with my_token will fail because the\nexpiration time of the token exceeds the new maximum expiration time. For more information, see Setting the maximum expiration time ."
        },
        {
            "name": "NETWORK_POLICY_EVALUATION   =   {   ENFORCED_REQUIRED   |   ENFORCED_NOT_REQUIRED   |   NOT_ENFORCED   }",
            "description": "Specifies how network policy requirements are handled for programmatic access tokens. By default, a user must be subject to a network policy with one or more network rules to generate or use programmatic access tokens: Service users (with TYPE=SERVICE) must be subject to a network policy to generate and use programmatic access tokens. Human users (with TYPE=PERSON) must be subject to a network policy to use programmatic access tokens. To override this behavior, set this property to one of the following values: The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication."
        },
        {
            "name": "ENFORCED_REQUIRED  (default behavior)",
            "description": "The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication."
        },
        {
            "name": "ENFORCED_NOT_REQUIRED",
            "description": "The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication."
        },
        {
            "name": "NOT_ENFORCED",
            "description": "The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a description of the policy."
        }
    ],
    "usage_notes": "After creating an authentication policy, you must use the ALTER ACCOUNT or\nALTER USER command to set it on an account or user before Snowflake enforces the policy.\nIf you want to update an existing authentication policy and need to see the definition of the policy, run the\nDESCRIBE AUTHENTICATION POLICY command or GET_DDL function.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-cortex-search",
    "title": "CREATE CORTEX SEARCH SERVICE",
    "description": "Creates a new Cortex Search service or replaces an existing one.",
    "syntax": "CREATE [ OR REPLACE ] CORTEX SEARCH SERVICE [ IF NOT EXISTS ] <name>\n  ON <search_column>\n  ATTRIBUTES <col_name> [ , ... ]\n  WAREHOUSE = <warehouse_name>\n  TARGET_LAG = '<num> { seconds | minutes | hours | days }'\n  [ EMBEDDING_MODEL = <embedding_model_name> ]\n  [ INITIALIZE = { ON_CREATE | ON_SCHEDULE } ]\n  [ COMMENT = '<comment>' ]\n  AS <query>;",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] CORTEX SEARCH SERVICE [ IF NOT EXISTS ] <name>\n  ON <search_column>\n  ATTRIBUTES <col_name> [ , ... ]\n  WAREHOUSE = <warehouse_name>\n  TARGET_LAG = '<num> { seconds | minutes | hours | days }'\n  [ EMBEDDING_MODEL = <embedding_model_name> ]\n  [ INITIALIZE = { ON_CREATE | ON_SCHEDULE } ]\n  [ COMMENT = '<comment>' ]\n  AS <query>;"
        },
        {
            "code": "Your service has not yet been loaded into our serving system. Please retry your request in a few minutes."
        },
        {
            "code": "CREATE OR REPLACE CORTEX SEARCH SERVICE mysvc\n  ON transcript_text\n  ATTRIBUTES region,agent_id\n  WAREHOUSE = mywh\n  TARGET_LAG = '1 hour'\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nAS (\n  SELECT\n      transcript_text,\n      date,\n      region,\n      agent_id\n  FROM support_db.public.transcripts_etl\n);"
        },
        {
            "code": "CREATE OR REPLACE CORTEX SEARCH SERVICE mysvc\n  ON transcript_text\n  ATTRIBUTES region\n  WAREHOUSE = mywh\n  TARGET_LAG = '1 hour'\n  INITIALIZE = ON_SCHEDULE\nAS SELECT * FROM support_db.public.transcripts_etl;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the Cortex Search service; must be unique for the schema in which the service is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "ON   search_column",
            "description": "Specifies the text column in the base table that you wish to search on. This column must be a text value."
        },
        {
            "name": "ATTRIBUTES   col_name   [   ,   ...   ]",
            "description": "Specifies comma-separated list of columns in the base table that you wish to filter on when issuing queries to the service.\nAttribute columns must be included in the source query, either via explicit enumeration or wildcard, ( * )."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Specifies the warehouse to use for running the source query, building the search index, and keeping it refreshed per the TARGET_LAG target."
        },
        {
            "name": "TARGET_LAG   =   ' num   {   seconds   |   minutes   |   hours   |   days   }'",
            "description": "Specifies the maximum amount of time that the Cortex Search service content should lag behind updates to the base tables specified in the source query."
        },
        {
            "name": "EMBEDDING_MODEL   =   <embedding_model_name>",
            "description": "Optional parameter that specifies the embedding model to use in the Cortex Search Service. This property cannot be altered after you create the Cortex\nSearch Service. To modify the property, recreate the Cortex Search Service with a CREATE OR REPLACE CORTEX SEARCH SERVICE command. Some embedding models are only available in certain cloud regions for Cortex Search.\nFor an availability list by model by region, see Cortex Search Regional Availability . Each model may incur a different cost per million input tokens processed.\nRefer to the Snowflake Service Consumption Table for each function’s cost in credits per million tokens. If the EMBEDDING_MODEL is not specified, the default model is used. The default model is snowflake-arctic-embed-m-v1.5 ."
        },
        {
            "name": "INITIALIZE",
            "description": "Specifies the behavior of the initial refresh of the Cortex Search Service. This property cannot be\naltered after you create the service. To modify the property, replace the cortex search service with a CREATE OR REPLACE CORTEX SEARCH SERVICE command. Refreshes the Cortex Search Service synchronously at creation. If this refresh fails, service creation fails and displays an error message. Refreshes the Cortex Search Service at the next scheduled refresh. The Cortex Search Service is populated when the refresh schedule process runs. No data is populated when the Cortex Search Service is created.\nIf you try to query the service, you might see the following error because the first scheduled refresh has not yet occurred. Default: ON_CREATE"
        },
        {
            "name": "COMMENT   =   ' comment '",
            "description": "Specifies a comment for the service."
        },
        {
            "name": "AS   query",
            "description": "Specifies a query defining the base table from which the service is created."
        },
        {
            "name": "ON_CREATE",
            "description": "Refreshes the Cortex Search Service synchronously at creation. If this refresh fails, service creation fails and displays an error message."
        },
        {
            "name": "ON_SCHEDULE",
            "description": "Refreshes the Cortex Search Service at the next scheduled refresh. The Cortex Search Service is populated when the refresh schedule process runs. No data is populated when the Cortex Search Service is created.\nIf you try to query the service, you might see the following error because the first scheduled refresh has not yet occurred."
        }
    ],
    "usage_notes": "Attention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe size of the Warehouse used to run the Cortex Search service source query does impact the speed and cost of each refresh. A\nlarger warehouse decreases build and refresh time. However, during this preview, Snowflake recommends using a warehouse size no larger\nthan MEDIUM for Cortex Search services.\nSnowflake recommends using a dedicated warehouse for each Cortex Search service so as to not interfere with other workloads.\nThe search index is built as part of the create statement, which means the CREATE CORTEX SEARCH SERVICE statement may take longer to\ncomplete for larger datasets.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-data-metric-function",
    "title": "CREATE DATA METRIC FUNCTION",
    "description": "Creates a new data metric function (DMF) in the current or specified schema, or replaces an existing data metric function.",
    "syntax": "CREATE [ OR REPLACE ] [ SECURE ] DATA METRIC FUNCTION [ IF NOT EXISTS ] <name>\n  ( <table_arg> TABLE( <col_arg> <data_type> [ , ... ] )\n    [ , <table_arg> TABLE( <col_arg> <data_type> [ , ... ] ) ] )\n  RETURNS NUMBER [ [ NOT ] NULL ]\n  [ LANGUAGE SQL ]\n  [ COMMENT = '<string_literal>' ]\n  AS\n  '<expression>'",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] [ SECURE ] DATA METRIC FUNCTION [ IF NOT EXISTS ] <name>\n  ( <table_arg> TABLE( <col_arg> <data_type> [ , ... ] )\n    [ , <table_arg> TABLE( <col_arg> <data_type> [ , ... ] ) ] )\n  RETURNS NUMBER [ [ NOT ] NULL ]\n  [ LANGUAGE SQL ]\n  [ COMMENT = '<string_literal>' ]\n  AS\n  '<expression>'"
        },
        {
            "code": "CREATE [ OR ALTER ] DATA METRIC FUNCTION ..."
        },
        {
            "code": "CREATE OR REPLACE DATA METRIC FUNCTION governance.dmfs.count_positive_numbers(\n  arg_t TABLE(\n    arg_c1 NUMBER,\n    arg_c2 NUMBER,\n    arg_c3 NUMBER\n  )\n)\nRETURNS NUMBER\nAS\n$$\n  SELECT\n    COUNT(*)\n  FROM arg_t\n  WHERE\n    arg_c1>0\n    AND arg_c2>0\n    AND arg_c3>0\n$$;"
        },
        {
            "code": "CREATE OR REPLACE DATA METRIC FUNCTION governance.dmfs.referential_check(\n  arg_t1 TABLE (arg_c1 INT), arg_t2 TABLE (arg_c2 INT))\nRETURNS NUMBER\nAS\n$$\n  SELECT\n    COUNT(*)\n    FROM arg_t1\n  WHERE\n    arg_c1 NOT IN (SELECT arg_c2 FROM arg_t2)\n$$;"
        },
        {
            "code": "CREATE OR ALTER SECURE DATA METRIC FUNCTION governance.dmfs.count_positive_numbers(\n  arg_t TABLE(\n    arg_c1 NUMBER,\n    arg_c2 NUMBER,\n    arg_c3 NUMBER\n  )\n)\nRETURNS NUMBER\nCOMMENT = \"count positive numbers\"\nAS\n$$\n  SELECT\n    COUNT(*)\n  FROM arg_t\n  WHERE\n    arg_c1>0\n    AND arg_c2>0\n    AND arg_c3>0\n$$;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the DMF; must be unique for your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "(   table_arg   TABLE(   col_arg   data_type   [   ,   ...   ]   )   [   ,   table_arg   TABLE(   col_arg   data_type   [   ,   ...   ]   )   ]   )",
            "description": "The signature for the DMF, which is used as input for the expression. You must specify: An argument name for each table ( table_arg ). For each table, an argument name for at least one column, along with its data type ( col_arg data_type ). You can optionally specify arguments for additional columns and their data types. The columns must be in the same table and cannot\nreference a different table."
        },
        {
            "name": "RETURNS   NUMBER",
            "description": "The data type of the output of the function. The data type can only be NUMBER."
        },
        {
            "name": "AS   expression",
            "description": "SQL expression that determines the output of the function. The expression must be deterministic and return a scalar value. The expression\ncan reference other table objects, such as by using a WITH clause or a WHERE clause. The delimiters around the expression can be either single quotes or a pair of dollar signs. Using $$ as the delimiter makes\nit easier to write expressions that contain single quotes. If the delimiter for the expression is the single quote character, then any single quotes within expression (for example, string literals) must be escaped by single quotes. The expression does not support the following: Using nondeterministic functions (for example, CURRENT_TIME ). Referencing an object that depends on a UDF or UDTF. Returning a nonscalar output."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the data metric function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "LANGUAGE   SQL",
            "description": "Specifies the language used to write the expression. SQL is the only supported language."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "A comment for the DMF."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc",
    "title": "DESCRIBE",
    "description": "Describes the details for the specified object."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-task",
    "title": "CREATE TASK",
    "description": "Creates a new task in the current/specified schema or replaces an existing task.",
    "syntax": "CREATE [ OR REPLACE ] TASK [ IF NOT EXISTS ] <name>\n    [ WITH TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n    [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n    [ { WAREHOUSE = <string> }\n      | { USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = <string> } ]\n    [ SCHEDULE = { '<num> { HOURS | MINUTES | SECONDS }'\n      | 'USING CRON <expr> <time_zone>' } ]\n    [ CONFIG = <configuration_string> ]\n    [ ALLOW_OVERLAPPING_EXECUTION = TRUE | FALSE ]\n    [ <session_parameter> = <value>\n      [ , <session_parameter> = <value> ... ] ]\n    [ USER_TASK_TIMEOUT_MS = <num> ]\n    [ SUSPEND_TASK_AFTER_NUM_FAILURES = <num> ]\n    [ ERROR_INTEGRATION = <integration_name> ]\n    [ SUCCESS_INTEGRATION = <integration_name> ]\n    [ LOG_LEVEL = '<log_level>' ]\n    [ COMMENT = '<string_literal>' ]\n    [ FINALIZE = <string> ]\n    [ TASK_AUTO_RETRY_ATTEMPTS = <num> ]\n    [ USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS = <num> ]\n    [ TARGET_COMPLETION_INTERVAL = '<num> { HOURS | MINUTES | SECONDS }' ]\n    [ SERVERLESS_TASK_MIN_STATEMENT_SIZE = '{ XSMALL | SMALL\n      | MEDIUM | LARGE | XLARGE | XXLARGE }' ]\n    [ SERVERLESS_TASK_MAX_STATEMENT_SIZE = '{ XSMALL | SMALL\n      | MEDIUM | LARGE | XLARGE | XXLARGE }' ]\n  [ AFTER <string> [ , <string> , ... ] ]\n  [ WHEN <boolean_expr> ]\n  AS\n    <sql>",
    "examples": [
        {
            "code": "CREATE TASK t1\n  SCHEDULE = 'USING CRON 0 9-17 * * SUN America/Los_Angeles'\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  AS\n    SELECT CURRENT_TIMESTAMP;"
        },
        {
            "code": "CREATE TASK mytask_hour\n  WAREHOUSE = mywh\n  SCHEDULE = 'USING CRON 0 9-17 * * SUN America/Los_Angeles'\n  AS\n    SELECT CURRENT_TIMESTAMP;"
        },
        {
            "code": "CREATE TASK t1\n  SCHEDULE = '60 MINUTES'\n  TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD HH24'\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  AS\n    INSERT INTO mytable(ts) VALUES(CURRENT_TIMESTAMP);"
        },
        {
            "code": "CREATE TASK mytask_minute\n  WAREHOUSE = mywh\n  SCHEDULE = '5 MINUTES'\n  AS\n    INSERT INTO mytable(ts) VALUES(CURRENT_TIMESTAMP);"
        },
        {
            "code": "CREATE TASK mytask1\n  WAREHOUSE = mywh\n  SCHEDULE = '5 MINUTES'\n  WHEN\n    SYSTEM$STREAM_HAS_DATA('MYSTREAM')\n  AS\n    INSERT INTO mytable1(id,name) SELECT id, name FROM mystream WHERE METADATA$ACTION = 'INSERT';"
        },
        {
            "code": "-- Create task5 and specify task2, task3, task4 as predecessors tasks.\n-- The new task is a serverless task that inserts the current timestamp into a table column.\nCREATE TASK task5\n  AFTER task2, task3, task4\nAS\n  INSERT INTO t1(ts) VALUES(CURRENT_TIMESTAMP);"
        },
        {
            "code": "-- Create a stored procedure that unloads data from a table\n-- The COPY statement in the stored procedure unloads data to files in a path identified by epoch time (using the Date.now() method)\nCREATE OR REPLACE PROCEDURE my_unload_sp()\n  returns string not null\n  language javascript\n  AS\n    $$\n      var my_sql_command = \"\"\n      var my_sql_command = my_sql_command.concat(\"copy into @mystage\",\"/\",Date.now(),\"/\",\" from mytable overwrite=true;\");\n      var statement1 = snowflake.createStatement( {sqlText: my_sql_command} );\n      var result_set1 = statement1.execute();\n    return my_sql_command; // Statement returned for info/debug purposes\n    $$;\n\n-- Create a task that calls the stored procedure every hour\nCREATE TASK my_copy_task\n  WAREHOUSE = mywh\n  SCHEDULE = '60 MINUTES'\n  AS\n    CALL my_unload_sp();"
        },
        {
            "code": "!set sql_delimiter=/\nCREATE OR REPLACE TASK test_logging\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  SCHEDULE = 'USING CRON  0 * * * * America/Los_Angeles'\n  AS\n    BEGIN\n      ALTER SESSION SET TIMESTAMP_OUTPUT_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF';\n      SELECT CURRENT_TIMESTAMP;\n    END;/\n!set sql_delimiter=';'"
        },
        {
            "code": "CREATE TASK t1\n  USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n  SCHEDULE = '15 SECONDS'\n  AS\n    EXECUTE IMMEDIATE\n    $$\n    DECLARE\n      radius_of_circle float;\n      area_of_circle float;\n    BEGIN\n      radius_of_circle := 3;\n      area_of_circle := pi() * radius_of_circle * radius_of_circle;\n      return area_of_circle;\n    END;\n    $$;"
        },
        {
            "code": "CREATE OR REPLACE TASK root_task_with_config\n  WAREHOUSE=mywarehouse\n  SCHEDULE='10 m'\n  CONFIG=$${\"output_dir\": \"/temp/test_directory/\", \"learning_rate\": 0.1}$$\n  AS\n    BEGIN\n      LET OUTPUT_DIR STRING := SYSTEM$GET_TASK_GRAPH_CONFIG('output_dir')::string;\n      LET LEARNING_RATE DECIMAL := SYSTEM$GET_TASK_GRAPH_CONFIG('learning_rate')::DECIMAL;\n    ...\n    END;"
        },
        {
            "code": "CREATE TASK finalize_task\n  WAREHOUSE = my_warehouse\n  FINALIZE = my_root_task\n  AS\n    CALL SYSTEM$SEND_EMAIL(\n      'my_email_int',\n      'first.last@example.com, first2.last2@example.com',\n      'Email Alert: Task A has finished.',\n      'Task A has successfully finished.\\nStart Time: 10:10:32\\nEnd Time: 12:15:45\\nTotal Records Processed: 115678'\n    );"
        },
        {
            "code": "CREATE TASK triggeredTask  WAREHOUSE = my_warehouse\n  WHEN system$stream_has_data('my_stream')\n  AS\n    INSERT INTO my_downstream_table\n    SELECT * FROM my_stream;\n\nALTER TASK triggeredTask RESUME;"
        },
        {
            "code": "CREATE OR ALTER TASK my_task\n  WAREHOUSE = my_warehouse\n  SCHEDULE = '60 MINUTES'\n  AS\n    SELECT PI();"
        },
        {
            "code": "CREATE OR ALTER TASK my_task\n  WAREHOUSE = regress\n  AFTER my_other_task\n  AS\n    SELECT 2 * PI();"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier for the task; must be unique for the schema in which the task is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes, such as \"My object\" . Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "sql",
            "description": "Any one of the following: Single SQL statement Call to a stored procedure Procedural logic using Snowflake Scripting The SQL code is executed when the task runs. Verify that the {sql} executes as expected before using it in a task."
        },
        {
            "name": "WAREHOUSE   =   string",
            "description": "Specifies the virtual warehouse that provides compute resources for task runs. Omit this parameter to use serverless compute resources for runs of this task. Snowflake automatically resizes and scales serverless\ncompute resources as required for each workload. When a schedule is specified for a task, Snowflake adjusts the resource size to\ncomplete future runs of the task within the specified time frame. To specify the initial warehouse size for the task, set the USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = string parameter."
        },
        {
            "name": "USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE   =   string",
            "description": "Applied only to serverless tasks. Specifies the size of the compute resources to provision for the first run of the task, before a task history is available for\nSnowflake to determine an ideal size. Once a task has successfully completed a few runs, Snowflake ignores this parameter setting. Note that if the task history is unavailable for a given task, the compute resources revert to this initial size. Note If a WAREHOUSE = string parameter value is specified, then setting this parameter produces a user error. The size is equivalent to the compute resources available when creating a warehouse (using CREATE WAREHOUSE ), such as SMALL , MEDIUM , or LARGE . The largest size supported by the parameter\nis XXLARGE . If the parameter is omitted, the first runs of the task are executed using a medium-sized ( MEDIUM ) warehouse. You can change the initial size (using ALTER TASK ) after the task is created but before it has run successfully once. Changing the parameter after the first run of this task starts has no effect on the\ncompute resources for current or future task runs. Note that suspending and resuming a task doesn’t remove the task history used to size the compute resources. The task history is\nonly removed if the task is recreated (using the CREATE OR REPLACE TASK syntax). For more information about this parameter, see USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE ."
        },
        {
            "name": "SCHEDULE   =   ...",
            "description": "Specifies the schedule for periodically running the task: Note For Triggered tasks , a schedule is not required. For other tasks, a schedule must be defined for a standalone task or the root task in a task graph ;\notherwise, the task only runs if manually executed using EXECUTE TASK . A schedule cannot be specified for child tasks in a task graph. Specifies a cron expression and time zone for periodically running the task. Supports a subset of standard cron utility syntax. ' expr ' : The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the task is scheduled for April, July, and October, which is every 3 months, starting with the\n4th month of the year. The same schedule is maintained in subsequent years. That is, the task is not scheduled to run in\nJanuary (3 months after the October run). Timing examples: SCHEDULE Value Description * * * * * UTC Every minute. UTC time zone. 0/5 * * * * UTC Every five minutes, starting at the top of the hour. UTC time zone. 5 * * * * UTC The 5th minute of every hour. UTC time zone. 30 3 * * * UTC Every night at 3:30 a.m. UTC time zone. 0 6,18 * * * UTC Twice daily, at 6:00 a.m. and 6:00 p.m.UTC time zone. 0 3 * * MON-FRI UTC Weekdays at 3:00 a.m. UTC time zone. 0 0 1 * * UTC At midnight on the first day of every month. UTC time zone. 0 0 L * * UTC At midnight on the last day of every month. UTC time zone. Note The cron expression defines all valid run times for the task. Snowflake attempts to run a task based on this schedule;\nhowever, any valid run time is skipped if a previous run hasn’t completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the task is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a task at midnight on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. To set a task to run in a shorter interval, use the SCHEDULE = ' <num> SECONDS' parameter instead. For example, SCHEDULE = '10 SECONDS' runs the task every 10 seconds. If a task is resumed during the minute defined in its cron expression,\nthe first scheduled run of the task is the next occurrence of the instance of the cron expression. For example, if task\nscheduled to run daily at midnight ( USING CRON 0 0 * * * ) is resumed at midnight plus 5 seconds ( 00:00:05 ), the\nfirst task run is scheduled for the following midnight. ' time_zone ' : The cron time zone for the task. The time zone is specified as a string literal. For a list of time zones, see the list of tz database time zones (in Wikipedia). Example: SCHEDULE Value Description 0 3 * * * America/Los_Angeles Every night at 3:00 a.m., Pacific Standard Time / Pacific Daylight Time (PST/PDT) time zone Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value for the account (or setting the value at the user or session level) does not change the time zone for the task. For time zones that observe daylight saving time, tasks scheduled during daylight saving time transitions can have unexpected behaviors. Examples: During the change from daylight saving time to standard time, a task scheduled to start at 1:00 a.m. in the America/Los_Angeles time zone ( 0 1 * * * America/Los_Angeles ) would run twice: at 1:00 a.m., and then again when 1:59:59 a.m. shifts to 1:00:00 a.m. local time. During the change from standard time to daylight saving time, a task scheduled to start at 2:00 a.m. in the America/Los_Angeles time zone ( 0 2 * * * America/Los_Angeles ) would not run because the local time shifts from 1:59:59 a.m. to 3:00:00 a.m. To avoid unexpected task executions due to daylight saving time, consider the following: Don’t schedule tasks to start between 1:00 a.m. and 2:59 a.m. Manually adjust the cron expression for tasks scheduled between 1 a.m. and 3 a.m. twice each year to compensate for the time change. Use a time format that does not apply daylight saving time, such as UTC. Do not change the time zone for the task. Specifies an interval of wait time between runs of the task. Snowflake sets the base interval time when the task is resumed ( ALTER TASK … RESUME ) or when a different interval is set ( ALTER TASK … SET SCHEDULE ). For example, if an INTERVAL value of 10 MINUTES is set and the task is enabled at 9:03 a.m., then the task runs at 9:13 a.m., 9:23 a.m., and\nso on. Snowflake ensures that a task won’t run before the set interval; however, Snowflake can’t guarantee task runs at precisely the specified interval. Values: { 10 - 691200 } SECONDS , { 1 - 11520 } MINUTES , or { 1-192 } HOURS (That is, from 10 seconds to the equivalent of 8 days). Accepts positive integers only. Also supports the notations: HOUR, MINUTE, SECOND, and H, M, S."
        },
        {
            "name": "CONFIG   =   configuration_string",
            "description": "Specifies a string representation of key value pairs that can be accessed by all tasks in the task graph. Must be in JSON format. For more information about getting the configuration string for the task that is currently running, see SYSTEM$GET_TASK_GRAPH_CONFIG . Note This parameter can only be set on a root task. The setting applies to all tasks in the task graph. The parameter can be set on standalone tasks but doesn’t affect the task behavior. Snowflake ensures only one instance of a\nstandalone task is running at a given time."
        },
        {
            "name": "ALLOW_OVERLAPPING_EXECUTION   =   TRUE   |   FALSE",
            "description": "Specifies whether to allow multiple instances of the task graph to run concurrently. Note This parameter can only be set on a root task. The setting applies to all tasks in the task graph. The parameter can be set on standalone tasks but doesn’t affect the task behavior. Snowflake ensures only one instance of a\nstandalone task is running at a given time. TRUE If the next scheduled run of the root task occurs while the current run of any child task is still in operation, another\ninstance of the task graph begins. If a root task is still running when the next scheduled run time occurs, then that scheduled time is\nskipped. FALSE The next run of a root task is scheduled only after all child tasks in the task graph have finished running. This means\nthat if the cumulative time required to run all tasks in the task graph exceeds the explicit scheduled time set in the definition of\nthe root task, at least one run of the Task Graph is skipped. Default: FALSE"
        },
        {
            "name": "session_parameter   =   value   [   ,   session_parameter   =   value   ...   ]",
            "description": "Specifies a comma-separated list of session parameters to set for the session when the task runs. A task supports all session\nparameters. For the complete list, see Session parameters ."
        },
        {
            "name": "'USING   CRON   expr   time_zone '",
            "description": "Specifies a cron expression and time zone for periodically running the task. Supports a subset of standard cron utility syntax."
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/ n",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the task is scheduled for April, July, and October, which is every 3 months, starting with the\n4th month of the year. The same schedule is maintained in subsequent years. That is, the task is not scheduled to run in\nJanuary (3 months after the October run)."
        },
        {
            "name": "' num   {   HOURS   |   MINUTES   |   SECONDS   }'",
            "description": "Specifies an interval of wait time between runs of the task. Snowflake sets the base interval time when the task is resumed ( ALTER TASK … RESUME ) or when a different interval is set ( ALTER TASK … SET SCHEDULE ). For example, if an INTERVAL value of 10 MINUTES is set and the task is enabled at 9:03 a.m., then the task runs at 9:13 a.m., 9:23 a.m., and\nso on. Snowflake ensures that a task won’t run before the set interval; however, Snowflake can’t guarantee task runs at precisely the specified interval. Values: { 10 - 691200 } SECONDS , { 1 - 11520 } MINUTES , or { 1-192 } HOURS (That is, from 10 seconds to the equivalent of 8 days). Accepts positive integers only. Also supports the notations: HOUR, MINUTE, SECOND, and H, M, S."
        },
        {
            "name": "USER_TASK_TIMEOUT_MS   =   num",
            "description": "Specifies the time limit on a single run of the task before it times out (in milliseconds). Note Before you increase the time limit on a task significantly, consider whether the SQL statement initiated by the task could be\noptimized (either by rewriting the statement or using a stored procedure) or the warehouse size should be increased. When both STATEMENT_TIMEOUT_IN_SECONDS and USER_TASK_TIMEOUT_MS are set, the timeout is the lowest non-zero value of the two parameters. When both STATEMENT_QUEUED_TIMEOUT_IN_SECONDS and USER_TASK_TIMEOUT_MS are set, the value of USER_TASK_TIMEOUT_MS takes precedence. For more information about this parameter, see USER_TASK_TIMEOUT_MS . Values: 0 - 604800000 (7 days). A value of 0 specifies that the maximum timeout value is enforced. Default: 3600000 (1 hour)"
        },
        {
            "name": "SUSPEND_TASK_AFTER_NUM_FAILURES   =   num",
            "description": "Specifies the number of consecutive failed task runs after which the current task is suspended automatically. Failed task runs\ninclude runs in which the SQL code in the task body either produces a user error or times out. Task runs that are skipped,\ncanceled, or that fail due to a system error are considered indeterminate and aren’t included in the count of failed task runs. Set the parameter on a standalone task or the root task in a task graph. When the parameter is set to a value greater than 0 , the\nfollowing behavior applies to runs of the standalone task or task graph: Standalone tasks are automatically suspended after the specified number of consecutive task runs either fail or time out. The root task is automatically suspended after the run of any single task in a task graph fails or times out the specified\nnumber of times in consecutive runs. When the parameter is set to 0 , failed tasks aren’t automatically suspended. The setting applies to tasks that rely on either serverless compute resources or virtual warehouse compute resources. For more information about this parameter, see SUSPEND_TASK_AFTER_NUM_FAILURES . Values: 0 - No upper limit. Default: 10"
        },
        {
            "name": "ERROR_INTEGRATION   =   ' integration_name '",
            "description": "Required only when configuring a task to send error notifications using Amazon Simple Notification Service (SNS), Microsoft Azure Event Grid, or Google Pub/Sub. Specifies the name of the notification integration used to communicate with Amazon SNS, MS Azure Event Grid, or Google Pub/Sub. For more information, see Enabling notifications for tasks ."
        },
        {
            "name": "SUCCESS_INTEGRATION   =   ' integration_name '",
            "description": "Required only when configuring a task to send success notifications using Amazon Simple Notification Service (SNS), Microsoft Azure Event Grid, or Google Pub/Sub. Specifies the name of the notification integration used to communicate with Amazon SNS, MS Azure Event Grid, or Google Pub/Sub. For more information, see Enabling notifications for tasks ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of events for this task that are ingested and made available in\nthe active event table. Events at the specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the task. Default: No value"
        },
        {
            "name": "AFTER   string   [   ,   string   ,   ...   ]",
            "description": "Specifies one or more predecessor tasks for the current task. Use this option to create a task graph or\nadd this task to an existing task graph. A task graph is a series of tasks that starts with a scheduled root task and is linked together\nby dependencies. Note that the structure of a task graph can be defined after all of its component tasks are created. Execute ALTER TASK … ADD AFTER statements to specify the predecessors for each task in the planned task graph. A task runs after all of its predecessor tasks have finished their own runs successfully (after a brief lag). Note The root task should have a defined schedule. Each child task must have one or more defined predecessor tasks, specified\nusing the AFTER parameter, to link the tasks together. A single task is limited to 100 predecessor tasks and 100 child tasks. In addition, a task graph is limited to a maximum of 1000 tasks total (including the root task) in either a resumed or suspended state. Accounts are currently limited to a maximum of 30000 resumed tasks. All tasks in a task graph must have the same task owner. A single role must have the OWNERSHIP privilege on all of the tasks in\nthe task graph. All tasks in a task graph must exist in the same schema. The root task must be suspended before any task is recreated (using the CREATE OR REPLACE TASK syntax) or a child task\nis added (using CREATE TASK … AFTER or ALTER TASK … ADD AFTER) or removed (using ALTER TASK … REMOVE AFTER). If any task in a task graph is cloned, the role that clones the task becomes the owner of the clone by default. If the owner of the original task creates the clone, then the task clone retains the link between the task and the predecessor\ntask. This means the same predecessor task triggers both the original task and the task clone. If another role creates the clone, then the task clone can have a schedule but not a predecessor. Current limitations: Snowflake guarantees that at most one instance of a task with a defined schedule is running at a given time; however, we cannot\nprovide the same guarantee for tasks with a defined predecessor task."
        },
        {
            "name": "WHEN   boolean_expr",
            "description": "Specifies a Boolean SQL expression; multiple conditions joined with AND/OR are supported. When a task is triggered (based on its SCHEDULE or AFTER setting), it validates the conditions of the expression to determine whether to execute. If the\nconditions of the expression are not met, then the task skips the current run. Any tasks that identify this task as a\npredecessor also don’t run. The following are supported in a task WHEN clause: SYSTEM$STREAM_HAS_DATA is supported for evaluation in the SQL expression. This function indicates whether a specified stream contains change tracking data. You can use this function to evaluate whether the specified stream contains\nchange data before starting the current run. If the result is FALSE, then the task doesn’t run. Note SYSTEM$STREAM_HAS_DATA is designed to avoid returning a FALSE value even when the stream contains\nchange data. However, this function isn’t guaranteed to avoid returning a TRUE value when the stream contains no change data. SYSTEM$GET_PREDECESSOR_RETURN_VALUE is supported for evaluation in the SQL expression. This function retrieves the return value for the predecessor task in a task graph.  The return value can be used as part of\na boolean expression.  When using SYSTEM$GET_PREDECESSOR_RETURN_VALUE, you can cast the returned value to\nthe appropriate numeric, string, or boolean type if required. Simple examples include: Note Use of PARSE_JSON in TASK … WHEN expressions isn’t supported as it requires warehouse based compute resources. Boolean operators such as AND, OR, NOT, and others. Simple example that runs whenever data changes in either of two streams: Casts between numeric, string, and boolean types. Comparison operators such as equal, not equal, greater than, less than, and others. Validating the conditions of the WHEN expression does not require compute resources. The validation is instead processed in the cloud\nservices layer. A nominal charge accrues each time a task evaluates its WHEN condition and doesn’t run. The charges accumulate each time\nthe task is triggered until it runs. At that time, the charge is converted to Snowflake credits and added to the compute resource usage\nfor the task run. Generally the compute time to validate the condition is insignificant compared to task execution time. As a best practice, align\nscheduled and actual task runs as closely as possible. Avoid task schedules that don’t align with task runs. For\nexample, if data is inserted into a table with a stream roughly every 24 hours, don’t schedule a task that checks for stream data\nevery minute. The charge to validate the WHEN expression with each run is generally insignificant, but the charges are cumulative. Note that daily consumption of cloud services that falls below the 10% quota of the daily usage of the compute resources accumulates no cloud services charges."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        },
        {
            "name": "FINALIZE   =   string",
            "description": "Specifies the name of a root task that the finalizer task is associated with. Finalizer tasks run after all other tasks in the task graph run to completion. You can define the SQL of a finalizer task to handle notifications and the release and cleanup of resources that a task graph uses. For more information, see Finalizer task . A root task can only have one finalizer task. If you create multiple finalizer tasks for a root task, the task creation will fail. A finalizer task cannot have any child tasks. Any command attempting to make the finalizer task a predecessor will fail. A finalizer task cannot have a schedule. Creating a finalizer task with a schedule will fail. Default: No value"
        },
        {
            "name": "TASK_AUTO_RETRY_ATTEMPTS   =   num",
            "description": "Specifies the number of automatic task graph retry attempts. If any task graphs complete in a FAILED state, Snowflake can automatically\nretry the task graphs from the last task in the graph that failed. The automatic task graph retry is disabled by default. To enable this feature, set TASK_AUTO_RETRY_ATTEMPTS to a value greater than 0 on the root task of a task graph. Note that this parameter must be set to the root task of a task graph. If it’s set to a child task, an error will be returned. Values: 0 - 30 . Default: 0"
        },
        {
            "name": "USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS   =   num",
            "description": "Defines how frequently a task can execute in seconds. If data changes occur more often than the specified minimum, changes will be\ngrouped and processed together. The task will run every 12 hours even if this value is set to more than 12 hours. Values: Minimum 10 , maximum 604800 . Default: 30"
        },
        {
            "name": "TARGET_COMPLETION_INTERVAL   =   ' num   {   HOURS   |   MINUTES   |   SECONDS   }'",
            "description": "Specifies the desired task completion time. This parameter only applies to serverless tasks. This property is only set on a Task. This parameter is required when you create serverless Triggered tasks . Values: { 10 - 86400 } SECONDS , { 1 - 1440 } MINUTES , or { 1-24 } HOURS (That is, from 10 seconds to the equivalent of 1 day). Accepts positive integers only. Also supports the notations: HOUR, MINUTE, SECOND, and H, M, S. Default: Snowflake resizes serverless compute resources to complete before the next scheduled execution time."
        },
        {
            "name": "SERVERLESS_TASK_MIN_STATEMENT_SIZE   =   string",
            "description": "Specifies the minimum allowed warehouse size for the serverless task. This parameter only applies to serverless tasks. This parameter can be specified on the Task, Schema, Database, or Account. Precedence follows the standard parameter hierarchy. Values: Minimum XSMALL , Maximum XXLARGE . Values are consistent with WAREHOUSE_SIZE values . Also supports the notation: X2LARGE. Default: XSMALL Note that if both SERVERLESS_TASK_MIN_STATEMENT_SIZE and USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE are specified, SERVERLESS_TASK_MIN_STATEMENT_SIZE must be equal to or smaller than USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE."
        },
        {
            "name": "SERVERLESS_TASK_MAX_STATEMENT_SIZE   =   string",
            "description": "Specifies the maximum allowed warehouse size for the serverless task. This parameter only applies to serverless tasks. This parameter can be specified on the Task, Schema, Database, or Account. Precedence follows the standard parameter hierarchy. Values: Minimum XSMALL , Maximum XXLARGE . Also supports the notation: X2LARGE. Default: XXLARGE Note that if both SERVERLESS_TASK_MIN_STATEMENT_SIZE and SERVERLESS_TASK_MAX_STATEMENT_SIZE are specified, SERVERLESS_TASK_MIN_STATEMENT_SIZE must be less than or equal to SERVERLESS_TASK_MAX_STATEMENT_SIZE. SERVERLESS_TASK_MAX_STATEMENT_SIZE must be equal to or greater than USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE"
        }
    ],
    "usage_notes": "Tasks run using the task owner’s privileges. For the list of minimum required privileges to run tasks, see\nTask security.\nRun the SQL statement or call the stored procedure, as the task owner role, before you include it in a task definition to ensure\nthe role has the required privileges on objects referenced by the SQL or stored procedure.\nFor serverless tasks:\nServerless compute resources for a task can range from the equivalent of XSMALL to XXLARGE in warehouse sizes. To request a\nsize increase, contact Snowflake Support.\nIndividual tasks in a task graph can use serverless or user-managed compute resources. Using the serverless compute for\nall tasks in the task graph isn’t required.\nIf a task fails with an unexpected error, you can receive a notification about the error.\nFor more information on configuring task error notifications, see Enabling notifications for tasks.\nBy default, a DML statement executed without explicitly starting a transaction is automatically committed on success or rolled back on\nfailure at the end of the statement. This behavior is called autocommit and is controlled with the AUTOCOMMIT parameter.\nThis parameter must be set to TRUE. If the AUTOCOMMIT parameter is set to FALSE at the account level, then set the parameter to\nTRUE for the individual task (using ALTER TASK … SET AUTOCOMMIT = TRUE); otherwise, any DML statement executed by the task fails.\nOnly one task should consume data from a stream. Create multiple streams for the same table to be consumed by more than one task. When a\ntask consumes the data in a stream using a DML statement, the stream advances the offset and change data is no longer available for the\nnext task to consume.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-snapshot",
    "title": "CREATE SNAPSHOT",
    "description": "Creates or replaces a snapshot of a named volume from the specified service instance. The snapshot is created in the current schema.",
    "syntax": "CREATE [ OR REPLACE ] SNAPSHOT [ IF NOT EXISTS ] <name>\n  FROM SERVICE <service_name>\n  VOLUME \"<volume_name>\"\n  INSTANCE <instance_id>\n  [ COMMENT = '<string_literal>']\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , ... ] ) ]",
    "examples": [
        {
            "code": "CREATE SNAPSHOT snapshot_0\n  FROM SERVICE example_service\n  VOLUME \"data\"\n  INSTANCE 0\n  COMMENT='new snapshot';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (that is, name) for the snapshot; must be unique for the schema in which the snapshot is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "FROM   SERVICE   service_name",
            "description": "Specifies the name of the service."
        },
        {
            "name": "VOLUME   \" volume_name \"",
            "description": "Specifies the name of the volume associated with the service. Snapshots can only be taken for block storage volumes (and not for local, memory, or stage volumes). Volume names are case-sensitive. Therefore, double quotes should always be used to match the corresponding name in the service specification."
        },
        {
            "name": "INSTANCE   instance_id",
            "description": "Index of the service instance. The service instance index starts at 0 and the range is [0, ..., MAX_INSTANCES - 1] . You can call the SYSTEM$GET_SERVICE_STATUS — Deprecated function to get the relevant information."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the service. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nSnowflake deletes job services approximately 10 minutes after its execution completes. To preserve the content of a block storage volume used by the job service, you must create a snapshot before Snowflake deletes the job. For example, you might use a stored procedure to first execute a job service and create a snapshot immediately following it.\nA schema cannot contain snapshots with the same name. When creating a snapshot, if a snapshot with the same name already exists in the schema, an error is returned and the snapshot is not created, unless the optional OR REPLACE keyword is included in the command, in which case Snowflake deletes the existing snapshot and creates a new snapshot.\nImportant\nA snapshot deleted using the DROP SNAPSHOT or the CREATE OR REPLACE SNAPSHOT command cannot be restored. For volumes containing critical data, create new snapshots with unique names, such as using timestamps in the snapshot name."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-notification-integration",
    "title": "CREATE NOTIFICATION INTEGRATION",
    "description": "Creates a new notification integration in the account or replaces an existing integration. A notification integration is a\nSnowflake object that provides an interface between Snowflake and third-party messaging services (third-party cloud message\nqueuing services, email services, webhooks, etc.)."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-alert",
    "title": "CREATE ALERT",
    "description": "Creates a new alert in the current schema.",
    "syntax": "CREATE [ OR REPLACE ] ALERT [ IF NOT EXISTS ] <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ SCHEDULE = '{ <num> MINUTE | USING CRON <expr> <time_zone> }' ]\n  [ WAREHOUSE = <warehouse_name> ]\n  [ COMMENT = '<string_literal>' ]\n  IF( EXISTS(\n    <condition>\n  ))\n  THEN\n    <action>",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] ALERT [ IF NOT EXISTS ] <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ SCHEDULE = '{ <num> MINUTE | USING CRON <expr> <time_zone> }' ]\n  [ WAREHOUSE = <warehouse_name> ]\n  [ COMMENT = '<string_literal>' ]\n  IF( EXISTS(\n    <condition>\n  ))\n  THEN\n    <action>"
        },
        {
            "code": "CREATE [ OR REPLACE ] ALERT <name> CLONE <source_alert>\n  [ ... ]"
        },
        {
            "code": "# __________ minute (0-59)\n# | ________ hour (0-23)\n# | | ______ day of month (1-31, or L)\n# | | | ____ month (1-12, JAN-DEC)\n# | | | | _ day of week (0-6, SUN-SAT, or L)\n# | | | | |\n# | | | | |\n  * * * * *"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the alert; must be unique for the schema in which the alert is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "IF(   EXISTS(   condition   ))",
            "description": "The SQL statement that represents the condition for the alert. You can use the following commands: SELECT SHOW <objects> CALL If the statement returns one or more rows, the action for the alert is executed."
        },
        {
            "name": "THEN   action",
            "description": "The SQL statement that should be executed if the condition returns one or more rows. To send a notification, you can call the SYSTEM$SEND_EMAIL or SYSTEM$SEND_SNOWFLAKE_NOTIFICATION stored procedure ."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Specifies the virtual warehouse that provides compute resources for executing this alert. Note For serverless alerts , do not set this property."
        },
        {
            "name": "SCHEDULE   ...",
            "description": "Specifies the schedule for periodically evaluating the condition for the alert on a schedule. When you create an alert, omitting this parameter or setting it to NULL creates an alert on new data . For alerts on a schedule, you can specify the schedule in one of the following ways: USING CRON expr time_zone Specifies a cron expression and time zone for periodically evaluating the condition for the alert. Supports a subset of\nstandard cron utility syntax. The cron expression consists of the following fields: The following special characters are supported: Special Character Description * Wildcard. When specified for a given field, the alert runs at every unit of time for that field. For example, * in the month field specifies that the alert runs every month. L Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday”\n(“5L”) of a given month. In the day-of-month field, it specifies the last day of the month. / n Indicates the n th instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is specified in the month field, then the evaluation of the condition is scheduled for April,\nJuly and October (i.e. every 3 months, starting with the 4th month of the year). The same schedule is maintained in subsequent years. That is, the condition is not scheduled to be evaluated in January\n(3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value for the account (or setting the value at the user or session level) does not change the time zone for the alert. The cron expression defines all valid times for the evaluation of the condition for the alert. Snowflake attempts\nto evaluate the condition based on this schedule; however, any valid run time is skipped if a previous run has not\ncompleted before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the evaluation of the\ncondition is scheduled on days satisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules an evaluation at 0AM on any 10th to 20th day of the month\nand also on any Tuesday or Thursday outside of those dates. num MINUTE Specifies an interval (in minutes) of wait time inserted between evaluations of the alert. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set when the alert is resumed (using ALTER ALERT … RESUME ). The base interval time starts the interval counter from the current clock time. For example, if an alert is created with 10 MINUTE and the alert is resumed at 9:03 AM, then the condition for the alert is evaluated at 9:13 AM, 9:23 AM, and so\non. Note that we make a best effort to ensure absolute precision, but only guarantee that conditions are not evaluated before their set interval occurs (e.g. in the current example, the condition could be evaluated first at 9:14 AM but\ndefinitely not at 9:12 AM). Note The maximum supported value is 11520 (8 days). Alerts that have a greater num MINUTE value never have their\nconditions evaluated."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the alert."
        }
    ],
    "usage_notes": "Alerts are executed using the privileges granted to the alert owner (i.e. the role that has the OWNERSHIP privilege on the\nalert). For the list of minimum required privileges to execute alerts, see Granting the privileges to create alerts.\nTo verify that the alert owner role has the required privileges to execute SQL statements for the condition and action, we\nrecommend that you execute these statements using the alert owner role before specifying them in CREATE ALERT.\nWhen you create an alert, the alert is suspended by default.\nTo make the alert active, you must execute ALTER ALERT … RESUME.\nWhen you execute CREATE ALERT or ALTER ALERT, some validation checks are not performed on the statements in the condition and\naction, including:\nThe resolution of the identifiers for objects.\nThe resolution of the data types of expressions.\nThe verification of the number and types of arguments in a function call.\nThe CREATE ALERT and ALTER ALERT commands do not fail if the SQL statement for a condition or action specifies an invalid\nidentifier, incorrect data type, incorrect number and types of function arguments, etc. Instead, the failure occurs when the\nalert executes.\nTo check for failures in an existing alert, use the ALERT_HISTORY table function.\nTo avoid these types of failures, before you specify the conditions and actions for alerts, verify the SQL expressions and\nstatements for those conditions and actions.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-hybrid-table",
    "title": "CREATE HYBRID TABLE",
    "description": "Creates a new hybrid table in the current/specified schema or replaces an existing table. A table can have multiple columns,\nwith each column definition consisting of a name, data type, and optionally whether the column:",
    "syntax": "CREATE [ OR REPLACE ] HYBRID TABLE [ IF NOT EXISTS ] <table_name>\n  ( <col_name> <col_type>\n    [\n      {\n        DEFAULT <expr>\n        | { AUTOINCREMENT | IDENTITY }\n          [\n            {\n              ( <start_num> , <step_num> )\n              | START <num> INCREMENT <num>\n            }\n          ]\n          [ { ORDER | NOORDER } ]\n      }\n    ]\n    [ NOT NULL ]\n    [ inlineConstraint ]\n    [ COMMENT '<string_literal>' ]\n    [ , <col_name> <col_type> [ ... ] ]\n    [ , outoflineConstraint ]\n    [ , outoflineIndex ]\n    [ , ... ]\n  )\n  [ COMMENT = '<string_literal>' ]\n\ninlineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE | PRIMARY KEY | { [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ] } }\n  [ <constraint_properties> ]\n\noutoflineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n      REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n  }\n  [ <constraint_properties> ]\n  [ COMMENT '<string_literal>' ]\n\noutoflineIndex ::=\n  INDEX <index_name> ( <col_name> [ , <col_name> , ... ] )\n    [ INCLUDE ( <col_name> [ , <col_name> , ... ] ) ]",
    "examples": [
        {
            "code": "CREATE HYBRID TABLE mytable (\n  customer_id INT AUTOINCREMENT PRIMARY KEY,\n  full_name VARCHAR(255),\n  email VARCHAR(255) UNIQUE,\n  extended_customer_info VARIANT,\n  INDEX index_full_name (full_name)\n);"
        },
        {
            "code": "+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table MYTABLE successfully created. |\n+-------------------------------------+"
        },
        {
            "code": "INSERT INTO mytable (customer_id, full_name, email, extended_customer_info)\n  SELECT 100, 'Jane Doe', 'jdoe@example.com',\n    parse_json('{\"address\": \"1234 Main St\", \"city\": \"San Francisco\", \"state\": \"CA\", \"zip\":\"94110\"}');"
        },
        {
            "code": "+-------------------------+\n| number of rows inserted |\n|-------------------------|\n|                       1 |\n+-------------------------+"
        },
        {
            "code": "200001 (22000): Primary key already exists"
        },
        {
            "code": "Duplicate key value violates unique constraint \"SYS_INDEX_MYTABLE_UNIQUE_EMAIL\""
        },
        {
            "code": "SHOW TABLES LIKE 'mytable';"
        },
        {
            "code": "+-------------------------------+---------+---------------+-------------+-------+-----------+---------+------------+------+-------+--------+----------------+----------------------+-----------------+---------------------+------------------------------+---------------------------+-------------+\n| created_on                    | name    | database_name | schema_name | kind  | is_hybrid | comment | cluster_by | rows | bytes | owner  | retention_time | automatic_clustering | change_tracking | search_optimization | search_optimization_progress | search_optimization_bytes | is_external |\n|-------------------------------+---------+---------------+-------------+-------+-----------+---------+------------+------+-------+--------+----------------+----------------------+-----------------+---------------------+------------------------------+---------------------------+-------------|\n| 2022-02-23 23:53:19.707 +0000 | MYTABLE | MYDB          | PUBLIC      | TABLE | Y         |         |            | NULL |  NULL | MYROLE | 10             | OFF                  | OFF             | OFF                 |                         NULL |                      NULL | N           |\n+-------------------------------+---------+---------------+-------------+-------+-----------+---------+------------+------+-------+--------+----------------+----------------------+-----------------+---------------------+------------------------------+---------------------------+-------------+"
        },
        {
            "code": "SHOW HYBRID TABLES;"
        },
        {
            "code": "+-------------------------------+---------------------------+---------------+-------------+--------------+--------------+------+-------+---------+\n| created_on                    | name                      | database_name | schema_name | owner        | datastore_id | rows | bytes | comment |\n|-------------------------------+---------------------------+---------------+-------------+--------------+--------------+------+-------+---------|\n| 2022-02-24 02:07:31.877 +0000 | MYTABLE                   | DEMO_DB       | PUBLIC      | ACCOUNTADMIN |         2002 | NULL |  NULL |         |\n+-------------------------------+---------------------------+---------------+-------------+--------------+--------------+------+-------+---------+"
        },
        {
            "code": "DESCRIBE TABLE mytable;"
        },
        {
            "code": "+-------------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name              | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|-------------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| CUSTOMER_ID       | NUMBER(38,0) | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | NULL    | NULL        |\n| FULL_NAME         | VARCHAR(256) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| APPLICATION_STATE | VARIANT      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+-------------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "SELECT customer_id, full_name, email, extended_customer_info\n  FROM mytable\n  WHERE extended_customer_info['state'] = 'CA';"
        },
        {
            "code": "+-------------+-----------+------------------+------------------------------+\n| CUSTOMER_ID | FULL_NAME | EMAIL            | EXTENDED_CUSTOMER_INFO       |\n|-------------+-----------+------------------+------------------------------|\n|         100 | Jane Doe  | jdoe@example.com | {                            |\n|             |           |                  |   \"address\": \"1234 Main St\", |\n|             |           |                  |   \"city\": \"San Francisco\",   |\n|             |           |                  |   \"state\": \"CA\",             |\n|             |           |                  |   \"zip\": \"94110\"             |\n|             |           |                  | }                            |\n+-------------+-----------+------------------+------------------------------+"
        },
        {
            "code": "CREATE OR REPLACE HYBRID TABLE team\n  (team_id INT PRIMARY KEY,\n  team_name VARCHAR(40),\n  stadium VARCHAR(40));\n\nCREATE OR REPLACE HYBRID TABLE player\n  (player_id INT PRIMARY KEY,\n  first_name VARCHAR(40),\n  last_name VARCHAR(40),\n  team_id INT,\n  FOREIGN KEY (team_id) REFERENCES team(team_id));"
        },
        {
            "code": "INSERT INTO team VALUES (1, 'Bayern Munich', 'Allianz Arena');\nINSERT INTO player VALUES (100, 'Harry', 'Kane', 1);\nINSERT INTO player VALUES (301, 'Gareth', 'Bale', 3);"
        },
        {
            "code": "200009 (22000): Foreign key constraint \"SYS_INDEX_PLAYER_FOREIGN_KEY_TEAM_ID_TEAM_TEAM_ID\" was violated."
        },
        {
            "code": "INSERT INTO player VALUES (200, 'Tommy', 'Atkins', NULL);"
        },
        {
            "code": "200009 (22000): Foreign key constraint \"SYS_INDEX_PLAYER_FOREIGN_KEY_TEAM_ID_TEAM_TEAM_ID\" was violated."
        },
        {
            "code": "SELECT * FROM team t, player p WHERE t.team_id=p.team_id;"
        },
        {
            "code": "+---------+---------------+---------------+-----------+------------+-----------+---------+\n| TEAM_ID | TEAM_NAME     | STADIUM       | PLAYER_ID | FIRST_NAME | LAST_NAME | TEAM_ID |\n|---------+---------------+---------------+-----------+------------+-----------+---------|\n|       1 | Bayern Munich | Allianz Arena |       100 | Harry      | Kane      |       1 |\n+---------+---------------+---------------+-----------+------------+-----------+---------+"
        },
        {
            "code": "INSERT INTO team VALUES (0, 'Unknown', 'Unknown');\nINSERT INTO player VALUES (200, 'Tommy', 'Atkins', 0);\n\nSELECT * FROM team t, player p WHERE t.team_id=p.team_id;"
        },
        {
            "code": "+---------+---------------+---------------+-----------+------------+-----------+---------+\n| TEAM_ID | TEAM_NAME     | STADIUM       | PLAYER_ID | FIRST_NAME | LAST_NAME | TEAM_ID |\n|---------+---------------+---------------+-----------+------------+-----------+---------|\n|       1 | Bayern Munich | Allianz Arena |       100 | Harry      | Kane      |       1 |\n|       0 | Unknown       | Unknown       |       200 | Tommy      | Atkins    |       0 |\n+---------+---------------+---------------+-----------+------------+-----------+---------+"
        },
        {
            "code": "CREATE HYBRID TABLE employee (\n    employee_id INT PRIMARY KEY,\n    employee_name VARCHAR(200),\n    employee_department VARCHAR(200),\n    INDEX idx_department (employee_department) INCLUDE (employee_name)\n);"
        },
        {
            "code": "INSERT INTO employee VALUES\n  (1, 'John Doe', 'Marketing'),\n  (2, 'Jane Smith', 'Sales'),\n  (3, 'Bob Johnson', 'Finance'),\n  (4, 'Alice Brown', 'Marketing');"
        },
        {
            "code": "SELECT employee_name FROM employee WHERE employee_department = 'Marketing';\nSELECT employee_name FROM employee WHERE employee_department IN ('Marketing', 'Sales');"
        },
        {
            "code": "CREATE OR REPLACE HYBRID TABLE ht1pk\n  (COL1 NUMBER(38,0) NOT NULL COMMENT 'Primary key',\n  COL2 NUMBER(38,0) NOT NULL,\n  COL3 VARCHAR(16777216),\n  CONSTRAINT PKEY_1 PRIMARY KEY (COL1));\n\nDESCRIBE TABLE ht1pk;"
        },
        {
            "code": "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+-------------+-------------+----------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment     | policy name | privacy domain |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+-------------+-------------+----------------|\n| COL1 | NUMBER(38,0)      | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | Primary key | NULL        | NULL           |\n| COL2 | NUMBER(38,0)      | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL        | NULL        | NULL           |\n| COL3 | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL        | NULL        | NULL           |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+-------------+-------------+----------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the table; must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "col_name",
            "description": "Specifies the column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. For more details, see Identifier requirements and Reserved & limited keywords . Note In addition to the standard reserved keywords, the following keywords cannot be used as column identifiers because they are reserved for ANSI-standard context functions: CURRENT_DATE CURRENT_ROLE CURRENT_TIME CURRENT_TIMESTAMP CURRENT_USER For the list of reserved keywords, see Reserved & limited keywords ."
        },
        {
            "name": "col_type",
            "description": "Specifies the data type for the column. For details about the data types that can be specified for table columns, see SQL data types reference ."
        },
        {
            "name": "PRIMARY   KEY   (   col_name   [   ,   col_name   ,   ...   ]   )",
            "description": "Specifies the required primary key constraint for the table, either within a column definition (inline) or separately (out-of-line).\nSee also Constraints for hybrid tables . For complete syntax details, see CREATE | ALTER TABLE … CONSTRAINT . For general information about constraints, see Constraints ."
        },
        {
            "name": "DEFAULT   ...   or   .   AUTOINCREMENT   ...",
            "description": "Specifies whether a default value is automatically inserted in the column if a value is not explicitly specified via an INSERT or\nCREATE HYBRID TABLE AS SELECT statement: Column default value is defined by the specified expression which can be any of the following: Constant value. Simple expression. Sequence reference ( seq_name .NEXTVAL ). A simple expression is an expression that returns a scalar value; however, the expression cannot contain\nreferences to: Subqueries. Aggregates. Window functions. External functions. When AUTOINCREMENT is used, the default value for the column starts with a specified number and each successive\nvalue is automatically generated. Values generated by an AUTOINCREMENT column are guaranteed to be unique. The\ndifference between any pair of the generated values is guaranteed to be a multiple of the increment amount. The optional ORDER and NOORDER parameters specify whether or not the generated values provide ordering\nguarantees as specified in Sequence Semantics . NOORDER is the default option for AUTOINCREMENT columns on hybrid tables. NOORDER typically provides significantly better performance for point writes. These parameters can only be used for columns with numeric data types (NUMBER, INT, FLOAT, etc.) AUTOINCREMENT and IDENTITY are synonymous. If either is specified for a column, Snowflake utilizes a\nsequence to generate the values for the column. For more information about sequences, see Using Sequences . The default value for both start and step/increment is 1 . Default: No value (the column has no default value) Note DEFAULT and AUTOINCREMENT are mutually exclusive; only one can be specified for a column. For performance-sensitive workloads, NOORDER is the recommended option for AUTOINCREMENT columns."
        },
        {
            "name": "CONSTRAINT   ...",
            "description": "Defines an inline or out-of-line constraint for the specified column(s) in the table. UNIQUE and FOREIGN KEY constraints\nare optional for hybrid table columns. See also Constraints for hybrid tables . For complete syntax details, see CREATE | ALTER TABLE … CONSTRAINT . For general information about constraints, see Constraints ."
        },
        {
            "name": "INDEX   index_name   (   col_name   [   ,   col_name   ,   ...   ]",
            "description": "Specifies a secondary index on one or more columns in the table. (When you define constraints on hybrid table columns,\nindexes are automatically created on those columns.) Indexes cannot be defined on the following columns: Semi-structured columns (VARIANT, OBJECT, ARRAY)\nbecause of space constraints associated with the underlying storage engines for the key of each record. Geospatial columns (GEOGRAPHY, GEOMETRY) or VECTOR columns . TIMESTAMP_TZ columns (or TIMESTAMP columns that resolve to TIMESTAMP_TZ). TIMESTAMP_NTZ columns are supported. Indexes can be defined when the table is created, or with the CREATE INDEX command. For more information about creating indexes for\nhybrid tables, see CREATE INDEX ."
        },
        {
            "name": "INCLUDE   (   col_name   [   ,   col_name   ,   ...   ]   )",
            "description": "Specifies one or more included columns for a secondary index. Using included columns with a secondary index is\nparticularly useful when queries frequently contain a set of columns in the SELECT list but not in\nthe list of WHERE predicates. See Create a secondary index with an INCLUDE column . INCLUDE columns cannot be semi-structured columns (VARIANT, OBJECT, ARRAY) or geospatial columns (GEOGRAPHY, GEOMETRY). INCLUDE columns can be specified only when creating a table with a secondary index. INCLUDE is not supported\nby CREATE INDEX ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment at the column, constraint, or table level. For details, see Comments on constraints . Default: No value"
        },
        {
            "name": "DEFAULT   expr",
            "description": "Column default value is defined by the specified expression which can be any of the following: Constant value. Simple expression. Sequence reference ( seq_name .NEXTVAL ). A simple expression is an expression that returns a scalar value; however, the expression cannot contain\nreferences to: Subqueries. Aggregates. Window functions. External functions."
        },
        {
            "name": "{   AUTOINCREMENT   |   IDENTITY   }   .   [   {   (   start_num   ,   step_num   )   |   START   num   INCREMENT   num   }   ]   .   [   {   ORDER   |   NOORDER   }   ]",
            "description": "When AUTOINCREMENT is used, the default value for the column starts with a specified number and each successive\nvalue is automatically generated. Values generated by an AUTOINCREMENT column are guaranteed to be unique. The\ndifference between any pair of the generated values is guaranteed to be a multiple of the increment amount. The optional ORDER and NOORDER parameters specify whether or not the generated values provide ordering\nguarantees as specified in Sequence Semantics . NOORDER is the default option for AUTOINCREMENT columns on hybrid tables. NOORDER typically provides significantly better performance for point writes. These parameters can only be used for columns with numeric data types (NUMBER, INT, FLOAT, etc.) AUTOINCREMENT and IDENTITY are synonymous. If either is specified for a column, Snowflake utilizes a\nsequence to generate the values for the column. For more information about sequences, see Using Sequences . The default value for both start and step/increment is 1 ."
        }
    ],
    "usage_notes": "To recreate or replace a hybrid table, call the GET_DDL function to see the definition of the\nhybrid table before running a CREATE OR REPLACE HYBRID TABLE command.\nYou cannot create hybrid tables that are temporary or transient. In turn, you cannot\ncreate hybrid tables within transient schemas or databases.\nA schema cannot contain tables and/or views with the same name. When creating a table:\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then\ncreating a new table with the same name.\nNote that the drop and create actions occur in a single atomic operation. This means that any queries concurrent with the\nCREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nFor information about cloning hybrid tables, see Clone databases that contain hybrid tables.\nSimilar to reserved keywords, ANSI-reserved function names\n(CURRENT_DATE, CURRENT_TIMESTAMP, etc.) cannot be used as\ncolumn names.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-image-repository",
    "title": "CREATE IMAGE REPOSITORY",
    "description": "Creates a new image repository in the\ncurrent schema.",
    "syntax": "CREATE [ OR REPLACE ] IMAGE REPOSITORY [ IF NOT EXISTS ] <name>\n  [ ENCRYPTION = ( TYPE = 'SNOWFLAKE_FULL' | TYPE = 'SNOWFLAKE_SSE' ) ]",
    "examples": [
        {
            "code": "CREATE OR REPLACE IMAGE REPOSITORY tutorial_repository;"
        },
        {
            "code": "CREATE OR REPLACE IMAGE REPOSITORY tutorial_repository\nENCRYPTION = (type = 'SNOWFLAKE_FULL');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (that is, the name) for the image repository; it must be unique for the schema in which the repository is created. Quoted names for special characters or case-sensitive names are not supported. The same constraint also applies to database and\nschema names where you create an image repository. That is, database and schema names without quotes are valid when creating an\nimage repository."
        },
        {
            "name": "ENCRYPTION   =   (   TYPE   =   'SNOWFLAKE_FULL'   |   TYPE   =   'SNOWFLAKE_SSE'   )",
            "description": "Specifies the type of encryption to use for binaries stored in the image repository. You cannot change the encryption type after you create the image repository. Specifies the encryption type to use. Important If you require Tri-Secret Secure for security compliance, use the SNOWFLAKE_FULL encryption type for internal stages. SNOWFLAKE_SSE does not support Tri-Secret Secure. Possible values are the following: SNOWFLAKE_FULL : On-host (image registry host) and server-side encryption. Data is first encrypted by Snowflake’s image registry service before sending the data to cloud service provider storage (for example, Amazon S3) where your Snowflake account is hosted. Snowflake uses AES-GCM with a 128-bit encryption key by default.\nYou can configure a 256-bit key by setting the CLIENT_ENCRYPTION_KEY_SIZE parameter. All binaries are also automatically encrypted using AES-256 strong encryption on the server side. Note With SNOWFLAKE_FULL encryption, Snowflake might throttle requests against the public image repository API. This throttling is triggered only when Snowflake detects an unusually large number of parallel requests against the repository. Note that, the service creation will never be impacted. SNOWFLAKE_SSE : Server-side encryption only. The binaries are encrypted by the cloud service provider (for example, Amazon S3) where your Snowflake account is hosted when they arrive on the image repository storage area. Default: SNOWFLAKE_SSE"
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type to use."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-external-volume",
    "title": "CREATE EXTERNAL VOLUME",
    "description": "Creates a new external volume for Apache Iceberg™ tables\nin the account or replaces an existing external volume.",
    "syntax": "CREATE [ OR REPLACE ] EXTERNAL VOLUME [IF NOT EXISTS]\n  <name>\n  STORAGE_LOCATIONS =\n    (\n      (\n        NAME = '<storage_location_name>'\n        { cloudProviderParams | s3CompatibleStorageParams }\n      )\n      [, (...), ...]\n    )\n  [ ALLOW_WRITES = { TRUE | FALSE }]\n  [ COMMENT = '<string_literal>' ]\n\ncloudProviderParams (for Amazon S3) ::=\n  STORAGE_PROVIDER = '{ S3 | S3GOV }'\n  STORAGE_AWS_ROLE_ARN = '<iam_role>'\n  STORAGE_BASE_URL = '<protocol>://<bucket>[/<path>/]'\n  [ STORAGE_AWS_ACCESS_POINT_ARN = '<string>' ]\n  [ STORAGE_AWS_EXTERNAL_ID = '<external_id>' ]\n  [ ENCRYPTION = ( [ TYPE = 'AWS_SSE_S3' ] |\n              [ TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = '<string>' ] ] |\n              [ TYPE = 'NONE' ] ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\ncloudProviderParams (for Google Cloud Storage) ::=\n  STORAGE_PROVIDER = 'GCS'\n  STORAGE_BASE_URL = 'gcs://<bucket>[/<path>/]'\n  [ ENCRYPTION = ( [ TYPE = 'GCS_SSE_KMS' ] [ KMS_KEY_ID = '<string>' ] |\n              [ TYPE = 'NONE' ] ) ]\n\ncloudProviderParams (for Microsoft Azure) ::=\n  STORAGE_PROVIDER = 'AZURE'\n  AZURE_TENANT_ID = '<tenant_id>'\n  STORAGE_BASE_URL = 'azure://...'\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\ns3CompatibleStorageParams ::=\n  STORAGE_PROVIDER = 'S3COMPAT'\n  STORAGE_BASE_URL = 's3compat://<bucket>[/<path>/]'\n  CREDENTIALS = ( AWS_KEY_ID = '<string>' AWS_SECRET_KEY = '<string>' )\n  STORAGE_ENDPOINT = '<s3_api_compatible_endpoint>'",
    "examples": [
        {
            "code": "CREATE OR REPLACE EXTERNAL VOLUME exvol\n  STORAGE_LOCATIONS =\n      (\n        (\n            NAME = 'my-s3-us-west-2'\n            STORAGE_PROVIDER = 'S3'\n            STORAGE_BASE_URL = 's3://my-example-bucket/'\n            STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/myrole'\n            ENCRYPTION=(TYPE='AWS_SSE_KMS' KMS_KEY_ID='1234abcd-12ab-34cd-56ef-1234567890ab')\n        )\n      )\n  ALLOW_WRITES = TRUE;"
        },
        {
            "code": "CREATE EXTERNAL VOLUME exvol\n  STORAGE_LOCATIONS =\n    (\n      (\n        NAME = 'my-us-east-1'\n        STORAGE_PROVIDER = 'GCS'\n        STORAGE_BASE_URL = 'gcs://mybucket1/path1/'\n        ENCRYPTION=(TYPE='GCS_SSE_KMS' KMS_KEY_ID = '1234abcd-12ab-34cd-56ef-1234567890ab')\n      )\n    )\n  ALLOW_WRITES = TRUE;"
        },
        {
            "code": "CREATE EXTERNAL VOLUME exvol\n  STORAGE_LOCATIONS =\n    (\n      (\n        NAME = 'my-azure-northeurope'\n        STORAGE_PROVIDER = 'AZURE'\n        STORAGE_BASE_URL = 'azure://exampleacct.blob.core.windows.net/my_container_northeurope/'\n        AZURE_TENANT_ID = 'a123b4c5-1234-123a-a12b-1a23b45678c9'\n      )\n    )\n  ALLOW_WRITES = TRUE;"
        },
        {
            "code": "CREATE OR REPLACE EXTERNAL VOLUME ext_vol_s3_compat\n  STORAGE_LOCATIONS = (\n    (\n      NAME = 'my_s3_compat_storage_location'\n      STORAGE_PROVIDER = 'S3COMPAT'\n      STORAGE_BASE_URL = 's3compat://mybucket/unload/mys3compatdata'\n      CREDENTIALS = (\n        AWS_KEY_ID = '1a2b3c...'\n        AWS_SECRET_KEY = '4x5y6z...'\n      )\n      STORAGE_ENDPOINT = 'example.com'\n    )\n  );"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (the name) for the external volume; must be unique in your account. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "STORAGE_LOCATIONS   =   (   (   NAME   =   ' storage_location_name '   {   cloudProviderParams   |   s3CompatibleStorageParams   }   )   [,   (...),   ...]   )",
            "description": "Set of named cloud storage locations in different regions and, optionally, cloud platforms. Note Each external volume that you create supports a single active storage location ."
        },
        {
            "name": "ALLOW_WRITES   =   '{   TRUE   |   FALSE   }'",
            "description": "Specifies whether write operations are allowed for the external volume; must be set to TRUE for\nIceberg tables that use Snowflake as the catalog. For Iceberg tables created from Delta table files, setting this parameter to TRUE enables Snowflake to write Iceberg\nmetadata to your external storage. For more information, see Delta-based tables . The value of this parameter must also match the permissions that you\nset on the cloud storage account for each specified storage location. Note If you plan to use the external volume for externally managed Iceberg tables, you can set this parameter to FALSE.\nSnowflake doesn’t write data or Iceberg metadata files to your cloud storage when you use an external Iceberg catalog. Default: TRUE"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the external volume. Default: No value"
        },
        {
            "name": "STORAGE_PROVIDER   =   '{   S3   |   S3GOV   }'",
            "description": "Specifies the cloud storage provider that stores your data files. 'S3' : S3 storage in public AWS regions outside of China. 'S3GOV' : S3 storage in AWS government regions ."
        },
        {
            "name": "STORAGE_AWS_ROLE_ARN   =   ' iam_role '",
            "description": "Specifies the case-sensitive Amazon Resource Name (ARN) of the AWS identity and access management (IAM) role that grants privileges on the S3 bucket\ncontaining your data files. For more information, see Configure an external volume for Amazon S3 ."
        },
        {
            "name": "STORAGE_BASE_URL   =   ' protocol :// bucket [/ path /]'",
            "description": "Specifies the base URL for your cloud storage location, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the STORAGE_AWS_ACCESS_POINT_ARN parameter. path is an optional path that can be used to provide granular control over objects in the bucket. Note Snowflake can’t support external volumes with S3 bucket names that contain dots (for example, my.s3.bucket ).\nS3 doesn’t support SSL for virtual-hosted-style buckets with dots in the name, and\nSnowflake uses virtual-host-style paths and HTTPS to access data in S3. Important To create an Iceberg table that uses an external catalog, your Parquet data files\nand Iceberg metadata files must be within the STORAGE_BASE_URL location."
        },
        {
            "name": "STORAGE_AWS_ACCESS_POINT_ARN   =   ' string '",
            "description": "Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage STORAGE_BASE_URL ."
        },
        {
            "name": "STORAGE_AWS_EXTERNAL_ID   =   ' external_id '",
            "description": "Optionally specifies an external ID that Snowflake uses to establish a trust relationship with AWS.\nYou must specify the same external ID in the trust policy of the IAM role\nthat you configured for this external volume. For more information,\nsee How to use an external ID when granting access to your AWS resources to a third party . If you don’t specify a value for this parameter, Snowflake automatically generates an external ID when you create the external volume."
        },
        {
            "name": "ENCRYPTION   =   (   [   TYPE   =   'AWS_SSE_S3'   ]   |   [   TYPE   =   'AWS_SSE_KMS'   [   KMS_KEY_ID   =   ' string '   ]   ]   |   [   TYPE   =   'NONE'   ]   )",
            "description": "Specifies the properties needed to encrypt data on the external volume. Specifies the encryption type used. Possible values are: 'AWS_SSE_S3' : Server-side encryption using S3-managed encryption keys. For more information, see Using server-side encryption with Amazon S3-managed encryption keys (SSE-S3) . 'AWS_SSE_KMS' : Server-side encryption using keys stored in KMS. For more information, see Using server-side encryption with AWS Key Management Service (SSE-KMS) . 'NONE' : No encryption. Optionally specifies the ID for the AWS KMS-managed key used to encrypt files written to the bucket. If no value is provided, your default KMS key is used to encrypt files for writing data. Note that this value is ignored when reading data."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter, see Private connectivity to external volumes for AWS ."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: 'AWS_SSE_S3' : Server-side encryption using S3-managed encryption keys. For more information, see Using server-side encryption with Amazon S3-managed encryption keys (SSE-S3) . 'AWS_SSE_KMS' : Server-side encryption using keys stored in KMS. For more information, see Using server-side encryption with AWS Key Management Service (SSE-KMS) . 'NONE' : No encryption."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  AWS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the AWS KMS-managed key used to encrypt files written to the bucket. If no value is provided, your default KMS key is used to encrypt files for writing data. Note that this value is ignored when reading data."
        },
        {
            "name": "STORAGE_PROVIDER   =   'GCS'",
            "description": "Specifies the cloud storage provider that stores your data files."
        },
        {
            "name": "STORAGE_BASE_URL   =   'gcs:// bucket [/ path /]'",
            "description": "Specifies the base URL for your cloud storage location, where: bucket is the name of a Cloud Storage bucket that stores your data files. path is an optional path that can be used to provide granular control over objects in the bucket. Important To create an Iceberg table that uses an external catalog, your Parquet data files\nand Iceberg metadata files must be within the STORAGE_BASE_URL location."
        },
        {
            "name": "ENCRYPTION   =   (   [   TYPE   =   'GCS_SSE_KMS'   ]   [   KMS_KEY_ID   =   ' string '   ]   |   [   TYPE   =   'NONE'   ]   )",
            "description": "Specifies the properties needed to encrypt data on the external volume. Specifies the encryption type used. Possible values are: 'GCS_SSE_KMS' : Server-side encryption using keys stored in KMS. For more information, see customer-managed encryption keys . 'NONE' : No encryption. Specifies the ID for the Cloud KMS-managed key used to encrypt files written to the bucket. Note that this value is ignored when reading data. The read operation should succeed if the service account has sufficient\npermissions to the data and any specified KMS keys."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: 'GCS_SSE_KMS' : Server-side encryption using keys stored in KMS. For more information, see customer-managed encryption keys . 'NONE' : No encryption."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  GCS_SSE_KMS  encryption only)",
            "description": "Specifies the ID for the Cloud KMS-managed key used to encrypt files written to the bucket. Note that this value is ignored when reading data. The read operation should succeed if the service account has sufficient\npermissions to the data and any specified KMS keys."
        },
        {
            "name": "STORAGE_PROVIDER   =   'AZURE'",
            "description": "Specifies the cloud storage provider that stores your data files."
        },
        {
            "name": "AZURE_TENANT_ID   =   ' tenant_id '",
            "description": "Specifies the ID for your Office 365 tenant that the storage location belongs to. An external volume can\nauthenticate to only one tenant, so the storage location must refer to a storage account\nthat belongs to this tenant. To find your tenant ID, log into the Azure portal and select Azure Active Directory » Properties . The tenant ID is\ndisplayed in the Tenant ID field."
        },
        {
            "name": "STORAGE_BASE_URL   =   'azure://...'",
            "description": "Specifies the base URL for your cloud storage location (case-sensitive). For Azure Blob Storage, specify azure:// account .blob.core.windows.net/ container [/ path /] , where: account is the name of your Azure account; for example, myaccount . container is the name of an Azure container that stores your data files. path is an optional path that can be used to provide granular control over logical directories in the container. For Fabric OneLake, specify azure://[ region -]onelake. dfs | blob .fabric.microsoft.com/ workspace / lakehouse / path / , where: region optionally specifies the endpoint region; for example, westus . If specified, this must be the same region used\nby your Microsoft Fabric capacity, and the same region in which your Snowflake account is hosted. dfs | blob specifies the endpoint type. workspace is either your Fabric workspace ID or workspace name; for example, cfafbeb1-8037-4d0c-896e-a46fb27ff227 or my_workspace .\nYou must use the same type of identifier (ID or name) for both your workspace and Lakehouse. lakehouse is either your Lakehouse ID or Lakehouse name. You must use the same type of identifier (ID or name)\nfor both your workspace and Lakehouse; for example, 5b218778-e7a5-4d73-8187-f10824047715 or my_lakehouse.Lakehouse . path is a path to your storage location in the specified Lakehouse and Workspace. Preview Feature — Open Available to all accounts This feature is not available in the People’s Republic of China. Note Use the azure:// prefix and not https:// . Important To create an Iceberg table that uses an external catalog, your Parquet data files\nand Iceberg metadata files must be within the STORAGE_BASE_URL location."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter, see Private connectivity to external volumes for Microsoft Azure ."
        },
        {
            "name": "STORAGE_PROVIDER   =   'S3COMPAT'",
            "description": "Specifies S3-compatible storage as your storage provider."
        },
        {
            "name": "STORAGE_BASE_URL   =   's3compat:// bucket [/ path /]'",
            "description": "Specifies the URL for the external location used to store data files (an existing bucket accessed using an S3-compatible API endpoint), where: bucket is the name of the bucket. path is an optional case-sensitive path (or prefix in S3 terminology) for files in the cloud storage location\n(files with names that begin with a common string)."
        },
        {
            "name": "CREDENTIALS   =   (   AWS_KEY_ID   =   ' string '   AWS_SECRET_KEY   =   ' string '   )",
            "description": "Specifies the security credentials for connecting to and accessing your S3-compatible storage location."
        },
        {
            "name": "STORAGE_ENDPOINT   =   ' s3_api_compatible_endpoint '",
            "description": "Specifies a fully qualified domain that points to your S3-compatible API endpoint. Note The storage endpoint should not include a bucket name; for example, specify example.com instead of my_bucket.example.com ."
        }
    ],
    "usage_notes": "Important\nExternal volumes in Amazon S3 storage only: If you recreate an external volume (using the CREATE OR REPLACE EXTERNAL VOLUME syntax)\nwithout specifying an external ID, you must repeat the steps to grant the AWS identity and access management (IAM) user\nfor your Snowflake account the access permissions required on the S3 storage location.\nFor more information, see the instructions for retrieving the AWS IAM user for your Snowflake\naccount in Configure an external volume for Amazon S3.\nYou can’t drop or replace an external volume if one or more Iceberg tables\nare associated with the external volume.\nTo view the tables that depend on an external volume,\nyou can use the SHOW ICEBERG TABLES command and\na query using the pipe operator that filters on the external_volume_name column.\nNote\nThe column identifier (external_volume_name) is case-sensitive.\nSpecify the column identifier exactly as it appears in the SHOW ICEBERG TABLES output.\nFor example:\nIf you use a regional endpoint for a Microsoft Fabric OneLake storage location,\nuse the same region as your Microsoft Fabric capacity. This must also be the same region that hosts your Snowflake account.\nFor S3 external volumes that use an S3 access point:\nYou must configure the IAM policy for the external volume\nto grant permission to your S3 access point. For more information,\nsee Step 1: Create an IAM policy that grants access to your S3 location.\nMulti-region access points aren’t supported.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-network-policy",
    "title": "CREATE NETWORK POLICY",
    "description": "Creates a network policy or replaces an existing network policy.",
    "syntax": "CREATE [ OR REPLACE ] NETWORK POLICY [ IF NOT EXISTS ] <name>\n  [ ALLOWED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n  [ BLOCKED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n  [ ALLOWED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' , ... ] ) ]\n  [ BLOCKED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' , ... ] ) ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] NETWORK POLICY [ IF NOT EXISTS ] <name>\n  [ ALLOWED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n  [ BLOCKED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n  [ ALLOWED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' , ... ] ) ]\n  [ BLOCKED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' , ... ] ) ]\n  [ COMMENT = '<string_literal>' ]"
        },
        {
            "code": "USE ROLE SECURITYADMIN;\n\nALTER ACCOUNT SET NETWORK_POLICY = <policy_name>;"
        },
        {
            "code": "CREATE NETWORK POLICY allow_vpceid_block_public_policy\n  ALLOWED_NETWORK_RULE_LIST = ('allow_vpceid_access')\n  BLOCKED_NETWORK_RULE_LIST = ('block_public_access');\n\nDESC NETWORK POLICY rule_based_policy;"
        },
        {
            "code": "+---------------------------+---------------------+\n| name                      | value               |\n|---------------------------+---------------------|\n| ALLOWED_NETWORK_RULE_LIST | ALLOW_VPCEID_ACCESS |\n+---------------------------+---------------------+\n| BLOCKED_NETWORK_RULE_LIST | BLOCK_PUBLIC_ACCESS |\n+---------------------------+---------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the network policy; must be unique for your account. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "ALLOWED_NETWORK_RULE_LIST   =   (   ' network_rule '   [   ,   ' network_rule '   ,   ...   ]   )",
            "description": "Specifies a list of network rules that contain the network identifiers that are allowed access to\nSnowflake. There is no limit on the number of network rules in the list."
        },
        {
            "name": "BLOCKED_NETWORK_RULE_LIST   =   (   ' network_rule '   [   ,   ' network_rule '   ,   ...   ]   )",
            "description": "Specifies a list of network rules that contain the network identifiers that are denied access to Snowflake. There is no limit on the\nnumber of network rules in the list."
        },
        {
            "name": "ALLOWED_IP_LIST   =   (   [   ip_address   ]   [   ,   ip_address   ,   ...   ]   )",
            "description": "Specifies a list of IPv4 addresses that are allowed access to your Snowflake account. This is referred to as the allowed list . Snowflake recommends using network rules in conjunction with network policies rather than using this property. Use the ALLOWED_NETWORK_RULE_LIST property to specify network rules that contain IPv4 addresses. If you are not yet using network rules, specify at least one IPv4 address or CIDR block range to allow access to your Snowflake\naccount. Additionally, if you are not using network rules and this property is specified with an empty list, no IPv4 addresses are\nallowed to access your Snowflake account."
        },
        {
            "name": "BLOCKED_IP_LIST   =   (   [   ip_address   ]   [   ,   ip_address   ,   ...   ]   )",
            "description": "Specifies a list of IPv4 addresses that are denied access to your Snowflake account. This is referred to as the blocked list .\nTo unset this parameter, specify a different CIDR block range, a series of IPv4 addresses, or a single IPv4 address. Snowflake recommends using network rules in conjunction with network policies rather than using this parameter. Use the BLOCKED_NETWORK_RULE_LIST property to specify network rules that contain IPv4 addresses. To block public access, use a network rule and add the network rule to the BLOCKED_NETWORK_RULE_LIST property. The result is\nthat only IP addresses that use private connectivity, such as AWS PrivateLink, can access your Snowflake account. Default: No value; no IP addresses in ALLOWED_IP_LIST property are blocked."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the network policy. Default: No value"
        }
    ],
    "usage_notes": "Use network rules in conjunction with the network policy to manage access to your Snowflake account.\nYou cannot execute a CREATE OR REPLACE NETWORK POLICY command to replace an existing network policy if that policy is currently assigned\nto an account, security integration, or user.\nEach ip_address can cover a range of addresses using Classless Inter-Domain Routing (CIDR) notation:\nip_address[/optional_prefix_length]\nFor example:\n192.168.1.0/24\nWhen a network policy includes values for both ALLOWED_IP_LIST and BLOCKED_IP_LIST, Snowflake applies the\nblocked list first.\nThe maximum number of characters for the ALLOWED_IP_LIST list is 100,000. Snowflake returns an error message when this\ncharacter limit is exceeded.\nAfter creating a network policy, you must associate it with your account before Snowflake enforces the policy. You can associate a\npolicy with your account through the ALTER ACCOUNT command, which must be run by a user with the SECURITYADMIN\nrole (or higher).\nFor example:\nFor more details, see Parameter management. Note that NETWORK_POLICY is currently the only account\nparameter that can be set by users with the SECURITYADMIN role.\nBefore associating a network policy with your account, your current IP address must be included in ALLOWED_IP_LIST; otherwise,\nthe ALTER ACCOUNT command returns an error. In addition, your current IP address cannot be included in BLOCKED_IP_LIST.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-materialized-view",
    "title": "CREATE MATERIALIZED VIEW",
    "description": "Creates a new materialized view in the current/specified schema, based on a query of an existing table, and populates the view with data.",
    "syntax": "CREATE [ OR REPLACE ] [ SECURE ] MATERIALIZED VIEW [ IF NOT EXISTS ] <name>\n  [ COPY GRANTS ]\n  ( <column_list> )\n  [ <col1> [ WITH ] MASKING POLICY <policy_name> [ USING ( <col1> , <cond_col1> , ... ) ]\n           [ WITH ] PROJECTION POLICY <policy_name>\n           [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ , <col2> [ ... ] ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ CLUSTER BY ( <expr1> [, <expr2> ... ] ) ]\n  AS <select_statement>",
    "examples": [
        {
            "code": "CREATE MATERIALIZED VIEW mymv\n    COMMENT='Test view'\n    AS\n    SELECT col1, col2 FROM mytable;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the view; must be unique for the schema in which the view is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "select_statement",
            "description": "Specifies the query used to create the view. This query serves as the text/definition for the view. This query is displayed in the output\nof SHOW VIEWS and SHOW MATERIALIZED VIEWS . There are limitations on the select_statement . For details, see: Usage notes . Limitations on Creating Materialized Views ."
        },
        {
            "name": "column_list :",
            "description": "If you do not want the column names in the view to be the same as the column names of the underlying table, you may include a column list in\nwhich you specify the column names. (You do not need to specify the data types of the columns.) If you include a CLUSTER BY clause for the materialized view, then you\nmust include the column name list."
        },
        {
            "name": "MASKING   POLICY   =   policy_name",
            "description": "Specifies the masking policy to set on a column."
        },
        {
            "name": "USING   (   col_name   ,   cond_col_1   ...   )",
            "description": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal masking policy ."
        },
        {
            "name": "PROJECTION   POLICY   policy_name",
            "description": "Specifies the projection policy to set on a column."
        },
        {
            "name": "string_literal",
            "description": "Specifies a comment for the view. The string literal should be in single quotes. (The string literal should not contain single\nquotes unless they are escaped.) Default: No value."
        },
        {
            "name": "expr#",
            "description": "Specifies an expression on which to cluster the materialized view. Typically, each expression is the name of a column in the\nmaterialized view. For more information about clustering materialized views, see Materialized Views and Clustering . For more\ninformation about clustering in general, see What is Data Clustering? ."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the view is secure. For more information about secure views, see Working with Secure Views . Default: No value (view is not secure)"
        },
        {
            "name": "COPY   GRANTS",
            "description": "If you are replacing an existing view by using the OR REPLACE clause, then the replacement view retains the access permissions\nfrom the original view. This parameter copies all privileges, except OWNERSHIP, from the existing view to the new view. The\nnew view does not inherit any future grants defined for the object type in the schema. By default, the role that executes\nthe CREATE MATERIALIZED VIEW statement owns the new view. If the parameter is not included in the CREATE VIEW statement, then the new view does not inherit any explicit access privileges\ngranted on the original view but does inherit any future grants defined for the object type in the schema. Note that the operation to copy grants occurs atomically with the CREATE VIEW statement (i.e. within the same transaction). Default: No value (grants are not copied)."
        },
        {
            "name": "ROW   ACCESS   POLICY   policy_name   ON   (   col_name   [   ,   col_name   ...   ]   )",
            "description": "Specifies the row access policy to set on the materialized view."
        },
        {
            "name": "AGGREGATION   POLICY   policy_name",
            "description": "Specifies the aggregation policy to set on the materialized view."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ],
    "usage_notes": "Creating a materialized view requires CREATE MATERIALIZED VIEW privilege on the schema, and SELECT privilege on\nthe base table. For more information about privileges and materialized views, see Privileges on a Materialized View’s Schema.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\ndefinition of the view, the function returns the database or schema that contains the view, not the database or schema in\nuse for the session.\nWhen you choose a name for the materialized view, note that a schema cannot contain a table and view with the same name. CREATE\n[ MATERIALIZED ] VIEW produces an error if a table with the same name already exists in the schema.\nWhen specifying the select_statement, note the following:\nYou cannot specify a HAVING clause or an ORDER BY clause.\nIf you include a CLUSTER BY clause for the materialized view, you must include the column_list clause.\nIf you refer to the base table more than once in the select_statement, use the same\nqualifier for all references for the base table.\nFor example, don’t use a mix of base_table, schema.base_table, and database.schema.base_table in the\nsame select_statement. Instead, choose one of these forms (e.g. database.schema.base_table), and use this\nconsistently throughout the select_statement.\nDo not query stream objects in the SELECT statement. Streams are not designed to serve as source objects for views or materialized\nviews.\nSome column names are not allowed in materialized views. If a column name is not allowed, you can define an alias for the\ncolumn. For details, see Handling Column Names That Are Not Allowed in Materialized Views.\nIf the materialized view queries external tables, you must refresh the file-level metadata\nfor the external tables to reflect changes in the referenced cloud storage location, including\nnew, updated, and removed files.\nYou can refresh the metadata for an external table\nautomatically using the event notification service\nfor your cloud storage service or manually using\nALTER EXTERNAL TABLE … REFRESH statements.\nMaterialized views have a number of other restrictions. For details, see\nLimitations on Creating Materialized Views and Limitations on Working With Materialized Views.\nView definitions are not updated if the schema of the underlying source table is changed so that the view definition becomes\ninvalid. For example:\nA view is created from a base table, and a column is subsequently dropped from that base table.\nThe base table for the materialized view is dropped.\nIn these scenarios, querying the view returns an error that includes the reason why the view was invalidated. For example:\nWhen this occurs, you can do the following:\nIf the base table has been dropped and this is within the\ndata retention period for Time Travel, you can\nundrop the base table to make the materialized view valid again.\nUse the CREATE OR REPLACE MATERIALIZED VIEW command to recreate the view.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nUsing OR REPLACE is the equivalent of using DROP MATERIALIZED VIEW on the existing materialized view and then creating a\nnew view with the same name.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE MATERIALIZED VIEW operation use either the old or new materialized\nview version.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nWhen creating a materialized view with a masking policy on one or more materialized view columns, or a row access policy added to the\nmaterialized view, use the POLICY_CONTEXT function to simulate a query on the column(s) protected by a\nmasking policy and the materialized view protected by a row access policy."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-model-monitor",
    "title": "CREATE MODEL MONITOR",
    "description": "Create or replace a model monitor in the current or specified schema.",
    "syntax": "CREATE [ OR REPLACE ] MODEL MONITOR [ IF NOT EXISTS ] <monitor_name> WITH\n    MODEL = <model_name>\n    VERSION = '<version_name>'\n    FUNCTION = '<function_name>'\n    SOURCE = <source_name>\n    WAREHOUSE = <warehouse_name>\n    REFRESH_INTERVAL = '<refresh_interval>'\n    AGGREGATION_WINDOW = '<aggregation_window>'\n    TIMESTAMP_COLUMN = <timestamp_name>\n    [ BASELINE = <baseline_name> ]\n    [ ID_COLUMNS = <id_column_name_array> ]\n    [ PREDICTION_CLASS_COLUMNS = <prediction_class_column_name_array> ]\n    [ PREDICTION_SCORE_COLUMNS = <prediction_column-name_array> ]\n    [ ACTUAL_CLASS_COLUMNS = <actual_class_column_name_array> ]\n    [ ACTUAL_SCORE_COLUMNS = <actual_column_name_array> ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] MODEL MONITOR [ IF NOT EXISTS ] <monitor_name> WITH\n    MODEL = <model_name>\n    VERSION = '<version_name>'\n    FUNCTION = '<function_name>'\n    SOURCE = <source_name>\n    WAREHOUSE = <warehouse_name>\n    REFRESH_INTERVAL = '<refresh_interval>'\n    AGGREGATION_WINDOW = '<aggregation_window>'\n    TIMESTAMP_COLUMN = <timestamp_name>\n    [ BASELINE = <baseline_name> ]\n    [ ID_COLUMNS = <id_column_name_array> ]\n    [ PREDICTION_CLASS_COLUMNS = <prediction_class_column_name_array> ]\n    [ PREDICTION_SCORE_COLUMNS = <prediction_column-name_array> ]\n    [ ACTUAL_CLASS_COLUMNS = <actual_class_column_name_array> ]\n    [ ACTUAL_SCORE_COLUMNS = <actual_column_name_array> ]"
        }
    ],
    "parameters": [
        {
            "name": "monitor_name",
            "description": "Specifies the identifier for the model monitor; must be unique in the schema where the monitor is created,\nand must be in the same schema as the model being monitored. If the monitor identifier is not fully qualified (in the form of db_name . schema_name . name or schema_name . name ), the command creates the model in the current schema for the session. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "MODEL   =   model_name",
            "description": "The name of the model to be monitored. Must be in the same schema where the monitor is created."
        },
        {
            "name": "VERSION   =   ' version_name '",
            "description": "Name of the model version to be monitored."
        },
        {
            "name": "FUNCTION   =   function_name",
            "description": "Name of the specific function in the model version to be monitored."
        },
        {
            "name": "SOURCE   =   source_name",
            "description": "Name of the source table or view that contains the feature, inferences and ground truth labels."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "The name of the Snowflake warehouse to use for the monitor’s internal compute operations."
        },
        {
            "name": "REFRESH_INTERVAL   =   ' refresh_interval '",
            "description": "The interval at which the monitor refreshes its internal state. The value must be a string representing a time period,\nsuch as '1 day' . Supported units include seconds, minutes, hours, days, weeks, months, quarters, and years.\nYou may use singular (“hour”) or plural (“hours”) for the interval name, but may not abbreviate."
        },
        {
            "name": "AGGREGATION_WINDOW   =   ' aggregation_window '",
            "description": "The window over which the monitor aggregates data. The value must be a string representing a time period, such as '1 day' . Only days are supported. You may use singular (“day”) or plural (“days”) for the interval name, but may not\nabbreviate."
        },
        {
            "name": "TIMESTAMP_COLUMN   =   timestamp_name",
            "description": "Name of the column in the source data that contains the timestamps. Must be of type TIMESTAMP_NTZ."
        },
        {
            "name": "BASELINE   =   baseline_name",
            "description": "Name of the baseline table that contains a snapshot of data similar to SOURCE, which is used to compute drift.\nA snapshot of this data is embedded within the monitor object. Although this parameter is optional, if is not set, the\nmonitor cannot detect drift."
        },
        {
            "name": "ID_COLUMNS   =   id_column_name_array",
            "description": "An array of string column names that, together, uniquely identify each row in the source data.  See ARRAY constants ."
        },
        {
            "name": "PREDICTION_CLASS_COLUMNS   =   prediction_class_column_name_array",
            "description": "An array of strings naming all prediction class columns in the data source. See ARRAY constants .\nIf the model task is TABULAR_BINARY_CLASSIFICATION or TABULAR_REGRESSION , the columns must be of type NUMBER.\nIf the model task is TABULAR_MULTI_CLASSIFICATION , the columns must be of type STRING."
        },
        {
            "name": "PREDICTION_SCORE_COLUMNS   =   prediction_column_name_array",
            "description": "An array of strings naming all prediction score columns in the data source. See ARRAY constants .\nColumns must be of type NUMBER."
        },
        {
            "name": "ACTUAL_CLASS_COLUMNS   =   actual_class_column_name_array",
            "description": "An array of strings naming all actual class columns in the data source.  See ARRAY constants .\nIf the model task is TABULAR_BINARY_CLASSIFICATION or TABULAR_REGRESSION , the columns must be of type NUMBER.\nIf the model task is TABULAR_MULTI_CLASSIFICATION , the columns must be of type STRING."
        },
        {
            "name": "ACTUAL_SCORE_COLUMNS   =   actual_column_name_array",
            "description": "An array of strings naming all actual score columns in the data source.  See ARRAY constants .\nColumns must be of type NUMBER."
        }
    ],
    "usage_notes": "The following requirements apply to the parameters:\nModel task must be tabular_binary_classification or tabular_regression.\nMultiple-output models are not currently supported. Although the prediction and actual columns are arrays, the arrays\nmust have only one element.\nAt least one of the prediction columns must be specified.\nActual columns are optional, but accuracy metrics are not computed if they are not specified.\nA column may be specified once across all parameters (for example, an ID column cannot also be a prediction column).\nThe number of monitored features is limited to 500.\nThe basic configuration of MODEL MONITOR instances, including the model it monitors and data sources it uses, cannot be\nchanged after the monitor is created. You can modify only a few options using\nALTER MODEL MONITOR. To change a monitor’s configuration, drop the instance and create a new\none.\nReplication is supported only for instances\nof the CUSTOM_CLASSIFIER class.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-database",
    "title": "CREATE DATABASE",
    "description": "Creates a new database in the system.",
    "syntax": "CREATE [ OR REPLACE ] [ TRANSIENT ] DATABASE [ IF NOT EXISTS ] <name>\n    [ CLONE <source_schema>\n        [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n        [ IGNORE TABLES WITH INSUFFICIENT DATA RETENTION ]\n        [ IGNORE HYBRID TABLES ] ]\n    [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n    [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n    [ EXTERNAL_VOLUME = <external_volume_name> ]\n    [ CATALOG = <catalog_integration_name> ]\n    [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n    [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n    [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n    [ COMMENT = '<string_literal>' ]\n    [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>' ]\n    [ CATALOG_SYNC_NAMESPACE_MODE = { NEST | FLATTEN } ]\n    [ CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER = '<string_literal>' ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n    [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nCREATE DATABASE <name> FROM LISTING '<listing_global_name>'\n\nCREATE DATABASE <name> FROM SHARE <provider_account>.<share_name>\n\nCREATE DATABASE <name>\n    AS REPLICA OF <account_identifier>.<primary_db_name>\n    [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]",
    "examples": [
        {
            "code": "CREATE DATABASE mytestdb;\n\nCREATE DATABASE mytestdb2 DATA_RETENTION_TIME_IN_DAYS = 10;\n\nSHOW DATABASES LIKE 'my%';\n\n+---------------------------------+------------+------------+------------+--------+----------+---------+---------+----------------+\n| created_on                      | name       | is_default | is_current | origin | owner    | comment | options | retention_time |\n|---------------------------------+------------+------------+------------+--------+----------+---------+---------+----------------|\n| Tue, 17 Mar 2016 16:57:04 -0700 | MYTESTDB   | N          | N          |        | PUBLIC   |         |         | 1              |\n| Tue, 17 Mar 2016 17:06:32 -0700 | MYTESTDB2  | N          | N          |        | PUBLIC   |         |         | 10             |\n+---------------------------------+------------+------------+------------+--------+----------+---------+---------+----------------+"
        },
        {
            "code": "CREATE TRANSIENT DATABASE mytransientdb;\n\nSHOW DATABASES LIKE 'my%';\n\n+---------------------------------+---------------+------------+------------+--------+----------+---------+-----------+----------------+\n| created_on                      | name          | is_default | is_current | origin | owner    | comment | options   | retention_time |\n|---------------------------------+---------------+------------+------------+--------+----------+---------+-----------+----------------|\n| Tue, 17 Mar 2016 16:57:04 -0700 | MYTESTDB      | N          | N          |        | PUBLIC   |         |           | 1              |\n| Tue, 17 Mar 2016 17:06:32 -0700 | MYTESTDB2     | N          | N          |        | PUBLIC   |         |           | 10             |\n| Tue, 17 Mar 2015 17:07:51 -0700 | MYTRANSIENTDB | N          | N          |        | PUBLIC   |         | TRANSIENT | 1              |\n+---------------------------------+---------------+------------+------------+--------+----------+---------+-----------+----------------+"
        },
        {
            "code": "CREATE DATABASE snow_sales FROM SHARE ab67890.sales_s;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the database; must be unique for your account. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements . Important As a best practice for Database Replication and Failover , we recommend giving each\nsecondary database the same name as its primary database. This practice supports referencing fully-qualified objects\n(i.e. '<db>.<schema>.<object>' ) by other objects in the same database, such as querying a fully-qualified table name in a view. If a secondary database has a different name from the primary database, then these object references would break in the secondary database."
        },
        {
            "name": "provider_account . share_name",
            "description": "Specifies the identifier of the share from which to create the database. As documented, the name of the\nshare must be fully-qualified with the name of the account providing the share."
        },
        {
            "name": "AS   REPLICA   OF   account_identifier . primary_db_name",
            "description": "Specifies the identifier for a primary database from which to create a replica (i.e. a secondary database). If the identifier contains spaces,\nspecial characters, or mixed-case characters, the entire string must be enclosed in double quotes. Requires the account identifier and name of the primary database. Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes . Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database. Note As a best practice for Database Replication and Failover, we recommend setting the optional parameter DATA_RETENTION_TIME_IN_DAYS to the same value on the secondary database as on the\nprimary database."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes ."
        },
        {
            "name": "primary_db_name",
            "description": "Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database."
        },
        {
            "name": "' listing_global_name '",
            "description": "Specifies the global name of the listing from which to create the database, which must meet the following requirements: Can’t be a paid listing. Listing terms, if not of type OFFLINE , must have been accepted using Snowsight. Listing data products must be available locally in the current region. Whether a listing is available in the local region can be determined by viewing the is_ready_for_import column\nof DESCRIBE AVAILABLE LISTING ."
        },
        {
            "name": "provider_account . share_name",
            "description": "Specifies the identifier of the share from which to create the database. As documented, the name of the\nshare must be fully-qualified with the name of the account providing the share."
        },
        {
            "name": "AS   REPLICA   OF   account_identifier . primary_db_name",
            "description": "Specifies the identifier for a primary database from which to create a replica (i.e. a secondary database). If the identifier contains spaces,\nspecial characters, or mixed-case characters, the entire string must be enclosed in double quotes. Requires the account identifier and name of the primary database. Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes . Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database. Note As a best practice for Database Replication and Failover, we recommend setting the optional parameter DATA_RETENTION_TIME_IN_DAYS to the same value on the secondary database as on the\nprimary database."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account that stores the primary database. The preferred identifier is organization_name . account_name .\nTo view the list of accounts enabled for replication in your organization, query SHOW REPLICATION ACCOUNTS. Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more details about using the account locator as an account identifier, see Database Replication Usage Notes ."
        },
        {
            "name": "primary_db_name",
            "description": "Name of the primary database. As a best practice, we recommend giving each secondary database the same name as its primary database."
        },
        {
            "name": "' listing_global_name '",
            "description": "Specifies the global name of the listing from which to create the database, which must meet the following requirements: Can’t be a paid listing. Listing terms, if not of type OFFLINE , must have been accepted using Snowsight. Listing data products must be available locally in the current region. Whether a listing is available in the local region can be determined by viewing the is_ready_for_import column\nof DESCRIBE AVAILABLE LISTING ."
        },
        {
            "name": "TRANSIENT",
            "description": "Specifies a database as transient. Transient databases do not have a Fail-safe period so they do not incur additional storage costs once\nthey leave Time Travel; however, this means they are also not protected by Fail-safe in the event of a data loss. For more information, see Understanding and viewing Fail-safe . In addition, by definition, all schemas (and consequently all tables) created in a transient database are transient. For more information about\ntransient tables, see CREATE TABLE . Default: No value (i.e. database is permanent)"
        },
        {
            "name": "CLONE   source_db",
            "description": "Specifies to create a clone of the specified source database. For more details about cloning a database, see CREATE <object> … CLONE ."
        },
        {
            "name": "AT   |   BEFORE   (   TIMESTAMP   =>   timestamp   |   OFFSET   =>   time_difference   |   STATEMENT   =>   id   )",
            "description": "When cloning a database, the AT | BEFORE clause specifies to use Time Travel to clone the database at or\nbefore a specific point in the past. If the specified Time Travel time is at or before the point in time when the database was created,\nthe cloning operation fails with an error."
        },
        {
            "name": "IGNORE   TABLES   WITH   INSUFFICIENT   DATA   RETENTION",
            "description": "Ignore tables that no longer have historical data available in Time Travel to clone. If the time in the past specified in the\nAT | BEFORE clause is beyond the data retention period for any child table in a database or schema, skip the cloning operation\nfor the child table. For more information, see Child Objects and Data Retention Time ."
        },
        {
            "name": "IGNORE   HYBRID   TABLES",
            "description": "Ignore hybrid tables, which will not be cloned. Use this option to clone a database that contains hybrid tables.\nThe cloned database includes other objects but skips hybrid tables. If you don’t use this option and your database contains one or more hybrid tables, the command ignores hybrid tables silently. However, the error handling for databases that contain hybrid tables will change in an upcoming release; therefore, you may want to add this parameter to your commands preemptively."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the\ndefault Time Travel retention time for all schemas created in the database. For more details, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent databases 0 or 1 for transient databases Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the account level) Note A value of 0 effectively disables Time Travel for the database."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the\ndatabase to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "EXTERNAL_VOLUME   =   external_volume_name",
            "description": "Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME ."
        },
        {
            "name": "CATALOG   =   catalog_integration_name",
            "description": "Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG ."
        },
        {
            "name": "REPLACE_INVALID_CHARACTERS   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for all schemas and tables added to the database. The\ndefault can be overridden at the schema and individual table level. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "STORAGE_SERIALIZATION_POLICY   =   {   COMPATIBLE   |   OPTIMIZED   }",
            "description": "Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the database. Default: No value"
        },
        {
            "name": "CATALOG_SYNC   =   ' snowflake_open_catalog_integration_name '",
            "description": "Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the database with an external catalog in your Snowflake Open Catalog\naccount. For more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value"
        },
        {
            "name": "CATALOG_SYNC_NAMESPACE_MODE   =   {   NEST   |   FLATTEN   }",
            "description": "Specifies the catalog sync namespace mode for Snowflake-managed Iceberg tables in the database that you sync with\nSnowflake Open Catalog. This property specifies whether Snowflake syncs the table to Open Catalog with one or two parent namespaces. It\nonly applies if you’re setting the CATALOG_SYNC parameter. After you create the database, you can’t alter this property. NEST : Snowflake syncs two parent namespaces with the table. For example, suppose you have a db2.public.table1 Iceberg table registered in Snowflake. You want to sync this table, along with its\ntwo parent namespaces, to the catalog2 external catalog in Open Catalog. To sync the table with its two parent namespaces, use the\ndefault for CATALOG_SYNC_NAMESPACE_MODE ( NEST ). If you don’t specify the CATALOG_SYNC_NAMESPACE_MODE property, the default for\nthis property is applied, which is NEST . Because you’re using the default for CATALOG_SYNC_NAMESPACE_MODE , you don’t need to specify CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER . As a result, Snowflake syncs the table to Open Catalog with the following fully qualified\nname: catalog2.db2.public.table1 . FLATTEN : Snowflake syncs one parent namespace with the table, which contains the delimiter you set by using the CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER property. Important If your third-party query engine can only query tables located up to the second namespace level in a catalog, you must set the CATALOG_SYNC_NAMESPACE_MODE property to FLATTEN . Otherwise, Snowflake will sync Snowflake-managed Iceberg tables to the\nthird namespace level in Open Catalog and you can’t query the table. For example, suppose that you have a db1.public.table1 Iceberg table registered in Snowflake. You want to sync this table and one parent\nnamespace named db1-public with the catalog1 external catalog in Open Catalog, so that the table is located at the second namespace level in Open Catalog. To sync the table with the db1-public parent namespace, set CATALOG_SYNC_NAMESPACE_MODE to FLATTEN and specify a hyphen ( - ) as the value\nfor CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER . As a result, Snowflake syncs this table to Open Catalog with the following\nfully-qualified name: catalog1.db1-public.table1 . Default: NEST"
        },
        {
            "name": "CATALOG_SYNC_NAMESPACE_FLATTEN_DELIMITER   =   ' string_literal '",
            "description": "Specifies a delimiter, which Snowflake inserts in the flattened namespace that results when Snowflake syncs a Snowflake-managed Iceberg\ntable to Snowflake Open Catalog with one parent namespace. This delimiter property only applies when you set the CATALOG_SYNC_NAMESPACE_MODE property to FLATTEN . Snowflake inserts this delimiter to avoid conflicts that could\narise from flattening parent namespaces for different tables. After you create the database, you can’t alter this property. For example, suppose you want to sync the customer.data.table1 and custom.erdata.table1 Snowflake-managed Iceberg tables to the catalog1 external catalog in Open Catalog. By setting the CATALOG_SYNC_NAMESPACE_MODE property set to FLATTEN and specifying a hyphen ( - ) for the\ndelimiter, Snowflake syncs these tables with Open Catalog with the following fully qualified names: catalog1.customer-data.table1 catalog1.custom-erdata.table1 If you set the CATALOG_SYNC_NAMESPACE_MODE property to FLATTEN , a non-empty delimiter value is required. However, if you set the CATALOG_SYNC_NAMESPACE_MODE property to NEST , this delimiter property doesn’t apply and the configured value will be ignored. Valid characters: 0-9 , A-Z , a-z , _ , $ , -"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-streamlit",
    "title": "CREATE STREAMLIT",
    "description": "Creates a new Streamlit application object in Snowflake or replaces an existing Streamlit\napplication object in the same schema.",
    "syntax": "CREATE [ OR REPLACE ] STREAMLIT [ IF NOT EXISTS ] <name>\n  FROM <source_location>\n  MAIN_FILE = '<path_to_main_file_in_root_directory>'\n  QUERY_WAREHOUSE = <warehouse_name>\n  [ COMMENT = '<string_literal>' ]\n  [ TITLE = '<app_title>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , ... ] ) ]\n  [ ROOT_LOCATION = '<stage_path_and_root_directory>' ]",
    "examples": [
        {
            "code": "CREATE STREAMLIT hello_streamlit\n  FROM @streamlit_db.streamlit_schema.streamlit_stage\n  MAIN_FILE = 'streamlit_main.py'\n  QUERY_WAREHOUSE = my_warehouse;"
        },
        {
            "code": "CREATE STREAMLIT hello_streamlit\n  FROM @streamlit_db.streamlit_schema.streamlit_repo/branches/streamlit_branch/\n  MAIN_FILE = 'streamlit_main.py'\n  QUERY_WAREHOUSE = my_warehouse;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the Streamlit object. This identifier must be unique for the schema in which\nthe table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters\nunless the entire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in\ndouble quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "FROM   source_location",
            "description": "Copies the source files from the specified location to initialize the app. This happens once. For example, FROM @streamlit_db.streamlit_schema.streamlit_stage ."
        },
        {
            "name": "MAIN_FILE   =   ' path_to_main_file_in_root_directory '",
            "description": "Specifies the filename of the Streamlit Python application. This filename is relative to the value of ROOT_LOCATION ."
        },
        {
            "name": "QUERY_WAREHOUSE   =   warehouse_name",
            "description": "Specifies the warehouse to run SQL queries issued by the Streamlit application. Note If you’re creating or replacing a Streamlit application object within the Snowflake Native App Framework, the QUERY_WAREHOUSE parameter is optional."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the Streamlit object. DEFAULT: No value"
        },
        {
            "name": "TITLE   =   ' app_title '",
            "description": "Specifies a title for the Streamlit app to display in Snowsight."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for the\nStreamlit app code to access external networks."
        },
        {
            "name": "ROOT_LOCATION   =   ' stage_path_and_root_directory '",
            "description": "Important ROOT_LOCATION is a legacy parameter. Snowflake recommends using FROM source_location . For Streamlit apps created using ROOT_LOCATION, multi-file editing and Git integration are not supported. Specifies the full path to the named stage containing the Streamlit Python files, media files, and the environment.yml file, for example: In this example, the Streamlit files are located on a named stage named streamlit_stage within a database named streamlit_db and schema named streamlit_schema . Note This parameter must point to a single directory inside a named internal stage. External stages are not supported for Streamlit in Snowflake. If you’re creating or replacing a Streamlit application object within the Snowflake Native App Framework, use FROM ' relative_path_from_stage_root_directory ' and not ROOT_LOCATION = ' stage_path_and_root_directory ' ."
        }
    ],
    "usage_notes": "When you clone a schema or database containing a Streamlit object, the Streamlit object is not cloned.\nTo specify the packages used by the Streamlit application, use an environment.yml file.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-external-table",
    "title": "CREATE EXTERNAL TABLE",
    "description": "Creates a new external table in the current/specified schema\nor replaces an existing external table. When queried, an external table reads\ndata from a set of one or more files in a specified external stage and outputs the data in a single VARIANT column.",
    "syntax": "-- Partitions computed from expressions\nCREATE [ OR REPLACE ] EXTERNAL TABLE [IF NOT EXISTS]\n  <table_name>\n    ( [ <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ]\n      [ inlineConstraint ]\n      [ , <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ... ]\n      [ , ... ] )\n  cloudProviderParams\n  [ PARTITION BY ( <part_col_name> [, <part_col_name> ... ] ) ]\n  [ WITH ] LOCATION = externalStage\n  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ PATTERN = '<regex_pattern>' ]\n  FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET } [ formatTypeOptions ] } )\n  [ AWS_SNS_TOPIC = '<string>' ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON (VALUE) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\n-- Partitions added and removed manually\nCREATE [ OR REPLACE ] EXTERNAL TABLE [IF NOT EXISTS]\n  <table_name>\n    ( [ <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ]\n      [ inlineConstraint ]\n      [ , <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ... ]\n      [ , ... ] )\n  cloudProviderParams\n  [ PARTITION BY ( <part_col_name> [, <part_col_name> ... ] ) ]\n  [ WITH ] LOCATION = externalStage\n  PARTITION_TYPE = USER_SPECIFIED\n  FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET } [ formatTypeOptions ] } )\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON (VALUE) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\n-- Delta Lake\nCREATE [ OR REPLACE ] EXTERNAL TABLE [IF NOT EXISTS]\n  <table_name>\n    ( [ <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ]\n      [ inlineConstraint ]\n      [ , <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ... ]\n      [ , ... ] )\n  cloudProviderParams\n  [ PARTITION BY ( <part_col_name> [, <part_col_name> ... ] ) ]\n  [ WITH ] LOCATION = externalStage\n  PARTITION_TYPE = USER_SPECIFIED\n  FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET } [ formatTypeOptions ] } )\n  [ TABLE_FORMAT = DELTA ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON (VALUE) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\ninlineConstraint ::=\n  [ NOT NULL ]\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE | PRIMARY KEY | [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> ] ) ] }\n  [ <constraint_properties> ]\n\ncloudProviderParams (for Google Cloud Storage) ::=\n  [ INTEGRATION = '<integration_name>' ]\n\ncloudProviderParams (for Microsoft Azure) ::=\n  [ INTEGRATION = '<integration_name>' ]\n\nexternalStage ::=\n  @[<namespace>.]<ext_stage_name>[/<path>]\n\nformatTypeOptions ::=\n-- If FILE_FORMAT = ( TYPE = CSV ... )\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string1>' [ , '<string2>' , ... ] )\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     ENCODING = '<string>'\n-- If FILE_FORMAT = ( TYPE = JSON ... )\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     MULTI_LINE = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n-- If FILE_FORMAT = ( TYPE = AVRO ... )\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n-- If FILE_FORMAT = ( TYPE = ORC ... )\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ]\n-- If FILE_FORMAT = ( TYPE = PARQUET ... )\n     COMPRESSION = AUTO | SNAPPY | NONE\n     BINARY_AS_TEXT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE",
    "examples": [
        {
            "code": "CREATE STAGE s1\n  URL='s3://mybucket/files/logs/'\n  ...\n  ;"
        },
        {
            "code": "CREATE STAGE s1\n  URL='gcs://mybucket/files/logs/'\n  ...\n  ;"
        },
        {
            "code": "CREATE STAGE s1\n  URL='azure://mycontainer/files/logs/'\n  ...\n  ;"
        },
        {
            "code": "SELECT metadata$filename FROM @s1/;\n\n+----------------------------------------+\n| METADATA$FILENAME                      |\n|----------------------------------------|\n| files/logs/2018/08/05/0524/log.parquet |\n| files/logs/2018/08/27/1408/log.parquet |\n+----------------------------------------+"
        },
        {
            "code": "CREATE EXTERNAL TABLE et1(\n date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n   || '/' || SPLIT_PART(metadata$filename, '/', 4)\n   || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n timestamp bigint AS (value:timestamp::bigint),\n col2 varchar AS (value:col2::varchar))\n PARTITION BY (date_part)\n LOCATION=@s1/logs/\n AUTO_REFRESH = true\n FILE_FORMAT = (TYPE = PARQUET)\n AWS_SNS_TOPIC = 'arn:aws:sns:us-west-2:001234567890:s3_mybucket';"
        },
        {
            "code": "CREATE EXTERNAL TABLE et1(\n  date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n    || '/' || SPLIT_PART(metadata$filename, '/', 4)\n    || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n  timestamp bigint AS (value:timestamp::bigint),\n  col2 varchar AS (value:col2::varchar))\n  PARTITION BY (date_part)\n  LOCATION=@s1/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET);"
        },
        {
            "code": "CREATE EXTERNAL TABLE et1(\n  date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n    || '/' || SPLIT_PART(metadata$filename, '/', 4)\n    || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n  timestamp bigint AS (value:timestamp::bigint),\n  col2 varchar AS (value:col2::varchar))\n  PARTITION BY (date_part)\n  INTEGRATION = 'MY_INT'\n  LOCATION=@s1/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET);"
        },
        {
            "code": "ALTER EXTERNAL TABLE et1 REFRESH;"
        },
        {
            "code": "SELECT timestamp, col2 FROM et1 WHERE date_part = to_date('08/05/2018');"
        },
        {
            "code": "CREATE STAGE s2\n  URL='s3://mybucket/files/logs/'\n  ...\n  ;"
        },
        {
            "code": "CREATE STAGE s2\n  URL='gcs://mybucket/files/logs/'\n  ...\n  ;"
        },
        {
            "code": "CREATE STAGE s2\n  URL='azure://mycontainer/files/logs/'\n  ...\n  ;"
        },
        {
            "code": "create external table et2(\n  col1 date as (parse_json(metadata$external_table_partition):COL1::date),\n  col2 varchar as (parse_json(metadata$external_table_partition):COL2::varchar),\n  col3 number as (parse_json(metadata$external_table_partition):COL3::number))\n  partition by (col1,col2,col3)\n  location=@s2/logs/\n  partition_type = user_specified\n  file_format = (type = parquet);"
        },
        {
            "code": "ALTER EXTERNAL TABLE et2 ADD PARTITION(col1='2022-01-24', col2='a', col3='12') LOCATION '2022/01';"
        },
        {
            "code": "+---------------------------------------+----------------+-------------------------------+\n|                       file            |     status     |          description          |\n+---------------------------------------+----------------+-------------------------------+\n| mycontainer/files/logs/2022/01/24.csv | REGISTERED_NEW | File registered successfully. |\n| mycontainer/files/logs/2022/01/25.csv | REGISTERED_NEW | File registered successfully. |\n+---------------------------------------+----------------+-------------------------------+"
        },
        {
            "code": "SELECT col1, col2, col3 FROM et1 WHERE col1 = TO_DATE('2022-01-24') AND col2 = 'a' ORDER BY METADATA$FILE_ROW_NUMBER;"
        },
        {
            "code": "CREATE MATERIALIZED VIEW et1_mv\n  AS\n  SELECT col2 FROM et1;"
        },
        {
            "code": "CREATE EXTERNAL TABLE mytable\n  USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    FROM TABLE(\n      INFER_SCHEMA(\n        LOCATION=>'@mystage',\n        FILE_FORMAT=>'my_parquet_format'\n      )\n    )\n  )\n  LOCATION=@mystage\n  FILE_FORMAT=my_parquet_format\n  AUTO_REFRESH=false;"
        },
        {
            "code": "CREATE EXTERNAL TABLE mytable\n  USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT('COLUMN_NAME',COLUMN_NAME, 'TYPE',TYPE, 'NULLABLE', NULLABLE, 'EXPRESSION',EXPRESSION))\n    FROM TABLE(\n      INFER_SCHEMA(\n        LOCATION=>'@mystage',\n        FILE_FORMAT=>'my_parquet_format'\n      )\n    )\n  )\n  LOCATION=@mystage\n  FILE_FORMAT=my_parquet_format\n  AUTO_REFRESH=false;"
        }
    ],
    "parameters": [
        {
            "name": "table_name",
            "description": "String that specifies the identifier (i.e. name) for the table; must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "[   WITH   ]   LOCATION   =",
            "description": "Specifies the external stage and optional path where the files containing data to be read are staged: @[ namespace .] ext_stage_name [/ path ] Files are in the specified named external stage. Neither string literals nor SQL variables are supported. Where: namespace is the database and/or schema in which the external stage resides, in the form of database_name . schema_name or schema_name . It is optional if a database and schema are currently in use within the user session; otherwise, it\nis required. path is an optional case-sensitive directory path for files in the cloud storage location that limits the set of files to load.\nPaths are alternatively called prefixes or folders by different cloud storage services. The external table appends this directory path to any path specified in the stage definition. To view the stage definition,\nexecute DESC STAGE stage_name and check the url property value. For example, if the stage URL includes\npath a and the external table location includes path b , then the external table reads files staged in stage /a/b . Note Specify a full directory path, and not a partial path (shared prefix) for files in your storage location (for example, use a path like @my_ext_stage/2025/ instead of @my_ext_stage/2025-* ). To filter for files that share a common prefix, use partition columns instead. Note that the [ WITH ] LOCATION value cannot reference specific filenames. To point an external table to individual\nstaged files, use the PATTERN parameter."
        },
        {
            "name": "FILE_FORMAT   =   (   FORMAT_NAME   =   ' file_format_name '   )   or   .   FILE_FORMAT   =   (   TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   [   ...   ]   )",
            "description": "String (constant) that specifies the file format: Specifies an existing named file format that describes the staged data files to scan. The named file format determines the format\ntype (CSV, JSON, etc.), as well as any other format options, for data files. Specifies the format type of the staged data files to scan when querying the external table. If a file format type is specified, additional format-specific options can be specified. For more details, see Format Type Options (in this topic). Default: TYPE = CSV . Important An external table does not inherit FILE_FORMAT options specified in a stage definition when that stage is used for loading data into the table. To specify FILE_FORMAT options, you must explicitly do so in the external table definition. Snowflake uses defaults for any FILE_FORMAT parameters omitted from the external table definition. Note FORMAT_NAME and TYPE are mutually exclusive; to avoid unintended behavior, you should only specify one or the other\nwhen creating an external table."
        },
        {
            "name": "FORMAT_NAME   =   file_format_name",
            "description": "Specifies an existing named file format that describes the staged data files to scan. The named file format determines the format\ntype (CSV, JSON, etc.), as well as any other format options, for data files."
        },
        {
            "name": "TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   [   ...   ]",
            "description": "Specifies the format type of the staged data files to scan when querying the external table. If a file format type is specified, additional format-specific options can be specified. For more details, see Format Type Options (in this topic)."
        },
        {
            "name": "col_name",
            "description": "String that specifies the column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. For more details, see Identifier requirements ."
        },
        {
            "name": "col_type",
            "description": "String (constant) that specifies the data type for the column. The data type must match the result of expr for the column. For details about the data types that can be specified for table columns, see SQL data types reference ."
        },
        {
            "name": "expr",
            "description": "String that specifies the expression for the column. When queried, the column returns results derived from this expression. External table columns are virtual columns, which are defined using an explicit expression. Add virtual columns as expressions using the\nVALUE column and/or the METADATA$FILENAME pseudocolumn: A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (i.e. {c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...} ). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file: A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage. For\nan example, see Partitions Added Automatically From Partition Column Expressions (in this topic)."
        },
        {
            "name": "CONSTRAINT   ...",
            "description": "String that defines an inline or out-of-line constraint for the specified column(s) in the table. For syntax details, see CREATE | ALTER TABLE … CONSTRAINT . For more information about constraints, see Constraints ."
        },
        {
            "name": "REFRESH_ON_CREATE   =     TRUE   |   FALSE",
            "description": "Specifies whether to automatically refresh the external table metadata once, immediately after the external table is created. Refreshing\nthe external table metadata synchronizes the metadata with the current list of data files in the specified stage path. This action is\nrequired for the metadata to register any existing data files in the named external stage specified in the [ WITH ] LOCATION = setting. Snowflake automatically refreshes the external table metadata once after creation. Note If the specified location contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the external table, refresh the metadata\nincrementally by executing ALTER EXTERNAL TABLE … REFRESH statements that specify subpaths in\nthe location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the external table metadata. To register any existing data files in the stage, you must\nmanually refresh the external table metadata once using ALTER EXTERNAL TABLE … REFRESH. Default: TRUE"
        },
        {
            "name": "AUTO_REFRESH   =     TRUE   |   FALSE",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the external table metadata when new or updated data\nfiles are available in the named external stage specified in the [ WITH ] LOCATION = setting. Note Setting this parameter to TRUE is not supported by partitioned external tables when partitions are added manually by the\nobject owner (i.e. when PARTITION_TYPE = USER_SPECIFIED ). Setting this parameter to TRUE is not supported for external tables that reference data files\nin S3-compatible storage (a storage application or device\nthat provides an API compliant with the S3 REST API). For more information, see Working with Amazon S3-compatible storage . You must manually refresh the metadata by executing an ALTER EXTERNAL TABLE … REFRESH command. You must configure an event notification for your storage location to notify Snowflake when new or updated data is available\nto read into the external table metadata. For more information, see the instructions for your cloud storage service: Refreshing external tables automatically for Amazon S3 Refreshing external tables automatically for Google Cloud Storage Refreshing external tables automatically for Azure Blob Storage When an external table is created, its metadata is refreshed automatically once unless REFRESH_ON_CREATE = FALSE . Snowflake enables triggering automatic refreshes of the external table metadata. Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table\nmetadata periodically using ALTER EXTERNAL TABLE … REFRESH to synchronize the metadata with the current list of files in the\nstage path. Default: TRUE"
        },
        {
            "name": "VALUE :",
            "description": "A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (i.e. {c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...} ). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
        },
        {
            "name": "METADATA$FILENAME :",
            "description": "A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage. For\nan example, see Partitions Added Automatically From Partition Column Expressions (in this topic)."
        },
        {
            "name": "CSV :",
            "description": "The VALUE column structures each row as an object with elements identified by column position (i.e. {c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...} ). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
        },
        {
            "name": "Semi-structured data :",
            "description": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the external table metadata once after creation. Note If the specified location contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the external table, refresh the metadata\nincrementally by executing ALTER EXTERNAL TABLE … REFRESH statements that specify subpaths in\nthe location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the external table metadata. To register any existing data files in the stage, you must\nmanually refresh the external table metadata once using ALTER EXTERNAL TABLE … REFRESH."
        },
        {
            "name": "Amazon S3 :",
            "description": "Refreshing external tables automatically for Amazon S3"
        },
        {
            "name": "Google Cloud Storage :",
            "description": "Refreshing external tables automatically for Google Cloud Storage"
        },
        {
            "name": "Microsoft Azure :",
            "description": "Refreshing external tables automatically for Azure Blob Storage"
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the external table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table\nmetadata periodically using ALTER EXTERNAL TABLE … REFRESH to synchronize the metadata with the current list of files in the\nstage path."
        },
        {
            "name": "PATTERN   =   ' regex_pattern '",
            "description": "A regular expression pattern string, enclosed in single quotes, specifying the filenames and/or paths on the external stage to match. Tip For the best performance, try to avoid applying patterns that filter on a large number of files."
        },
        {
            "name": "AWS_SNS_TOPIC   =   ' string '",
            "description": "Required only when configuring AUTO_REFRESH for Amazon S3 stages using Amazon Simple Notification Service (SNS). Specifies the\nAmazon Resource Name (ARN) for the SNS topic for your S3 bucket. The CREATE EXTERNAL TABLE statement subscribes the Amazon Simple Queue\nService (SQS) queue to the specified SNS topic. Event notifications via the SNS topic trigger metadata refreshes. For more information,\nsee Refreshing external tables automatically for Amazon S3 ."
        },
        {
            "name": "TABLE_FORMAT   =   DELTA",
            "description": "Note This feature is still supported but will be deprecated in a future release. Consider using an Apache Iceberg™ table instead. Iceberg tables\nuse an external volume to connect to Delta table files in your cloud storage. For more information, see Iceberg tables and CREATE ICEBERG TABLE (Delta files in object storage) .\nYou can also Migrate a Delta external table to Apache Iceberg™ . Identifies the external table as referencing a Delta Lake on the cloud storage location. A Delta Lake on Amazon S3, Google Cloud Storage,\nor Microsoft Azure cloud storage is supported. Note This preview feature is available to all accounts. When this parameter is set, the external table scans for Delta Lake transaction log files in the [ WITH ] LOCATION location.\nDelta log files have names like _delta_log/00000000000000000000.json , _delta_log/00000000000000000010.checkpoint.parquet , etc. When the metadata for an external table is refreshed, Snowflake parses the Delta Lake transaction logs and determines which Parquet\nfiles are current. In the background, the refresh performs add and remove file operations to keep the external table metadata in sync. Note The external stage and optional path specified in [ WITH ] LOCATION = must contain the data files and metadata for a single Delta Lake table only. That is, the specified storage location can only contain one __delta_log directory. The ordering of event notifications triggered by DDL operations in cloud storage is not guaranteed. Therefore, the ability to\nautomatically refresh is not available for external tables that reference Delta Lake files. Both REFRESH_ON_CREATE and AUTO_REFRESH must be set to FALSE. Periodically execute an ALTER EXTERNAL TABLE … REFRESH statement to register any\nadded or removed files. The FILE_FORMAT value must specify Parquet as the file type. For optimal performance, we recommend defining partition columns for the external table. The following parameters are not supported when referencing a Delta Lake: AWS_SNS_TOPIC = ' string ' PATTERN = ' regex_pattern '"
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access permissions from the original table when an external table is recreated using the CREATE OR REPLACE TABLE\nvariant. The parameter copies all permissions, except OWNERSHIP, from the existing table to the new table. By default, the role\nthat executes the CREATE EXTERNAL TABLE command owns the new external table. Note The operation to copy grants occurs atomically in the CREATE EXTERNAL TABLE command (i.e. within the same transaction)."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the external table. Default: No value"
        },
        {
            "name": "ROW   ACCESS   POLICY   <policy_name>   ON   (VALUE)",
            "description": "Specifies the row access policy to set on the table. Specify the VALUE column when applying a row access policy to an external table."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        },
        {
            "name": "part_col_name   col_type   AS   part_expr",
            "description": "Defines one or more partition columns in the external table. The format of a partition column definition differs depending on whether partitions are computed and added automatically from an\nexpression in each partition column or the partitions are added manually. A partition column must evaluate as an expression that parses the path and/or filename information in the METADATA$FILENAME\npseudocolumn. Partition columns optimize query performance by pruning out the data files that do not need to be scanned (i.e.\npartitioning the external table). A partition consists of all data files that match the path and/or filename in the expression for\nthe partition column. part_col_name String that specifies the partition column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. col_type String (constant) that specifies the data type for the column. The data type must match the result of part_expr for the column. part_expr String that specifies the expression for the column. The expression must include the METADATA$FILENAME pseudocolumn. External tables currently support the following subset of functions in partition expressions: List of supported functions: = , <> , > , >= , < , <= || + , - - (negate) * AND , OR ARRAY_CONSTRUCT CASE CAST , :: CONCAT , || ENDSWITH IS [ NOT ] NULL IFF IFNULL [ NOT ] IN LOWER NOT NULLIF NVL2 SPLIT_PART STARTSWITH SUBSTR , SUBSTRING UPPER ZEROIFNULL Required: Also set the PARTITION_TYPE parameter value to USER_SPECIFIED . A partition column definition is an expression that parses the column metadata in the internal (hidden)\nMETADATA$EXTERNAL_TABLE_PARTITION column. Essentially, the definition only defines the data type for the column. The format of the\npartition column definition is as follows: part_col_name col_type AS ( PARSE_JSON (METADATA$EXTERNALTABLE_PARTITION): part_col_name :: data_type ) For example, suppose columns col1 , col2 , and col3 contain varchar, number, and timestamp (time zone) data, respectively: After defining any partition columns for the table, identify these columns using the PARTITION BY clause. Note The maximum length of user-specified partition column names is 32 characters."
        },
        {
            "name": "PARTITION_TYPE   =   USER_SPECIFIED",
            "description": "Defines the partition type for the external table as user-defined . The owner of the external table (i.e. the role that has the\nOWNERSHIP privilege on the external table) must add partitions to the external metadata manually by executing ALTER EXTERNAL\nTABLE … ADD PARTITION statements. Do not set this parameter if partitions are added to the external table metadata automatically upon evaluation of expressions\nin the partition columns."
        },
        {
            "name": "[   PARTITION   BY   (   part_col_name   [,   part_col_name   ...   ]   )   ]",
            "description": "Specifies any partition columns to evaluate for the external table. When querying an external table, include one or more partition columns in a WHERE clause, e.g.: ... WHERE part_col_name = ' filter_value ' Snowflake filters on the partition columns to restrict the set of data files to scan. Note that all rows in these files are scanned.\nIf a WHERE clause includes non-partition columns, those filters are evaluated after the data files have been filtered. A common practice is to partition the data files based on increments of time; or, if the data files are staged from multiple sources,\nto partition by a data source identifier and date or timestamp."
        },
        {
            "name": "Added from an expression :",
            "description": "A partition column must evaluate as an expression that parses the path and/or filename information in the METADATA$FILENAME\npseudocolumn. Partition columns optimize query performance by pruning out the data files that do not need to be scanned (i.e.\npartitioning the external table). A partition consists of all data files that match the path and/or filename in the expression for\nthe partition column. part_col_name String that specifies the partition column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. col_type String (constant) that specifies the data type for the column. The data type must match the result of part_expr for the column. part_expr String that specifies the expression for the column. The expression must include the METADATA$FILENAME pseudocolumn. External tables currently support the following subset of functions in partition expressions: List of supported functions: = , <> , > , >= , < , <= || + , - - (negate) * AND , OR ARRAY_CONSTRUCT CASE CAST , :: CONCAT , || ENDSWITH IS [ NOT ] NULL IFF IFNULL [ NOT ] IN LOWER NOT NULLIF NVL2 SPLIT_PART STARTSWITH SUBSTR , SUBSTRING UPPER ZEROIFNULL"
        },
        {
            "name": "Added manually :",
            "description": "Required: Also set the PARTITION_TYPE parameter value to USER_SPECIFIED . A partition column definition is an expression that parses the column metadata in the internal (hidden)\nMETADATA$EXTERNAL_TABLE_PARTITION column. Essentially, the definition only defines the data type for the column. The format of the\npartition column definition is as follows: part_col_name col_type AS ( PARSE_JSON (METADATA$EXTERNALTABLE_PARTITION): part_col_name :: data_type ) For example, suppose columns col1 , col2 , and col3 contain varchar, number, and timestamp (time zone) data, respectively:"
        },
        {
            "name": "Usage :",
            "description": "When querying an external table, include one or more partition columns in a WHERE clause, e.g.: ... WHERE part_col_name = ' filter_value ' Snowflake filters on the partition columns to restrict the set of data files to scan. Note that all rows in these files are scanned.\nIf a WHERE clause includes non-partition columns, those filters are evaluated after the data files have been filtered. A common practice is to partition the data files based on increments of time; or, if the data files are staged from multiple sources,\nto partition by a data source identifier and date or timestamp."
        },
        {
            "name": "part_col_name   col_type   AS   part_expr",
            "description": "Defines one or more partition columns in the external table. The format of a partition column definition differs depending on whether partitions are computed and added automatically from an\nexpression in each partition column or the partitions are added manually. A partition column must evaluate as an expression that parses the path and/or filename information in the METADATA$FILENAME\npseudocolumn. Partition columns optimize query performance by pruning out the data files that do not need to be scanned (i.e.\npartitioning the external table). A partition consists of all data files that match the path and/or filename in the expression for\nthe partition column. part_col_name String that specifies the partition column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. col_type String (constant) that specifies the data type for the column. The data type must match the result of part_expr for the column. part_expr String that specifies the expression for the column. The expression must include the METADATA$FILENAME pseudocolumn. External tables currently support the following subset of functions in partition expressions: List of supported functions: = , <> , > , >= , < , <= || + , - - (negate) * AND , OR ARRAY_CONSTRUCT CASE CAST , :: CONCAT , || ENDSWITH IS [ NOT ] NULL IFF IFNULL [ NOT ] IN LOWER NOT NULLIF NVL2 SPLIT_PART STARTSWITH SUBSTR , SUBSTRING UPPER ZEROIFNULL Required: Also set the PARTITION_TYPE parameter value to USER_SPECIFIED . A partition column definition is an expression that parses the column metadata in the internal (hidden)\nMETADATA$EXTERNAL_TABLE_PARTITION column. Essentially, the definition only defines the data type for the column. The format of the\npartition column definition is as follows: part_col_name col_type AS ( PARSE_JSON (METADATA$EXTERNALTABLE_PARTITION): part_col_name :: data_type ) For example, suppose columns col1 , col2 , and col3 contain varchar, number, and timestamp (time zone) data, respectively: After defining any partition columns for the table, identify these columns using the PARTITION BY clause. Note The maximum length of user-specified partition column names is 32 characters."
        },
        {
            "name": "PARTITION_TYPE   =   USER_SPECIFIED",
            "description": "Defines the partition type for the external table as user-defined . The owner of the external table (i.e. the role that has the\nOWNERSHIP privilege on the external table) must add partitions to the external metadata manually by executing ALTER EXTERNAL\nTABLE … ADD PARTITION statements. Do not set this parameter if partitions are added to the external table metadata automatically upon evaluation of expressions\nin the partition columns."
        },
        {
            "name": "[   PARTITION   BY   (   part_col_name   [,   part_col_name   ...   ]   )   ]",
            "description": "Specifies any partition columns to evaluate for the external table. When querying an external table, include one or more partition columns in a WHERE clause, e.g.: ... WHERE part_col_name = ' filter_value ' Snowflake filters on the partition columns to restrict the set of data files to scan. Note that all rows in these files are scanned.\nIf a WHERE clause includes non-partition columns, those filters are evaluated after the data files have been filtered. A common practice is to partition the data files based on increments of time; or, if the data files are staged from multiple sources,\nto partition by a data source identifier and date or timestamp."
        },
        {
            "name": "Added from an expression :",
            "description": "A partition column must evaluate as an expression that parses the path and/or filename information in the METADATA$FILENAME\npseudocolumn. Partition columns optimize query performance by pruning out the data files that do not need to be scanned (i.e.\npartitioning the external table). A partition consists of all data files that match the path and/or filename in the expression for\nthe partition column. part_col_name String that specifies the partition column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. col_type String (constant) that specifies the data type for the column. The data type must match the result of part_expr for the column. part_expr String that specifies the expression for the column. The expression must include the METADATA$FILENAME pseudocolumn. External tables currently support the following subset of functions in partition expressions: List of supported functions: = , <> , > , >= , < , <= || + , - - (negate) * AND , OR ARRAY_CONSTRUCT CASE CAST , :: CONCAT , || ENDSWITH IS [ NOT ] NULL IFF IFNULL [ NOT ] IN LOWER NOT NULLIF NVL2 SPLIT_PART STARTSWITH SUBSTR , SUBSTRING UPPER ZEROIFNULL"
        },
        {
            "name": "Added manually :",
            "description": "Required: Also set the PARTITION_TYPE parameter value to USER_SPECIFIED . A partition column definition is an expression that parses the column metadata in the internal (hidden)\nMETADATA$EXTERNAL_TABLE_PARTITION column. Essentially, the definition only defines the data type for the column. The format of the\npartition column definition is as follows: part_col_name col_type AS ( PARSE_JSON (METADATA$EXTERNALTABLE_PARTITION): part_col_name :: data_type ) For example, suppose columns col1 , col2 , and col3 contain varchar, number, and timestamp (time zone) data, respectively:"
        },
        {
            "name": "Usage :",
            "description": "When querying an external table, include one or more partition columns in a WHERE clause, e.g.: ... WHERE part_col_name = ' filter_value ' Snowflake filters on the partition columns to restrict the set of data files to scan. Note that all rows in these files are scanned.\nIf a WHERE clause includes non-partition columns, those filters are evaluated after the data files have been filtered. A common practice is to partition the data files based on increments of time; or, if the data files are staged from multiple sources,\nto partition by a data source identifier and date or timestamp."
        },
        {
            "name": "INTEGRATION   =   integration_name",
            "description": "Specifies the name of the notification integration used to automatically refresh the external table metadata using Google Pub/Sub\nevent notifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party\ncloud message queuing services. This parameter is required to enable auto-refresh operations for the external table. For instructions on configuring the\nauto-refresh capability, see Refreshing external tables automatically for Google Cloud Storage ."
        },
        {
            "name": "INTEGRATION   =   integration_name",
            "description": "Specifies the name of the notification integration used to automatically refresh the external table metadata using Azure Event Grid\nnotifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party cloud\nmessage queuing services. This parameter is required to enable auto-refresh operations for the external table. For instructions on configuring the auto-refresh\ncapability, see Refreshing external tables automatically for Azure Blob Storage ."
        }
    ],
    "usage_notes": "External tables support external (S3, Azure, or GCS) stages only; internal (Snowflake) stages are not supported.\nExternal tables don’t support storage versioning (S3 versioning, Object Versioning in Google Cloud Storage, or versioning for Azure Storage).\nYou cannot access data held in archival cloud storage classes that requires restoration before it can be retrieved. These archival storage classes include, for example, the Amazon S3 Glacier Flexible Retrieval or Glacier Deep Archive storage class, or Microsoft Azure Archive Storage.\nSnowflake does not enforce integrity constraints on external tables. In particular, unlike normal tables, Snowflake does not enforce\nNOT NULL constraints.\nExternal tables include the following metadata column:\nMETADATA$FILENAME: Name of each staged data file included in the external table. Includes the path to the data file in the stage.\nMETADATA$FILE_ROW_NUMBER: Row number for each record in the staged data file.\nThe following are not supported for external tables:\nClustering keys\nCloning\nData in XML format\nTime Travel is not supported for external tables.\nFor details about using an external table with a policy, see:\nMasking policies and external tables.\nRow access policies and external tables.\nUsing OR REPLACE is the equivalent of using DROP EXTERNAL TABLE on the existing external table and then creating a new\nexternal table with the same name.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE EXTERNAL TABLE operation use either the old or new external table version.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nWhen creating an external table with a row access policy added to the external table, use the\nPOLICY_CONTEXT function to simulate a query on the external table protected by a row access policy.\nSELECT * always returns the VALUE column, in which all regular or semi-structured data is cast to variant rows.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-stage",
    "title": "CREATE STAGE",
    "description": "Creates a new named internal or external stage to use for loading data from files into Snowflake tables and unloading data from\ntables into files:",
    "syntax": "-- Internal stage\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] STAGE [ IF NOT EXISTS ] <internal_stage_name>\n    internalStageParams\n    directoryTableParams\n  [ FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM } [ formatTypeOptions ] } ) ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n\n-- External stage\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] STAGE [ IF NOT EXISTS ] <external_stage_name>\n    externalStageParams\n    directoryTableParams\n  [ FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM } [ formatTypeOptions ] } ) ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n\ninternalStageParams ::=\n  [ ENCRYPTION = (   TYPE = 'SNOWFLAKE_FULL'\n                   | TYPE = 'SNOWFLAKE_SSE' ) ]\n\nexternalStageParams (for Amazon S3) ::=\n  URL = '<protocol>://<bucket>[/<path>/]'\n  [ AWS_ACCESS_POINT_ARN = '<string>' ]\n  [ { STORAGE_INTEGRATION = <integration_name> } | { CREDENTIALS = ( {  { AWS_KEY_ID = '<string>' AWS_SECRET_KEY = '<string>' [ AWS_TOKEN = '<string>' ] } | AWS_ROLE = '<string>'  } ) } ]\n  [ ENCRYPTION = ( [ TYPE = 'AWS_CSE' ] MASTER_KEY = '<string>'\n                   | TYPE = 'AWS_SSE_S3'\n                   | TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = '<string>' ]\n                   | TYPE = 'NONE' ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\nexternalStageParams (for Google Cloud Storage) ::=\n  URL = 'gcs://<bucket>[/<path>/]'\n  [ STORAGE_INTEGRATION = <integration_name> ]\n  [ ENCRYPTION = (   TYPE = 'GCS_SSE_KMS' [ KMS_KEY_ID = '<string>' ]\n                   | TYPE = 'NONE' ) ]\n\nexternalStageParams (for Microsoft Azure) ::=\n  URL = 'azure://<account>.blob.core.windows.net/<container>[/<path>/]'\n  [ { STORAGE_INTEGRATION = <integration_name> } | { CREDENTIALS = ( [ AZURE_SAS_TOKEN = '<string>' ] ) } ]\n  [ ENCRYPTION = (   TYPE = 'AZURE_CSE' MASTER_KEY = '<string>'\n                   | TYPE = 'NONE' ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\nexternalStageParams (for Amazon S3-compatible Storage) ::=\n  URL = 's3compat://{bucket}[/{path}/]'\n  ENDPOINT = '<s3_api_compatible_endpoint>'\n  [ { CREDENTIALS = ( AWS_KEY_ID = '<string>' AWS_SECRET_KEY = '<string>' ) } ]\n\ndirectoryTableParams (for internal stages) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ AUTO_REFRESH = { TRUE | FALSE } ] ) ]\n\ndirectoryTableParams (for Amazon S3) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n                  [ AUTO_REFRESH = { TRUE | FALSE } ] ) ]\n\ndirectoryTableParams (for Google Cloud Storage) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ AUTO_REFRESH = { TRUE | FALSE } ]\n                  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n                  [ NOTIFICATION_INTEGRATION = '<notification_integration_name>' ] ) ]\n\ndirectoryTableParams (for Microsoft Azure) ::=\n  [ DIRECTORY = ( ENABLE = { TRUE | FALSE }\n                  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n                  [ AUTO_REFRESH = { TRUE | FALSE } ]\n                  [ NOTIFICATION_INTEGRATION = '<notification_integration_name>' ] ) ]\n\nformatTypeOptions ::=\n-- If TYPE = CSV\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     FILE_EXTENSION = '<string>'\n     PARSE_HEADER = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     ESCAPE = '<character>' | NONE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n     ENCODING = '<string>' | UTF8\n-- If TYPE = JSON\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     TRIM_SPACE = TRUE | FALSE\n     MULTI_LINE = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     FILE_EXTENSION = '<string>'\n     ENABLE_OCTAL = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n-- If TYPE = AVRO\n     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = ORC\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = PARQUET\n     COMPRESSION = AUTO | LZO | SNAPPY | NONE\n     SNAPPY_COMPRESSION = TRUE | FALSE\n     BINARY_AS_TEXT = TRUE | FALSE\n     USE_LOGICAL_TYPE = TRUE | FALSE\n     TRIM_SPACE = TRUE | FALSE\n     USE_VECTORIZED_SCANNER = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = XML\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     PRESERVE_SPACE = TRUE | FALSE\n     STRIP_OUTER_ELEMENT = TRUE | FALSE\n     DISABLE_AUTO_CONVERT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE",
    "examples": [
        {
            "code": "CREATE STAGE my_int_stage\n  ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');"
        },
        {
            "code": "CREATE TEMPORARY STAGE my_temp_int_stage;"
        },
        {
            "code": "CREATE TEMPORARY STAGE my_int_stage\n  FILE_FORMAT = my_csv_format;"
        },
        {
            "code": "CREATE STAGE mystage\n  DIRECTORY = (ENABLE = TRUE)\n  FILE_FORMAT = myformat;"
        },
        {
            "code": "CREATE STAGE my_ext_stage\n  URL='s3://load/files/'\n  STORAGE_INTEGRATION = myint;"
        },
        {
            "code": "CREATE STAGE my_ext_stage1\n  URL='s3://load/files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z');"
        },
        {
            "code": "CREATE STAGE my_ext_stage2\n  URL='s3://load/encrypted_files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z')\n  ENCRYPTION=(MASTER_KEY = 'eSx...');"
        },
        {
            "code": "CREATE STAGE my_ext_stage3\n  URL='s3://load/encrypted_files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z')\n  ENCRYPTION=(TYPE='AWS_SSE_KMS' KMS_KEY_ID = 'aws/key');"
        },
        {
            "code": "CREATE STAGE my_ext_stage3\n  URL='s3://load/encrypted_files/'\n  CREDENTIALS=(AWS_ROLE='arn:aws:iam::001234567890:role/mysnowflakerole')\n  ENCRYPTION=(TYPE='AWS_SSE_KMS' KMS_KEY_ID = 'aws/key');"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='s3://load/files/'\n  STORAGE_INTEGRATION = my_storage_int\n  DIRECTORY = (\n    ENABLE = true\n    AUTO_REFRESH = true\n  );"
        },
        {
            "code": "CREATE STAGE my_ext_stage\n  URL='gcs://load/files/'\n  STORAGE_INTEGRATION = myint;"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='gcs://load/files/'\n  STORAGE_INTEGRATION = my_storage_int\n  DIRECTORY = (\n    ENABLE = true\n    AUTO_REFRESH = true\n    NOTIFICATION_INTEGRATION = 'MY_NOTIFICATION_INT'\n  );"
        },
        {
            "code": "CREATE STAGE my_ext_stage2\n  URL='gcs://load/encrypted_files/'\n  STORAGE_INTEGRATION = my_storage_int\n  ENCRYPTION=(TYPE = 'GCS_SSE_KMS' KMS_KEY_ID = '{a1b2c3});"
        },
        {
            "code": "CREATE STAGE my_ext_stage\n  URL='azure://myaccount.blob.core.windows.net/load/files/'\n  STORAGE_INTEGRATION = myint;"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='azure://myaccount.blob.core.windows.net/mycontainer/files/'\n  CREDENTIALS=(AZURE_SAS_TOKEN='?sv=2016-05-31&ss=b&srt=sco&sp=rwdl&se=2018-06-27T10:05:50Z&st=2017-06-27T02:05:50Z&spr=https,http&sig=bgqQwoXwxzuD2GJfagRg7VOS8hzNr3QLT7rhS8OFRLQ%3D')\n  ENCRYPTION=(TYPE='AZURE_CSE' MASTER_KEY = 'kPx...');"
        },
        {
            "code": "CREATE STAGE mystage\n  URL='azure://myaccount.blob.core.windows.net/load/files/'\n  STORAGE_INTEGRATION = my_storage_int\n  DIRECTORY = (\n    ENABLE = true\n    AUTO_REFRESH = true\n    NOTIFICATION_INTEGRATION = 'MY_NOTIFICATION_INT'\n  );"
        },
        {
            "code": "CREATE OR ALTER STAGE my_int_stage\n  COMMENT='my_comment'\n  ;"
        },
        {
            "code": "CREATE OR ALTER STAGE my_int_stage\n  DIRECTORY=(ENABLE=true);"
        },
        {
            "code": "CREATE OR ALTER STAGE my_ext_stage\n  URL='s3://load/files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z');"
        },
        {
            "code": "CREATE OR ALTER STAGE my_ext_stage\n  URL='s3://load/files/'\n  CREDENTIALS=(AWS_KEY_ID='1a2b3c' AWS_SECRET_KEY='4x5y6z')\n  DIRECTORY=(ENABLE=true);"
        }
    ],
    "parameters": [
        {
            "name": "internal_stage_name   or   .   external_stage_name",
            "description": "Specifies the identifier for the stage; must be unique for the schema in which the stage is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   }",
            "description": "Specifies that the stage created is temporary and will be dropped at the end of the session in which it was created. Note: When a temporary external stage is dropped, only the stage itself is dropped; the data files are not removed. When a temporary internal stage is dropped, all of the files in the stage are purged from Snowflake, regardless of their load status.\nThis prevents files in temporary internal stages from using data storage and, consequently, accruing storage charges. However, this also\nmeans that the staged files cannot be recovered through Snowflake once the stage is dropped. Tip If you plan to create and use temporary internal stages, you should maintain copies of your data files outside of Snowflake."
        },
        {
            "name": "FILE_FORMAT   =   (   FORMAT_NAME   =   ' file_format_name '   )   or   .   FILE_FORMAT   =   (   TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   |   CUSTOM   [   ...   ]   )",
            "description": "Specifies the file format for the stage, which can be either: Specifies an existing named file format to use for the stage. The named file format determines the format type (CSV, JSON, etc.), as\nwell as any other format options, for the data files loaded using this stage. For more details, see CREATE FILE FORMAT . Specifies the type of files for the stage: Loading data from a stage (using COPY INTO <table> ) accommodates all of the supported format types. Unloading data into a stage (using COPY INTO <location> ) accommodates CSV , JSON , or PARQUET . If a file format type is specified, additional format-specific options can be specified. For more details, see Format type options (formatTypeOptions) (in this topic). The CUSTOM format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option. Default: TYPE = CSV"
        },
        {
            "name": "FORMAT_NAME   =   ' file_format_name '",
            "description": "Specifies an existing named file format to use for the stage. The named file format determines the format type (CSV, JSON, etc.), as\nwell as any other format options, for the data files loaded using this stage. For more details, see CREATE FILE FORMAT ."
        },
        {
            "name": "TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   |   CUSTOM   [   ...   ]",
            "description": "Specifies the type of files for the stage: Loading data from a stage (using COPY INTO <table> ) accommodates all of the supported format types. Unloading data into a stage (using COPY INTO <location> ) accommodates CSV , JSON , or PARQUET . If a file format type is specified, additional format-specific options can be specified. For more details, see Format type options (formatTypeOptions) (in this topic). The CUSTOM format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the stage. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "[   ENCRYPTION   =   (   TYPE   =   'SNOWFLAKE_FULL'   |   TYPE   =   'SNOWFLAKE_SSE'   )   ]",
            "description": "Specifies the type of encryption supported for all files stored on the stage. You cannot change the encryption type after you create the stage. Specifies the encryption type used. Important If you require Tri-Secret Secure for security compliance, use the SNOWFLAKE_FULL encryption type for internal stages. SNOWFLAKE_SSE does not support Tri-Secret Secure. Possible values are: SNOWFLAKE_FULL : Client-side and server-side encryption. The files are encrypted by a client when it uploads them to the internal stage\nusing PUT . Snowflake uses a 128-bit encryption key by default. You can configure a 256-bit key by setting the CLIENT_ENCRYPTION_KEY_SIZE parameter. All files are also automatically encrypted using AES-256 strong encryption on the server side. SNOWFLAKE_SSE : Server-side encryption only. The files are encrypted when they arrive on the stage by the cloud service\nwhere your Snowflake account is hosted. Specify server-side encryption if you plan to query pre-signed URLs for your staged files. For more information, see Types of URLs available to access files . Default: SNOWFLAKE_FULL"
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Important If you require Tri-Secret Secure for security compliance, use the SNOWFLAKE_FULL encryption type for internal stages. SNOWFLAKE_SSE does not support Tri-Secret Secure. Possible values are: SNOWFLAKE_FULL : Client-side and server-side encryption. The files are encrypted by a client when it uploads them to the internal stage\nusing PUT . Snowflake uses a 128-bit encryption key by default. You can configure a 256-bit key by setting the CLIENT_ENCRYPTION_KEY_SIZE parameter. All files are also automatically encrypted using AES-256 strong encryption on the server side. SNOWFLAKE_SSE : Server-side encryption only. The files are encrypted when they arrive on the stage by the cloud service\nwhere your Snowflake account is hosted. Specify server-side encryption if you plan to query pre-signed URLs for your staged files. For more information, see Types of URLs available to access files ."
        },
        {
            "name": "URL   =   ' cloud_specific_url '",
            "description": "If this parameter is omitted, Snowflake creates an internal stage Important Enclose the URL in single quotes ( '' ) in order for Snowflake to identify the string. If the quotes are omitted, any credentials\nyou supply may be displayed in plain text in the history. We strongly recommend verifying the syntax of the CREATE STAGE statement\nbefore you execute it. When you create a stage in the Snowflake web interface, the interface automatically encloses field values in quotation characters,\nas needed. Append a forward slash ( / ) to the URL to filter to the specified folder path. If the forward slash is omitted, all files and\nfolders starting with the prefix for the specified path are included. Note that the forward slash is required to access and retrieve unstructured data files in the stage. Amazon S3 Specifies the URL for the external location (existing S3 bucket) used to store data files for loading/unloading, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . Accessing cloud storage in a government region using a storage integration is limited to Snowflake\naccounts hosted in the same government region. Similarly, if you need to access cloud storage in a region in China, you can use a storage integration only from a Snowflake\naccount hosted in the same region in China. In these cases, use the CREDENTIALS parameter in the CREATE STAGE command (rather than using a storage\nintegration) to provide the credentials for authentication. bucket is the name of the S3 bucket or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the AWS_ACCESS_POINT_ARN parameter. path is an optional case-sensitive path for files in the cloud storage location (files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage URL . Google Cloud Storage Specifies the URL for the external location (existing GCS bucket) used to store data files for loading/unloading, where: bucket is the name of the GCS bucket. path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Microsoft Azure Specifies the URL for the external location (existing Azure container) used to store data files for loading, where: account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all\nsupported types of Azure blob storage accounts, including Data Lake Storage Gen2. Note that currently, accessing Azure blob storage in government regions using a storage\nintegration is limited to Snowflake accounts hosted on Azure in the same government region. Accessing your blob storage from an\naccount hosted outside of the government region using direct credentials is supported. container is the name of the Azure container (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Default: No value (an internal stage is created)"
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name   or   .   CREDENTIALS   =   (   cloud_specific_credentials   )",
            "description": "Required only if the storage location is private/protected; not required for public buckets/containers Amazon S3 Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing S3 storage in government regions using a storage integration is limited to Snowflake accounts hosted on AWS in\nthe same government region. Accessing your S3 storage from an account hosted outside of the government region using direct\ncredentials is supported. Specifies the security credentials for connecting to AWS and accessing the private/protected S3 bucket where the files to\nload/unload are staged. For more information, see Configuring secure access to Amazon S3 . The credentials you specify depend on whether you associated the Snowflake access permissions for the bucket with an AWS IAM\n(Identity & Access Management) user or role: IAM user: IAM credentials are required. Temporary (aka “scoped”) credentials are generated by AWS Security Token Service\n(STS) and consist of three components: AWS_KEY_ID AWS_SECRET_KEY AWS_TOKEN All three are required to access a private/protected bucket. After a designated period of time, temporary credentials\nexpire and can no longer be used. You must then generate a new set of valid temporary credentials. Important The COPY command also allows permanent (aka “long-term”) credentials to be used; however, for security reasons, Snowflake does not recommend using them. If you must use permanent credentials, Snowflake recommends periodically generating new\npermanent credentials for external stages. IAM role: Omit the security credentials and access keys and, instead, identify the role using AWS_ROLE and specify\nthe AWS role ARN (Amazon Resource Name). Google Cloud Storage Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Microsoft Azure Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing Azure blob storage in government regions using a storage integration is limited to Snowflake accounts hosted on Azure in the\nsame government region. Accessing your blob storage from an account hosted outside\nof the government region using direct credentials is supported. Specifies the SAS (shared access signature) token for connecting to Azure and accessing the private/protected container\nwhere the files containing loaded data are staged. Credentials are generated by Azure. Default: No value (no credentials are provided for the external stage)"
        },
        {
            "name": "URL   =   ' protocol :// bucket [/ path /]'",
            "description": "Specifies the URL for the external location (existing S3 bucket) used to store data files for loading/unloading, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . Accessing cloud storage in a government region using a storage integration is limited to Snowflake\naccounts hosted in the same government region. Similarly, if you need to access cloud storage in a region in China, you can use a storage integration only from a Snowflake\naccount hosted in the same region in China. In these cases, use the CREDENTIALS parameter in the CREATE STAGE command (rather than using a storage\nintegration) to provide the credentials for authentication. bucket is the name of the S3 bucket or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the AWS_ACCESS_POINT_ARN parameter. path is an optional case-sensitive path for files in the cloud storage location (files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "AWS_ACCESS_POINT_ARN   =   ' string '",
            "description": "Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage URL ."
        },
        {
            "name": "URL   =   'gcs:// bucket [/ path /]'",
            "description": "Specifies the URL for the external location (existing GCS bucket) used to store data files for loading/unloading, where: bucket is the name of the GCS bucket. path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "URL   =   'azure:// account .blob.core.windows.net/ container [/ path /]'",
            "description": "Specifies the URL for the external location (existing Azure container) used to store data files for loading, where: account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all\nsupported types of Azure blob storage accounts, including Data Lake Storage Gen2. Note that currently, accessing Azure blob storage in government regions using a storage\nintegration is limited to Snowflake accounts hosted on Azure in the same government region. Accessing your blob storage from an\naccount hosted outside of the government region using direct credentials is supported. container is the name of the Azure container (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing S3 storage in government regions using a storage integration is limited to Snowflake accounts hosted on AWS in\nthe same government region. Accessing your S3 storage from an account hosted outside of the government region using direct\ncredentials is supported."
        },
        {
            "name": "CREDENTIALS   =   (   AWS_KEY_ID   =   ' string '   AWS_SECRET_KEY   =   ' string '   [   AWS_TOKEN   =   ' string '   ]   )   or   .   CREDENTIALS   =   (   AWS_ROLE   =   ' string '   )",
            "description": "Specifies the security credentials for connecting to AWS and accessing the private/protected S3 bucket where the files to\nload/unload are staged. For more information, see Configuring secure access to Amazon S3 . The credentials you specify depend on whether you associated the Snowflake access permissions for the bucket with an AWS IAM\n(Identity & Access Management) user or role: IAM user: IAM credentials are required. Temporary (aka “scoped”) credentials are generated by AWS Security Token Service\n(STS) and consist of three components: AWS_KEY_ID AWS_SECRET_KEY AWS_TOKEN All three are required to access a private/protected bucket. After a designated period of time, temporary credentials\nexpire and can no longer be used. You must then generate a new set of valid temporary credentials. Important The COPY command also allows permanent (aka “long-term”) credentials to be used; however, for security reasons, Snowflake does not recommend using them. If you must use permanent credentials, Snowflake recommends periodically generating new\npermanent credentials for external stages. IAM role: Omit the security credentials and access keys and, instead, identify the role using AWS_ROLE and specify\nthe AWS role ARN (Amazon Resource Name)."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION ."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a\nSnowflake identity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using\nthe CREDENTIALS parameter when creating stages or loading data. Accessing Azure blob storage in government regions using a storage integration is limited to Snowflake accounts hosted on Azure in the\nsame government region. Accessing your blob storage from an account hosted outside\nof the government region using direct credentials is supported."
        },
        {
            "name": "CREDENTIALS   =   (   AZURE_SAS_TOKEN   =   ' string '   )",
            "description": "Specifies the SAS (shared access signature) token for connecting to Azure and accessing the private/protected container\nwhere the files containing loaded data are staged. Credentials are generated by Azure."
        },
        {
            "name": "ENCRYPTION   =   (   cloud_specific_encryption   )",
            "description": "Required when loading from encrypted files or unloading into encrypted files. Not required if storage location and files are unencrypted. Modifies the encryption settings used to decrypt encrypted files in the storage location and extract data. Modifies the encryption settings used to encrypt files unloaded to the storage location. Amazon S3 ENCRYPTION = ( [ TYPE = 'AWS_CSE' ] MASTER_KEY = ' string ' | TYPE = 'AWS_SSE_S3' | TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = ' string ' ] | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: AWS_CSE : Client-side encryption (requires a MASTER_KEY value). Currently, the client-side master key you provide can only be a symmetric key. When a MASTER_KEY value is provided, Snowflake assumes TYPE = AWS_CSE (when a MASTER_KEY value is\nprovided, TYPE is not required). AWS_SSE_S3 : Server-side encryption that requires no additional encryption settings. AWS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information about the encryption types, see the AWS documentation for client-side encryption or server-side encryption . NONE : No encryption. Specifies the client-side master key used to encrypt the files in the bucket. The master key must be a 128-bit or 256-bit key\nin Base64-encoded form. Optionally specifies the ID for the AWS KMS-managed key used to encrypt files unloaded into the bucket. If no value\nis provided, your default KMS key ID is used to encrypt files on unload. Note that this value is ignored for data loading. Default: NONE Google Cloud Storage ENCRYPTION = ( TYPE = 'GCS_SSE_KMS' [ KMS_KEY_ID = ' string ' ] | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information, see the Google Cloud documentation: https://cloud.google.com/storage/docs/encryption/customer-managed-keys https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys NONE : No encryption. Optionally specifies the ID for the Cloud KMS-managed key that is used to encrypt files unloaded into the bucket. If\nno value is provided, your default KMS key ID set on the bucket is used to encrypt files on unload. Note that this value is ignored for data loading. The load operation should succeed if the service account has sufficient\npermissions to decrypt data in the bucket. Default: NONE Microsoft Azure ENCRYPTION = ( TYPE = 'AZURE_CSE' MASTER_KEY = ' string ' | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: AZURE_CSE : Client-side encryption (requires a MASTER_KEY value). For information, see the Client-side encryption information in the Microsoft Azure documentation. NONE : No encryption. Specifies the client-side master key used to encrypt or decrypt files. The master key must be a 128-bit or 256-bit key in\nBase64-encoded form. Default: NONE"
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use private connectivity for an external stage to harden your\nsecurity posture. If the external stage uses a storage integration, and that integration is configured for private connectivity, set this parameter to\nFALSE. For information about using this parameter, see one of the following: AWS private connectivity to external stages . Azure private connectivity for external stages and Snowpipe automation ."
        },
        {
            "name": "Data loading :",
            "description": "Modifies the encryption settings used to decrypt encrypted files in the storage location and extract data."
        },
        {
            "name": "Data unloading :",
            "description": "Modifies the encryption settings used to encrypt files unloaded to the storage location."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: AWS_CSE : Client-side encryption (requires a MASTER_KEY value). Currently, the client-side master key you provide can only be a symmetric key. When a MASTER_KEY value is provided, Snowflake assumes TYPE = AWS_CSE (when a MASTER_KEY value is\nprovided, TYPE is not required). AWS_SSE_S3 : Server-side encryption that requires no additional encryption settings. AWS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information about the encryption types, see the AWS documentation for client-side encryption or server-side encryption . NONE : No encryption."
        },
        {
            "name": "MASTER_KEY   =   ' string '  (applies to  AWS_CSE  encryption only)",
            "description": "Specifies the client-side master key used to encrypt the files in the bucket. The master key must be a 128-bit or 256-bit key\nin Base64-encoded form."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  AWS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the AWS KMS-managed key used to encrypt files unloaded into the bucket. If no value\nis provided, your default KMS key ID is used to encrypt files on unload. Note that this value is ignored for data loading."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information, see the Google Cloud documentation: https://cloud.google.com/storage/docs/encryption/customer-managed-keys https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys NONE : No encryption."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  GCS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the Cloud KMS-managed key that is used to encrypt files unloaded into the bucket. If\nno value is provided, your default KMS key ID set on the bucket is used to encrypt files on unload. Note that this value is ignored for data loading. The load operation should succeed if the service account has sufficient\npermissions to decrypt data in the bucket."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: AZURE_CSE : Client-side encryption (requires a MASTER_KEY value). For information, see the Client-side encryption information in the Microsoft Azure documentation. NONE : No encryption."
        },
        {
            "name": "MASTER_KEY   =   ' string '  (applies to AZURE_CSE encryption only)",
            "description": "Specifies the client-side master key used to encrypt or decrypt files. The master key must be a 128-bit or 256-bit key in\nBase64-encoded form."
        },
        {
            "name": "URL   =   's3compat:// bucket [/ path /]'",
            "description": "Specifies the URL for the external location (existing bucket accessed using an S3-compatible API endpoint) used to store data files, where: bucket is the name of the bucket. path is an optional case-sensitive path (or prefix in S3 terminology) for files in the cloud storage location (i.e. files with names that begin with a common string)."
        },
        {
            "name": "ENDPOINT   =   ' s3_api_compatible_endpoint '",
            "description": "Fully-qualified domain that points to the S3-compatible API endpoint."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to enable a directory table on the internal named stage. Default: FALSE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should automatically refresh the directory table metadata when new or updated data files are available on the internal named stage . Snowflake automatically refreshes the directory table metadata. Snowflake does not automatically refresh the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path. Default: FALSE"
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to add a directory table to the stage. When the value is TRUE, a directory table is created with the stage. Default: FALSE"
        },
        {
            "name": "REFRESH_ON_CREATE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically refresh the directory table metadata once, immediately after the stage is\ncreated. Refreshing the directory table metadata synchronizes the metadata with the current list of data files\nin the specified stage path. This action is required for the metadata to register any existing data\nfiles in the named stage specified in the URL = setting. Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH. Default: TRUE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the directory table metadata when new or updated data files are available in the named external stage specified in the URL value. Snowflake enables triggering automatic refreshes of the directory table metadata. Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path. Default: FALSE"
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH."
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to add a directory table to the stage. When the value is TRUE, a directory table is created with the stage. Default: FALSE"
        },
        {
            "name": "REFRESH_ON_CREATE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically refresh the directory table metadata once, immediately after the stage is\ncreated. Refreshing the directory table metadata synchronizes the metadata with the current list of data files\nin the specified stage path. This action is required for the metadata to register any existing data\nfiles in the named stage specified in the URL = setting. Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH. Default: TRUE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the directory table metadata when new or updated data files are available in the named external stage specified in the [ WITH ] LOCATION = setting. Snowflake enables triggering automatic refreshes of the directory table metadata. Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "NOTIFICATION_INTEGRATION   =   ' notification_integration_name '",
            "description": "Specifies the name of the notification integration used to automatically refresh the directory table metadata using GCS Pub/Sub\nnotifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party cloud\nmessage queuing services."
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH."
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        },
        {
            "name": "ENABLE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to add a directory table to the stage. When the value is TRUE, a directory table is created with the stage. Default: FALSE"
        },
        {
            "name": "REFRESH_ON_CREATE   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically refresh the directory table metadata once, immediately after the stage is\ncreated. Refreshing the directory table metadata synchronizes the metadata with the current list of data files\nin the specified stage path. This action is required for the metadata to register any existing data\nfiles in the named stage specified in the URL = setting. Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH. Default: TRUE"
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the directory table metadata when new or updated data files are available in the named external stage specified in the [ WITH ] LOCATION = setting. Snowflake enables triggering automatic refreshes of the directory table metadata. Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path. Default: FALSE"
        },
        {
            "name": "NOTIFICATION_INTEGRATION   =   ' notification_integration_name '",
            "description": "Specifies the name of the notification integration used to automatically refresh the directory table metadata using Azure Event Grid\nnotifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party cloud\nmessage queuing services."
        },
        {
            "name": "TRUE",
            "description": "Snowflake automatically refreshes the directory table metadata once after the stage creation. Note If the specified cloud storage URL contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE . After creating the stage, refresh the directory table metadata\nincrementally by executing ALTER STAGE … REFRESH statements that specify subpaths in\nthe storage location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not automatically refresh the directory table metadata. To register any data files that\nexist in the stage, you must manually refresh the directory table metadata once using ALTER STAGE … REFRESH."
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the directory table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the directory table metadata. You must manually refresh the directory\ntable metadata periodically using ALTER STAGE … REFRESH to synchronize the metadata with the current\nlist of files in the stage path."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-table",
    "title": "CREATE TABLE",
    "description": "Creates a new table in the current/specified schema, replaces an existing table, or alters an existing table. A table can have multiple\ncolumns, with each column definition consisting of a name, data type, and optionally whether the column:",
    "syntax": "CREATE [ OR REPLACE ]\n    [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE | TRANSIENT } ]\n  TABLE [ IF NOT EXISTS ] <table_name>\n\n  (\n    -- Column definition\n    <col_name> <col_type>\n      [ inlineConstraint ]\n      [ NOT NULL ]\n      [ COLLATE '<collation_specification>' ]\n      [\n        {\n          DEFAULT <expr>\n          | { AUTOINCREMENT | IDENTITY }\n            [\n              {\n                ( <start_num> , <step_num> )\n                | START <num> INCREMENT <num>\n              }\n            ]\n            [ { ORDER | NOORDER } ]\n        }\n      ]\n      [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n      [ [ WITH ] PROJECTION POLICY <policy_name> ]\n      [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n    -- Out-of-line constraints\n    [ , outoflineConstraint [ ... ] ]\n  )\n\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE } ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] JOIN POLICY <policy_name> [ ALLOWED JOIN KEYS ( <col_name> [ , ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\ninlineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE\n    | PRIMARY KEY\n    | [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ]\n  }\n  [ <constraint_properties> ]\n\noutoflineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n      REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n  }\n  [ <constraint_properties> ]\n  [ COMMENT '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE TABLE mytable (amount NUMBER);\n\n+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table MYTABLE successfully created. |\n+-------------------------------------+\n\nINSERT INTO mytable VALUES(1);\n\nSHOW TABLES like 'mytable';\n\n+---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:32:28 -0700 | MYTABLE | TESTDB        | PUBLIC      | TABLE |         |            |    1 |  1024 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n\nDESC TABLE mytable;\n\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name   | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| AMOUNT | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+"
        },
        {
            "code": "CREATE TABLE example (col1 NUMBER COMMENT 'a column comment') COMMENT='a table comment';\n\n+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table EXAMPLE successfully created. |\n+-------------------------------------+\n\nSHOW TABLES LIKE 'example';\n\n+---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment         | cluster_by | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:35:59 -0700 | EXAMPLE | TESTDB        | PUBLIC      | TABLE | a table comment |            |    0 |     0 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------+\n\nDESC TABLE example;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment          |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------|\n| COL1 | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | a column comment |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------+"
        },
        {
            "code": "CREATE TABLE mytable_copy (b) AS SELECT * FROM mytable;\n\nDESC TABLE mytable_copy;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| B    | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nCREATE TABLE mytable_copy2 AS SELECT b+1 AS c FROM mytable_copy;\n\nDESC TABLE mytable_copy2;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| C    | NUMBER(39,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nSELECT * FROM mytable_copy2;\n\n+---+\n| C |\n|---|\n| 2 |\n+---+"
        },
        {
            "code": "CREATE TABLE testtable_summary (name, summary_amount) AS SELECT name, amount1 + amount2 FROM testtable;"
        },
        {
            "code": "CREATE OR REPLACE TABLE parquet_col (\n  custKey NUMBER DEFAULT NULL,\n  orderDate DATE DEFAULT NULL,\n  orderStatus VARCHAR(100) DEFAULT NULL,\n  price VARCHAR(255)\n)\nAS SELECT\n  $1:o_custkey::number,\n  $1:o_orderdate::date,\n  $1:o_orderstatus::text,\n  $1:o_totalprice::text\nFROM @my_stage;\n\n+-----------------------------------------+\n| status                                  |\n|-----------------------------------------|\n| Table PARQUET_COL successfully created. |\n+-----------------------------------------+\n\nDESC TABLE parquet_col;\n\n+-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name        | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| CUSTKEY     | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| ORDERDATE   | DATE         | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| ORDERSTATUS | VARCHAR(100) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| PRICE       | VARCHAR(255) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+"
        },
        {
            "code": "CREATE TABLE mytable (amount NUMBER);\n\nINSERT INTO mytable VALUES(1);\n\nSELECT * FROM mytable;\n\n+--------+\n| AMOUNT |\n|--------|\n|      1 |\n+--------+\n\nCREATE TABLE mytable_2 LIKE mytable;\n\nDESC TABLE mytable_2;\n\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name   | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| AMOUNT | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nSELECT * FROM mytable_2;\n\n+--------+\n| AMOUNT |\n|--------|\n+--------+"
        },
        {
            "code": "CREATE TABLE mytable (date TIMESTAMP_NTZ, id NUMBER, content VARIANT) CLUSTER BY (date, id);\n\nSHOW TABLES LIKE 'mytable';\n\n+---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment | cluster_by       | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:20:41 -0700 | MYTABLE | TESTDB        | PUBLIC      | TABLE |         | LINEAR(DATE, ID) |    0 |     0 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE collation_demo (\n  uncollated_phrase VARCHAR, \n  utf8_phrase VARCHAR COLLATE 'utf8',\n  english_phrase VARCHAR COLLATE 'en',\n  spanish_phrase VARCHAR COLLATE 'es');\n\nINSERT INTO collation_demo (\n      uncollated_phrase, \n      utf8_phrase, \n      english_phrase, \n      spanish_phrase) \n   VALUES (\n     'pinata', \n     'pinata', \n     'pinata', \n     'piñata');"
        },
        {
            "code": "CREATE TABLE mytable\n  USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    WITHIN GROUP (ORDER BY order_id)\n      FROM TABLE(\n        INFER_SCHEMA(\n          LOCATION=>'@mystage',\n          FILE_FORMAT=>'my_parquet_format'\n        )\n      ));"
        },
        {
            "code": "CREATE TEMPORARY TABLE demo_temporary (i INTEGER);\nCREATE TEMP TABLE demo_temp (i INTEGER);"
        },
        {
            "code": "CREATE LOCAL TEMPORARY TABLE demo_local_temporary (i INTEGER);\nCREATE LOCAL TEMP TABLE demo_local_temp (i INTEGER);\n\nCREATE GLOBAL TEMPORARY TABLE demo_global_temporary (i INTEGER);\nCREATE GLOBAL TEMP TABLE demo_global_temp (i INTEGER);\n\nCREATE VOLATILE TABLE demo_volatile (i INTEGER);"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(a INT);"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    b VARCHAR(200)\n  )\n  DATA_RETENTION_TIME_IN_DAYS = 5\n  DEFAULT_DDL_COLLATION = 'de';"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    c VARCHAR(200)\n  )\n  DEFAULT_DDL_COLLATION = 'de';"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    b INT\n  );"
        },
        {
            "code": "INSERT INTO my_table VALUES (1, 2), (2, 3);\n\nSELECT * FROM my_table;"
        },
        {
            "code": "+---+---+\n| A | B |\n|---+---|\n| 1 | 2 |\n| 2 | 3 |\n+---+---+"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    c INT\n  );"
        },
        {
            "code": "SELECT * FROM my_table;"
        },
        {
            "code": "+---+------+\n| A | C    |\n|---+------|\n| 1 | NULL |\n| 2 | NULL |\n+---+------+"
        },
        {
            "code": "CREATE TABLE t(a INT);"
        },
        {
            "code": "CREATE OR ALTER TABLE t(a INT PRIMARY KEY);"
        },
        {
            "code": "DESC TABLE t;"
        },
        {
            "code": "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name | privacy domain |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------|\n| A    | NUMBER(38,0) | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | NULL    | NULL        | NULL           |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE t(a INT);"
        },
        {
            "code": "INSERT INTO t VALUES (null);"
        },
        {
            "code": "CREATE OR ALTER TABLE t(a INT PRIMARY KEY);"
        },
        {
            "code": "001471 (42601): SQL compilation error:\nColumn 'A' contains null values. Not null constraint cannot be added."
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the table; must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "col_name",
            "description": "Specifies the column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. For more details, see Identifier requirements and Reserved & limited keywords . Note In addition to the standard reserved keywords, the following keywords cannot be used as column identifiers because they are\nreserved for ANSI-standard context functions: CURRENT_DATE CURRENT_ROLE CURRENT_TIME CURRENT_TIMESTAMP CURRENT_USER For the list of reserved keywords, see Reserved & limited keywords ."
        },
        {
            "name": "col_type",
            "description": "Specifies the data type for the column. For details about the data types that can be specified for table columns, see SQL data types reference ."
        },
        {
            "name": "query",
            "description": "Required for CTAS and USING TEMPLATE. For CTAS, specifies the SELECT statement that populates the table. This query must be\nspecified last in the CTAS statement, regardless of the other parameters that you include. For CREATE TABLE … USING TEMPLATE, specifies the subquery that calls the INFER_SCHEMA function and\nformats the output as an array. Alternatively, USING TEMPLATE accepts the INFER_SCHEMA output as a string\nliteral or variable."
        },
        {
            "name": "source_table",
            "description": "Required for LIKE and CLONE. For CREATE TABLE … LIKE, specifies the table from which properties and column definitions are copied. For CREATE TABLE … CLONE, specifies the table to use as the source for the clone."
        },
        {
            "name": "{   [   {   LOCAL   |   GLOBAL   }   ]   TEMP   [   READ   ONLY]   |   .   TEMPORARY   [   READ   ONLY]   |   .   VOLATILE   |   .   TRANSIENT   }",
            "description": "Specifies that the table persists only for the duration of the session that you created it in. A\ntemporary table and all its contents are dropped at the end of the session. The synonyms and abbreviations for TEMPORARY (e.g. GLOBAL TEMPORARY ) are provided for compatibility with other databases\n(e.g. to prevent errors when migrating CREATE TABLE statements). Tables created with any of these keywords appear and behave identically\nto a table created with the TEMPORARY keyword. Default: No value. If a table is not declared as TEMPORARY or TRANSIENT , the table is permanent. If you want to avoid unexpected conflicts, avoid naming temporary tables after tables that already exist in the schema. If you created a temporary table with the same name as another table in the schema, all queries and operations used on the table only\naffect the temporary table in the session, until you drop the temporary table. If you drop the table, you drop the temporary table, and\nnot the table that already exists in the schema. For information about temporary or transient tables, and how they can affect storage and cost, refer to the following resources: Working with Temporary and Transient Tables Storage costs for Time Travel and Fail-safe Specifies that the table is read-only. READ ONLY is valid only for a temporary table that is being created with the CREATE TABLE … CLONE variant of the CREATE TABLE command. A read-only table does not allow DML operations and only allows the following subset of DDL operations: ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } COMMENT ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } MASKING POLICY ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } TAG ALTER TABLE … RENAME COLUMN … TO ALTER TABLE … RENAME TO ALTER TABLE … { SET | UNSET } COMMENT ALTER TABLE … { SET | UNSET } TAG COMMENT DESCRIBE DROP SHOW UNDROP Read-only tables have a METADATA$ROW_POSITION column. This metadata column assigns a row number to each row in\nthe table that is continuous and starts from 0. The row number assigned to each row remains unchanged until the\nread-only table is dropped."
        },
        {
            "name": "TRANSIENT",
            "description": "Specifies that the table is transient. Like a permanent table, a transient table exists until explicitly dropped and is visible to any\nuser with the appropriate privileges. However, transient tables have a lower level of data protection than permanent tables, meaning\nthat data in a transient table might be lost in the event of a system failure. As such, transient tables should only be used for data\nthat can be recreated externally to Snowflake. Default: No value. If a table is not declared as TRANSIENT or TEMPORARY , the table is permanent. Note Transient tables have some storage considerations. For more information about these and other considerations when deciding whether to create temporary or transient tables, see Working with Temporary and Transient Tables and Storage costs for Time Travel and Fail-safe ."
        },
        {
            "name": "CONSTRAINT   ...",
            "description": "Defines an inline or out-of-line constraint for the specified column(s) in the table. For syntax details, see CREATE | ALTER TABLE … CONSTRAINT . For more information about constraints, see Constraints ."
        },
        {
            "name": "COLLATE   ' collation_specification '",
            "description": "Specifies the collation to use for column operations such as string comparison. This option applies only to text columns\n(VARCHAR, STRING, TEXT, etc.). For more details, see Collation specifications ."
        },
        {
            "name": "DEFAULT   ...   or   .   AUTOINCREMENT   ...",
            "description": "Specifies whether a default value is automatically inserted in the column if a value is not explicitly specified via an INSERT\nor CREATE TABLE AS SELECT statement: Column default value is defined by the specified expression which can be any of the following: Constant value. Sequence reference ( seq_name .NEXTVAL ). Simple expression that returns a scalar value. The simple expression can include a SQL UDF (user-defined function) if the UDF is not a secure UDF . Note If a default expression refers to a SQL UDF, then the function is replaced by its\ndefinition at table creation time. If the user-defined function is redefined in the future, this does not\nupdate the column’s default expression. The simple expression cannot contain references to: Subqueries. Aggregates. Window functions. Secure UDFs. UDFs written in languages other than SQL (e.g. Java, JavaScript). External functions. When you specify AUTOINCREMENT or IDENTITY, the default value for the column starts with a specified number and each\nsuccessive value automatically increments by the specified amount. AUTOINCREMENT and IDENTITY are synonymous and can be used only for columns with numeric data types, such as NUMBER, INT,\nFLOAT. Caution Snowflake uses a sequence to generate the values for an auto-incremented column. Sequences have limitations;\nsee Sequence Semantics . The default value for both the start value and the step/increment value is 1 . Note Manually inserting values into an AUTOINCREMENT or IDENTITY column can result in duplicate values. If you manually insert the\nvalue 5 into an AUTOINCREMENT or IDENTITY column, a subsequently inserted row might use the same value 5 as the\ndefault value for the column. Use ORDER or NOORDER to specify whether or not the values are generated for the auto-incremented column in increasing or decreasing order . ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). If you do not specify ORDER or NOORDER, the NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is\nset. Note DEFAULT and AUTOINCREMENT are mutually exclusive; only one can be specified for a column."
        },
        {
            "name": "MASKING   POLICY   =   policy_name",
            "description": "Specifies the masking policy to set on a column. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "PROJECTION   POLICY   policy_name",
            "description": "Specifies the projection policy to set on a column. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "COMMENT   ' string_literal '",
            "description": "Specifies a comment for the column. (Note that comments can be specified at the column level or the table level. The syntax for each is slightly different.)"
        },
        {
            "name": "USING   (   col_name   ,   cond_col_1   ...   )",
            "description": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal masking policy ."
        },
        {
            "name": "CLUSTER   BY   (   expr   [   ,   expr   ,   ...   ]   )",
            "description": "Specifies one or more columns or column expressions in the table as the clustering key. For more details, see Clustering Keys & Clustered Tables . Default: No value (no clustering key is defined for the table) Important Clustering keys are not intended or recommended for all tables; they typically benefit very large (i.e. multi-terabyte)\ntables. Before you specify a clustering key for a table, you should understand micro-partitions. For more information, see Understanding Snowflake Table Structures ."
        },
        {
            "name": "ENABLE_SCHEMA_EVOLUTION   =   {   TRUE   |   FALSE   }",
            "description": "Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support . The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER , ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false."
        },
        {
            "name": "READ   ONLY",
            "description": "Specifies that the table is read-only. READ ONLY is valid only for a temporary table that is being created with the CREATE TABLE … CLONE variant of the CREATE TABLE command. A read-only table does not allow DML operations and only allows the following subset of DDL operations: ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } COMMENT ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } MASKING POLICY ALTER TABLE … { ALTER | MODIFY } COLUMN … { SET | UNSET } TAG ALTER TABLE … RENAME COLUMN … TO ALTER TABLE … RENAME TO ALTER TABLE … { SET | UNSET } COMMENT ALTER TABLE … { SET | UNSET } TAG COMMENT DESCRIBE DROP SHOW UNDROP Read-only tables have a METADATA$ROW_POSITION column. This metadata column assigns a row number to each row in\nthe table that is continuous and starts from 0. The row number assigned to each row remains unchanged until the\nread-only table is dropped."
        },
        {
            "name": "DEFAULT   expr",
            "description": "Column default value is defined by the specified expression which can be any of the following: Constant value. Sequence reference ( seq_name .NEXTVAL ). Simple expression that returns a scalar value. The simple expression can include a SQL UDF (user-defined function) if the UDF is not a secure UDF . Note If a default expression refers to a SQL UDF, then the function is replaced by its\ndefinition at table creation time. If the user-defined function is redefined in the future, this does not\nupdate the column’s default expression. The simple expression cannot contain references to: Subqueries. Aggregates. Window functions. Secure UDFs. UDFs written in languages other than SQL (e.g. Java, JavaScript). External functions."
        },
        {
            "name": "{   AUTOINCREMENT   |   IDENTITY   }   .   [   {   (   start_num   ,   step_num   )   |   START   num   INCREMENT   num   }   ]   .   [   {   ORDER   |   NOORDER   }   ]",
            "description": "When you specify AUTOINCREMENT or IDENTITY, the default value for the column starts with a specified number and each\nsuccessive value automatically increments by the specified amount. AUTOINCREMENT and IDENTITY are synonymous and can be used only for columns with numeric data types, such as NUMBER, INT,\nFLOAT. Caution Snowflake uses a sequence to generate the values for an auto-incremented column. Sequences have limitations;\nsee Sequence Semantics . The default value for both the start value and the step/increment value is 1 . Note Manually inserting values into an AUTOINCREMENT or IDENTITY column can result in duplicate values. If you manually insert the\nvalue 5 into an AUTOINCREMENT or IDENTITY column, a subsequently inserted row might use the same value 5 as the\ndefault value for the column. Use ORDER or NOORDER to specify whether or not the values are generated for the auto-incremented column in increasing or decreasing order . ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). If you do not specify ORDER or NOORDER, the NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is\nset."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the retention period for the table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical\ndata in the table. For more details, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "CHANGE_TRACKING   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to enable change tracking on the table. TRUE enables change tracking on the table. This setting adds a pair of hidden columns to the source table and begins\nstoring change-tracking metadata in the columns. These columns consume a small amount of storage. The change-tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the table. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for the columns in the table, including columns\nadded to the table in the future. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original table when a new table is created using any of the following\nCREATE TABLE variants: CREATE OR REPLACE TABLE CREATE TABLE … LIKE CREATE TABLE … CLONE The parameter copies all privileges, except OWNERSHIP, from the existing table to the new table. The new table does not inherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE TABLE statement\nowns the new table. If the parameter is not included in the CREATE TABLE statement, then the new table does not inherit any explicit access\nprivileges granted on the original table, but does inherit any future grants defined for the object type in the schema. Note: With data sharing : If the existing table was shared to another account, the replacement table is also shared. If the existing table was shared with your account as a data consumer, and access was further granted to other roles in\nthe account (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement\ntable. The SHOW GRANTS output for the replacement table lists the grantee for the copied privileges as the\nrole that executed the CREATE TABLE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE TABLE command (i.e. within the same transaction). This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the table. Default: No value (Note that comments can be specified at the column level, constraint level, or table level. The syntax for each is slightly different.)"
        },
        {
            "name": "ROW   ACCESS   POLICY   policy_name   ON   (   col_name   [   ,   col_name   ...   ]   )",
            "description": "Specifies the row access policy to set on a table. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "AGGREGATION   POLICY   policy_name   [   ENTITY   KEY   (   col_name   [   ,   col_name   ...   ]   )   ]",
            "description": "Specifies an aggregation policy to set on a table. You can apply one or more aggregation\npolicies on a table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see Implementing entity-level privacy with aggregation policies . You can specify one or more entity keys for an aggregation policy. This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "JOIN   POLICY   policy_name   [   ALLOWED   JOIN   KEYS   (   col_name   [   ,   ...   ]   )   ]",
            "description": "Specifies the join policy to set on a table. Use the optional ALLOWED JOIN KEYS parameter to define which columns are allowed to be used as joining columns when\nthis policy is in effect. For more information, see Join policies . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ],
    "usage_notes": "A schema cannot contain tables and/or views with the same name. When creating a table:\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then creating a new table\nwith the same name; however, the dropped table is not permanently removed from the system. Instead, it is retained in\nTime Travel. This is important to note because dropped tables in Time Travel can be recovered, but they also contribute to data\nstorage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data. Any stream on the table becomes stale. In\naddition, any stream on a view that has this table as an underlying table, becomes stale. A stale stream is unreadable.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nSimilar to reserved keywords, ANSI-reserved function names\n(CURRENT_DATE, CURRENT_TIMESTAMP, etc.) cannot be used as column\nnames.\nCREATE OR ALTER TABLE:\nFor more information, see CREATE OR ALTER TABLE usage notes.\nCREATE TABLE … CLONE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is suspended\nfor the new table – even if Automatic Clustering was not suspended for the source table.\nCREATE TABLE … CHANGE_TRACKING = TRUE:\nWhen change tracking is enabled, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nCREATE TABLE … LIKE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is not\nsuspended for the new table – even if Automatic Clustering was suspended for the source table.\nCREATE TABLE … AS SELECT (CTAS):\nIf the aliases for the column names in the SELECT list are valid columns, then the column definitions\nare not required in the CTAS statement; if omitted, the column names and types are inferred from the underlying query:\nAlternatively, the names can be explicitly specified using the following syntax:\nThe number of column names specified must match the number of SELECT list items in the query; the\ntypes of the columns are inferred from the types produced by the query.\nWhen clustering keys are specified in a CTAS statement:\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table – even if Automatic Clustering is suspended for the\nsource table.\nIf you want the table to be created with rows in a specific order, then use an ORDER BY sub-clause in the SELECT clause of the\nCTAS. Specifying CLUSTER BY does not cluster the data at the time that the table is created; instead, CLUSTER BY relies on\nautomatic clustering to recluster the data over time.\nThe ORDER BY sub-clause in a CREATE TABLE statement does not affect the order of the rows returned by future SELECT statements\non that table. To specify the order of rows in future SELECT statements, use an ORDER BY sub-clause in those statements.\nInside a transaction, any DDL statement (including CREATE TEMPORARY/TRANSIENT TABLE) commits\nthe transaction before executing the DDL statement itself. The DDL statement then runs in its own transaction. The\nnext statement after the DDL statement starts a new transaction. Therefore, you can’t create, use, and drop a\ntemporary or transient table within a single transaction. If you want to use a temporary or transient table inside a\ntransaction, then create the table before the transaction, and drop the table after the transaction.\nRecreating a table (using the optional OR REPLACE keyword) drops its history, which makes any stream on the table stale.\nA stale stream is unreadable.\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen creating a table with a masking policy on one or more table columns, or a row access policy added to the table, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the table\nprotected by a row access policy.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-resource-monitor",
    "title": "CREATE RESOURCE MONITOR",
    "description": "Creates a new resource monitor. This command can only be executed by account administrators.",
    "syntax": "CREATE [ OR REPLACE ] RESOURCE MONITOR [ IF NOT EXISTS ] <name> WITH\n                      [ CREDIT_QUOTA = <number> ]\n                      [ FREQUENCY = { MONTHLY | DAILY | WEEKLY | YEARLY | NEVER } ]\n                      [ START_TIMESTAMP = { <timestamp> | IMMEDIATELY } ]\n                      [ END_TIMESTAMP = <timestamp> ]\n                      [ NOTIFY_USERS = ( <user_name> [ , <user_name> , ... ] ) ]\n                      [ TRIGGERS triggerDefinition [ triggerDefinition ... ] ]\n\ntriggerDefinition ::=\n    ON <threshold> PERCENT DO { SUSPEND | SUSPEND_IMMEDIATE | NOTIFY }",
    "examples": [
        {
            "code": "CREATE OR REPLACE RESOURCE MONITOR limiter\n  WITH CREDIT_QUOTA = 5000\n  TRIGGERS ON 75 PERCENT DO NOTIFY\n           ON 100 PERCENT DO SUSPEND\n           ON 110 PERCENT DO SUSPEND_IMMEDIATE;"
        },
        {
            "code": "CREATE OR REPLACE RESOURCE MONITOR limiter\n  WITH CREDIT_QUOTA = 5000\n       NOTIFY_USERS = (JDOE, \"Jane Smith\", \"John Doe\")\n  TRIGGERS ON 75 PERCENT DO NOTIFY\n           ON 100 PERCENT DO SUSPEND\n           ON 110 PERCENT DO SUSPEND_IMMEDIATE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the resource monitor; must be unique for your account. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier string\nis enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "CREDIT_QUOTA   =   num",
            "description": "The number of credits allocated to the resource monitor per frequency interval. When total usage for all warehouses assigned to the\nmonitor reaches this number for the current frequency interval, the resource monitor is considered to be at 100% of quota. If a value is not specified for a resource monitor, the monitor has no quota and will never reach 100% usage within the specified interval. Default: No value (i.e. no credit quota)"
        },
        {
            "name": "FREQUENCY   =   MONTHLY   |   DAILY   |   WEEKLY   |   YEARLY   |   NEVER",
            "description": "The frequency interval at which the credit usage resets to 0 . If you set a frequency for a resource monitor, you must also set START_TIMESTAMP . If you specify NEVER for the frequency, the credit usage for the warehouse does not reset. Default: No value (i.e. legacy behavior, whereby the credit quota resets at the beginning of each calendar month)"
        },
        {
            "name": "START_TIMESTAMP   =   timestamp   |   IMMEDIATELY",
            "description": "The date and time when the resource monitor starts monitoring credit usage for the assigned warehouses. If you set a timestamp for a resource monitor, you must also set FREQUENCY . If you specify IMMEDIATELY for the start timestamp, the current timestamp is used. If you specify a date without a time, the current time is used. If you set a time without specifying a time zone, UTC is used as the default time zone. Default: No value (i.e. legacy behavior, whereby the resource monitor starts monitoring warehouses immediately)"
        },
        {
            "name": "END_TIMESTAMP   =   timestamp",
            "description": "The date and time when the resource monitor suspends the assigned warehouses. Default: No value (i.e. no warehouse suspension date)"
        },
        {
            "name": "NOTIFY_USERS   =   (   user_name   [   ,   user_name   ,   ...   ]   )",
            "description": "Specifies the list of users to receive email notifications on resource monitors. If a user identifier includes spaces or special\ncharacters or is case-sensitive, then the identifier must be enclosed in double quotes (e.g. “Mary Smith”). See Identifier requirements for details. The user identifier, user_name , is the value of the name column from the output of SHOW USERS . Each user listed must have a verified email address. For instructions on verifying email addresses in the web interface, see: For Classic Console: Verifying Your Email Address in the Classic Console . For Snowsight: Verify your email address . Email notifications for non-administrator users do not supersede email notifications for administrators. Any account administrators that\nhave enabled email notifications will continue to receive email notifications. Note The following limitations apply for non-administrator users: Non-administrator users can only receive notifications for warehouse monitors . Non-administrator users are notified by email but can’t see notifications in the Classic Console. Non-administrator users can’t create resource monitors. Non-administrator users can’t assign other users to be notified."
        },
        {
            "name": "TRIGGERS   ...  (aka  actions )",
            "description": "Specifies one or more triggers for the resource monitor. Each trigger definition consists of the following: A numeric value specified as a percentage of the credit quota for the resource monitor; values larger than 100 are supported.\nOnce usage reaches this threshold for the current frequency interval, the trigger fires. Specifies the action performed by the trigger when the threshold is reached: SUSPEND : Suspend all assigned warehouses while allowing currently running queries to complete. No new queries can be executed\nby the warehouses until the credit quota for the resource monitor is increased. In addition, this action sends a notification to all\nusers who have enabled notifications for themselves. SUSPEND_IMMEDIATE :  Suspend all assigned warehouses immediately and cancel any currently running queries or statements using\nthe warehouses. In addition, this action sends a notification to all users who have enabled notifications for themselves. NOTIFY : Send a notification (to all account administrators with notifications enabled), but do not take any other action. Default: No value (i.e. resource monitor performs no actions)"
        },
        {
            "name": "ON   threshold   PERCENT",
            "description": "A numeric value specified as a percentage of the credit quota for the resource monitor; values larger than 100 are supported.\nOnce usage reaches this threshold for the current frequency interval, the trigger fires."
        },
        {
            "name": "DO   SUSPEND   |   SUSPEND_IMMEDIATE   |   NOTIFY",
            "description": "Specifies the action performed by the trigger when the threshold is reached: SUSPEND : Suspend all assigned warehouses while allowing currently running queries to complete. No new queries can be executed\nby the warehouses until the credit quota for the resource monitor is increased. In addition, this action sends a notification to all\nusers who have enabled notifications for themselves. SUSPEND_IMMEDIATE :  Suspend all assigned warehouses immediately and cancel any currently running queries or statements using\nthe warehouses. In addition, this action sends a notification to all users who have enabled notifications for themselves. NOTIFY : Send a notification (to all account administrators with notifications enabled), but do not take any other action."
        }
    ],
    "usage_notes": "Triggers are optional; however, at least one trigger must be added to a resource monitor before it can perform any actions.\nEach resource monitor supports up to a maximum of 5 NOTIFY action triggers.\nAfter a resource monitor is created, it must be assigned to a warehouse or account before it can perform any monitoring actions:\nTo assign a warehouse to a resource monitor, use ALTER WAREHOUSE (or CREATE WAREHOUSE if you are creating the warehouse).\nTo assign a resource monitor at the account level, use ALTER ACCOUNT. The NOTIFY_USERS parameter must be null.\nTo view all resource monitors created in your account and their assignment, use the SHOW RESOURCE MONITORS command. The command\noutput displays NULL in the level column for resource monitors that are not assigned to the account or any warehouses\nand, therefore, are not monitoring any credit usage.\nIf frequency and start_timestamp parameters are set on a resource monitor, the day for the credit usage reset is\ncalculated based on those parameters. The time the credit usage resets to 0 is 12:00 AM UTC regardless of the time specified in\nstart_timestamp.\nIf you specify an end_timestamp, monitoring ends at that specified date and time and all assigned warehouses are suspended\nat that date and time even if the credit quota has not been reached.\nWhen this occurs, a notification is sent that states the resource monitor has reached a percentage of its quota and has triggered a\nsuspend immediate action. The percentage of the quota reflects the number of credits used in the current interval up to the end date\nand might not be a threshold you specified.\nIf there are non-administrator users in the notification list, the following notes apply:\nIf any user in the notification list does not have a verified email,\nthe SQL statement fails.\nIf any user in the notification list changes their email address and does not verify the new email address, the\nnotification silently fails.\nThe notification list is limited to a maximum number of 5 non-administrator users.\nAccount administrators can view the notification list of non-administrator users in the output of\nSHOW RESOURCE MONITORS in the notify_user column.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nImportant\nTo receive notifications generated by resource monitors, account administrators and non-administrator users in the notification\nlist must explicitly enable notifications in their preferences. In addition, to receive email notifications, users must have a\nverified email in their preferences. Preferences can only\nbe set in the Snowflake web interface. For more information, see Enabling receipt of notifications."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-organization-profile",
    "title": "CREATE ORGANIZATION PROFILE",
    "description": "Create the organization profile that forms part of the Uniform Listing Locator (ULL)\nused to publish organizational listings or query organizational listing information\nwithout mounting the listing. To create an organization profile, you modify the\nlisting manifest and then move it to a stage where you can publish or unpublish it.",
    "syntax": "CREATE ORGANIZATION PROFILE [ IF NOT EXISTS ] <name>\n\nCREATE ORGANIZATION PROFILE [ IF NOT EXISTS ] <name>\n  AS '<yaml_manifest_string>'\n  [ VERSION <version_alias_name> ]\n  [ PUBLISH = { TRUE | FALSE } ]\n\nCREATE ORGANIZATION PROFILE [ IF NOT EXISTS ] <name>\n  FROM @<yaml_manifest_stage_location>\n  [ VERSION <version_alias_name> ]\n  [ PUBLISH = { TRUE | FALSE } ]",
    "examples": [
        {
            "code": "CREATE DATABASE OrgProfileDB;\nCREATE STAGE my_test_stage_org_profile;\nCOPY INTO @my_test_stage_org_profile/manifest.yml\n  FROM (\n    SELECT $$\n      title: \"MY_ORG_PROFILE\"\n      description: \"Profile for SE Business Unit\"\n      contact: \"contact_name@myemail.com\"\n      approver_contact: \"approver_name@email.com\"\n      allowed_publishers:\n        access:\n          - all_internal_accounts: \"true\"\n      logo: \"urn:icon:shieldlock:blue\"\n    $$\n  )\n  SINGLE = TRUE\n  OVERWRITE = TRUE\n  FILE_FORMAT = (\n    COMPRESSION = NONE\n    ESCAPE_UNENCLOSED_FIELD = NONE\n  );"
        },
        {
            "code": "CREATE ORGANIZATION PROFILE MYPROFILENAME\n FROM @my_test_stage_org_profile\n PUBLISH=TRUE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (name) for the organization profile. It must be unique within the current organization. The identifier must conform to Snowflake identifier requirements. See Identifier requirements . Additionally, organization profile names can only contain uppercase characters or numbers, they must start with an uppercase character, and the name length cannot exceed 128 characters."
        },
        {
            "name": "AS   ' yaml_manifest_string '",
            "description": "Specifies the YAML manifest for the organization profile.\nFor organizational listing profile manifest fields,\nsee Organization profile manifest reference . Inline manifests are normally provided as dollar-quoted strings.\nFor more information, see Dollar-quoted string constants ."
        },
        {
            "name": "FROM   @ yaml_manifest_stage_location",
            "description": "Specifies the external stage, internal stage, or Git repository clone YAML format manifest stage location."
        },
        {
            "name": "VERSION   version_alias_name",
            "description": "Optional. Specifies the unique version identifier for the version being added. If VERSION version_name isn’t specified, an alias isn’t created. If the identifier contains spaces, special characters, or mixed-case characters, the entire identifier must be enclosed in double quotes. Identifiers enclosed in double quotes are also case sensitive. The FIRST, LAST, DEFAULT or LIVE keywords are reserved as version shortcuts and can’t be used. The unique version identifier can’t start with “version$” and can’t contain slashes ( / ). For information about identifier syntax, see Identifier requirements ."
        },
        {
            "name": "PUBLISH   =   {   TRUE   |   FALSE   }",
            "description": "Optional. Specifies how the organization profile should be published. If TRUE, the organization profile is published immediately. Default: FALSE."
        }
    ],
    "usage_notes": "Organization profiles created using CREATE ORGANIZATION PROFILE are DRAFT until they are published.\nFor usage examples of organization profile manifests, see Manage organizational listings."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-organization-listing",
    "title": "CREATE ORGANIZATION LISTING",
    "description": "Create an organization listing to share data products securely within your organization.",
    "syntax": "CREATE ORGANIZATION LISTING [ IF NOT EXISTS ] <name>\n  [ { SHARE <share_name>  |  APPLICATION PACKAGE <package_name> } ]\n  AS '<yaml_manifest_string>'\n  [ PUBLISH = { TRUE | FALSE } ]",
    "examples": [
        {
            "code": "USE ROLE <organization_listing_role>;\n\nCREATE ORGANIZATION LISTING MYORGLISTING\nSHARE <share_name> AS\n$$\ntitle: \"My title\"\ndescription: \"One region, all accounts\"\norganization_profile: \"INTERNAL\"\norganization_targets:\ndiscovery:\n   - account: \"<account_name>\"\n     roles:\n     - \"<role>\"\naccess:\n   - account: \"<account_name>\"\n     roles:\n     - \"<role>\"\nsupport_contact: \"support@somedomain.com\"\napprover_contact: \"approver@somedomain.com\"\nlocations:\n   access_regions:\n   - name: \"PUBLIC.<snowflake_region>\"\n$$\n;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (name) for the listing. It must conform to the following: Must be unique within an account, regardless of which Snowflake Region the account is located in. The Uniform Listing Locator (ULL) must be unique within an organization. Cannot contain embedded dollar signs. Must conform to Snowflake identifier requirements. See Identifier requirements ."
        },
        {
            "name": "SHARE   share_name",
            "description": "Specifies the identifier for the share to attach to the listing."
        },
        {
            "name": "APPLICATION   PACKAGE   package_name",
            "description": "Specifies the application package attached to the listing. See also SHOW APPLICATION PACKAGES ."
        },
        {
            "name": "AS   ' yaml_manifest_string '",
            "description": "The YAML manifest for the organization profile. For manifest field details and examples,\nsee Organization listing manifest reference . Manifests are normally provided as dollar-quoted strings. For more information, see Dollar-quoted string constants ."
        },
        {
            "name": "PUBLISH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies how to publish the listing. If TRUE, the listing is published to the Internal Marketplace immediately. Default: TRUE."
        }
    ],
    "usage_notes": "Listings created using CREATE ORGANIZATION LISTING … are automatically published."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-feature-policy",
    "title": "CREATE FEATURE POLICY",
    "description": "Creates a new feature policy.",
    "syntax": "CREATE [ OR REPLACE ] FEATURE POLICY [ IF NOT EXISTS ] <name>\n  BLOCKED_OBJECT_TYPES_FOR_CREATION = ( <type> [ , ... ] )\n  [ COMMENT = '<string-literal>' ]",
    "examples": [
        {
            "code": "CREATE FEATURE POLICY block_create_db_policy\n  BLOCKED_OBJECT_TYPES_FOR_CREATION = (DATABASES);"
        },
        {
            "code": "CREATE FEATURE POLICY block_nothing_policy\n  BLOCKED_OBJECT_TYPES_FOR_CREATION = ();"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the feature policy. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "BLOCKED_OBJECT_TYPES_FOR_CREATION   =   (   type   [   ,   ...   ]   )",
            "description": "Specifies a list of objects that an app can’t create in the consumer account. The\nfollowing objects can be blocked: COMPUTE POOLS WAREHOUSES TASKS DATABASES"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the feature policy. Default: No value"
        }
    ],
    "usage_notes": "If a policy is bound to an object, for example an account or an app, the policy cannot be replaced.\nUse the ALTER FEATURE POLICY to update or rename the feature policy."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-dynamic-table",
    "title": "CREATE DYNAMIC TABLE",
    "description": "Creates a dynamic table, based on a specified query.",
    "syntax": "CREATE [ OR REPLACE ] [ TRANSIENT ] DYNAMIC TABLE [ IF NOT EXISTS ] <name> (\n    -- Column definition\n    <col_name> <col_type>\n      [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n      [ [ WITH ] PROJECTION POLICY <policy_name> ]\n      [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n      [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n  )\n  TARGET_LAG = { '<num> { seconds | minutes | hours | days }' | DOWNSTREAM }\n  WAREHOUSE = <warehouse_name>\n  [ REFRESH_MODE = { AUTO | FULL | INCREMENTAL } ]\n  [ INITIALIZE = { ON_CREATE | ON_SCHEDULE } ]\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ COMMENT = '<string_literal>' ]\n  [ COPY GRANTS ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ REQUIRE USER ]\n  AS <query>",
    "examples": [
        {
            "code": "CREATE OR REPLACE DYNAMIC TABLE my_dynamic_table\n  TARGET_LAG = '20 minutes'\n  WAREHOUSE = mywh\n  AS\n    SELECT product_id, product_name FROM staging_table;"
        },
        {
            "code": "CREATE DYNAMIC ICEBERG TABLE my_dynamic_table (date TIMESTAMP_NTZ, id NUMBER, content STRING)\n  TARGET_LAG = '20 minutes'\n  WAREHOUSE = mywh\n  EXTERNAL_VOLUME = 'my_external_volume'\n  CATALOG = 'SNOWFLAKE'\n  BASE_LOCATION = 'my_iceberg_table'\n  AS\n    SELECT product_id, product_name FROM staging_table;"
        },
        {
            "code": "CREATE DYNAMIC TABLE my_dynamic_table (date TIMESTAMP_NTZ, id NUMBER, content VARIANT)\n  TARGET_LAG = '20 minutes'\n  WAREHOUSE = mywh\n  CLUSTER BY (date, id)\n  AS\n    SELECT product_id, product_name FROM staging_table;"
        },
        {
            "code": "CREATE DYNAMIC TABLE my_cloned_dynamic_table CLONE my_dynamic_table AT (TIMESTAMP => TO_TIMESTAMP_TZ('04/05/2013 01:02:03', 'mm/dd/yyyy hh24:mi:ss'));"
        },
        {
            "code": "CREATE DYNAMIC TABLE my_dynamic_table\n  TARGET_LAG = 'DOWNSTREAM'\n  WAREHOUSE = mywh\n  INITIALIZE = on_schedule\n  REQUIRE USER\n  AS\n    SELECT product_id, product_name FROM staging_table;"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table REFRESH COPY SESSION;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the dynamic table; must be unique for the schema in which the dynamic table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TARGET_LAG   =   {   num   {   seconds   |   minutes   |   hours   |   days   }   |   DOWNSTREAM   }",
            "description": "Specifies the lag for the dynamic table: Specifies the maximum amount of time that the dynamic table’s content should lag behind updates to the source tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes . If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours . Must be a minimum of 60 seconds. If the dynamic table depends on another dynamic table, the minimum target lag must\nbe greater than or equal to the target lag of the dynamic table it depends on. Specifies that the dynamic table should be refreshed only when dynamic tables that depend on it are refreshed."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Specifies the name of the warehouse that provides the compute resources for refreshing the dynamic table. You must use a role that has the USAGE privilege on this warehouse in order to create the dynamic table. For limitations and more\ninformation, see Privileges to create a dynamic table ."
        },
        {
            "name": "AS   query",
            "description": "Specifies the query whose results the dynamic table should contain."
        },
        {
            "name": "' num   seconds   |   minutes   |   hours   |   days'",
            "description": "Specifies the maximum amount of time that the dynamic table’s content should lag behind updates to the source tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes . If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours . Must be a minimum of 60 seconds. If the dynamic table depends on another dynamic table, the minimum target lag must\nbe greater than or equal to the target lag of the dynamic table it depends on."
        },
        {
            "name": "DOWNSTREAM",
            "description": "Specifies that the dynamic table should be refreshed only when dynamic tables that depend on it are refreshed."
        },
        {
            "name": "TRANSIENT",
            "description": "Specifies that the table is transient. Like permanent dynamic tables, transient dynamic tables exist until\nthey’re explicitly dropped, and are available to any user with the appropriate privileges. Transient dynamic\ntables don’t retain data in fail-safe storage, which helps reduce storage costs, especially for tables that\nrefresh frequently. Due to this reduced level of durability, transient dynamic tables are best used for\ntransitory data that doesn’t need the same level of data protection and recovery provided by permanent tables. Default: No value. If a dynamic table is not declared as TRANSIENT , it is permanent."
        },
        {
            "name": "REFRESH_MODE   =   {   AUTO   |   FULL   |   INCREMENTAL   }",
            "description": "Specifies the refresh mode for the dynamic table. This property cannot be altered after you create the dynamic table. To modify the property, recreate the dynamic table with a CREATE OR\nREPLACE DYNAMIC TABLE command. When refresh mode is AUTO , the system attempts to apply an incremental refresh by default. However, when incremental refresh isn’t\nsupported or expected to perform well, the dynamic table automatically selects full refresh instead. For more information, see Dynamic table refresh modes and Best practices for choosing dynamic table refresh modes . To determine the best mode for your use case, experiment with refresh modes and automatic recommendations. For consistent behavior across\nSnowflake releases, explicitly set the refresh mode on all dynamic tables. To verify the refresh mode for your dynamic tables, see View dynamic table refresh mode . Enforces a full refresh of the dynamic table, even if the dynamic table can be incrementally refreshed. Enforces an incremental refresh of the dynamic table. If the query that underlies the dynamic table can’t perform an incremental refresh,\ndynamic table creation fails and displays an error message. Default: AUTO"
        },
        {
            "name": "AUTO",
            "description": "When refresh mode is AUTO , the system attempts to apply an incremental refresh by default. However, when incremental refresh isn’t\nsupported or expected to perform well, the dynamic table automatically selects full refresh instead. For more information, see Dynamic table refresh modes and Best practices for choosing dynamic table refresh modes . To determine the best mode for your use case, experiment with refresh modes and automatic recommendations. For consistent behavior across\nSnowflake releases, explicitly set the refresh mode on all dynamic tables. To verify the refresh mode for your dynamic tables, see View dynamic table refresh mode ."
        },
        {
            "name": "FULL",
            "description": "Enforces a full refresh of the dynamic table, even if the dynamic table can be incrementally refreshed."
        },
        {
            "name": "INCREMENTAL",
            "description": "Enforces an incremental refresh of the dynamic table. If the query that underlies the dynamic table can’t perform an incremental refresh,\ndynamic table creation fails and displays an error message."
        },
        {
            "name": "INITIALIZE",
            "description": "Specifies the behavior of the initial refresh of the dynamic table. This property cannot be\naltered after you create the dynamic table. To modify the property, replace the dynamic table with a CREATE OR REPLACE DYNAMIC TABLE command. Refreshes the dynamic table synchronously at creation. If this refresh fails, dynamic table creation fails and displays an error message. Refreshes the dynamic table at the next scheduled refresh. The dynamic table is populated when the refresh schedule process runs. No data is populated when the dynamic table is created. If you try to\nquery the table using SELECT * FROM DYNAMIC TABLE , you might see the following error because the first scheduled refresh has not yet\noccurred. Default: ON_CREATE"
        },
        {
            "name": "COMMENT   ' string_literal '",
            "description": "Specifies a comment for the column. (Note that comments can be specified at the column level or the table level. The syntax for each is slightly different.)"
        },
        {
            "name": "MASKING   POLICY   =   policy_name",
            "description": "Specifies the masking policy to set on a column."
        },
        {
            "name": "PROJECTION   POLICY   policy_name",
            "description": "Specifies the projection policy to set on a column."
        },
        {
            "name": "ON_CREATE",
            "description": "Refreshes the dynamic table synchronously at creation. If this refresh fails, dynamic table creation fails and displays an error message."
        },
        {
            "name": "ON_SCHEDULE",
            "description": "Refreshes the dynamic table at the next scheduled refresh. The dynamic table is populated when the refresh schedule process runs. No data is populated when the dynamic table is created. If you try to\nquery the table using SELECT * FROM DYNAMIC TABLE , you might see the following error because the first scheduled refresh has not yet\noccurred."
        },
        {
            "name": "column_list",
            "description": "If you want to change the name of a column or add a comment to a column in the dynamic table,\ninclude a column list that specifies the column names and, if needed, comments about\nthe columns. You do not need to specify the data types of the columns. If any of the columns in the dynamic table are based on expressions - for example, not simple column names -\nthen you must supply a column name for each column in the dynamic table. For instance, the column names are\nrequired in the following case: You can specify an optional comment for each column. For example:"
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        },
        {
            "name": "CLUSTER   BY   (   expr   [   ,   expr   ,   ...   ]   )",
            "description": "Specifies one or more columns or column expressions in the dynamic table as the clustering key. Before you specify a clustering\nkey for a dynamic table, you should understand micro-partitions. For more information, see Understanding Snowflake Table Structures . Note the following when using clustering keys with dynamic tables: Column definitions are required and must be explicitly specified in the statement. By default, Automatic Clustering is not suspended for the new dynamic table, even if Automatic Clustering is suspended for the\nsource table. Clustering keys are not intended or recommended for all tables; they typically benefit very large (for example\nmulti-terabyte) tables. Specifying CLUSTER BY doesn’t cluster the data at creation time; instead, CLUSTER BY relies on\nAutomatic Clustering to recluster the data over time. For more information, see Clustering Keys & Clustered Tables . Default: No value (no clustering key is defined for the table)"
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the retention period for the dynamic table so that Time Travel actions (SELECT, CLONE) can be performed on historical\ndata in the dynamic table. Time Travel behaves the same way for dynamic tables as it behaves for traditional tables. For more\ninformation, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "An object parameter that sets the maximum number of days Snowflake can extend the data retention period to prevent streams on the dynamic\ntable from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the dynamic table. (Note that comments can be specified at the column level or the table level. The syntax for each is slightly different.) Default: No value."
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original table when a new dynamic table is created using any of the following CREATE DYNAMIC TABLE variants: CREATE OR REPLACE DYNAMIC TABLE CREATE OR REPLACE DYNAMIC ICEBERG TABLE CREATE OR REPLACE DYNAMIC TABLE … CLONE This parameter copies all privileges except OWNERSHIP from the existing dynamic table to the new dynamic table. The new dynamic\ntable does not inherit any future grants defined for the object type in the schema. By default, the role that executes the\nCREATE DYNAMIC TABLE statement owns the new dynamic table. If this parameter is not included in the CREATE DYNAMIC TABLE statement, then the new table does not inherit any explicit access\nprivileges granted on the original dynamic table, but does inherit any future grants defined for the object type in the schema. If the statement is replacing an existing table of the same name, then the grants are copied from the table being replaced. If there is\nno existing table of that name, then the grants are copied. For example, the following statement creates a dynamic table dt1 cloned from dt0 with all grants copied from dt0 . The first\ntime you run the command, dt1 copies all grants from dt0 . If you run the same command again, dt1 will copy all grants from dt1 and not dt0 . Note the following: With data sharing : If the existing dynamic table was shared to another account, the replacement dynamic table is also shared. If the existing dynamic table was shared with your account as a data consumer, and access was further granted to other roles in\nthe account (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement dynamic\ntable. The SHOW GRANTS output for the replacement dynamic table lists the grantee for the copied privileges as the\nrole that executed the CREATE TABLE statement, with the current timestamp when the statement was executed. The SHOW GRANTS output for the replacement dynamic table lists the grantee for the copied privileges as the\nrole that executed the CREATE TABLE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE DYNAMIC TABLE command (i.e. within the same transaction). Important The COPY GRANTS parameter can be placed anywhere in a CREATE [ OR REPLACE ] DYNAMIC TABLE command, except after the query\ndefinition. For example, the following dynamic table will fail to create:"
        },
        {
            "name": "ROW   ACCESS   POLICY   policy_name   ON   (   col_name   [   ,   col_name   ...   ]   )",
            "description": "Specifies the row access policy to set on a dynamic table."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "AGGREGATION   POLICY   policy_name   [   ENTITY   KEY   (   col_name   [   ,   col_name   ...   ]   )   ]",
            "description": "Specifies an aggregation policy to set on a dynamic table. You can apply one or more aggregation\npolicies on a table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For more information,\nsee Implementing entity-level privacy with aggregation policies . You can specify one or more entity keys for an aggregation policy."
        },
        {
            "name": "REQUIRE   USER",
            "description": "When specified, the dynamic table cannot run unless a user is specified. The dynamic table is not able to refresh unless a user is set\nin a manual refresh with the COPY SESSION parameter specified. If this option is enabled, the dynamic table must be created with the ON_SCHEDULE parameter for INITIALIZE ."
        }
    ],
    "usage_notes": "When you execute the CREATE DYNAMIC TABLE command, the current role in use becomes\nthe owner of the dynamic table. This role is used to perform refreshes of the dynamic\ntable in the background.\nYou cannot make changes to the schema after you create a dynamic table.\nDynamic tables are updated as underlying database objects change. Change tracking must\nbe enabled on all underlying objects used by a dynamic table. See\nEnable change tracking.\nIf you want to replace an existing dynamic table and need to see its current definition,\ncall the GET_DDL function.\nUsing ORDER BY in the definition of a dynamic table\nmight produce results sorted in an unexpected order. You can use ORDER BY when querying\nyour dynamic table to ensure that rows selected return in a specific order.\nSnowflake doesn’t support using ORDER BY to create a view that selects from a dynamic\ntable.\nSome expressions, clauses, and functions are not currently supported in dynamic tables.\nFor a complete list, see Dynamic table limitations.\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-contact",
    "title": "CREATE CONTACT",
    "description": "Creates a new contact or replaces an existing contact.",
    "syntax": "CREATE [ OR REPLACE ] CONTACT [ IF NOT EXISTS ] <name>\n  [ {\n    USERS = ( <user-name> [ , <user_name> ... ] )\n    | EMAIL_DISTRIBUTION_LIST = '<email>'\n    | URL = '<url>'\n    } ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE CONTACT my_contact\n  EMAIL_DISTRIBUTION_LIST = 'comany_support@example.com';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the name of the new contact. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "USERS   =   (   user_name   [   ,   user_name   ...   ]   )",
            "description": "Comma-delimited list of Snowflake users who can be contacted, specified by the name of their user objects."
        },
        {
            "name": "EMAIL_DISTRIBUTION_LIST   =   ' email '",
            "description": "A valid email address, which can be a distribution list if you want users to be able to contact more than one individual."
        },
        {
            "name": "URL   =   ' url '",
            "description": "A URL that can be used to contact people about an object."
        },
        {
            "name": "COMMENT",
            "description": "A user-defined string. Specifies a comment for the contact."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-tag",
    "title": "CREATE TAG",
    "description": "Creates a new tag or replaces an existing tag in the system.",
    "syntax": "CREATE [ OR REPLACE ] TAG [ IF NOT EXISTS ] <name>\n    [ ALLOWED_VALUES '<val_1>' [ , '<val_2>' [ , ... ] ] ]\n    [ PROPAGATE = { ON_DEPENDENCY_AND_DATA_MOVEMENT | ON_DEPENDENCY | ON_DATA_MOVEMENT }\n      [ ON_CONFLICT = { '<string>' | ALLOWED_VALUES_SEQUENCE } ] ]\n    [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE TAG cost_center COMMENT = 'cost_center tag';"
        },
        {
            "code": "CREATE OR ALTER TAG cost_center ALLOWED_VALUES 'finance', 'engineering', 'sales';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the tag. Assign the tag string value on an object using either a CREATE <object> statement or an ALTER <object> statement. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. “My object”). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "ALLOWED_VALUES   ' val_1 '   [   ,   ' val_2 '   [   ,   ...   ]   ]",
            "description": "Specifies a comma-separated list of the possible string values that can be assigned to the tag when the tag is set on an object using the corresponding CREATE <object> or ALTER <object> command. Must come before all other parameters to work. The maximum number of tag values in this list is 300. If a tag is configured to automatically propagate to target objects, the order of values in the allowed list can affect how conflicts are\nresolved. For more information, see Tag propagation conflicts . Default: NULL (all string values are allowed, including an empty string value (that is, ' ' ))."
        },
        {
            "name": "PROPAGATE   =   {   ON_DEPENDENCY_AND_DATA_MOVEMENT   |   ON_DEPENDENCY   |   ON_DATA_MOVEMENT   }",
            "description": "Enterprise Edition Feature This parameter requires Enterprise Edition or higher. To inquire about upgrading, please contact Snowflake Support . Specifies that the tag will be automatically propagated from source objects to target\nobjects. You can configure the tag to propagate when there is an object dependency , data movement , or both. Propagates the tag when there is an object dependency or data movement. Propagates the tag for object dependencies, but not for data movement. Propagates the tag when there is data movement, but not for object dependencies."
        },
        {
            "name": "ON_CONFLICT   =   {   ' string '   |   ALLOWED_VALUES_SEQUENCE   }",
            "description": "Enterprise Edition Feature This parameter requires Enterprise Edition or higher. To inquire about upgrading, please contact Snowflake Support . Specifies what happens when there is a conflict between the values of propagated tags . If you don’t set this parameter and there is a conflict, the value of the tag is set to the string CONFLICT . Possible values are: When there is a conflict, the value of the tag is set to the specified string. The order of the values in the ALLOWED_VALUES property of the tag determines which value is used when there is a conflict. For example,\nsuppose you created a tag with the following statement: If there is a conflict, then the value of my_tag will be blue because it comes before red in the allowed values list. Default: Set the value of the tag to CONFLICT ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the tag. Default: No value"
        },
        {
            "name": "ON_DEPENDENCY_AND_DATA_MOVEMENT",
            "description": "Propagates the tag when there is an object dependency or data movement."
        },
        {
            "name": "ON_DEPENDENCY",
            "description": "Propagates the tag for object dependencies, but not for data movement."
        },
        {
            "name": "ON_DATA_MOVEMENT",
            "description": "Propagates the tag when there is data movement, but not for object dependencies."
        },
        {
            "name": "' string '",
            "description": "When there is a conflict, the value of the tag is set to the specified string."
        },
        {
            "name": "ALLOWED_VALUES_SEQUENCE",
            "description": "The order of the values in the ALLOWED_VALUES property of the tag determines which value is used when there is a conflict. For example,\nsuppose you created a tag with the following statement: If there is a conflict, then the value of my_tag will be blue because it comes before red in the allowed values list."
        }
    ],
    "usage_notes": "Snowflake limits the number of tags in an account to 10,000.\nFor more information about how tags can be associated with Snowflake objects, see Introduction to object tagging.\nFor more information about tag DDL authorization, see required privileges.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-aggregation-policy",
    "title": "CREATE AGGREGATION POLICY",
    "description": "Creates a new aggregation policy in the current/specified schema or replaces an existing\naggregation policy.",
    "syntax": "CREATE [ OR REPLACE ] AGGREGATION POLICY [ IF NOT EXISTS ] <name>\n  AS () RETURNS AGGREGATION_CONSTRAINT -> <body>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE AGGREGATION POLICY my_policy AS ()\n  RETURNS AGGREGATION_CONSTRAINT ->\n  AGGREGATION_CONSTRAINT(MIN_GROUP_SIZE => 5);"
        },
        {
            "code": "CREATE AGGREGATION POLICY my_policy AS ()\n  RETURNS AGGREGATION_CONSTRAINT ->\n    CASE\n      WHEN CURRENT_ROLE() = 'ADMIN'\n        THEN NO_AGGREGATION_CONSTRAINT()\n      ELSE AGGREGATION_CONSTRAINT(MIN_GROUP_SIZE => 5)\n    END;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the aggregation policy; must be unique for your schema. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "body",
            "description": "SQL expression that determines the restrictions of an aggregation policy. To define the constraints of the aggregation policy, use the SQL expression to call one or more of the following functions: When the policy body returns a value from this function, queries can return data from an aggregation-constrained table or view\nwithout restriction. For example, the body of the policy could call this function when an administrator needs to obtain unaggregated\nresults from the aggregation-constrained table or view. Call NO_AGGREGATION_CONSTRAINT without an argument. When the policy body returns a value from this function, queries must aggregate data in order to return results. Use the\nMIN_GROUP_SIZE argument to specify how many records must be included in each aggregation group. The syntax of the AGGREGATION_CONSTRAINT function is: Where: Specifies how many rows or entities must be included in the groups returned by\na query against the aggregation-constrained table or view. There is a difference between passing a 1 and a 0 as the argument to the function. Both require results to be aggregated. Passing a 1 also requires that each aggregation group contain at least one record from the aggregation-constrained table. So for\nouter joins, at least one record from the aggregation-constrained table must match a record from an unprotected table. Passing a 0 allows the query to return groups that consist entirely of records from another table. So for outer joins between an\naggregation-constrained table and an unprotected table, a group could consist of records from the unprotected table that do not match\nany records in the aggregation-constrained table. The body of a policy cannot reference user-defined functions, tables, or views."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the aggregation policy."
        },
        {
            "name": "NO_AGGREGATION_CONSTRAINT",
            "description": "When the policy body returns a value from this function, queries can return data from an aggregation-constrained table or view\nwithout restriction. For example, the body of the policy could call this function when an administrator needs to obtain unaggregated\nresults from the aggregation-constrained table or view. Call NO_AGGREGATION_CONSTRAINT without an argument."
        },
        {
            "name": "AGGREGATION_CONSTRAINT",
            "description": "When the policy body returns a value from this function, queries must aggregate data in order to return results. Use the\nMIN_GROUP_SIZE argument to specify how many records must be included in each aggregation group. The syntax of the AGGREGATION_CONSTRAINT function is: Where: Specifies how many rows or entities must be included in the groups returned by\na query against the aggregation-constrained table or view. There is a difference between passing a 1 and a 0 as the argument to the function. Both require results to be aggregated. Passing a 1 also requires that each aggregation group contain at least one record from the aggregation-constrained table. So for\nouter joins, at least one record from the aggregation-constrained table must match a record from an unprotected table. Passing a 0 allows the query to return groups that consist entirely of records from another table. So for outer joins between an\naggregation-constrained table and an unprotected table, a group could consist of records from the unprotected table that do not match\nany records in the aggregation-constrained table."
        },
        {
            "name": "MIN_GROUP_SIZE   =>   integer_expression",
            "description": "Specifies how many rows or entities must be included in the groups returned by\na query against the aggregation-constrained table or view. There is a difference between passing a 1 and a 0 as the argument to the function. Both require results to be aggregated. Passing a 1 also requires that each aggregation group contain at least one record from the aggregation-constrained table. So for\nouter joins, at least one record from the aggregation-constrained table must match a record from an unprotected table. Passing a 0 allows the query to return groups that consist entirely of records from another table. So for outer joins between an\naggregation-constrained table and an unprotected table, a group could consist of records from the unprotected table that do not match\nany records in the aggregation-constrained table."
        }
    ],
    "usage_notes": "If you want to update an existing aggregation policy and need to see the current body of the policy, run the\nDESCRIBE AGGREGATION POLICY command or GET_DDL function.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-pipe",
    "title": "CREATE PIPE",
    "description": "Creates a new pipe in the system for defining the COPY INTO <table> statement used by\nSnowpipe to load data from an ingestion queue into tables.",
    "syntax": "CREATE [ OR REPLACE ] PIPE [ IF NOT EXISTS ] <name>\n  [ AUTO_INGEST = [ TRUE | FALSE ] ]\n  [ ERROR_INTEGRATION = <integration_name> ]\n  [ AWS_SNS_TOPIC = '<string>' ]\n  [ INTEGRATION = '<string>' ]\n  [ COMMENT = '<string_literal>' ]\n  AS <copy_statement>",
    "examples": [
        {
            "code": "CREATE PIPE mypipe\n  AS\n  COPY INTO mytable\n  FROM @mystage\n  FILE_FORMAT = (TYPE = 'JSON');"
        },
        {
            "code": "CREATE PIPE mypipe2\n  AS\n  COPY INTO mytable(C1, C2)\n  FROM (SELECT $5, $4 FROM @mystage)\n  FILE_FORMAT = (TYPE = 'JSON');"
        },
        {
            "code": "CREATE PIPE mypipe3\n  AS\n  (COPY INTO mytable\n    FROM @mystage\n    MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n    INCLUDE_METADATA = (c1= METADATA$START_SCAN_TIME, c2=METADATA$FILENAME)\n    FILE_FORMAT = (TYPE = 'JSON'));"
        },
        {
            "code": "CREATE PIPE mypipe_s3\n  AUTO_INGEST = TRUE\n  AWS_SNS_TOPIC = 'arn:aws:sns:us-west-2:001234567890:s3_mybucket'\n  AS\n  COPY INTO snowpipe_db.public.mytable\n  FROM @snowpipe_db.public.mystage\n  FILE_FORMAT = (TYPE = 'JSON');"
        },
        {
            "code": "CREATE PIPE mypipe_gcs\n  AUTO_INGEST = TRUE\n  INTEGRATION = 'MYINT'\n  AS\n  COPY INTO snowpipe_db.public.mytable\n  FROM @snowpipe_db.public.mystage\n  FILE_FORMAT = (TYPE = 'JSON');"
        },
        {
            "code": "CREATE PIPE mypipe_azure\n  AUTO_INGEST = TRUE\n  INTEGRATION = 'MYINT'\n  AS\n  COPY INTO snowpipe_db.public.mytable\n  FROM @snowpipe_db.public.mystage\n  FILE_FORMAT = (TYPE = 'JSON');"
        },
        {
            "code": "CREATE PIPE mypipe_aws\n  AUTO_INGEST = TRUE\n  AS\n  COPY INTO snowpipe_db.public.mytable\n  FROM @snowpipe_db.public.mystage\n  FILE_FORMAT = (TYPE = 'JSON');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the pipe; must be unique for the schema in which the pipe is created. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "copy_statement",
            "description": "COPY INTO <table> statement used to load data from queued files into a Snowflake table. This statement serves\nas the text/definition for the pipe and is displayed in the SHOW PIPES output."
        },
        {
            "name": "AUTO_INGEST   =   TRUE   |   FALSE",
            "description": "Specifies whether to automatically load data files from the internal or external stage: TRUE enables automatic data loading. Snowpipe supports loading from external stages (Amazon S3, Google Cloud Storage, or Microsoft Azure). FALSE disables automatic data loading. You must make calls to the Snowpipe REST API endpoints to load data files. Snowpipe supports loading from internal stages (i.e. Snowflake named stages or table stages, but not user stages) or\nexternal stage (Amazon S3, Google Cloud Storage, or Microsoft Azure)."
        },
        {
            "name": "ERROR_INTEGRATION   =   ' integration_name '",
            "description": "Required only when configuring Snowpipe to send error notifications to a cloud messaging service. Specifies the name of the notification integration used to communicate with the messaging service. For more information, see Snowpipe error notifications ."
        },
        {
            "name": "AWS_SNS_TOPIC   =   ' string '",
            "description": "Required only when configuring AUTO_INGEST for Amazon S3 external stages using SNS. Specifies the Amazon Resource Name (ARN) for the SNS topic for your S3 bucket. The CREATE PIPE statement subscribes the\nAmazon Simple Queue Service (SQS) queue to the specified SNS topic. The pipe copies files to the ingest queue triggered by event\nnotifications via the SNS topic. For more information, see Automating Snowpipe for Amazon S3 ."
        },
        {
            "name": "INTEGRATION   =   ' string '",
            "description": "Required only when configuring AUTO_INGEST for Google Cloud Storage or Microsoft Azure external stages. Specifies the existing notification integration used to access the storage queue. For more information, see: Automating Snowpipe for Google Cloud Storage Automating Snowpipe for Microsoft Azure Blob Storage The integration name must be typed in all uppercase."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the pipe. Default: No value"
        }
    ],
    "usage_notes": "This SQL command requires the following minimum permissions:\nPrivilege\nObject\nNotes\nCREATE PIPE\nSchema\nUSAGE\nStage in the pipe definition\nExternal stages only\nUSAGE\nIntegration\nRequired for receiving Snowpipe error notifications\nREAD\nStage in the pipe definition\nInternal stages only\nSELECT, INSERT\nTable in the pipe definition\nSQL operations on schema objects also require the USAGE privilege on the database and schema that contain the object.\nAll COPY INTO <table> copy options are supported except for the following:\nFILES = ( 'file_name1' [ , 'file_name2', ... ] )\nON_ERROR = ABORT_STATEMENT\nSIZE_LIMIT = num\nPURGE = TRUE | FALSE (i.e. automatic purging while loading)\nFORCE = TRUE | FALSE\nNote that you can manually remove files from an internal (i.e. Snowflake) stage (after they’ve been loaded) using the\nREMOVE command.\nRETURN_FAILED_ONLY = TRUE | FALSE\nVALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS\nThe PATTERN = 'regex_pattern' copy option filters the set of files to load using a regular expression. Pattern matching\nbehaves as follows depending on the AUTO_INGEST parameter value:\nAUTO_INGEST = TRUE: The regular expression filters the list of files in the stage and optional path (i.e. cloud storage location)\nin the COPY INTO <table> statement.\n:AUTO_INGEST = FALSE: The regular expression filters the list of files submitted in calls to the Snowpipe REST API\ninsertFiles endpoint.\nNote that Snowpipe trims any path segments in the stage definition from the storage location and applies the regular expression to any\nremaining path segments and filenames. To view the stage definition, execute the DESCRIBE STAGE command for the\nstage. The URL property consists of the bucket or container name and zero or more path segments. For example, if the FROM location in\na COPY INTO <table> statement is @s/path1/path2/ and the URL value for stage @s is s3://mybucket/path1/, then Snowpipe\ntrims /path1/ from the storage location in the FROM clause and applies the regular expression to path2/ plus the filenames in\nthe path.\nImportant\nSnowflake recommends that you enable cloud event filtering for Snowpipe to reduce costs, event noise, and latency. Only use\nthe PATTERN option when your cloud provider’s event filtering feature is not sufficient. For more information about configuring\nevent filtering for each cloud provider, see the following pages:\nAmazon S3: Configuring event notifications using object key name filtering\nMicrosoft Azure Event Grid: Understand event filtering for Event Grid subscriptions\nGoogle Cloud Pub/Sub: Filtering messages\nUsing a query as the source for the COPY statement for column reordering, column omission, and casts (i.e. transforming data during\na load) is supported. For usage examples, see Transforming data during a load. Note that only simple SELECT statements are\nsupported. Filtering using a WHERE clause is not supported.\nPipe definitions are not dynamic (i.e. a pipe is not automatically updated if the underlying stage or table changes, such as renaming\nor dropping the stage/table). Instead, you must create a new pipe and submit this pipe name in future Snowpipe REST API calls.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nImportant\nIf you recreate a pipe (using the CREATE OR REPLACE PIPE syntax), see Recreating pipes for related\nconsiderations and best practices."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/anomaly-detection/commands/create-anomaly-detection.html#label-class-anomaly-detection-create",
    "title": "CREATE SNOWFLAKE.ML.ANOMALY_DETECTION",
    "description": "Creates a new anomaly detection model or replaces an existing one using\nthe training data you provide.",
    "syntax": "CREATE [ OR REPLACE ] SNOWFLAKE.ML.ANOMALY_DETECTION <model_name>(\n  INPUT_DATA => <reference_to_training_data>,\n  [ SERIES_COLNAME => '<series_column_name>', ]\n  TIMESTAMP_COLNAME => '<timestamp_column_name>',\n  TARGET_COLNAME => '<target_column_name>',\n  LABEL_COLNAME => '<label_column_name>',\n  [ CONFIG_OBJECT => <config_object> ]\n)\n[ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n[ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] SNOWFLAKE.ML.ANOMALY_DETECTION <model_name>(\n  INPUT_DATA => <reference_to_training_data>,\n  [ SERIES_COLNAME => '<series_column_name>', ]\n  TIMESTAMP_COLNAME => '<timestamp_column_name>',\n  TARGET_COLNAME => '<target_column_name>',\n  LABEL_COLNAME => '<label_column_name>',\n  [ CONFIG_OBJECT => <config_object> ]\n)\n[ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n[ COMMENT = '<string_literal>' ]"
        }
    ],
    "parameters": [
        {
            "name": "model_name",
            "description": "Specifies the identifier ( model_name ) for the anomaly detector object; must be unique for the schema in which the object is\ncreated. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        }
    ],
    "usage_notes": "If the column names specified by the TIMESTAMP_COLNAME, TARGET_COLNAME, or LABEL_COLNAME arguments do not exist in the table,\nview, or query specified by the INPUT_DATA argument, an error occurs.\nReplication is supported only for instances\nof the CUSTOM_CLASSIFIER class."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-service",
    "title": "CREATE SERVICE",
    "description": "Creates a new Snowpark Container Services service\nin the current schema. If a service with that name already exists, use the DROP SERVICE command to delete the previously\ncreated service.",
    "syntax": "CREATE SERVICE [ IF NOT EXISTS ] <name>\n  IN COMPUTE POOL <compute_pool_name>\n  {\n     fromSpecification\n     | fromSpecificationTemplate\n  }\n  [ AUTO_SUSPEND_SECS = <num> ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <EAI_name> [ , ... ] ) ]\n  [ AUTO_RESUME = { TRUE | FALSE } ]\n  [ MIN_INSTANCES = <num> ]\n  [ MIN_READY_INSTANCES = <num> ]\n  [ MAX_INSTANCES = <num> ]\n  [ LOG_LEVEL = '<log_level>' ]\n  [ QUERY_WAREHOUSE = <warehouse_name> ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COMMENT = '{string_literal}']\n\nfromSpecification ::=\n  {\n    FROM SPECIFICATION_FILE = '<yaml_file_path>' -- for native app service.\n    | FROM @<stage> SPECIFICATION_FILE = '<yaml_file_path>' -- for non-native app service.\n    | FROM SPECIFICATION <specification_text>\n  }\n\nfromSpecificationTemplate ::=\n  {\n    FROM SPECIFICATION_TEMPLATE_FILE = '<yaml_file_stage_path>' -- for native app service.\n    | FROM @<stage> SPECIFICATION_TEMPLATE_FILE = '<yaml_file_stage_path>' -- for non-native app service.\n    | FROM SPECIFICATION_TEMPLATE <specification_text>\n  }\n  USING ( <key> => <value> [ , <key> => <value> [ , ... ] ]  )",
    "examples": [
        {
            "code": "CREATE SERVICE echo_service\n  IN COMPUTE POOL tutorial_compute_pool\n  FROM @tutorial_stage\n  SPECIFICATION_FILE='echo_spec.yaml'\n  MIN_INSTANCES=2\n  MAX_INSTANCES=2"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (that is, the name) for the service; it must be unique for the schema in which the service\nis created. Quoted names for special characters or case-sensitive names are not supported. The same constraint also applies to database\nand schema names where you create a service. That is, database and schema names without quotes are valid when creating a\nservice."
        },
        {
            "name": "IN   COMPUTE   POOL   compute_pool_name",
            "description": "Specifies the name of the compute pool in your account on which to run the service."
        },
        {
            "name": "FROM   ...",
            "description": "Identifies the specification or\nthe template specification for the service. Using a service specification You can either define the specification either inline or in a separate file . Specifies the file containing the service specification or the service specification inline. If your service specification is in a file, use SPECIFICATION_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file. Using a service specification template You can either define the template specification either inline or in a separate file . Specifies the file containing the service specification template or the service specification template inline. If your service specification template is in a file, use SPECIFICATION_TEMPLATE_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file. When using template specification, you should also include the USING parameter. Specifies the template variables and the values of those variables. key is the name of the template variable. The template variable name can optionally be enclosed in double quotes\n( \" ). value is the value to assign to the variable in the template. String values must be enclosed in ' or $$ . The value must either be alphanumeric or valid JSON. Use a comma between each key-value pair."
        },
        {
            "name": "SPECIFICATION_FILE   =   ' yaml_file_path '   or   .   @ stage   SPECIFICATION_FILE   =   ' yaml_file_path '   or   .   SPECIFICATION   specification_text",
            "description": "Specifies the file containing the service specification or the service specification inline. If your service specification is in a file, use SPECIFICATION_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file."
        },
        {
            "name": "SPECIFICATION_TEMPLATE_FILE   =   ' yaml_file_path '   or   .   @ stage   SPECIFICATION_TEMPLATE_FILE   =   ' yaml_file_path '   or   .   SPECIFICATION_TEMPLATE   specification_text",
            "description": "Specifies the file containing the service specification template or the service specification template inline. If your service specification template is in a file, use SPECIFICATION_TEMPLATE_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file. When using template specification, you should also include the USING parameter."
        },
        {
            "name": "USING   (   key   =>   value   [   ,   key   =>   value   [   ,   ...   ]   ]    )",
            "description": "Specifies the template variables and the values of those variables. key is the name of the template variable. The template variable name can optionally be enclosed in double quotes\n( \" ). value is the value to assign to the variable in the template. String values must be enclosed in ' or $$ . The value must either be alphanumeric or valid JSON. Use a comma between each key-value pair."
        },
        {
            "name": "AUTO_SUSPEND_SECS   =   num",
            "description": "Specifies the number of seconds of inactivity (service is idle) after which Snowflake automatically suspends the service. Inactivity means no queries (that invoke a service function) executed for the time period specified by AUTO_SUSPEND_SECS. You can configure this value to 300 seconds or more to enable auto-suspension. For more information, see Suspending a service . Default: 0 seconds, which indicates Snowflake does not suspend the service automatically. Preview Feature — Open Configuring the automatic suspension of a Snowpark Container Services service using the AUTO_SUSPEND_SECS property is a preview feature ."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   EAI_name   [   ,   ...   ]   )",
            "description": "Specifies the names of the external access integrations that allow your service to access external sites.\nThe names in this list are case-sensitive. By default, application containers don’t have\npermission to access the internet. If you want to allow your service to access an external site, create an External Access Integration\n(EAI), and configure your service to use that integration. For more\ninformation, see Configuring network egress ."
        },
        {
            "name": "AUTO_RESUME   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically resume a service when user performs one of the following actions that depend on the service: Executing a query is that uses a service function . Sending a request to the public endpoint exposed by the service ( ingress ). If AUTO_RESUME is FALSE, you need to explicitly resume the service (using ALTER SERVICE … RESUME ). Default: TRUE."
        },
        {
            "name": "MIN_INSTANCES   =   num",
            "description": "Specifies the minimum number of service instances to run. Default: 1."
        },
        {
            "name": "MIN_READY_INSTANCES   =   num",
            "description": "Indicates the minimum service instances that must be ready for Snowflake to consider the service is ready to process requests.\nMIN_READY_INSTANCES must be equal to or less than MIN_INSTANCES. For more information, see Scaling services . Default: The value of the MIN_INSTANCES property."
        },
        {
            "name": "MAX_INSTANCES   =   num",
            "description": "Specifies the maximum number of service instances to run. Default: The value of the MIN_INSTANCES property."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested.\nCurrently, LOG_LEVEL is supported only for platform events , Changing LOG_LEVEL for container logs is not supported. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "QUERY_WAREHOUSE   =   warehouse_name",
            "description": "Warehouse to use if a service container connects to Snowflake to execute a query but does not explicitly specify a warehouse\nto use. This parameter also supports object references in Native Apps. For more information, see Request references and object-level privileges from consumers . Default: none."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the service. Default: No value"
        }
    ],
    "usage_notes": "When calling CREATE SERVICE, the parameters should be provided in this order: specify compute pool, followed by the service specification (either provider specification file on stage or inline specification), and then other properties.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-packages-policy",
    "title": "CREATE PACKAGES POLICY",
    "description": "Creates a new packages policy or replaces an\nexisting packages policy.",
    "syntax": "CREATE [ OR REPLACE ] PACKAGES POLICY [ IF NOT EXISTS ] <name>\n  LANGUAGE PYTHON\n  [ ALLOWLIST = ( [ '<packageSpec>' ] [ , '<packageSpec>' ... ] ) ]\n  [ BLOCKLIST = ( [ '<packageSpec>' ] [ , '<packageSpec>' ... ] ) ]\n  [ ADDITIONAL_CREATION_BLOCKLIST = ( [ '<packageSpec>' ] [ , '<packageSpec>' ... ] ) ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE PACKAGES POLICY yourdb.yourschema.packages_policy_prod_1\n  LANGUAGE PYTHON\n  ALLOWLIST = ('numpy', 'pandas==1.2.3', ...)\n  BLOCKLIST = ('numpy==1.2.3', 'bad_package', ...)\n  COMMENT = 'Packages policy for the prod_1 environment';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the packages policy; must be unique for the schema in which the packages policy is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "LANGUAGE   PYTHON",
            "description": "Specifies the language that this packages policy will apply to."
        },
        {
            "name": "ALLOWLIST   =   (   [   ' packageSpec '   ]   [   ,   ' packageSpec '   ...   ]   )",
            "description": "Specifies a list of package specs that are allowed. Default: ('*') (i.e. allow all packages)."
        },
        {
            "name": "BLOCKLIST   =   (   [   ' packageSpec '   ]   [   ,   ' packageSpec '   ...   ]   )",
            "description": "Specifies a list of package specs that are blocked. To unset this parameter, specify an empty list. Default: () (i.e. do not block any packages)."
        },
        {
            "name": "ADDITIONAL_CREATION_BLOCKLIST   =   (   [   ' packageSpec '   ]   [   ,   ' packageSpec '   ...   ]   )",
            "description": "Specifies a list of package specs that are blocked at creation time. To unset this parameter, specify an empty list.\nIf the ADDITIONAL_CREATION_BLOCKLIST is set, it is appended to the basic BLOCKLIST at the creation time.\nFor temporary UDFs and anonymous stored procedures, the ADDITIONAL_CREATION_BLOCKLIST is appended to the basic BLOCKLIST at both creation and execution time. Default: () (i.e. do not block any packages)."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the packages policy."
        }
    ],
    "usage_notes": "The OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-model",
    "title": "CREATE MODEL",
    "description": "Creates a new machine learning model in the current/specified schema or replaces an existing model.",
    "syntax": "CREATE [ OR REPLACE ] MODEL [ IF NOT EXISTS ] <name> [ WITH VERSION <version_name> ]\n    FROM MODEL <source_model_name> [ VERSION <source_version_or_alias_name> ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] MODEL [ IF NOT EXISTS ] <name> [ WITH VERSION <version_name> ]\n    FROM MODEL <source_model_name> [ VERSION <source_version_or_alias_name> ]"
        },
        {
            "code": "CREATE [ OR REPLACE ] MODEL [ IF NOT EXISTS ] <name> FROM internalStage"
        },
        {
            "code": "internalStage ::=\n    @[<namespace>.]<int_stage_name>[/<path>]\n  | @[<namespace>.]%<table_name>[/<path>]\n  | @~[/<path>]"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the new model; must be unique for the schema in which the model\nis created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "FROM   MODEL   source_model_name",
            "description": "Specifies the name of the model from which to create the new model."
        },
        {
            "name": "FROM   internalStage",
            "description": "Specifies the internal stage that contains the model’s files. The required layout of these files is not currently\ndocumented."
        },
        {
            "name": "Required if not using FROM internalStage variant",
            "description": "Specifies the name of the model from which to create the new model."
        },
        {
            "name": "Required if using FROM internalStage variant",
            "description": "Specifies the internal stage that contains the model’s files. The required layout of these files is not currently\ndocumented."
        },
        {
            "name": "WITH   VERSION   version_name",
            "description": "Specifies the name of the version to create in the new model."
        },
        {
            "name": "VERSION   source_version_or_alias_name",
            "description": "Specifies the name or alias of the version to be copied from the source model. If not specified, uses the default version\nfrom the source model."
        },
        {
            "name": "For use with FROM MODEL variant",
            "description": "Specifies the name of the version to create in the new model."
        },
        {
            "name": "For use with FROM MODEL variant",
            "description": "Specifies the name or alias of the version to be copied from the source model. If not specified, uses the default version\nfrom the source model."
        }
    ],
    "usage_notes": "The OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-dbt-project",
    "title": "CREATE DBT PROJECT",
    "description": "Creates a new dbt project object or replaces an existing dbt project. Running CREATE DBT PROJECT with the OR REPLACE option resets the version identifier to version$1 and removes all version name aliases. For more information, see Versioning for dbt project objects and files.",
    "syntax": "CREATE [ OR REPLACE ] DBT PROJECT [ IF NOT EXISTS ] <name>\n  [ FROM '<source_location>' ]\n  [ DEFAULT_ARGS = '<string_literal>' ]\n  [ DEFAULT_VERSION = { FIRST | LAST | VERSION$<num> | '<version_name_alias>' } ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE DBT PROJECT sales_db.dbt_projects_schema.sales_model\n  FROM '@sales_db.integrations_schema.sales_dbt_git_stage/branches/main'\n  COMMENT = 'generates sales data models';"
        },
        {
            "code": "CREATE DBT PROJECT sales_db.dbt_projects_schema.sw_region_sales_model\n  FROM '@sales_db.integrations_schema.sales_dbt_git_stage/branches/main/sales_dbt_projects_parent/sw_region_dbt_project'\n  COMMENT = 'generates data models for sw sales region';"
        },
        {
            "code": "CREATE DBT PROJECT sales_db.dbt_projects_schema.sales_model_nw_region\n  FROM 'snow://dbt/sales_db.dbt_projects_schema.sales_model/versions/version$2'\n  DEFAULT_ARGS = '--select \"tag:nw_region\"'\n  COMMENT = 'generates data models for the NW sales region';"
        },
        {
            "code": "-- Create a dbt project object from a workspace named \"My dbt Project Workspace\" in the user's personal database.\n\nCREATE DBT PROJECT sales_db.dbt_projects_schema.sales_model_from_workspace\n  FROM 'snow://workspace/user$.public.\"My dbt Project Workspace\"/versions/live'\n\n-- Execute the dbt project, specifying a subdirectory path for a dbt project within the workspace\n\nEXECUTE DBT PROJECT sales_db.dbt_projects_schema.sales_model_from_workspace\n  PROJECT_ROOT = 'project2'\n  ARGS = 'run --target prod';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (that is, the name) for the dbt project object within Snowflake; must be unique for the schema in which the dbt project is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "FROM   ' source_location '",
            "description": "A string that specifies the location in Snowflake of the source files for the dbt project object. This can be a parent directory that contains multiple dbt projects, or a specific subdirectory that contains a dbt project and dbt_project.yml file. If the specified location doesn’t contain a dbt_project.yml file, the EXECUTE DBT PROJECT command must use the PROJECT_ROOT parameter to specify the subdirectory path to a dbt_project.yml file. If no value is specified, Snowflake creates an empty dbt project. The dbt project source files can be in any one of the following locations: A Git repository stage , for example: '@my_db.my_schema.my_git_repository_stage/branches/my_branch/path/to/dbt_project_or_projects_parent' For more information about creating a Git repository object in Snowflake that connects a Git repository to a workspace for dbt Projects on Snowflake, see Create a workspace connected to your Git repository . For more information about creating and managing a Git repository object and stage without using a workspace, see Using a Git repository in Snowflake and CREATE GIT REPOSITORY . An existing dbt project stage , for example: 'snow://dbt/my_db.my_schema.my_existing_dbt_project_object/versions/last' The version specifier is required and can be last (as shown in the previous example), first , or the specifier for any existing version in the form version$<num> . For more information, see Versioning for dbt project objects and files . An internal named stage , for example: '@my_db.my_schema.my_internal_named_stage/path/to/dbt_projects_or_projects_parent' Internal user stages and table stages aren’t supported. A workspace for dbt on Snowflake , for example: 'snow://workspace/user$.public.\"my_workspace_name\"/versions/live/path/to/dbt_projects_or_projects_parent' We recommend enclosing the workspace name in double quotes because workspace names are case-sensitive and can contain special characters. The version specifier is required and can be last , first , live , or the specifier for any existing version in the form version$<num> . For more information, see Versioning for dbt project objects and files . Default: No value"
        },
        {
            "name": "DEFAULT_ARGS   =   ' string_literal '",
            "description": "A string that specifies the default dbt command and command line options to use if EXECUTE DBT PROJECT specifies no command. Important Arguments that you explicitly specify in an EXECUTE DBT PROJECT command overwrite any and all DEFAULT_ARGS specified in the DBT PROJECT definition. Default: No value"
        },
        {
            "name": "DEFAULT_VERSION   =   {   FIRST   |   LAST   |   VERSION$ num   |   ' version_name_alias '   }",
            "description": "Specifies the default version of the dbt project object to use if EXECUTE DBT PROJECT doesn’t specify a version. For more information, see Versioning for dbt project objects and files . Default: No value"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the dbt project object. Default: No value"
        }
    ],
    "usage_notes": "The OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-privacy-policy",
    "title": "CREATE PRIVACY POLICY",
    "description": "Creates a new privacy policy or replaces an existing privacy policy.",
    "syntax": "CREATE [ OR REPLACE ] PRIVACY POLICY [ IF NOT EXISTS ] <name>\n  AS () RETURNS PRIVACY_BUDGET -> <body>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE PRIVACY POLICY my_priv_policy\n  AS ( ) RETURNS PRIVACY_BUDGET ->\n  PRIVACY_BUDGET(BUDGET_NAME=> 'analysts');"
        },
        {
            "code": "CREATE PRIVACY POLICY my_priv_policy\n  AS () RETURNS PRIVACY_BUDGET ->\n    CASE\n      WHEN CURRENT_USER() = 'ADMIN'\n        THEN NO_PRIVACY_POLICY()\n      ELSE PRIVACY_BUDGET(BUDGET_NAME => 'analysts')\n    END;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (that is, name) for the privacy policy; must be unique for the schema in which the privacy policy is\ncreated. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "body",
            "description": "The SQL expression of the body calls two functions to control the return value of the policy:\nNO_PRIVACY_POLICY and PRIVACY_BUDGET. When a query is executed against a table that has been assigned the\npolicy, Snowflake evaluates the conditions of the body to call the appropriate function and return a value. This return value determines\nwhich privacy budget, if any, is associated with the query against the privacy-protected table. The expression can use context functions such as CURRENT_ROLE or INVOKER_ROLE to associate a user or group of users with a privacy budget. If you use a CASE block in the body’s expression, it must include an ELSE statement that\ncalls either NO_PRIVACY_POLICY or PRIVACY_BUDGET. Every user must either be associated with a privacy budget or have unrestricted access to\nthe privacy-protected table. If a user should not have any access to a privacy-protected table or view, revoke SELECT privileges rather than\ntrying to define this in the privacy policy. Use the body’s expression to call the NO_PRIVACY_POLICY function when you want a query to have unrestricted access to the table or view to which the privacy policy is assigned. Use the body’s expression to call the PRIVACY_BUDGET function when you want to return a privacy budget from the policy. The\nexpression can contain conditions that allow the policy to return different privacy budgets for different queries based on factors like\nthe user who is executing the query. In cross-account collaboration, privacy budgets are automatically namespaced by the account identifier of the consumer account, which\nprevents two different consumer accounts from sharing the same privacy budget even if the name of the privacy budget is the same. Using\nthe CURRENT_ACCOUNT function to concatenate the name of the account with the name of the privacy budget\ncan help distinguish between privacy budgets. For example, you could call the function as follows: PRIVACY_BUDGET(BUDGET_NAME => 'external_budget.' || CURRENT_ACCOUNT()) . The signature of the PRIVACY_BUDGET function is: Privacy budget arguments: Resolves to the name of a privacy budget. Snowflake creates the privacy budget automatically when its name is\nspecified in the body of the privacy policy. A decimal number > 0 that specifies the budget limit for this privacy policy.\nThis controls the total amount of privacy loss allowed. Adjusting this value\nchanges how many total differentially private aggregates can be calculated\nagainst tables protected by this privacy budget during the refresh period. When a query is run that would\ncause the cumulative privacy loss to exceed this number, the query will fail.\nAs a rough estimate, a budget\nlimit of 233 with MAX_BUDGET_PER_AGGREGATE=1 permits about 1000 aggregates\nper refresh period. Default: 233.0 Specifies how much privacy budget is used for each aggregate function in a\nquery. Adjusting this value changes the amount of noise added to each aggregate\nquery, as well as the number of aggregates that can be calculated before the budget limit is reached. As an example, the query select count(*), avg(a) ... has two aggregates: count(*) and avg(a) . Specify a decimal value > 0. Default: 0.5 How often the privacy budget is refreshed, that is, has its cumulative privacy loss reset to 0. Valid values: Daily : Refreshed every day at 12:00 AM UTC Weekly : Refreshed every Sunday at 12:00 AM UTC Monthly : Refreshed on the first day of the calendar month at 12:00 AM UTC Yearly : Refreshed on January 1 at 12:00 AM UTC Never : Privacy budget is never refreshed. Default: Weekly"
        },
        {
            "name": "NO_PRIVACY_POLICY",
            "description": "Use the body’s expression to call the NO_PRIVACY_POLICY function when you want a query to have unrestricted access to the table or view to which the privacy policy is assigned."
        },
        {
            "name": "PRIVACY_BUDGET",
            "description": "Use the body’s expression to call the PRIVACY_BUDGET function when you want to return a privacy budget from the policy. The\nexpression can contain conditions that allow the policy to return different privacy budgets for different queries based on factors like\nthe user who is executing the query. In cross-account collaboration, privacy budgets are automatically namespaced by the account identifier of the consumer account, which\nprevents two different consumer accounts from sharing the same privacy budget even if the name of the privacy budget is the same. Using\nthe CURRENT_ACCOUNT function to concatenate the name of the account with the name of the privacy budget\ncan help distinguish between privacy budgets. For example, you could call the function as follows: PRIVACY_BUDGET(BUDGET_NAME => 'external_budget.' || CURRENT_ACCOUNT()) . The signature of the PRIVACY_BUDGET function is: Privacy budget arguments: Resolves to the name of a privacy budget. Snowflake creates the privacy budget automatically when its name is\nspecified in the body of the privacy policy. A decimal number > 0 that specifies the budget limit for this privacy policy.\nThis controls the total amount of privacy loss allowed. Adjusting this value\nchanges how many total differentially private aggregates can be calculated\nagainst tables protected by this privacy budget during the refresh period. When a query is run that would\ncause the cumulative privacy loss to exceed this number, the query will fail.\nAs a rough estimate, a budget\nlimit of 233 with MAX_BUDGET_PER_AGGREGATE=1 permits about 1000 aggregates\nper refresh period. Default: 233.0 Specifies how much privacy budget is used for each aggregate function in a\nquery. Adjusting this value changes the amount of noise added to each aggregate\nquery, as well as the number of aggregates that can be calculated before the budget limit is reached. As an example, the query select count(*), avg(a) ... has two aggregates: count(*) and avg(a) . Specify a decimal value > 0. Default: 0.5 How often the privacy budget is refreshed, that is, has its cumulative privacy loss reset to 0. Valid values: Daily : Refreshed every day at 12:00 AM UTC Weekly : Refreshed every Sunday at 12:00 AM UTC Monthly : Refreshed on the first day of the calendar month at 12:00 AM UTC Yearly : Refreshed on January 1 at 12:00 AM UTC Never : Privacy budget is never refreshed. Default: Weekly"
        },
        {
            "name": "BUDGET_NAME   =>   expression",
            "description": "Resolves to the name of a privacy budget. Snowflake creates the privacy budget automatically when its name is\nspecified in the body of the privacy policy."
        },
        {
            "name": "BUDGET_LIMIT   =>   decimal",
            "description": "A decimal number > 0 that specifies the budget limit for this privacy policy.\nThis controls the total amount of privacy loss allowed. Adjusting this value\nchanges how many total differentially private aggregates can be calculated\nagainst tables protected by this privacy budget during the refresh period. When a query is run that would\ncause the cumulative privacy loss to exceed this number, the query will fail.\nAs a rough estimate, a budget\nlimit of 233 with MAX_BUDGET_PER_AGGREGATE=1 permits about 1000 aggregates\nper refresh period. Default: 233.0"
        },
        {
            "name": "MAX_BUDGET_PER_AGGREGATE   =>   decimal",
            "description": "Specifies how much privacy budget is used for each aggregate function in a\nquery. Adjusting this value changes the amount of noise added to each aggregate\nquery, as well as the number of aggregates that can be calculated before the budget limit is reached. As an example, the query select count(*), avg(a) ... has two aggregates: count(*) and avg(a) . Specify a decimal value > 0. Default: 0.5"
        },
        {
            "name": "BUDGET_WINDOW   =>   string",
            "description": "How often the privacy budget is refreshed, that is, has its cumulative privacy loss reset to 0. Valid values: Daily : Refreshed every day at 12:00 AM UTC Weekly : Refreshed every Sunday at 12:00 AM UTC Monthly : Refreshed on the first day of the calendar month at 12:00 AM UTC Yearly : Refreshed on January 1 at 12:00 AM UTC Never : Privacy budget is never refreshed. Default: Weekly"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the privacy policy. Default: No value"
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-iceberg-table",
    "title": "CREATE ICEBERG TABLE",
    "description": "Creates or replaces an Apache Iceberg™ table in the current/specified schema.",
    "syntax": "CREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name> (\n    -- Column definition\n    <col_name> <col_type>\n      [ inlineConstraint ]\n      [ NOT NULL ]\n      [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n      [ [ WITH ] PROJECTION POLICY <policy_name> ]\n      [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n    -- Out-of-line constraints\n    [ , outoflineConstraint [ ... ] ]\n  )\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = 'SNOWFLAKE' ]\n  [ BASE_LOCATION = '<directory_for_table_files>' ]\n  [ CATALOG_SYNC = '<open_catalog_integration_name>']\n  [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\ninlineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE\n    | PRIMARY KEY\n    | [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ]\n  }\n  [ <constraint_properties> ]\n\noutoflineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n      REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n  }\n  [ <constraint_properties> ]\n\nCREATE [ OR REPLACE ] ICEBERG TABLE <table_name> [ ( <col_name> [ <col_type> ] , <col_name> [ <col_type> ] , ... ) ]\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = 'SNOWFLAKE' ]\n  [ BASE_LOCATION = '<relative_path_from_external_volume>' ]\n  [ COPY GRANTS ]\n  [ ... ]\n  AS SELECT <query>\n\nCREATE [ OR REPLACE ] ICEBERG TABLE <table_name> LIKE <source_table>\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ COPY GRANTS ]\n  [ ... ]\n\nCREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name>\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = '<catalog_integration_name>' ]\n  CATALOG_TABLE_NAME = '<rest_catalog_table_name>'\n  [ CATALOG_NAMESPACE = '<catalog_namespace>' ]\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nCREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name>\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = '<catalog_integration_name>' ]\n  BASE_LOCATION = '<relative_path_from_external_volume>'\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nCREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name>\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = '<catalog_integration_name>' ]\n  METADATA_FILE_PATH = '<metadata_file_path>'\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name> (\n    -- Column definition\n    <col_name> <col_type>\n      [ inlineConstraint ]\n      [ NOT NULL ]\n      [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n      [ [ WITH ] PROJECTION POLICY <policy_name> ]\n      [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n    -- Out-of-line constraints\n    [ , outoflineConstraint [ ... ] ]\n  )\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = 'SNOWFLAKE' ]\n  [ BASE_LOCATION = '<directory_for_table_files>' ]\n  [ CATALOG_SYNC = '<open_catalog_integration_name>']\n  [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]"
        },
        {
            "code": "inlineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE\n    | PRIMARY KEY\n    | [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ]\n  }\n  [ <constraint_properties> ]"
        },
        {
            "code": "outoflineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n      REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n  }\n  [ <constraint_properties> ]"
        },
        {
            "code": "CREATE [ OR REPLACE ] ICEBERG TABLE <table_name> [ ( <col_name> [ <col_type> ] , <col_name> [ <col_type> ] , ... ) ]\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = 'SNOWFLAKE' ]\n  [ BASE_LOCATION = '<relative_path_from_external_volume>' ]\n  [ COPY GRANTS ]\n  [ ... ]\n  AS SELECT <query>"
        },
        {
            "code": "CREATE [ OR REPLACE ] ICEBERG TABLE <table_name> LIKE <source_table>\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ COPY GRANTS ]\n  [ ... ]"
        },
        {
            "code": "CREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name>\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = '<catalog_integration_name>' ]\n  CATALOG_TABLE_NAME = '<rest_catalog_table_name>'\n  [ CATALOG_NAMESPACE = '<catalog_namespace>' ]\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]"
        },
        {
            "code": "CREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name>\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = '<catalog_integration_name>' ]\n  BASE_LOCATION = '<relative_path_from_external_volume>'\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]"
        },
        {
            "code": "CREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <table_name>\n  [ EXTERNAL_VOLUME = '<external_volume_name>' ]\n  [ CATALOG = '<catalog_integration_name>' ]\n  METADATA_FILE_PATH = '<metadata_file_path>'\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-listing",
    "title": "CREATE LISTING",
    "description": "Create a free listing to share directly with specific consumers, with an inline YAML manifest, or from a file located in a stage location.",
    "syntax": "CREATE EXTERNAL LISTING [ IF NOT EXISTS ] <name>\n  [ { SHARE <share_name>  |  APPLICATION PACKAGE <package_name> } ]\n  AS '<yaml_manifest_string>'\n  [ PUBLISH = { TRUE | FALSE } ]\n  [ REVIEW = { TRUE | FALSE } ]\n  [ COMMENT = '<string>' ]\n\nCREATE EXTERNAL LISTING [ IF NOT EXISTS ] <name>\n  [ { SHARE <share_name>  |  APPLICATION PACKAGE <package_name> } ]\n  FROM '<yaml_manifest_stage_location>'\n  [ PUBLISH = { TRUE | FALSE } ]\n  [ REVIEW = { TRUE | FALSE } ]",
    "examples": [
        {
            "code": "CREATE EXTERNAL LISTING MYLISTING\nSHARE MySHARE AS\n$$\ntitle: \"MyListing\"\nsubtitle: \"Subtitle for MyListing\"\ndescription: \"Description for MyListing\"\nlisting_terms:\n   type: \"STANDARD\"\ntargets:\n    accounts: [\"Org1.Account1\"]\nusage_examples:\n    - title: \"this is a test sql\"\n      description: \"Simple example\"\n      query: \"select *\"\n$$\n;"
        },
        {
            "code": "CREATE EXTERNAL LISTING MYLISTING\nSHARE MySHARE AS\n$$\ntitle: \"MyListing\"\nsubtitle: \"Subtitle for MyListing\"\ndescription: \"Description for MyListing\"\nlisting_terms:\n  type: \"OFFLINE\"\ntargets:\n   regions: [\"PUBLIC.AWS_US_EAST_1\", \"PUBLIC.AZURE_WESTUS2\"]\nusage_examples:\n   - title: \"this is a test sql\"\n     description: \"Simple example\"\n     query: \"select *\"\n$$ PUBLISH=FALSE REVIEW=FALSE;"
        },
        {
            "code": "CREATE EXTERNAL LISTING MYLISTING\nSHARE MySHARE FROM @dbforstage.public.listingstage/listingmanifests;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the listing identifier (name). It must conform to the following: Must be unique within an organization, regardless of which Snowflake Region the account is located in. Must start with an alphabetic character and cannot contain spaces or special characters except for underscores ( _ )."
        },
        {
            "name": "SHARE   share_name",
            "description": "Specifies the identifier for the share to attach to the listing."
        },
        {
            "name": "APPLICATION   PACKAGE   package_name",
            "description": "Specifies the application package attached to the listing. See also SHOW APPLICATION PACKAGES ."
        },
        {
            "name": "AS   ' yaml_manifest_string '",
            "description": "Specifies the YAML manifest for the listing. For manifest parameters, see Listing manifest reference . Manifests are normally provided as dollar quoted strings.\nFor more information, see Dollar-quoted string constants ."
        },
        {
            "name": "FROM   ' yaml_manifest_stage_location '",
            "description": "Specifies the path for the internal stage or Git repository clone manifest.yml file."
        },
        {
            "name": "PUBLISH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies how the listing should be published. If TRUE, listing is published immediately on listing to Marketplace Ops for review. Default: TRUE."
        },
        {
            "name": "REVIEW   =    {   TRUE   |   FALSE   }",
            "description": "Specifies whether the listing should or should not submitted to Marketplace Ops review. Default: TRUE."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "A comment for the listing. Default: No value"
        }
    ],
    "usage_notes": "Listings created using CREATE LISTING … are automatically published. For information about unpublish and publish operations, see ALTER LISTING."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool",
    "title": "CREATE COMPUTE POOL",
    "description": "Creates a new compute pool in the current account.",
    "syntax": "CREATE COMPUTE POOL [ IF NOT EXISTS ] <name>\n  [ FOR APPLICATION <app-name> ]\n  MIN_NODES = <num>\n  MAX_NODES = <num>\n  INSTANCE_FAMILY = <instance_family_name>\n  [ AUTO_RESUME = { TRUE | FALSE } ]\n  [ INITIALLY_SUSPENDED = { TRUE | FALSE } ]\n  [ AUTO_SUSPEND_SECS = <num>  ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE COMPUTE POOL tutorial_compute_pool\n  MIN_NODES = 1\n  MAX_NODES = 1\n  INSTANCE_FAMILY = CPU_X64_XS;"
        },
        {
            "code": "CREATE COMPUTE POOL tutorial_compute_pool\n  MIN_NODES = 1\n  MAX_NODES = 1\n  INSTANCE_FAMILY = CPU_X64_XS\n  AUTO_RESUME = FALSE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (that is, the name) for the compute pool; it must be unique for your account. Quoted names for special characters or case-sensitive names are not supported."
        },
        {
            "name": "MIN_NODES   =   num",
            "description": "Specifies the minimum number of nodes for the compute pool. This value must be greater than 0. For more information, see Creating a compute pool ."
        },
        {
            "name": "MAX_NODES   =   num",
            "description": "Specifies the maximum number of nodes for the compute pool."
        },
        {
            "name": "INSTANCE_FAMILY   =   instance_family_name",
            "description": "Identifies the type of machine you want to provision for the nodes in the compute pool.  The machine type determines the amount\nof compute resources in the compute pool and, therefore, the number of credits consumed while the compute pool is running. The INSTANCE_FAMILY values in the following table can be grouped into 3 categories: Generic instance types: Provide a balance of CPU, memory and disk. This does not include GPU. These instance family names\nstart with “CPU”. High memory instance types: Similar to generic instance types, but these provide more memory. These instance family\nnames start with “HighMemory”. Instance types with GPU attached: These instance family names start with “GPU”. INSTANCE_FAMILY, Snowflake Service Consumption Table Mapping vCPU Memory (GiB) Storage (GB) Bandwidth limit (Gbps) GPU GPU Memory per GPU (GiB) Node limit Description CPU_X64_XS, . CPU | XS 1 6 100 Up to 12.5 n/a n/a 50 Smallest instance available for Snowpark Containers. Ideal for cost-savings and getting started. CPU_X64_S, . CPU | S 3 13 100 Up to 12.5 n/a n/a 50 Ideal for hosting multiple services/jobs while saving cost. CPU_X64_M, . CPU | M 6 28 100 Up to 12.5 n/a n/a 50 Ideal for having a full stack application or multiple services CPU_X64_L, . CPU | L 28 116 100 12.5 n/a n/a 50 For applications which need an unusually large number of CPUs, memory and Storage. HIGHMEM_X64_S, . High-Memory CPU | S 6 58 100 AWS and GCP: Up to 12.5, Azure: 8 n/a n/a 50 For memory intensive applications. HIGHMEM_X64_M, . High-Memory CPU | M . (Azure only) 28 AWS: 240, Azure and GCP: 244 100 AWS: 12.5, Azure and GCP: 16 n/a n/a 50 For hosting multiple memory intensive applications on a single machine. HIGHMEM_X64_L, . High-Memory CPU | L . (AWS only) 124 984 100 50 n/a n/a 20 Largest AWS high-memory machine available for processing large in-memory data. HIGHMEM_X64_SL, . High-Memory CPU | L . (Azure and GCP only) 92 654 100 32 n/a n/a 20 Largest Azure high-memory machine available for processing large in-memory data. GPU_NV_S, . GPU | S . (AWS only, except Singapore, Switzerland North, Paris, and Osaka regions) 6 27 300 (NVMe) Up to 10 1 NVIDIA A10G 24 10 Our smallest NVIDIA GPU size available for Snowpark Containers to get started. GPU_NV_M, . GPU | M . (AWS only, except Singapore, Switzerland North, Paris, and Osaka regions) 44 178 3.4 TB (NVMe) 40 4 NVIDIA A10G 24 10 Optimized for intensive GPU usage scenarios like Computer Vision or LLMs/VLMs. GPU_NV_L, . GPU | L . (AWS only, available only in AWS US West and US East regions by request; limited availability might be possible in other regions upon request) 92 1112 6.8 TB (NVMe) 400 8 NVIDIA A100 40 On request Largest GPU instance for specialized and advanced GPU cases like LLMs and Clustering, etc. GPU_NV_XS, . GPU | XS . (Azure only, except Switzerland North, UAE North, Central US, and UK South regions) 3 26 100 8 1 NVIDIA T4 16 10 Our smallest Azure NVIDIA GPU size available for Snowpark Containers to get started. GPU_NV_SM, . GPU | SM . (Azure only, except Central US region) 32 424 100 40 1 NVIDIA A10 24 10 A smaller Azure NVIDIA GPU size available for Snowpark Containers to get started. GPU_NV_2M, . GPU | 2M . (Azure only, except Central US region) 68 858 100 80 2 NVIDIA A10 24 5 Optimized for intensive GPU usage scenarios like Computer Vision or LLMs/VLMs. GPU_NV_3M, . GPU | 3M . (Azure only, except Central US, North Europe, and UAE North regions) 44 424 100 40 2 NVIDIA A100 80 On request Optimized for memory-intensive GPU usage scenarios like Computer Vision or LLMs/VLMs. GPU_NV_SL, . GPU | SL . (Azure only, except Central US, North Europe, and UAE North regions) 92 858 100 80 4 NVIDIA A100 80 On request Largest GPU instance for specialized and advanced GPU cases like LLMs and Clustering, etc. GPU_GCP_NV_L4_1_24G (Google Cloud only) 6 28 300 Up to 16 1 NVIDIA L4 24 10 Our smallest NVIDIA GPU size available for Snowpark Containers to get started. GPU_GCP_NV_L4_4_24G (Google Cloud only) 44 178 1200 Up to 50 4 NVIDIA L4 24 10 GPU usage scenarios like Computer Vision or LLMs. GPU_GCP_NV_A100_8_40G (Google Cloud only, available only in GCP US Central1 and Europe West4 regions by request) 92 654 2500 Up to 100 8 NVIDIA A100 40 On request Optimized for memory-intensive GPU usage scenarios like Computer Vision or LLMs/VLMs. Note the following: The consumption table link in the first column heading provides information about the credit consumption rate for the specific INSTANCE_FAMILY . The Node limit column indicates the maximum number of nodes a Snowflake account can provision for the specific INSTANCE_FAMILY type. Contact your account representative to increase the limit."
        },
        {
            "name": "FOR   APPLICATION   app_name",
            "description": "Specifies the Snowflake Native App name. If specified, the compute pool can only be used by the native app. The SHOW COMPUTE POOLS command output includes the is_exclusive and application columns to indicate whether the compute pool is created exclusively for an app and provides the app name."
        },
        {
            "name": "AUTO_RESUME   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically resume a compute pool when a service or job is submitted to it. If AUTO_RESUME is FALSE, you need to explicitly resume the compute pool (using ALTER COMPUTE POOL RESUME) before you can\nstart a service or job on the compute pool. If AUTO_RESUME is TRUE, if you start a new service on a suspended compute pool, Snowflake starts the compute pool. Similarly,\nwhen you use a service either by invoking a service function or accessing ingress (see Using a service ), Snowflake starts the previously suspended compute pool and resumes\nthe service. Default: TRUE"
        },
        {
            "name": "INITIALLY_SUSPENDED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether the compute pool is created initially in the suspended state. If you create a compute pool with\nINITIALLY_SUSPENDED set to TRUE, Snowflake will not provision any nodes requested for the compute pool at the compute pool\ncreation time. You can start the  suspended compute pool using ALTER COMPUTE POOL … RESUME . Default: FALSE"
        },
        {
            "name": "AUTO_SUSPEND_SECS   =   num",
            "description": "Number of seconds of inactivity after which you want Snowflake to automatically suspend the compute pool. An inactive compute\npool is one in which no services or jobs are currently active on any node in the pool. If auto_suspend_secs is set to 0,\nSnowflake does not suspend the compute pool automatically. Default: 3600 seconds"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the compute pool. Default: No value"
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-function",
    "title": "CREATE FUNCTION",
    "description": "Creates a new UDF (user-defined function). Depending on how you configure it, the function can\nreturn either scalar results or tabular results.",
    "syntax": "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <java_jdk_version> ]\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name_and_version>' [ , ... ] ) ]\n  HANDLER = '<path_to_method>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <java_jdk_version> ]\n  [ COMMENT = '<string_literal>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  HANDLER = '<path_to_method>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE JAVASCRIPT\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ COMMENT = '<string_literal>' ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] [ AGGREGATE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE PYTHON\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  RUNTIME_VERSION = <python_version>\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name>[==<version>]' [ , ... ] ) ]\n  [ ARTIFACT_REPOSITORY = '<repository_name>' ]\n  HANDLER = '<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] [ AGGREGATE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  LANGUAGE PYTHON\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  RUNTIME_VERSION = <python_version>\n  [ COMMENT = '<string_literal>' ]\n  [ ARTIFACT_REPOSITORY = '<repository_name>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  [ PACKAGES = ( '<package_name>[==<version>]' [ , ... ] ) ]\n  HANDLER = '<module_file_name>.<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  LANGUAGE SCALA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <scala_version> ]\n  [ COMMENT = '<string_literal>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ PACKAGES = ( '<package_name_and_version>' [ , ... ] ) ]\n  HANDLER = '<path_to_method>'\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  AS '<function_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION [ IF NOT EXISTS ] <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  LANGUAGE SCALA\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ RUNTIME_VERSION = <scala_version> ]\n  [ COMMENT = '<string_literal>' ]\n  IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] )\n  HANDLER = '<path_to_method>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] FUNCTION <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( <col_name> <col_data_type> [ , ... ] ) }\n  [ [ NOT ] NULL ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ MEMOIZABLE ]\n  [ COMMENT = '<string_literal>' ]\n  AS '<function_definition>'",
    "examples": [
        {
            "code": "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE JAVA\n  CALLED ON NULL INPUT\n  HANDLER = 'TestFunc.echoVarchar'\n  TARGET_PATH = '@~/testfunc.jar'\n  AS\n  'class TestFunc {\n    public static String echoVarchar(String x) {\n      return x;\n    }\n  }';"
        },
        {
            "code": "create function my_decrement_udf(i numeric(9, 0))\n    returns numeric\n    language java\n    imports = ('@~/my_decrement_udf_package_dir/my_decrement_udf_jar.jar')\n    handler = 'my_decrement_udf_package.my_decrement_udf_class.my_decrement_udf_method'\n    ;"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION js_factorial(d double)\n  RETURNS double\n  LANGUAGE JAVASCRIPT\n  STRICT\n  AS '\n  if (D <= 0) {\n    return 1;\n  } else {\n    var result = 1;\n    for (var i = 2; i <= D; i++) {\n      result = result * i;\n    }\n    return result;\n  }\n  ';"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION py_udf()\n  RETURNS VARIANT\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.10'\n  PACKAGES = ('numpy','pandas','xgboost==1.5.0')\n  HANDLER = 'udf'\nAS $$\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\ndef udf():\n    return [np.__version__, pd.__version__, xgb.__version__]\n$$;"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION dream(i int)\n  RETURNS VARIANT\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.10'\n  HANDLER = 'sleepy.snore'\n  IMPORTS = ('@my_stage/sleepy.py')"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE SCALA\n  RUNTIME_VERSION = 2.12\n  HANDLER='Echo.echoVarchar'\n  AS\n  $$\n  class Echo {\n    def echoVarchar(x : String): String = {\n      return x\n    }\n  }\n  $$;"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION echo_varchar(x VARCHAR)\n  RETURNS VARCHAR\n  LANGUAGE SCALA\n  RUNTIME_VERSION = 2.12\n  IMPORTS = ('@udf_libs/echohandler.jar')\n  HANDLER='Echo.echoVarchar';"
        },
        {
            "code": "CREATE FUNCTION pi_udf()\n  RETURNS FLOAT\n  AS '3.141592654::FLOAT'\n  ;"
        },
        {
            "code": "CREATE FUNCTION simple_table_function ()\n  RETURNS TABLE (x INTEGER, y INTEGER)\n  AS\n  $$\n    SELECT 1, 2\n    UNION ALL\n    SELECT 3, 4\n  $$\n  ;"
        },
        {
            "code": "SELECT * FROM TABLE(simple_table_function());"
        },
        {
            "code": "SELECT * FROM TABLE(simple_table_function());\n+---+---+\n| X | Y |\n|---+---|\n| 1 | 2 |\n| 3 | 4 |\n+---+---+"
        },
        {
            "code": "CREATE FUNCTION multiply1 (a number, b number)\n  RETURNS number\n  COMMENT='multiply two numbers'\n  AS 'a * b';"
        },
        {
            "code": "CREATE OR REPLACE FUNCTION get_countries_for_user ( id NUMBER )\n  RETURNS TABLE (country_code CHAR, country_name VARCHAR)\n  AS 'SELECT DISTINCT c.country_code, c.country_name\n      FROM user_addresses a, countries c\n      WHERE a.user_id = id\n      AND c.country_code = a.country_code';"
        },
        {
            "code": "CREATE OR ALTER FUNCTION multiply(a NUMBER, b NUMBER)\n  RETURNS NUMBER\n  AS 'a * b';"
        },
        {
            "code": "CREATE OR ALTER SECURE FUNCTION multiply(a NUMBER, b NUMBER)\n  RETURNS NUMBER\n  COMMENT = 'Multiply two numbers.'\n  AS 'a * b';"
        }
    ],
    "parameters": [
        {
            "name": "name   (   [   arg_name   arg_data_type   [   DEFAULT   default_value   ]   ]   [   ,   ...   ]   )",
            "description": "Specifies the identifier ( name ), any input arguments, and the default values for any optional arguments for the UDF. For the identifier: The identifier does not need to be unique for the schema in which the function is created because UDFs are identified and resolved by the combination of the name and argument types . The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, “My object”). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements . For the input arguments: For arg_name , specify the name of the input argument. For arg_data_type , use the Snowflake data type that corresponds to the handler language that you are using. For Java handlers , see SQL-Java Data Type Mappings . For JavaScript handlers , see SQL and JavaScript data type mapping . For Python handlers , see SQL-Python Data Type Mappings . For Scala handlers , see SQL-Scala Data Type Mappings . To indicate that an argument is optional, use DEFAULT default_value to specify the default value of the argument.\nFor the default value, you can use a literal or an expression. If you specify any optional arguments, you must place these after the required arguments. If a function has optional arguments, you cannot define additional functions with the same name and different signatures. For details, see Specify optional arguments ."
        },
        {
            "name": "RETURNS   ...",
            "description": "Specifies the results returned by the UDF, which determines the UDF type: result_data_type : Creates a scalar UDF that returns a single value with the specified data type. Note For UDF handlers written in Java, Python, or Scala, the result_data_type must be in the SQL Data Type column of the\nfollowing table corresponding to the handler language: SQL-Java Type Mappings table SQL-Python Type Mappings table SQL-Scala Type Mappings table TABLE ( col_name col_data_type , ... ) : Creates a table UDF that returns tabular results with the specified table column(s)\nand column type(s). Note For Scala UDFs, the TABLE return type is not supported."
        },
        {
            "name": "AS   function_definition",
            "description": "Defines the handler code executed when the UDF is called. The function_definition value must be source code in one of the\nlanguages supported for handlers. The code may be: Java. For more information, see Introduction to Java UDFs . JavaScript. For more information, see Introduction to JavaScript UDFs . Python. For more information, see Introduction to Python UDFs . Scala. For more information, see Introduction to Scala UDFs . A SQL expression. For more information, see Introduction to SQL UDFs . For more details, see General usage notes (in this topic). Note The AS clause is not required when the UDF handler code is referenced on a stage with the IMPORTS clause."
        },
        {
            "name": "LANGUAGE   JAVA",
            "description": "Specifies that the code is in the Java language."
        },
        {
            "name": "RUNTIME_VERSION   =   java_jdk_version",
            "description": "Specifies the Java JDK runtime version to use. The supported versions of Java are: 11.x 17.x If RUNTIME_VERSION is not set, Java JDK 11 is used."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import. A file can be a JAR file or another type of file. If the file is a JAR file, it can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode). Java UDFs can also read non-JAR files. For an example, see Reading a file specified statically in IMPORTS . If you plan to copy a file (JAR file or other file) to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file\nto a stage. External stages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom each file name in the IMPORTS clause, even if the files are in different subdirectories or different stages. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an existing file. For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF. For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files. For Snowflake system packages, such the Snowpark package ,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value. In-line Java In-line Java UDFs require a function definition ."
        },
        {
            "name": "HANDLER   =   handler_name",
            "description": "The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod . If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class."
        },
        {
            "name": "AS   function_definition",
            "description": "In-line Java UDFs require a function definition ."
        },
        {
            "name": "LANGUAGE   JAVASCRIPT",
            "description": "Specifies that the code is in the JavaScript language."
        },
        {
            "name": "LANGUAGE   PYTHON",
            "description": "Specifies that the code is in the Python language."
        },
        {
            "name": "RUNTIME_VERSION   =   python_version",
            "description": "Specifies the Python version to use. The supported versions of Python are: 3.9 3.10 3.11 3.12"
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import. A file can be a .py file or another type of file. Python UDFs can also read non-Python files, such as text files. For an example, see Reading a file . If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the\nPUT command supports copying files to named internal stages, and the PUT command is usually the easiest way to move a file\nto a stage. External stages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. When the handler code is stored in a stage, you must use the IMPORTS clause to specify the handler code’s location. For an in-line Python UDF, the IMPORTS clause is needed only if the UDF handler needs to access other files, such as\npackages or text files. For packages included on the Snowflake system, such numpy ,\nyou can specify the package with the PACKAGES clause alone, omitting the package’s source as an IMPORTS value."
        },
        {
            "name": "HANDLER   =   handler_name",
            "description": "The name of the handler function or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a function name. If the handler code\nis in-line with the CREATE FUNCTION statement, you can use the function name alone. When the handler code is referenced at a stage, this\nvalue should be qualified with the module name, as in the following form: my_module.my_function . If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class."
        },
        {
            "name": "LANGUAGE   SCALA",
            "description": "Specifies that the code is in the Scala language."
        },
        {
            "name": "RUNTIME_VERSION   =   scala_version",
            "description": "Specifies the Scala runtime version to use. The supported versions of Scala are: 2.12 If RUNTIME_VERSION is not set, Scala 2.12 is used."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import, such as a JAR or other kind of file. The JAR file might contain handler dependency libraries. It can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java\nbytecode). A non-JAR file might a file read by handler code. For an example, see Reading a file specified statically in IMPORTS . If you plan to copy a file to a stage, then Snowflake recommends using a named internal stage because the PUT command supports\ncopying files to named internal stages, and the PUT command is usually the easiest way to move a JAR file to a stage. External\nstages are allowed, but are not supported by PUT. Each file in the IMPORTS clause must have a unique name, even if the files are in different stage subdirectories or different stages. If both the IMPORTS and TARGET_PATH clauses are present, the file name in the TARGET_PATH clause must be different\nfrom that of any file listed in the IMPORTS clause, even if the files are in different stage subdirectories or different stages. For a UDF whose handler is on a stage, the IMPORTS clause is required because it specifies the location of the JAR file that\ncontains the UDF. For UDF whose handler code is in-line, the IMPORTS clause is needed only if the in-line UDF needs to access other files, such as\nlibraries or text files. For Snowflake system packages, such the Snowpark package ,\nyou can specify the package with the PACKAGES clause rather than specifying its JAR file with IMPORTS. When you do, the package\nJAR file need not be included in an IMPORTS value. In-line Scala UDFs with in-line Scala handler code require a function definition ."
        },
        {
            "name": "HANDLER   =   handler_name",
            "description": "The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following\nform: MyClass.myMethod ."
        },
        {
            "name": "AS   function_definition",
            "description": "UDFs with in-line Scala handler code require a function definition ."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the function is secure. For more information about secure functions, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   }",
            "description": "Specifies that the function persists only for the duration of the session that you created it in. A\ntemporary function is dropped at the end of the session. Default: No value. If a function is not declared as TEMPORARY , the function is permanent. You cannot create temporary user-defined functions that have the same name as a function that already\nexists in the schema."
        },
        {
            "name": "[   [   NOT   ]   NULL   ]",
            "description": "Specifies whether the function can return NULL values or must return only NON-NULL values. The default is NULL (i.e. the function can\nreturn NULL). Note Currently, the NOT NULL clause is not enforced for SQL UDFs.\nSQL UDFs declared as NOT NULL can return NULL values. Snowflake recommends avoiding NOT NULL for SQL UDFs unless the code in the function is written to ensure that NULL values are never returned."
        },
        {
            "name": "CALLED   ON   NULL   INPUT   or   .   {   RETURNS   NULL   ON   NULL   INPUT   |   STRICT   }",
            "description": "Specifies the behavior of the UDF when called with null inputs. In contrast to system-defined functions, which always return null when any\ninput is null, UDFs can handle null inputs, returning non-null values even when an input is null: CALLED ON NULL INPUT will always call the UDF with null inputs. It is up to the UDF to handle such values appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT ) will not call the UDF if any input is null. Instead, a null value\nwill always be returned for that row. Note that the UDF might still return null for non-null inputs. Note RETURNS NULL ON NULL INPUT ( STRICT ) is not supported for SQL UDFs. SQL UDFs effectively use CALLED ON NULL INPUT . In your SQL UDFs, you must handle null input values. Default: CALLED ON NULL INPUT"
        },
        {
            "name": "{   VOLATILE   |   IMMUTABLE   }",
            "description": "Specifies the behavior of the UDF when returning results: VOLATILE : UDF might return different values for different rows, even for the same input (e.g. due to non-determinism and\nstatefullness). IMMUTABLE : UDF assumes that the function, when called with the same inputs, will always return the same result. This guarantee\nis not checked. Specifying IMMUTABLE for a UDF that returns different values for the same input will result in undefined\nbehavior. Default: VOLATILE Note IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the UDF, which is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output. Default: user-defined function"
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original function when a new function is created using CREATE OR REPLACE FUNCTION. The parameter copies all privileges, except OWNERSHIP, from the existing function to the new function. The new function will\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE FUNCTION\nstatement owns the new function. Note: With data sharing , if the existing function was shared to another account, the replacement function is\nalso shared. The SHOW GRANTS output for the replacement function lists the grantee for the copied privileges as the\nrole that executed the CREATE FUNCTION statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE FUNCTION command (i.e. within the same transaction)."
        },
        {
            "name": "PACKAGES   =   (   ' package_name_and_version '   [   ,   ...   ]   )",
            "description": "The name and version number of Snowflake system packages required as dependencies. The value should be of the form package_name : version_number , where package_name is snowflake_domain : package . Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause. In-line Java Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nfunction’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "AGGREGATE",
            "description": "Specifies that the function is an aggregate function. For more information about user-defined aggregate functions, see Python user-defined aggregate functions . Preview Feature — Open Using Python to write a handler for a user-defined aggregate function (UDAF) is a preview feature that is available to all accounts. Note IMMUTABLE is not supported on an aggregate function (when you use the AGGREGATE parameter). Therefore, all aggregate functions are\nVOLATILE by default."
        },
        {
            "name": "PACKAGES   =   (   ' package_name_and_version '   [   ,   ...   ]   )",
            "description": "The name and version number of packages required as dependencies. The value should be of the form package_name == version_number . If you omit the version number, Snowflake will use the latest package available on the\nsystem. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For more information about included packages, see Using third-party packages . Preview Feature — Open Specifying a range of Python package versions is available as a preview feature to all accounts. You can specify package versions by using these version\nspecifiers: == , <= , >= , < ,or > . For example:"
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nfunction’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "MEMOIZABLE",
            "description": "Specifies that the function is memoizable. For details, see Memoizable UDFs ."
        },
        {
            "name": "PACKAGES   =   (   ' package_name_and_version '   [   ,   ...   ]   )",
            "description": "The name and version number of Snowflake system packages required as dependencies. The value should be of the form package_name : version_number , where package_name is snowflake_domain : package . Note that you can\nspecify latest as the version number in order to have Snowflake use the latest version available on the system. For example: You can discover the list of supported system packages by executing the following SQL in Snowflake: For a dependency you specify with PACKAGES, you do not need to also specify its JAR file in an IMPORTS clause. Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the function_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. The generated JAR file remains until you explicitly delete it, even if you drop the function. When you drop the UDF you should\nseparately remove the JAR file because the JAR is no longer needed to support the UDF. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this UDF to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-secret",
    "title": "CREATE SECRET",
    "description": "Creates a new secret in the current or specified schema or replaces an existing secret.",
    "syntax": "CREATE [ OR REPLACE ] SECRET [ IF NOT EXISTS ] <name>\n  TYPE = OAUTH2\n  API_AUTHENTICATION = <security_integration_name>\n  OAUTH_SCOPES = ( '<scope_1>' [ , '<scope_2>' ... ] )\n  [ COMMENT = '<string_literal>' ]\n\nCREATE [ OR REPLACE ] SECRET [ IF NOT EXISTS ] <name>\n  TYPE = OAUTH2\n  OAUTH_REFRESH_TOKEN = '<string_literal>'\n  OAUTH_REFRESH_TOKEN_EXPIRY_TIME = '<string_literal>'\n  API_AUTHENTICATION = <security_integration_name>;\n  [ COMMENT = '<string_literal>' ]\n\nCREATE [ OR REPLACE ] SECRET [ IF NOT EXISTS ] <name>\n  TYPE = CLOUD_PROVIDER_TOKEN\n  API_AUTHENTICATION = '<cloud_provider_security_integration>'\n  ENABLED = { TRUE | FALSE }\n  [ COMMENT = '<string_literal>' ]\n\nCREATE [ OR REPLACE ] SECRET [ IF NOT EXISTS ] <name>\n  TYPE = PASSWORD\n  USERNAME = '<username>'\n  PASSWORD = '<password>'\n  [ COMMENT = '<string_literal>' ]\n\nCREATE [ OR REPLACE ] SECRET [ IF NOT EXISTS ] <name>\n  TYPE = GENERIC_STRING\n  SECRET_STRING = '<string_literal>'\n  [ COMMENT = '<string_literal>' ]\n\nCREATE [ OR REPLACE ] SECRET [ IF NOT EXISTS ] <name>\nTYPE = SYMMETRIC_KEY\nALGORITHM = GENERIC",
    "examples": [
        {
            "code": "CREATE OR REPLACE SECRET mysecret\n  TYPE = OAUTH2\n  API_AUTHENTICATION = mysecurityintegration\n  OAUTH_SCOPES = ('useraccount')\n  COMMENT = 'secret for the service now connector'"
        },
        {
            "code": "CREATE SECRET service_now_creds_oauth_code\n  TYPE = OAUTH2\n  OAUTH_REFRESH_TOKEN = '34n;vods4nQsdg09wee4qnfvadH'\n  OAUTH_REFRESH_TOKEN_EXPIRY_TIME = '2022-01-06 20:00:00'\n  API_AUTHENTICATION = sn_oauth;"
        },
        {
            "code": "CREATE SECRET aws_secret\n  TYPE = CLOUD_PROVIDER_TOKEN\n  API_AUTHENTICATION = myawsiamintegration\n  ENABLED = TRUE;"
        },
        {
            "code": "CREATE SECRET service_now_creds_pw\n  TYPE = password\n  USERNAME = 'jsmith1'\n  PASSWORD = 'W3dr@fg*7B1c4j';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TYPE   =   OAUTH2",
            "description": "Specifies a secret to use with an OAuth grant flow."
        },
        {
            "name": "API_AUTHENTICATION   =   security_integration_name",
            "description": "Specifies the name value of the Snowflake security integration that connects Snowflake to an external service."
        },
        {
            "name": "OAUTH_SCOPES   =   (   ' scope_1 '   [   ,   ' scope_2 '   ...   ]   )",
            "description": "Specifies a comma-separated list of scopes to use when making a request from the OAuth server by a role with USAGE on the integration\nduring the OAuth client credentials flow. This list must be a subset of the scopes defined in the OAUTH_ALLOWED_SCOPES property of the security integration. If the OAUTH_SCOPES property values are not specified, the secret inherits all of the scopes that are specified in the security\nintegration. For the ServiceNow connector, the only possible scope value is 'useraccount' ."
        },
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TYPE   =   OAUTH2",
            "description": "Specifies a secret to use with the OAuth grant flow."
        },
        {
            "name": "OAUTH_REFRESH_TOKEN   =   ' string_literal '",
            "description": "Specifies the token as a string that is used to obtain a new access token from the OAuth authorization server when the access token\nexpires."
        },
        {
            "name": "OAUTH_REFRESH_TOKEN_EXPIRY_TIME   =   ' string_literal '",
            "description": "Specifies the timestamp as a string when the OAuth refresh token expires."
        },
        {
            "name": "API_AUTHENTICATION   =   security_integration_name",
            "description": "Specifies the name value of the Snowflake security integration that connects Snowflake to an external service."
        },
        {
            "name": "TYPE   =   CLOUD_PROVIDER_TOKEN",
            "description": "Specifies that this is secret for use with a cloud provider, such as Amazon Web Services (AWS)."
        },
        {
            "name": "API_AUTHENTICATION   =   ' cloud_provider_security_integration '",
            "description": "Specifies the name value of the Snowflake security integration that connects Snowflake to a cloud provider."
        },
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TYPE   =   PASSWORD",
            "description": "Specifies a secret to use with basic authentication. When specifying this type you must specify values for the username and password properties."
        },
        {
            "name": "USERNAME   =   ' username '",
            "description": "Specifies the username value to store in the secret. Specify this value when setting the TYPE value to PASSWORD for use with basic authentication."
        },
        {
            "name": "PASSWORD   =   ' password '",
            "description": "Specifies the password value to store in the secret. Specify this value when setting the TYPE value to PASSWORD for use with basic authentication."
        },
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TYPE   =   GENERIC_STRING",
            "description": "Specifies a secret to store a sensitive string value."
        },
        {
            "name": "SECRET_STRING   =   ' string_literal '",
            "description": "Specifies the string to store in the secret. The string can be an API token or a string of sensitive value that can be used in the handler code of a UDF or stored procedure. For\ndetails, see Creating and using an external access integration . You should not use this property to store any kind of OAuth token; use one of the other secret types for your OAuth use cases."
        },
        {
            "name": "ALGORITHM",
            "description": "Specifies which algorithm to use to generate the symmetric key. The only value supported is GENERIC , which generates a 256-bit key."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the secret. Default: No value"
        }
    ],
    "usage_notes": "The OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-role",
    "title": "CREATE ROLE",
    "description": "Create a new role or replace an existing role in the system.",
    "syntax": "CREATE [ OR REPLACE ] ROLE [ IF NOT EXISTS ] <name>\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]",
    "examples": [
        {
            "code": "CREATE ROLE myrole;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the role; must be unique for your account. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the role. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-schema",
    "title": "CREATE SCHEMA",
    "description": "Creates a new schema in the current database.",
    "syntax": "CREATE [ OR REPLACE ] [ TRANSIENT ] SCHEMA [ IF NOT EXISTS ] <name>\n  [ CLONE <source_schema>\n      [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n      [ IGNORE TABLES WITH INSUFFICIENT DATA RETENTION ]\n      [ IGNORE HYBRID TABLES ] ]\n  [ WITH MANAGED ACCESS ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ EXTERNAL_VOLUME = <external_volume_name> ]\n  [ CATALOG = <catalog_integration_name> ]\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n  [ CLASSIFICATION_PROFILE = '<classification_profile>' ]\n  [ COMMENT = '<string_literal>' ]\n  [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]",
    "examples": [
        {
            "code": "CREATE SCHEMA myschema;\n\nSHOW SCHEMAS;\n\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+---------+----------------+\n| created_on                    | name               | is_default | is_current | database_name | owner        | comment                                                   | options | retention_time |\n|-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+---------+----------------|\n| 2018-12-10 09:34:02.127 -0800 | INFORMATION_SCHEMA | N          | N          | MYDB          |              | Views describing the contents of schemas in this database |         | 1              |\n| 2018-12-10 09:33:56.793 -0800 | MYSCHEMA           | N          | Y          | MYDB          | PUBLIC       |                                                           |         | 1              |\n| 2018-11-26 06:08:24.263 -0800 | PUBLIC             | N          | N          | MYDB          | PUBLIC       |                                                           |         | 1              |\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+---------+----------------+"
        },
        {
            "code": "CREATE TRANSIENT SCHEMA tschema;\n\nSHOW SCHEMAS;\n\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+-----------+----------------+\n| created_on                    | name               | is_default | is_current | database_name | owner        | comment                                                   | options   | retention_time |\n|-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+-----------+----------------|\n| 2018-12-10 09:34:02.127 -0800 | INFORMATION_SCHEMA | N          | N          | MYDB          |              | Views describing the contents of schemas in this database |           | 1              |\n| 2018-12-10 09:33:56.793 -0800 | MYSCHEMA           | N          | Y          | MYDB          | PUBLIC       |                                                           |           | 1              |\n| 2018-11-26 06:08:24.263 -0800 | PUBLIC             | N          | N          | MYDB          | PUBLIC       |                                                           |           | 1              |\n| 2018-12-10 09:35:32.326 -0800 | TSCHEMA            | N          | Y          | MYDB          | PUBLIC       |                                                           | TRANSIENT | 1              |\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+-----------+----------------+"
        },
        {
            "code": "CREATE SCHEMA mschema WITH MANAGED ACCESS;\n\nSHOW SCHEMAS;\n\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+----------------+----------------+\n| created_on                    | name               | is_default | is_current | database_name | owner        | comment                                                   | options        | retention_time |\n|-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+----------------+----------------|\n| 2018-12-10 09:34:02.127 -0800 | INFORMATION_SCHEMA | N          | N          | MYDB          |              | Views describing the contents of schemas in this database |                | 1              |\n| 2018-12-10 09:36:47.738 -0800 | MSCHEMA            | N          | Y          | MYDB          | ROLE1        |                                                           | MANAGED ACCESS | 1              |\n| 2018-12-10 09:33:56.793 -0800 | MYSCHEMA           | N          | Y          | MYDB          | PUBLIC       |                                                           |                | 1              |\n| 2018-11-26 06:08:24.263 -0800 | PUBLIC             | N          | N          | MYDB          | PUBLIC       |                                                           |                | 1              |\n| 2018-12-10 09:35:32.326 -0800 | TSCHEMA            | N          | Y          | MYDB          | PUBLIC       |                                                           | TRANSIENT      | 1              |\n+-------------------------------+--------------------+------------+------------+---------------+--------------+-----------------------------------------------------------+----------------+----------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the schema; must be unique for the database in which the schema is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TRANSIENT",
            "description": "Specifies a schema as transient. Transient schemas do not have a Fail-safe period so they do not incur additional storage costs once\nthey leave Time Travel; however, this means they are also not protected by Fail-safe in the event of a data loss. For more information,\nsee Understanding and viewing Fail-safe . In addition, by definition, all tables created in a transient schema are transient. For more information about transient tables, see CREATE TABLE . Default: No value (i.e. schema is permanent)"
        },
        {
            "name": "CLONE   source_schema",
            "description": "Specifies to create a clone of the specified source schema. For more details about cloning a schema, see CREATE <object> … CLONE ."
        },
        {
            "name": "AT   |   BEFORE   (   TIMESTAMP   =>   timestamp   |   OFFSET   =>   time_difference   |   STATEMENT   =>   id   )",
            "description": "When cloning a schema, the AT | BEFORE clause specifies to use Time Travel to clone the schema at or\nbefore a specific point in the past."
        },
        {
            "name": "IGNORE   TABLES   WITH   INSUFFICIENT   DATA   RETENTION",
            "description": "Ignore tables that no longer have historical data available in Time Travel to clone. If the time in the past specified in the\nAT | BEFORE clause is beyond the data retention period for any child table in a database or schema, skip the cloning operation\nfor the child table. For more information, see Child Objects and Data Retention Time ."
        },
        {
            "name": "IGNORE   HYBRID   TABLES",
            "description": "Ignore hybrid tables, which will not be cloned. Use this option to clone a schema that contains hybrid tables.\nThe cloned schema includes other objects but skips hybrid tables. If you don’t use this option and your schema contains one or more hybrid tables, the command ignores hybrid tables silently. However, the error handling for schemas that contain hybrid tables will change in an upcoming release; therefore, you may want to add this parameter to your commands preemptively."
        },
        {
            "name": "WITH   MANAGED   ACCESS",
            "description": "Specifies a managed schema. Managed access schemas centralize privilege management with the schema owner. In regular schemas, the owner of an object (i.e. the role that has the OWNERSHIP privilege on the object) can grant further privileges\non their objects to other roles. In managed schemas, the schema owner manages all privilege grants, including future grants , on objects in the schema. Object owners retain the OWNERSHIP\nprivileges on the objects; however, only the schema owner can manage privilege grants on the objects."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the schema, as well as specifying the\ndefault Time Travel retention time for all tables created in the schema. For more details, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . For more information about table-level retention time, see CREATE TABLE and Understanding & using Time Travel . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent schemas 0 or 1 for transient schemas Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the database or account level) Note A value of 0 effectively disables Time Travel for the schema."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in\nthe schema to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "EXTERNAL_VOLUME   =   external_volume_name",
            "description": "Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME ."
        },
        {
            "name": "CATALOG   =   catalog_integration_name",
            "description": "Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG ."
        },
        {
            "name": "REPLACE_INVALID_CHARACTERS   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for all tables added to the schema. The default\ncan be overridden at the individual table level. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "STORAGE_SERIALIZATION_POLICY   =   {   COMPATIBLE   |   OPTIMIZED   }",
            "description": "Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED"
        },
        {
            "name": "CLASSIFICATION_PROFILE   =   ' classification_profile '",
            "description": "Associates the schema with a classification profile so that sensitive data in the schema is automatically classified ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the schema. Default: No value"
        },
        {
            "name": "CATALOG_SYNC   =   ' snowflake_open_catalog_integration_name '",
            "description": "Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the schema with an external catalog in your Snowflake Open Catalog account.\nFor more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-connection",
    "title": "CREATE CONNECTION",
    "description": "Creates a new connection in the account.",
    "syntax": "CREATE CONNECTION [ IF NOT EXISTS ] <name>\n  [ COMMENT = '<string_literal>' ]\n\nCREATE CONNECTION [ IF NOT EXISTS ] <name>\n  AS REPLICA OF <organization_name>.<account_name>.<name>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE CONNECTION IF NOT EXISTS myconnection;"
        },
        {
            "code": "CREATE CONNECTION myconnection AS REPLICA OF myorg.myaccount1.myconnection;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the connection. It must conform to the following: Must start with an alphabetic character and may only\ncontain letters, decimal digits (0-9), and underscores (_). For a primary connection, the name must be unique across connection names and account names in the organization. For a secondary connection, the name must match the name of its primary connection."
        },
        {
            "name": "AS   REPLICA   OF   organization_name . account_name . name",
            "description": "Specifies the identifier for a primary connection from which to create a replica (i.e. a secondary connection). Specifies the identifier for the organization. Specifies the identifier for the account. Specifies the identifier for the primary connection."
        },
        {
            "name": "organization_name",
            "description": "Specifies the identifier for the organization."
        },
        {
            "name": "account_name",
            "description": "Specifies the identifier for the account."
        },
        {
            "name": "name",
            "description": "Specifies the identifier for the primary connection."
        },
        {
            "name": "AS   REPLICA   OF   organization_name . account_name . name",
            "description": "Specifies the identifier for a primary connection from which to create a replica (i.e. a secondary connection). Specifies the identifier for the organization. Specifies the identifier for the account. Specifies the identifier for the primary connection."
        },
        {
            "name": "organization_name",
            "description": "Specifies the identifier for the organization."
        },
        {
            "name": "account_name",
            "description": "Specifies the identifier for the account."
        },
        {
            "name": "name",
            "description": "Specifies the identifier for the primary connection."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the connection. Default: No value"
        }
    ],
    "usage_notes": "If private connectivity to the Snowflake service is enabled for your Snowflake account, your network manager must create and manage\na DNS CNAME record. For more details, see Configuring the DNS settings for private connectivity to the Snowflake service.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-replication-group",
    "title": "CREATE REPLICATION GROUP",
    "description": "Creates a new replication group of specified objects in the system.",
    "syntax": "CREATE REPLICATION GROUP [ IF NOT EXISTS ] <name>\n    OBJECT_TYPES = <object_type> [ , <object_type> , ... ]\n    [ ALLOWED_DATABASES = <db_name> [ , <db_name> , ... ] ]\n    [ ALLOWED_SHARES = <share_name> [ , <share_name> , ... ] ]\n    [ ALLOWED_INTEGRATION_TYPES = <integration_type_name> [ , <integration_type_name> , ... ] ]\n    ALLOWED_ACCOUNTS = <org_name>.<target_account_name> [ , <org_name>.<target_account_name> , ... ]\n    [ IGNORE EDITION CHECK ]\n    [ REPLICATION_SCHEDULE = '{ <num> MINUTE | USING CRON <expr> <time_zone> }' ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n    [ ERROR_INTEGRATION = <integration_name> ]\n\nCREATE REPLICATION GROUP [ IF NOT EXISTS ] <secondary_name>\n    AS REPLICA OF <org_name>.<source_account_name>.<name>",
    "examples": [
        {
            "code": "CREATE REPLICATION GROUP myrg\n    OBJECT_TYPES = DATABASES\n    ALLOWED_DATABASES = db1\n    ALLOWED_ACCOUNTS = myorg.myaccount2\n    REPLICATION_SCHEDULE = '10 MINUTE';"
        },
        {
            "code": "CREATE REPLICATION GROUP myrg\n    AS REPLICA OF myorg.myaccount1.myrg;"
        },
        {
            "code": "CREATE REPLICATION GROUP myrg\n    OBJECT_TYPES = DATABASES, SHARES\n    ALLOWED_DATABASES = db1\n    ALLOWED_SHARES = s1\n    ALLOWED_ACCOUNTS = myorg.myaccount2\n    REPLICATION_SCHEDULE = '10 MINUTE';"
        },
        {
            "code": "CREATE REPLICATION GROUP myrg\n    AS REPLICA OF myorg.myaccount1.myrg;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the replication group. The identifier must start with an alphabetic character and cannot contain spaces or\nspecial characters unless the identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double\nquotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "OBJECT_TYPES   =   object_type   [   ,   object_type   ,   ...   ]",
            "description": "Type(s) of objects for which you are enabling replication from the source account to the target account. The following object types are supported: Requires Business Critical Edition (or higher). All account-level parameters. This includes account parameters and parameters that can be set for your account . Add database objects to the list of object types. If database objects are included in the list of specified object types, the ALLOWED_DATABASES parameter must be set. Requires Business Critical Edition (or higher). Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set. Requires Business Critical Edition (or higher). All network policies in the source account. Requires Business Critical Edition (or higher). All resource monitors in the source account. Requires Business Critical Edition (or higher). All roles in the source account. Replicating roles implicitly includes all grants for object types included in the replication group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated. Add share objects to the list of object types. If share objects are included in the list of specified object types, the ALLOWED_SHARES parameter must be set. Requires Business Critical Edition (or higher). All users in the source account. Requires Business Critical Edition (or higher). All warehouses in the source account. Note If you replicate users and roles, programmatic access tokens for users are replicated automatically. To modify the list of replicated object types to a specified target account, use ALTER REPLICATION GROUP to reset the list of object types."
        },
        {
            "name": "ALLOWED_DATABASES   =   db_name   [   ,   db_name   ,   ...   ]",
            "description": "Specifies the database or list of databases for which you are enabling replication from the source account to the target account.\nIn order for you to set this parameter, the OBJECT_TYPES list must include DATABASES ."
        },
        {
            "name": "ALLOWED_SHARES   =   share_name   [   ,   share_name   ,   ...   ]",
            "description": "Specifies the share or list of shares for which you are enabling replication from the source account to the target account. In order\nfor you to set this parameter, the OBJECT_TYPES list must include SHARES ."
        },
        {
            "name": "ALLOWED_INTEGRATION_TYPES   =   integration_type_name   [   ,   integration_type_name   ,   ...   ]",
            "description": "Requires Business Critical Edition (or higher). Type(s) of integrations for which you are enabling replication from the source account to the target account. This property requires that the OBJECT_TYPES list include INTEGRATIONS to set this parameter. The following integration types are supported: Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES . Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations . Specifies storage integrations. Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) . Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "ALLOWED_ACCOUNTS   =   org_name . target_account_name1   [   ,   org_name . target_account_name2   ,   ...   ]",
            "description": "Specifies the target account or list of target accounts to which replication of specified objects from the source account is\nenabled. Name of your Snowflake organization. Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "IGNORE   EDITION   CHECK",
            "description": "Allows replicating objects to accounts on lower editions in either of the following scenarios: A primary replication group with only database and/or share objects is in a Business Critical (or higher) account but\none or more accounts approved for replication are on lower editions. Business Critical Edition is intended for Snowflake accounts\nwith extremely sensitive data. A primary replication group with any object type is in a Business\nCritical (or higher) account and a signed business associate agreement is in place to store PHI data in the account per HIPAA and HITRUST regulations. However, no such agreement is in place for one or more of the accounts approved\nfor replication, regardless if they are Business Critical (or higher) accounts. Both scenarios are prohibited by default in an effort to help prevent account administrators for Business Critical (or higher) accounts\nfrom inadvertently replicating sensitive data to accounts on lower editions."
        },
        {
            "name": "REPLICATION_SCHEDULE   ...",
            "description": "Specifies the schedule for refreshing secondary replication groups. Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs."
        },
        {
            "name": "ACCOUNT PARAMETERS :",
            "description": "Requires Business Critical Edition (or higher). All account-level parameters. This includes account parameters and parameters that can be set for your account ."
        },
        {
            "name": "DATABASES :",
            "description": "Add database objects to the list of object types. If database objects are included in the list of specified object types, the ALLOWED_DATABASES parameter must be set."
        },
        {
            "name": "INTEGRATIONS :",
            "description": "Requires Business Critical Edition (or higher). Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set."
        },
        {
            "name": "NETWORK POLICIES :",
            "description": "Requires Business Critical Edition (or higher). All network policies in the source account."
        },
        {
            "name": "RESOURCE MONITORS :",
            "description": "Requires Business Critical Edition (or higher). All resource monitors in the source account."
        },
        {
            "name": "ROLES :",
            "description": "Requires Business Critical Edition (or higher). All roles in the source account. Replicating roles implicitly includes all grants for object types included in the replication group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated."
        },
        {
            "name": "SHARES :",
            "description": "Add share objects to the list of object types. If share objects are included in the list of specified object types, the ALLOWED_SHARES parameter must be set."
        },
        {
            "name": "USERS :",
            "description": "Requires Business Critical Edition (or higher). All users in the source account."
        },
        {
            "name": "WAREHOUSES :",
            "description": "Requires Business Critical Edition (or higher). All warehouses in the source account."
        },
        {
            "name": "SECURITY INTEGRATIONS :",
            "description": "Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES ."
        },
        {
            "name": "API INTEGRATIONS :",
            "description": "Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations ."
        },
        {
            "name": "STORAGE INTEGRATIONS :",
            "description": "Specifies storage integrations."
        },
        {
            "name": "EXTERNAL ACCESS INTEGRATIONS :",
            "description": "Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) ."
        },
        {
            "name": "NOTIFICATION INTEGRATIONS :",
            "description": "Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "target_account_name",
            "description": "Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "USING   CRON   expr   time_zone",
            "description": "Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates."
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/ n",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run)."
        },
        {
            "name": "num   MINUTE",
            "description": "Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "ERROR_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the notification integration to use to email/push notifications when refresh errors occur for the replication\ngroup. For more details, see Error notifications for replication and failover groups ."
        },
        {
            "name": "secondary_name",
            "description": "Specifies the identifier for the secondary replication group. The identifier must start with an alphabetic character and cannot contain\nspaces or special characters unless the identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in\ndouble quotes are also case-sensitive. For more details, see Identifier requirements . The identifiers for the secondary replication group ( secondary_name ) and primary replication group ( name ) can be, but\nare not required to be, identical."
        },
        {
            "name": "AS   REPLICA   OF   org_name . source_account_name . name",
            "description": "Specifies the identifier of the primary replication group from which to create a secondary replication group. Name of your Snowflake organization. Source account from which you are enabling replication of the specified objects. Identifier for the primary replication group in the source account."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "source_account_name",
            "description": "Source account from which you are enabling replication of the specified objects."
        },
        {
            "name": "name",
            "description": "Identifier for the primary replication group in the source account."
        }
    ],
    "usage_notes": "Identifiers for failover groups and replication groups in an account must be unique.\nA database can only be added to one replication or failover group.\nInbound shares (shares from providers) cannot be added to a replication or failover group.\nTo retrieve the set of accounts in your organization that are enabled for replication, use\nSHOW REPLICATION ACCOUNTS.\nTo retrieve the list of replication and failover groups in your organization, use SHOW REPLICATION GROUPS.\nThe allowed_accounts column lists all target accounts enabled for object replication from a source account.\nIf there are account objects (for example, users or roles) in a target account that you do not want to drop during replication,\nuse the SYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME system function to apply a global identifier to objects\ncreated by means other than replication. For more information, see\nApply Global IDs to Objects Created by Scripts in Target Accounts before\nyou create a replication group.\nAutomatically scheduled refresh operations are executed using the role with the OWNERSHIP\nprivilege on the group. If a scheduled refresh operation fails due to insufficient privileges, grant the required privileges\nto the role with the OWNERSHIP privilege on the group.\nIf you create a replication or failover group with a tag or modify a replication or failover group by setting a tag on it,\ntag inheritance does not apply to any objects that you specify in the replication or failover group.\nTag inheritance is only applicable to objects with a parent-child relationship, such\ndatabase, schema, and table. There are no child objects of replication or failover groups.\nYou cannot set a tag or modify a tag on a secondary replication or failover group because these objects are read\nonly.\nWhen you refresh a secondary replication or failover group, any tags that are set on the primary group are then set on\nthe secondary group.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/custom_classifier/commands/create-custom-classifier",
    "title": "CREATE CUSTOM_CLASSIFIER",
    "description": "Fully qualified name: SNOWFLAKE.DATA_PRIVACY.CUSTOM_CLASSIFIER",
    "syntax": "CREATE [ OR REPLACE ] SNOWFLAKE.DATA_PRIVACY.CUSTOM_CLASSIFIER\n[ IF NOT EXISTS ] <custom_classifier_name>()",
    "examples": [
        {
            "code": "CREATE OR REPLACE SNOWFLAKE.DATA_PRIVACY.CUSTOM_CLASSIFIER medical_codes();"
        }
    ],
    "parameters": [
        {
            "name": "custom_classifier_name ()",
            "description": "Specifies the identifier (name) for the instance; the name must be unique for the schema in which the object is created. You must add the\nparentheses at the end of the identifier when creating the object. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        }
    ],
    "usage_notes": "SNOWFLAKE.DATA_PRIVACY.CUSTOM_CLASSIFIER is a name that Snowflake defines and maintains. Use this object name every time you want to create\nan instance of this class. Alternatively, update your search path to make it easier to use the instance."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-git-repository",
    "title": "CREATE GIT REPOSITORY",
    "description": "Creates a Snowflake Git repository clone in the schema or replaces an\nexisting Git repository clone.",
    "syntax": "CREATE [ OR REPLACE ] GIT REPOSITORY [ IF NOT EXISTS ] <name>\n  ORIGIN = '<repository_url>'\n  API_INTEGRATION = <integration_name>\n  [ GIT_CREDENTIALS = <secret_name> ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]",
    "examples": [
        {
            "code": "CREATE OR REPLACE GIT REPOSITORY snowflake_extensions\n  API_INTEGRATION = git_api_integration\n  GIT_CREDENTIALS = git_secret\n  ORIGIN = 'https://github.com/my-account/snowflake-extensions.git';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the Git repository clone to create. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "ORIGIN   =   ' repository_url '",
            "description": "Specifies the origin URL of the remote Git repository that this Git repository clone represents. The URL must use HTTPS. Snowflake supports any HTTPS Git repository URL. For example, you can specify a custom URL to a corporate Git server within your own\ndomain. From the command line, you can use the git config command from within your local repository to get the value to use for the\nORIGIN parameter, as shown in the following example:"
        },
        {
            "name": "API_INTEGRATION   =   integration_name",
            "description": "Specifies the API INTEGRATION that contains information about the remote Git\nrepository such as allowed credentials and prefixes for target URLs. The API integration you specify here must have an API_PROVIDER parameter whose value is set to git_https_api . For reference information about API integrations, see CREATE API INTEGRATION ."
        },
        {
            "name": "GIT_CREDENTIALS   =   secret_name",
            "description": "Specifies the Snowflake secret containing the credentials to use for authenticating with the\nremote Git repository. Omit this parameter to use the default secret specified by the API integration or if this integration does not require\nauthentication. As a best practice, use a personal access token for the secret’s PASSWORD value. For information about creating a personal access token\nin GitHub, see Managing your personal access tokens in the GitHub documentation. The secret you specify here must be a secret specified by the ALLOWED_AUTHENTICATION_SECRETS parameter of the API integration you specify\nwith this command’s API_INTEGRATION parameter. Default: No value For reference information about secrets, see CREATE SECRET ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the external access integration. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "The OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-event-table",
    "title": "CREATE EVENT TABLE",
    "description": "Creates an event table that captures events, including logged messages\nfrom functions and procedures.",
    "syntax": "CREATE [ OR REPLACE ] EVENT TABLE [ IF NOT EXISTS ] <name>\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COPY GRANTS ]\n  [ [ WITH ] COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]",
    "examples": [
        {
            "code": "CREATE EVENT TABLE my_events;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (the name) for the event table; must be unique for the schema in which the event table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "source_table",
            "description": "Required for CLONE. Specifies the event table to use as the source for the clone."
        },
        {
            "name": "CLUSTER   BY   (   expr   [   ,   expr   ,   ...   ]   )",
            "description": "Specifies one or more columns or column expressions in the table as the clustering key. For more details, see Clustering Keys & Clustered Tables . Default: No value (no clustering key is defined for the table) Important Clustering keys are not intended or recommended for all tables; they typically benefit very large (i.e. multi-terabyte)\ntables. Before you specify a clustering key for a table, please read Understanding Snowflake Table Structures ."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the retention period for the table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical\ndata in the table. For more details, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "CHANGE_TRACKING   =     TRUE   |   FALSE",
            "description": "Specifies whether to enable change tracking on the table. TRUE enables change tracking on the table. This setting adds a pair of hidden columns to the source table and begins\nstoring change tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the table. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for the columns in the table. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original table when a new table is created using any of the following\nCREATE TABLE variants: CREATE OR REPLACE TABLE CREATE TABLE … CLONE The parameter copies all privileges, except OWNERSHIP, from the existing table to the new table. The new table does not inherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE EVENT TABLE statement\nowns the new table. If the parameter is not included in the CREATE EVENT TABLE statement, then the new table does not inherit any explicit access\nprivileges granted on the original table, but does inherit any future grants defined for the object type in the schema. Note: With data sharing : If the existing table was shared to another account, the replacement table is also shared. If the existing table was shared with your account as a data consumer, and access was further granted to other roles in\nthe account (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement\ntable. The SHOW GRANTS output for the replacement table lists the grantee for the copied privileges as the\nrole that executed the CREATE EVENT TABLE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE EVENT TABLE command (i.e. within the same transaction)."
        },
        {
            "name": "ROW   ACCESS   POLICY   policy_name   ON   (   col_name   [   ,   col_name   ...   ]   )",
            "description": "Specifies the row access policy to set on a table."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the table. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ],
    "usage_notes": "A schema cannot contain event tables, tables, and/or views with the same name. When creating an event table:\nIf a table or view with the same name already exists in the schema, an error is returned and the event table is not created.\nIf an event table with the same name already exists in the schema, an error is returned and the event table is not created,\nunless the optional OR REPLACE keyword is included in the command.\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing event table and then\ncreating a new event table with the same name; however, the dropped table is not permanently removed from the system.\nInstead, it is retained in Time Travel. This is important to note because dropped tables in Time Travel can be recovered, but\nthey also contribute to data storage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE EVENT TABLE operation use either the old or new table version.\nRecreating a table (using the optional OR REPLACE keyword) drops its history, which makes any stream on the table stale.\nA stale stream is unreadable.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE EVENT TABLE … CLONE:\nIf the source event table has clustering keys, the new event table has clustering keys. By default, Automatic Clustering is suspended\nfor the new event table—even if Automatic Clustering was not suspended for the source table.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-application",
    "title": "CREATE APPLICATION",
    "description": "Creates a new application object using an application package or listing. This command is used by\nproviders to test an application package before publishing a Snowflake Native App.",
    "syntax": "CREATE APPLICATION <name> FROM APPLICATION PACKAGE <package_name>\n   [ COMMENT = '<string_literal>' ]\n   [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , ... ] ) ]\n   [ AUTHORIZE_TELEMETRY_EVENT_SHARING = { TRUE | FALSE } ]\n   [ WITH FEATURE POLICY = <policy_name> ]\n\nCREATE APPLICATION <name> FROM APPLICATION PACKAGE <package_name>\n  USING <path_to_version_directory>\n  [ DEBUG_MODE = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [, ...] ) ]\n  [ AUTHORIZE_TELEMETRY_EVENT_SHARING = { TRUE | FALSE } ]\n  [ WITH FEATURE POLICY = <policy_name> ]\n\n\nCREATE APPLICATION <name> FROM APPLICATION PACKAGE <package_name>\n  USING VERSION  <version_identifier> [ PATCH <patch_num> ]\n  [ DEBUG_MODE = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , ... ] ) ]\n  [ AUTHORIZE_TELEMETRY_EVENT_SHARING = { TRUE | FALSE } ]\n  [ WITH FEATURE POLICY = <policy_name> ]\n\nCREATE APPLICATION <name> FROM LISTING <listing_name>\n   [ COMMENT = '<string_literal>' ]\n   [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , ... ] ) ]\n   [ BACKGROUND_INSTALL = { TRUE | FALSE } ]\n   [ AUTHORIZE_TELEMETRY_EVENT_SHARING = { TRUE | FALSE } ]\n   [ WITH FEATURE POLICY = <policy_name> ]",
    "examples": [
        {
            "code": "CREATE APPLICATION hello_snowflake_app\n  FROM APPLICATION PACKAGE hello_snowflake_package\n  USING VERSION v1;"
        },
        {
            "code": "+---------------------------------------------------------+\n| status                                                  |\n|---------------------------------------------------------|\n| Application 'hello_snowflake_app' created successfully. |\n+---------------------------------------------------------+"
        },
        {
            "code": "CREATE APPLICATION hello_snowflake_app\n  FROM APPLICATION PACKAGE hello_snowflake_package\n  USING '@hello_snowflake_code.core.hello_snowflake_stage';"
        },
        {
            "code": "+---------------------------------------------------------+\n| status                                                  |\n|---------------------------------------------------------|\n| Application 'hello_snowflake_app' created successfully. |\n+---------------------------------------------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the application. Must be unique for your account. In addition, the identifier must start with an alphabetic character and cannot contain spaces\nor special characters unless the entire identifier string is enclosed in double quotes\n(for example, \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, refer to Identifier requirements ."
        },
        {
            "name": "FROM   APPLICATION   PACKAGE   package_name",
            "description": "Specifies the name of the application package used to create the application. To use this\nclause to create an application from an application package without specifying a stage or a\nversion/patch, the application package must have a default release directive defined. This clause can only be used to create an application in the same account as the application\npackage. This clause cannot be used to create an application in development mode."
        },
        {
            "name": "FROM   LISTING   listing_name",
            "description": "Specifies the name of listing containing the application package used to create the application."
        },
        {
            "name": "USING   path_to_version_directory",
            "description": "Specifies the path to the stage containing the application files for the application."
        },
        {
            "name": "USING   version   [   PATCH   patch_num   ]",
            "description": "Specifies the version, and optionally the patch, defined in the application package\nused to create the application."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the application. Default: No value"
        },
        {
            "name": "DEBUG_MODE   =   {   TRUE   |   FALSE   }",
            "description": "Enables or disables debug mode for the application\nobject being created. Debug mode allows a provider to see the contents of the application object. TRUE enables debug mode for the installed application. FAlSE disables debug mode for the installed application. Note You can only set DEBUG_MODE if the application meets the following requirements: The application is in the same account as the application package. The application is being created from a specific version or from files on a named stage."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "BACKGROUND_INSTALL   =   {   TRUE   |   FALSE   }",
            "description": "Creates the application object from a listing in the background. If you specify this clause, the\ncommand returns you to the prompt immediately, and the installation process continues in the\nbackground. To monitor that status of the installation, use the DESCRIBE APPLICATION command. Note When using this clause, the application object is created even if the command fails. In this\nsituation, use the DROP APPLICATION command to delete the object before\nrunning the CREATE APPLICATION command again. This clause is primarily used by Snowsight to install a Snowflake Native App in the background. Background\ninstallation allows the consumer to navigate away from the listing in Snowsight during\ninstallation. A provider might use this clause when testing the installation of a Snowflake Native App\nfrom a listing before publishing the listing."
        },
        {
            "name": "AUTHORIZE_TELEMETRY_EVENT_SHARING   =   {   TRUE   |   FALSE   }",
            "description": "Enables logging and event sharing in the app."
        },
        {
            "name": "WITH   FEATURE   POLICY   =   policy_name",
            "description": "Create the app with the specified feature policy. If the app attempts to create an object that the feature policy prohibits (such as a database), the command fails."
        }
    ],
    "usage_notes": "To create an application directly from an application package, you must specify a default release directive in the application package.\nThe application object differs from a database in the following ways:\nAn application may not be transient.\nThe role with the OWNERSHIP privilege on the application:\nCan drop the database or modify the COMMENT property as well as any properties that are specific to the application.\nCannot see or modify the contents of the application except via the privileges granted the application roles. Also, this role\ncannot create a database-level object, such as a schema or a database role.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-user",
    "title": "CREATE USER",
    "description": "Creates a new user or replaces an existing user in the system. For more details, see User management.",
    "syntax": "CREATE [ OR REPLACE ] USER [ IF NOT EXISTS ] <name>\n  [ objectProperties ]\n  [ objectParams ]\n  [ sessionParams ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n\nobjectProperties ::=\n  PASSWORD = '<string>'\n  LOGIN_NAME = <string>\n  DISPLAY_NAME = <string>\n  FIRST_NAME = <string>\n  MIDDLE_NAME = <string>\n  LAST_NAME = <string>\n  EMAIL = <string>\n  MUST_CHANGE_PASSWORD = TRUE | FALSE\n  DISABLED = TRUE | FALSE\n  DAYS_TO_EXPIRY = <integer>\n  MINS_TO_UNLOCK = <integer>\n  DEFAULT_WAREHOUSE = <string>\n  DEFAULT_NAMESPACE = <string>\n  DEFAULT_ROLE = <string>\n  DEFAULT_SECONDARY_ROLES = ( 'ALL' ) | ()\n  MINS_TO_BYPASS_MFA = <integer>\n  RSA_PUBLIC_KEY = <string>\n  RSA_PUBLIC_KEY_FP = <string>\n  RSA_PUBLIC_KEY_2 = <string>\n  RSA_PUBLIC_KEY_2_FP = <string>\n  TYPE = PERSON | SERVICE | LEGACY_SERVICE | NULL\n  COMMENT = '<string_literal>'\n\nobjectParams ::=\n  ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR = TRUE | FALSE\n  ENABLE_UNREDACTED_SECURE_OBJECT_ERROR = TRUE | FALSE\n  NETWORK_POLICY = <string>\n\nsessionParams ::=\n  ABORT_DETACHED_QUERY = TRUE | FALSE\n  AUTOCOMMIT = TRUE | FALSE\n  BINARY_INPUT_FORMAT = <string>\n  BINARY_OUTPUT_FORMAT = <string>\n  DATE_INPUT_FORMAT = <string>\n  DATE_OUTPUT_FORMAT = <string>\n  DEFAULT_NULL_ORDERING = <string>\n  ERROR_ON_NONDETERMINISTIC_MERGE = TRUE | FALSE\n  ERROR_ON_NONDETERMINISTIC_UPDATE = TRUE | FALSE\n  JSON_INDENT = <num>\n  LOCK_TIMEOUT = <num>\n  QUERY_TAG = <string>\n  ROWS_PER_RESULTSET = <num>\n  SIMULATED_DATA_SHARING_CONSUMER = <string>\n  STATEMENT_TIMEOUT_IN_SECONDS = <num>\n  STRICT_JSON_OUTPUT = TRUE | FALSE\n  TIMESTAMP_DAY_IS_ALWAYS_24H = TRUE | FALSE\n  TIMESTAMP_INPUT_FORMAT = <string>\n  TIMESTAMP_LTZ_OUTPUT_FORMAT = <string>\n  TIMESTAMP_NTZ_OUTPUT_FORMAT = <string>\n  TIMESTAMP_OUTPUT_FORMAT = <string>\n  TIMESTAMP_TYPE_MAPPING = <string>\n  TIMESTAMP_TZ_OUTPUT_FORMAT = <string>\n  TIMEZONE = <string>\n  TIME_INPUT_FORMAT = <string>\n  TIME_OUTPUT_FORMAT = <string>\n  TRANSACTION_DEFAULT_ISOLATION_LEVEL = <string>\n  TWO_DIGIT_CENTURY_START = <num>\n  UNSUPPORTED_DDL_ACTION = <string>\n  USE_CACHED_RESULT = TRUE | FALSE\n  WEEK_OF_YEAR_POLICY = <num>\n  WEEK_START = <num>",
    "examples": [
        {
            "code": "CREATE USER user1 PASSWORD='abc123' DEFAULT_ROLE = myrole DEFAULT_SECONDARY_ROLES = ('ALL') MUST_CHANGE_PASSWORD = TRUE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the user; must be unique for your account. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR   =   {   TRUE   |   FALSE   }",
            "description": "Controls how queries that fail due to syntax or parsing errors show up in a query history. If FALSE, the contents of a\nfailed query is redacted from the views, pages, and functions that provide a query history. This parameter controls behavior for the user viewing the query history, not the user who executed the query. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter."
        },
        {
            "name": "ENABLE_UNREDACTED_SECURE_OBJECT_ERROR   =   {   TRUE   |   FALSE   }",
            "description": "Controls whether error messages related to secure objects are redacted in metadata. For more information about\nerror message redaction for secure objects, see Secure objects: Redaction of information in error messages . Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_SECURE_OBJECT_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the\nredacted error messages in metadata, not the user who caused the error."
        },
        {
            "name": "NETWORK_POLICY   =   string",
            "description": "Specifies an existing network policy is active for the user. The network policy restricts the\nlist of user IP addresses when exchanging an authorization code for an access or refresh token and when using a refresh token to\nobtain a new access token. If this parameter is not set, the network policy for the account (if any) is used instead."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-failover-group",
    "title": "CREATE FAILOVER GROUP",
    "description": "Creates a new failover group of specified objects in the system.",
    "syntax": "CREATE FAILOVER GROUP [ IF NOT EXISTS ] <name>\n    OBJECT_TYPES = <object_type> [ , <object_type> , ... ]\n    [ ALLOWED_DATABASES = <db_name> [ , <db_name> , ... ] ]\n    [ ALLOWED_SHARES = <share_name> [ , <share_name> , ... ] ]\n    [ ALLOWED_INTEGRATION_TYPES = <integration_type_name> [ , <integration_type_name> , ... ] ]\n    ALLOWED_ACCOUNTS = <org_name>.<target_account_name> [ , <org_name>.<target_account_name> ,  ... ]\n    [ IGNORE EDITION CHECK ]\n    [ REPLICATION_SCHEDULE = '{ <num> MINUTE | USING CRON <expr> <time_zone> }' ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n    [ ERROR_INTEGRATION = <integration_name> ]\n\nCREATE FAILOVER GROUP [ IF NOT EXISTS ] <secondary_name>\n    AS REPLICA OF <org_name>.<source_account_name>.<name>",
    "examples": [
        {
            "code": "CREATE FAILOVER GROUP myfg\n    OBJECT_TYPES = DATABASES\n    ALLOWED_DATABASES = db1\n    ALLOWED_ACCOUNTS = myorg.myaccount2\n    REPLICATION_SCHEDULE = '10 MINUTE';"
        },
        {
            "code": "CREATE FAILOVER GROUP myfg\n    AS REPLICA OF myorg.myaccount1.myfg;"
        },
        {
            "code": "CREATE FAILOVER GROUP myfg\n    OBJECT_TYPES = DATABASES\n    ALLOWED_DATABASES = db1, db2, db3\n    ALLOWED_ACCOUNTS = myorg.myaccount2\n    REPLICATION_SCHEDULE = '10 MINUTE';"
        },
        {
            "code": "CREATE FAILOVER GROUP myfg\n    AS REPLICA OF myorg.myaccount1.myfg;"
        },
        {
            "code": "CREATE FAILOVER GROUP myfg\n    OBJECT_TYPES = USERS, ROLES, WAREHOUSES, RESOURCE MONITORS, INTEGRATIONS\n    ALLOWED_INTEGRATION_TYPES = STORAGE INTEGRATIONS, NOTIFICATION INTEGRATIONS\n    ALLOWED_ACCOUNTS = myorg.myaccount2\n    REPLICATION_SCHEDULE = '10 MINUTE';"
        },
        {
            "code": "CREATE FAILOVER GROUP myfg\n    AS REPLICA OF myorg.myaccount1.myfg;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the failover group. The identifier must start with an alphabetic character and cannot contain spaces or\nspecial characters unless the identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double\nquotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "OBJECT_TYPES   =   object_type   [   ,   object_type   ,   ...   ]",
            "description": "Type(s) of objects for which you are enabling replication and failover from the source account to the target account. The following object types are supported: All account-level parameters. This includes account parameters and parameters that can be set for\nyour account . Add database objects to the list of object types. If database objects are included in the list of specified object types, the ALLOWED_DATABASES parameter must be set. Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set. All network policies in the source account. All resource monitors in the source account. All roles in the source account. Replicating roles implicitly includes all grants for object types included in the replication group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated. Add share objects to the list of object types. If share objects are included in the list of specified object types, the ALLOWED_SHARES parameter must be set. All users in the source account. All warehouses in the source account. Note If you replicate users and roles, programmatic access tokens for users are replicated automatically. To modify the list of replicated object types to a specified target account, use ALTER FAILOVER GROUP to reset the list of\nobject types."
        },
        {
            "name": "ACCOUNT PARAMETERS :",
            "description": "All account-level parameters. This includes account parameters and parameters that can be set for\nyour account ."
        },
        {
            "name": "DATABASES :",
            "description": "Add database objects to the list of object types. If database objects are included in the list of specified object types, the ALLOWED_DATABASES parameter must be set."
        },
        {
            "name": "INTEGRATIONS :",
            "description": "Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set."
        },
        {
            "name": "NETWORK POLICIES :",
            "description": "All network policies in the source account."
        },
        {
            "name": "RESOURCE MONITORS :",
            "description": "All resource monitors in the source account."
        },
        {
            "name": "ROLES :",
            "description": "All roles in the source account. Replicating roles implicitly includes all grants for object types included in the replication group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated."
        },
        {
            "name": "SHARES :",
            "description": "Add share objects to the list of object types. If share objects are included in the list of specified object types, the ALLOWED_SHARES parameter must be set."
        },
        {
            "name": "USERS :",
            "description": "All users in the source account."
        },
        {
            "name": "WAREHOUSES :",
            "description": "All warehouses in the source account."
        },
        {
            "name": "ALLOWED_DATABASES   =   db_name   [   ,   db_name   ,   ...   ]",
            "description": "Specifies the database or list of databases for which you are enabling replication and failover from the source account to the target\naccount.  In order for you to set this parameter, the OBJECT_TYPES list must include DATABASES . Specifies the identifier for the database."
        },
        {
            "name": "ALLOWED_SHARES   =   share_name   [   ,   share_name   ,   ...   ]",
            "description": "Specifies the share or list of shares for which you are enabling replication and failover from the source account to the target account.\nIn order for you to set this parameter, the OBJECT_TYPES list must include SHARES . Specifies the identifier for the share."
        },
        {
            "name": "ALLOWED_INTEGRATION_TYPES   =   integration_type_name   [   ,   integration_type_name   ,   ...   ]",
            "description": "Type(s) of integrations for which you are enabling replication and failover from the source account to the target account. This property requires that the OBJECT_TYPES list include INTEGRATIONS to set this parameter. The following integration types are supported: Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES . Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations . Specifies storage integrations. Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) . Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "ALLOWED_ACCOUNTS   =   org_name . target_account_name   [   ,   org_name . target_account_name   ,   ...   ]",
            "description": "Specifies the target account or list of target accounts to which replication and failover of specified objects from the source account is\nenabled. Secondary failover groups in the target accounts in this list can be promoted to serve as the primary failover group in\ncase of failover. Name of your Snowflake organization. Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "IGNORE   EDITION   CHECK",
            "description": "Allows replicating objects to accounts in the following scenario: The primary failover group is in a Business Critical (or higher) account and a signed business associate agreement is in place to\nstore PHI data in the account per HIPAA and HITRUST regulations. However, no such agreement is in place\nfor one or more of the accounts approved for replication, regardless if they are Business Critical (or higher) accounts. This scenario is prohibited by default."
        },
        {
            "name": "REPLICATION_SCHEDULE   ...",
            "description": "Specifies the schedule for refreshing secondary failover groups. Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs."
        },
        {
            "name": "db_name",
            "description": "Specifies the identifier for the database."
        },
        {
            "name": "share_name",
            "description": "Specifies the identifier for the share."
        },
        {
            "name": "SECURITY INTEGRATIONS :",
            "description": "Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES ."
        },
        {
            "name": "API INTEGRATIONS :",
            "description": "Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations ."
        },
        {
            "name": "STORAGE INTEGRATIONS :",
            "description": "Specifies storage integrations."
        },
        {
            "name": "EXTERNAL ACCESS INTEGRATIONS :",
            "description": "Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) ."
        },
        {
            "name": "NOTIFICATION INTEGRATIONS :",
            "description": "Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "target_account_name",
            "description": "Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "USING   CRON   expr   time_zone",
            "description": "Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates."
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/ n",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run)."
        },
        {
            "name": "num   MINUTE",
            "description": "Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "ERROR_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the notification integration to use to send notifications when refresh errors occur for the failover\ngroup. For more details, see Error notifications for replication and failover groups ."
        },
        {
            "name": "secondary_name",
            "description": "Specifies the identifier for the secondary failover group. The identifier must start with an alphabetic character and cannot contain\nspaces or special characters unless the identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in\ndouble quotes are also case-sensitive. For more details, see Identifier requirements . The identifiers for the secondary failover group ( secondary_name ) and primary failover group ( name ) can be, but\nare not required to be, identical."
        },
        {
            "name": "AS   REPLICA   OF   org_name . source_account_name . name",
            "description": "Specifies the identifier of the primary failover group from which to create a secondary failover group. Name of your Snowflake organization. Source account from which you are enabling replication and failover of the specified objects. Identifier for the primary failover group in the source account."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "source_account_name",
            "description": "Source account from which you are enabling replication and failover of the specified objects."
        },
        {
            "name": "name",
            "description": "Identifier for the primary failover group in the source account."
        }
    ],
    "usage_notes": "Identifiers for failover groups and replication groups in an account must be unique.\nObjects other than databases and shares must be in the same failover group.\nA database can only be added to one failover group.\nInbound shares (shares from providers) cannot be added to a replication or failover group.\nTo retrieve the set of accounts in your organization that are enabled for replication, use\nSHOW REPLICATION ACCOUNTS.\nTo retrieve the list of failover groups in your organization, use SHOW FAILOVER GROUPS.\nIf there are account objects (for example, users or roles) in a target account that you do not want to drop during replication,\nuse the SYSTEM$LINK_ACCOUNT_OBJECTS_BY_NAME system function to apply a global identifier to objects\ncreated by means other than replication. For more information, see\nApply Global IDs to Objects Created by Scripts in Target Accounts before\nyou create a failover group.\nAutomatically scheduled refresh operations are executed using the role with the OWNERSHIP\nprivilege on the group. If a scheduled refresh operation fails due to insufficient privileges, grant the required privileges\nto the role with the OWNERSHIP privilege on the group.\nIf you create a replication or failover group with a tag or modify a replication or failover group by setting a tag on it,\ntag inheritance does not apply to any objects that you specify in the replication or failover group.\nTag inheritance is only applicable to objects with a parent-child relationship, such\ndatabase, schema, and table. There are no child objects of replication or failover groups.\nYou cannot set a tag or modify a tag on a secondary replication or failover group because these objects are read\nonly.\nWhen you refresh a secondary replication or failover group, any tags that are set on the primary group are then set on\nthe secondary group.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nFor an account that is newly upgraded to Business Critical Edition (or higher), it might take up to 12 hours for failover capabilities to\nbecome available."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-row-access-policy",
    "title": "CREATE ROW ACCESS POLICY",
    "description": "Creates a new row access policy in the current/specified schema or replaces an existing row access policy.",
    "syntax": "CREATE [ OR REPLACE ] ROW ACCESS POLICY [ IF NOT EXISTS ] <name> AS\n( <arg_name> <arg_type> [ , ... ] ) RETURNS BOOLEAN -> <body>\n[ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "create or replace row access policy rap_it as (empl_id varchar) returns boolean ->\n  case\n      when 'it_admin' = current_role() then true\n      else false\n  end\n;"
        },
        {
            "code": "use role securityadmin;\n\ncreate or replace row access policy rap_sales_manager_regions_1 as (sales_region varchar) returns boolean ->\n  'sales_executive_role' = current_role()\n      or exists (\n            select 1 from salesmanagerregions\n              where sales_manager = current_role()\n                and region = sales_region\n          )\n;"
        },
        {
            "code": "create or replace row access policy rap_test2 as (n number, v varchar)\n  returns boolean -> true;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the row access policy; must be unique for your schema. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. “My object”). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements"
        },
        {
            "name": "AS   (   <arg_name>   <arg_type>   [   ,   ...   ]   )",
            "description": "The signature for the row access policy. A signature specifies a set of attributes that must be considered to determine whether the row is accessible. The attribute values come\nfrom the database object (e.g. table or view) to be protected by the row access policy."
        },
        {
            "name": "RETURNS   BOOLEAN",
            "description": "A row access policy must evaluate to true or false. A user that queries a table protected by a row access policy sees rows in the output\nbased on how the body is written."
        },
        {
            "name": "body",
            "description": "SQL expression that operates on the argument values in the signature to determine which rows to return for a query on a table that is\nprotected by a row access policy. The body can be any boolean-valued SQL expression. Snowflake supports expressions that invoke User-defined functions overview , Writing external functions , and expressions that use sub-queries."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the row access policy. Default: No value"
        }
    ],
    "usage_notes": "Including one or more subqueries in the policy body may cause errors. When possible, limit the\nnumber of subqueries, limit the number of JOIN operations, and simplify WHERE clause conditions.\nIf a database object has both a row access policy and one or more masking policy, the row access\npolicy is evaluated first.\nFor more information on row access policies during query runtime, see Understanding row access policies.\nA given table or view column can be specified in either a masking policy signature or a row access policy signature. In other words, the\nsame column cannot be specified in both a masking policy signature and a row access policy signature at the same time.\nFor more information, see CREATE MASKING POLICY.\nYou cannot change the policy signature (i.e. argument name or input/output data type) using\nCREATE OR REPLACE ROW ACCESS POLICY if the policy is attached to a table or view, or using\nALTER ROW ACCESS POLICY. If you need to change the signature, execute a\nDROP ROW ACCESS POLICY statement on the policy and create a new row access policy.\nIf the policy body contains a mapping table lookup, create a centralized mapping table and store the mapping table\nin the same database as the protected table. This is particularly important if the body calls the\nIS_DATABASE_ROLE_IN_SESSION function. For details, see the function usage notes.\nA data sharing provider cannot create a row access policy in a reader account.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\nbody of a masking or row access policy, the function returns the database or schema that contains the protected table, not the database or\nschema in use for the session.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-external-function",
    "title": "CREATE EXTERNAL FUNCTION",
    "description": "Creates a new external function.",
    "syntax": "CREATE [ OR REPLACE ] [ SECURE ] EXTERNAL FUNCTION <name> ( [ <arg_name> <arg_data_type> ] [ , ... ] )\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ]\n  [ COMMENT = '<string_literal>' ]\n  API_INTEGRATION = <api_integration_name>\n  [ HEADERS = ( '<header_1>' = '<value_1>' [ , '<header_2>' = '<value_2>' ... ] ) ]\n  [ CONTEXT_HEADERS = ( <context_function_1> [ , <context_function_2> ...] ) ]\n  [ MAX_BATCH_ROWS = <integer> ]\n  [ COMPRESSION = <compression_type> ]\n  [ REQUEST_TRANSLATOR = <request_translator_udf_name> ]\n  [ RESPONSE_TRANSLATOR = <response_translator_udf_name> ]\n  AS '<url_of_proxy_and_resource>';",
    "examples": [
        {
            "code": "CREATE OR REPLACE EXTERNAL FUNCTION local_echo(string_col VARCHAR)\n  RETURNS VARIANT\n  API_INTEGRATION = demonstration_external_api_integration_01\n  AS 'https://xyz.execute-api.us-west-2.amazonaws.com/prod/remote_echo';"
        },
        {
            "code": "CREATE OR ALTER SECURE EXTERNAL FUNCTION local_echo(string_col VARCHAR)\n  RETURNS VARIANT\n  API_INTEGRATION = demonstration_external_api_integration_01\n  HEADERS = ('header_variable1'='header_value', 'header_variable2'='header_value2')\n  CONTEXT_HEADERS = (current_account)\n  MAX_BATCH_ROWS = 100\n  COMPRESSION = \"GZIP\"\n  AS 'https://xyz.execute-api.us-west-2.amazonaws.com/prod/remote_echo';"
        }
    ],
    "parameters": [
        {
            "name": "name :",
            "description": "Specifies the identifier for the function. The identifier can contain the schema name and database name, as well as the function name. The identifier does not need to be unique for the schema in which the function is created because functions are\nidentified and resolved by their name and argument types. However, the signature (name and argument data types)\nmust be unique within the schema. The name must follow the rules for Snowflake identifiers .\nFor more details, see Identifier requirements . Setting name the same as the remote service name can make the relationship more clear.\nHowever, this is not required."
        },
        {
            "name": "(   [   arg_name   arg_data_type   ]   [   ,   ...   ]   )",
            "description": "Specifies the arguments/inputs for the external function. These should correspond to the arguments that the remote\nservice expects. If there are no arguments, then include the parentheses without any argument name(s) and data type(s)."
        },
        {
            "name": "RETURNS   result_data_type",
            "description": "Specifies the data type returned by the function."
        },
        {
            "name": "API_INTEGRATION   =   api_integration_name",
            "description": "This is the name of the API integration object that should be used to authenticate the call to the proxy service."
        },
        {
            "name": "AS   ' url_of_proxy_and_resource '",
            "description": "This is the invocation URL of the proxy service (e.g. API Gateway or API Management service) and resource through\nwhich Snowflake calls the remote service."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the function is secure. If a function is secure, the URL, the HTTP headers, and the context headers\nare hidden from all users who are not owners of the function."
        },
        {
            "name": "[   [   NOT   ]   NULL   ]",
            "description": "This clause indicates whether the function can return NULL values or must return only NON-NULL values.\nIf NOT NULL is specified, the function must return only non-NULL values. If NULL is specified, the\nfunction can return NULL values. Default: The default is NULL (i.e. the function can return NULL values)."
        },
        {
            "name": "CALLED   ON   NULL   INPUT   or   .   {   RETURNS   NULL   ON   NULL   INPUT   |   STRICT   }",
            "description": "Specifies the behavior of the function when called with null inputs. In contrast to system-defined functions,\nwhich always return null when any input is null, external functions can handle null inputs,\nreturning non-null values even when an input is null: CALLED ON NULL INPUT will always call the function with null inputs. It is up to the function to\nhandle such values appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT ) will not call the function if any input\nis null. Instead, a null value will always be returned for that row. Note that the function might\nstill return null for non-null inputs. Default: CALLED ON NULL INPUT"
        },
        {
            "name": "{   VOLATILE   |   IMMUTABLE   }",
            "description": "Specifies the behavior of the function when returning results: VOLATILE : The function can return different values for different rows, even for the same input (e.g.\ndue to non-determinism and statefulness). IMMUTABLE : The function always returns the same result when called with the same input.\nSnowflake does not check or guarantee this; the remote service must be designed to behave this way.\nSpecifying IMMUTABLE for a function that actually returns different values for the same input will\nresult in undefined behavior. Default: VOLATILE Snowflake recommends that you set this explicitly rather than accept the default. Setting this\nexplicitly reduces the chance of error, and tells users how the function behaves.\n(The SHOW EXTERNAL FUNCTIONS command shows whether a function is volatile or immutable.) For important additional information about VOLATILE vs. IMMUTABLE external functions, see Categorize your function as volatile or immutable ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the function, which is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW EXTERNAL FUNCTIONS output. Default: user-defined function"
        },
        {
            "name": "HEADERS   =   (   ' header_1 '   =   ' value_1 '   [   ,   ' header_2 '   =   ' value_2 '   ...   ]   )",
            "description": "This clause allows users to specify key-value metadata that is sent with every request.\nThe creator of the external function decides what goes into the headers, and the caller does not have any control\nover it. Snowflake prepends all of the specified header names with the prefix “sf-custom-”, and sends them as HTTP\nheaders. The value must be a constant string, not an expression. Here’s an example: This causes Snowflake to add 2 HTTP headers into every HTTPS request: sf-custom-volume-measure and sf-custom-distance-measure , with their corresponding values. The rules for header names are different from the rules for Snowflake database identifiers. Header names can be\ncomposed of most visible standard ASCII characters (decimal 32 - 126) except the following: the space character ( ) , / : ; < > = \" ? @ [ ] \\ { } _ Note specifically that the underscore character is not allowed in header names. The header name and value are delimited by single quotes, so any single quotes inside the header name or value\nmust be escaped with the backslash character. If the backslash character is used as a literal character inside a header value, it must be escaped. In header values, both spaces and tabs are allowed, but header values should not contain more than one whitespace\ncharacter in a row. This restriction applies to combinations of whitespace characters (e.g. a space followed by a\ntab) as well as individual whitespace characters (e.g. two spaces in a row). If the function author marks the function as secure (with CREATE SECURE EXTERNAL FUNCTION... ), then the\nheaders, the context headers, the binary context headers, and the URL are not visible to function users. The sum of the sizes of the header names and header values for an external function must be less than or equal\nto 8 KB."
        },
        {
            "name": "CONTEXT_HEADERS   =   (   context_function_1   [   ,   context_function_2   ...]   )",
            "description": "This is similar to HEADERS, but instead of using constant strings, it binds Snowflake context function results to HTTP headers.\n(For more information about Snowflake context functions, see: Context functions .) Not all context functions are supported in context headers. The following are supported: CURRENT_ACCOUNT() CURRENT_CLIENT() CURRENT_DATABASE() CURRENT_DATE() CURRENT_IP_ADDRESS() CURRENT_REGION() CURRENT_ROLE() CURRENT_SCHEMA() CURRENT_SCHEMAS() CURRENT_SESSION() CURRENT_STATEMENT() CURRENT_TIME() CURRENT_TIMESTAMP() CURRENT_TRANSACTION() CURRENT_USER() CURRENT_VERSION() CURRENT_WAREHOUSE() LAST_QUERY_ID() LAST_TRANSACTION() LOCALTIME() LOCALTIMESTAMP() When function names are listed in the CONTEXT_HEADERS clause, the function names should not be quoted. Snowflake prepends sf-context to the header before it is written to the HTTP request. Example: In this example, Snowflake writes the header sf-context-current-timestamp into the HTTP request. The characters allowed in context header names and values are the same as the characters allowed in custom header names and values . Context functions can generate characters that are illegal in HTTP header values, including (but not limited to): newline Ä Î ß ë ¬ ± © ® Snowflake replaces each sequence of one or more illegal characters with one space character. (The replacement\nis per sequence, not per character.) For example, suppose that the context function CURRENT_STATEMENT() returns: The value sent in sf-context-current-statement is: To ensure that remote services can access the original result (with illegal characters) from the context function\neven if illegal characters have been replaced, Snowflake also sends a binary context header that contains the\ncontext function result encoded in base64 . In the example above, the value sent in the base64 header is the result of calling: The remote service is responsible for decoding the base64 value if needed. Each such base64 header is named according to the following convention: In the example above, the name of the header would be If no context headers are sent, then no base64 context headers are sent. If the rows sent to an external function are split across multiple batches, then all batches contain the same\ncontext headers and the same binary context headers."
        },
        {
            "name": "MAX_BATCH_ROWS   =   integer",
            "description": "This specifies the maximum number of rows in each batch sent to the proxy service. The purpose of this parameter is to limit batch sizes for remote services that have memory constraints or other\nlimitations. This parameter is not a performance tuning parameter. This parameter specifies a maximum\nsize, not a recommended size. If you do not specify MAX_BATCH_ROWS, Snowflake estimates the optimal batch size and uses that. Snowflake recommends leaving this parameter unset unless the remote service requires a limit."
        },
        {
            "name": "COMPRESSION   =   compression_type",
            "description": "If this clause is specified, the JSON payload is compressed when sent from Snowflake to the proxy service, and when\nsent back from the proxy service to Snowflake. Valid values are: NONE . GZIP . DEFLATE . AUTO . On AWS, AUTO is equivalent to GZIP . On Azure, AUTO is equivalent to NONE . On GCP, AUTO is equivalent to NONE . The Amazon API Gateway automatically compresses/decompresses requests. For more information about\nAmazon API Gateway compression and decompression, see: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-gzip-compression-decompression.html For information about compression and decompression for other cloud platform proxy services, see the documentation\nfor those cloud platforms. Default: The default is AUTO ."
        },
        {
            "name": "REQUEST_TRANSLATOR   =   request_translator_udf_name",
            "description": "This specifies the name of the request translator function. For more information, see Using request and response translators with data for a remote service ."
        },
        {
            "name": "RESPONSE_TRANSLATOR   =   response_translator_udf_name",
            "description": "This specifies the name of the response translator function. For more information, see Using request and response translators with data for a remote service ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/classification_profile/commands/create-classification-profile",
    "title": "CREATE CLASSIFICATION_PROFILE",
    "description": "Fully qualified name: SNOWFLAKE.DATA_PRIVACY.CLASSIFICATION_PROFILE",
    "syntax": "CREATE [ OR REPLACE ] SNOWFLAKE.DATA_PRIVACY.CLASSIFICATION_PROFILE\n  [ IF NOT EXISTS ] <classification_profile_name> (  <config_object> )",
    "examples": [
        {
            "code": "CREATE OR REPLACE SNOWFLAKE.DATA_PRIVACY.CLASSIFICATION_PROFILE\n  my_classification_profile(\n    {\n      'minimum_object_age_for_classification_days': 0,\n      'maximum_classification_validity_days': 30,\n      'auto_tag': true\n    });"
        },
        {
            "code": "CREATE SNOWFLAKE.DATA_PRIVACY.CLASSIFICATION_PROFILE my_classification_profile(\n  {\n    'minimum_object_age_for_classification_days':0,\n    'auto_tag':true,\n    'tag_map':{\n      'column_tag_map':[\n        {\n          'tag_name':'tag_db.sch.pii'\n        }\n      ]\n    }\n  }\n);"
        },
        {
            "code": "CREATE OR REPLACE SNOWFLAKE.DATA_PRIVACY.CLASSIFICATION_PROFILE\n  my_classification_profile(\n    {\n      'minimum_object_age_for_classification_days':0,\n      'auto_tag':true,\n      'tag_map': {\n        'column_tag_map':[\n          {\n            'tag_name':'test_ac_db.test_ac_schema.pii',\n            'tag_value':'important',\n            'semantic_categories':['NAME']\n          },\n          {\n            'tag_name':'test_ac_db.test_ac_schema.pii',\n            'tag_value':'pii',\n            'semantic_categories':['EMAIL','NATIONAL_IDENTIFIER']\n          }\n        ]\n      }\n    }\n  );"
        },
        {
            "code": "CREATE SNOWFLAKE.DATA_PRIVACY.CLASSIFICATION_PROFILE my_classification_profile(\n  {\n    'minimum_object_age_for_classification_days':0,\n    'auto_tag':true,\n    'custom_classifiers': {\n      'medical_codes': medical_codes!list(),\n      'finance_codes': finance_codes!list()\n    }\n  }\n);"
        }
    ],
    "parameters": [
        {
            "name": "classification_profile_name",
            "description": "Specifies the identifier (name) for the instance of the CLASSIFICATION_PROFILE class; must be unique for the schema in which the object\nis created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        }
    ],
    "usage_notes": "Executing a CREATE OR REPLACE command removes the classification profile from all schemas, which turns off automatic classification.\nTo refer to this class by its unqualified name, include the database and schema of the class in your\nsearch path.\nIf the same tag and semantic category is mapped to two different values, then the order of the objects in the column_tag_map\ndetermines the tag and string value to set on a column. Order the column_tag_map arrays from highest preference to lowest\npreference."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-dataset",
    "title": "CREATE DATASET",
    "description": "Creates a new machine learning dataset in the current schema or the schema that you specify.",
    "syntax": "CREATE [ OR REPLACE ] [ IF NOT EXISTS ] DATASET <name>",
    "examples": [
        {
            "code": "CREATE DATASET my_dataset;"
        },
        {
            "code": "CREATE OR REPLACE DATASET my_dataset;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "The name of the dataset that you’re creating within the current schema or a schema that you specify."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-procedure",
    "title": "CREATE PROCEDURE",
    "description": "Creates a new stored procedure.",
    "syntax": "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '<java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '<java_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS <result_data_type> [ NOT NULL ]\n  LANGUAGE JAVASCRIPT\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  [ ARTIFACT_REPOSITORY = `<repository_name>` ]\n  [ PACKAGES = ( '<package_name>' [ , ... ] ) ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ { TEMP | TEMPORARY } ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '<python_version>'\n  [ ARTIFACT_REPOSITORY = `<repository_name>` ]\n  [ PACKAGES = ( '<package_name>' [ , ... ] ) ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<module_file_name>.<function_name>'\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <name_of_integration> [ , ... ] ) ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name> [ , ... ] ) ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n\nCREATE [ OR REPLACE ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '<scala_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ TARGET_PATH = '<stage_path_and_file_name_to_write>' ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS '<procedure_definition>'\n\nCREATE [ OR REPLACE ] [ SECURE ] PROCEDURE <name> (\n    [ <arg_name> <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> [ [ NOT ] NULL ] | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  LANGUAGE SCALA\n  RUNTIME_VERSION = '<scala_runtime_version>'\n  PACKAGES = ( 'com.snowflake:snowpark:<version>' [, '<package_name_and_version>' ...] )\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [, '<stage_path_and_file_name_to_read>' ...] ) ]\n  HANDLER = '<fully_qualified_method_name>'\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ VOLATILE | IMMUTABLE ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n\nCREATE [ OR REPLACE ] PROCEDURE <name> (\n    [ <arg_name> [ { IN | INPUT | OUT | OUTPUT } ] <arg_data_type> [ DEFAULT <default_value> ] ] [ , ... ] )\n  [ COPY GRANTS ]\n  RETURNS { <result_data_type> | TABLE ( [ <col_name> <col_data_type> [ , ... ] ] ) }\n  [ NOT NULL ]\n  LANGUAGE SQL\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  [ { VOLATILE | IMMUTABLE } ] -- Note: VOLATILE and IMMUTABLE are deprecated.\n  [ COMMENT = '<string_literal>' ]\n  [ EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER } ]\n  AS <procedure_definition>",
    "examples": [
        {
            "code": "CREATE OR REPLACE PROCEDURE sp_pi()\n    RETURNS FLOAT NOT NULL\n    LANGUAGE JAVASCRIPT\n    AS\n    $$\n    return 3.1415926;\n    $$\n    ;"
        },
        {
            "code": "CREATE OR REPLACE PROCEDURE stproc1(FLOAT_PARAM1 FLOAT)\n    RETURNS STRING\n    LANGUAGE JAVASCRIPT\n    STRICT\n    EXECUTE AS OWNER\n    AS\n    $$\n    var sql_command = \n     \"INSERT INTO stproc_test_table1 (num_col1) VALUES (\" + FLOAT_PARAM1 + \")\";\n    try {\n        snowflake.execute (\n            {sqlText: sql_command}\n            );\n        return \"Succeeded.\";   // Return a success/error indicator.\n        }\n    catch (err)  {\n        return \"Failed: \" + err;   // Return a success/error indicator.\n        }\n    $$\n    ;"
        },
        {
            "code": "CREATE OR REPLACE PROCEDURE my_proc(from_table STRING, to_table STRING, count INT)\n  RETURNS STRING\n  LANGUAGE PYTHON\n  RUNTIME_VERSION = '3.9'\n  PACKAGES = ('snowflake-snowpark-python')\n  HANDLER = 'run'\nAS\n$$\ndef run(session, from_table, to_table, count):\n  session.table(from_table).limit(count).write.save_as_table(to_table)\n  return \"SUCCESS\"\n$$;"
        },
        {
            "code": "CREATE OR REPLACE PROCEDURE my_proc(fromTable STRING, toTable STRING, count INT)\n  RETURNS STRING\n  LANGUAGE JAVA\n  RUNTIME_VERSION = '11'\n  PACKAGES = ('com.snowflake:snowpark:latest')\n  IMPORTS = ('@mystage/myjar.jar')\n  HANDLER = 'MyClass.myMethod';"
        }
    ],
    "parameters": [
        {
            "name": "name   (   [   arg_name   [   {   IN   |   INPUT   |   OUT   |   OUTPUT   }   ]   arg_data_type   .   [   DEFAULT   {default_value}   ]   ]   [   ,   ...   ]   )",
            "description": "Specifies the identifier ( name ), any arguments, and the default values for any optional arguments for the\nstored procedure. For the identifier: The identifier does not need to be unique for the schema in which the procedure is created because stored procedures are identified and resolved by the combination of the name and argument types . The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. “My object”). Identifiers enclosed in double quotes are also\ncase-sensitive. See Identifier requirements . For the arguments: For arg_name , specify the name of the argument. For { IN | INPUT | OUT | OUTPUT } , specify the type of the argument (input or output). The type specification is only valid\nfor a Snowflake Scripting stored procedure. For more information, see Using arguments passed to a stored procedure . For arg_data_type , use the Snowflake data type that corresponds to the language that you are using. For Java stored procedures , see SQL-Java Data Type Mappings . For JavaScript stored procedures , see SQL and JavaScript data type mapping . For Python stored procedures , see SQL-Python Data Type Mappings . For Scala stored procedures , see SQL-Scala Data Type Mappings . For Snowflake Scripting, a SQL data type . Note For stored procedures you write in Java, Python, or Scala (which use Snowpark APIs), omit the argument for the Snowpark Session object. The Session argument is not a formal parameter that you specify in CREATE PROCEDURE or CALL. When you call your\nstored procedure, Snowflake automatically creates a Session object and passes it to the handler function for your\nstored procedure. To indicate that an argument is optional, use DEFAULT default_value to specify the default value of the argument.\nFor the default value, you can use a literal or an expression. If you specify any optional arguments, you must place these after the required arguments. If a procedure has optional arguments, you cannot define additional procedures with the same name and different signatures. For details, see Specify optional arguments ."
        },
        {
            "name": "RETURNS   {   result_data_type   [   [   NOT   ]   NULL   ]   |   TABLE   (   [   col_name   col_data_type   [   ,   ...   ]   ]   )   }",
            "description": "Specifies the type of the result returned by the stored procedure. For result_data_type , use the Snowflake data type that corresponds to the type of the language that you are using. For Java stored procedures , see SQL-Java Data Type Mappings . For JavaScript stored procedures , see SQL and JavaScript data type mapping . For Python stored procedures , see SQL-Python Data Type Mappings . For Scala stored procedures , see SQL-Scala Data Type Mappings . For Snowflake Scripting, a SQL data type . Note Stored procedures you write in Snowpark (Java or Scala) must have a return value. In Snowpark (Python), when a stored procedure\nreturns no value, it is considered to be returning None . Note that every CREATE PROCEDURE statement must include a RETURNS\nclause that defines a return type, even if the procedure does not explicitly return anything. For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ) , if you know the Snowflake data types of the columns in the returned table, specify the column names and\ntypes: Otherwise (for example, if you are determining the column types during run time), you can omit the column names and types: Note Currently, in the RETURNS TABLE(...) clause, you can’t specify GEOGRAPHY as a column type. This\napplies whether you are creating a stored or anonymous procedure. If you attempt to specify GEOGRAPHY as a column type, calling the stored procedure results in the error: To work around this issue, you can omit the column arguments and types in RETURNS TABLE() . RETURNS TABLE(…) is supported only when the handler is written in the following languages: Java Python Scala Snowflake Scripting As a practical matter, outside of a Snowflake Scripting block , the returned value cannot be used because the call cannot be part of an expression ."
        },
        {
            "name": "LANGUAGE   language",
            "description": "Specifies the language of the stored procedure code. Note that this is optional for stored procedures written with Snowflake Scripting . Currently, the supported values for language include: JAVA (for Java ) JAVASCRIPT (for JavaScript ) PYTHON (for Python ) SCALA (for Scala ) SQL (for Snowflake Scripting ) Default: SQL ."
        },
        {
            "name": "AS   procedure_definition",
            "description": "Defines the code executed by the stored procedure. The definition can consist of any valid code. Note the following: For stored procedures for which the code is not in-line, omit the AS clause. This includes stored procedures with staged handlers. Instead, use the IMPORTS clause to specify the location of the file containing the code for the stored procedure. For\ndetails, see: Writing stored procedures with SQL and Python Writing Java handlers for stored procedures created with SQL Writing Scala handlers for stored procedures created with SQL For more information on in-line and staged handlers, see Keeping handler code in-line or on a stage . You must use string literal delimiters ( ' or $$ ) around procedure definition if: You are using a language other than Snowflake Scripting. You are creating a Snowflake Scripting procedure in SnowSQL or the Classic Console. See Using Snowflake Scripting in Snowflake CLI, SnowSQL, the Classic Console, and Python Connector . For stored procedures in JavaScript, if you are writing a string that contains newlines, you can use\nbackquotes (also called “backticks”) around the string. The following example of a JavaScript stored procedure uses $$ and backquotes because the body of the stored procedure\ncontains single quotes and double quotes: Snowflake does not completely validate the code when you execute the CREATE PROCEDURE command. For example, for Snowpark (Scala) stored procedures, the number and types of arguments are validated, but the body of\nthe function is not validated. If the number or types do not match (e.g. if the Snowflake data type NUMBER is used when the\nargument is a non-numeric type), executing the CREATE PROCEDURE command causes an error. If the code is not valid, the CREATE PROCEDURE command will succeed, and errors will be returned when the stored procedure is\ncalled. For more details about stored procedures, see Working with stored procedures ."
        },
        {
            "name": "RUNTIME_VERSION   =   ' language_runtime_version '",
            "description": "The language runtime version to use. Currently, the supported versions are: 11"
        },
        {
            "name": "PACKAGES   =   (   ' snowpark_package_name '   [,   ' package_name '   ...]   )",
            "description": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler code’s\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API . By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Java, query the INFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: To specify the package name and version number use the following form: To specify the latest version, specify latest for version . For example, to include a package from the latest Snowpark library in Snowflake, use the following: When specifying a package from the Snowpark library, you must specify version 1.3.0 or later."
        },
        {
            "name": "HANDLER   =   ' fully_qualified_method_name '",
            "description": "Use the fully qualified name of the method or function for the stored procedure. This is typically in the\nfollowing form: where: corresponds to the package containing the object or class:"
        },
        {
            "name": "RUNTIME_VERSION   =   ' language_runtime_version '",
            "description": "The language runtime version to use. Currently, the supported versions are: 3.9 3.10 3.11 3.12"
        },
        {
            "name": "PACKAGES   =   (   ' snowpark_package_name '   [,   ' package_name '   ...]   )",
            "description": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler code’s\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API . By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Python, query the INFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: Snowflake includes a large number of packages available through Anaconda; for more information, see Using third-party packages . To specify the package name and version number use the following form: To specify the latest version, omit the version number. For example, to include the spacy package version 2.3.5 (along with the latest version of the required Snowpark package), use the\nfollowing: When specifying a package from the Snowpark library, you must specify version 0.4.0 or later. Omit the version number to use the\nlatest version available in Snowflake. Preview Feature — Open Specifying a range of Python package versions is available as a preview feature to all accounts. You can specify package versions by using these version\nspecifiers: == , <= , >= , < ,or > . For example:"
        },
        {
            "name": "HANDLER   =   ' fully_qualified_method_name '",
            "description": "Use the name of the stored procedure’s function or method. This can differ depending on whether the code is in-line or\nreferenced at a stage. When the code is in-line, you can specify just the function name, as in the following example: When the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name> ."
        },
        {
            "name": "RUNTIME_VERSION   =   ' language_runtime_version '",
            "description": "The language runtime version to use. Currently, the supported versions are: 2.12"
        },
        {
            "name": "PACKAGES   =   (   ' snowpark_package_name '   [,   ' package_name '   ...]   )",
            "description": "A comma-separated list of the names of packages deployed in Snowflake that should be included in the handler code’s\nexecution environment. The Snowpark package is required for stored procedures, so it must always be referenced in the PACKAGES clause.\nFor more information about Snowpark, see Snowpark API . By default, the environment in which Snowflake runs stored procedures includes a selected set of packages for supported languages.\nWhen you reference these packages in the PACKAGES clause, it is not necessary to reference a file containing the package in the IMPORTS\nclause because the package is already available in Snowflake. You can also specify the package version. For the list of supported packages and versions for Scala, query the INFORMATION_SCHEMA.PACKAGES view for rows, specifying the language. For example: To specify the package name and version number use the following form: To specify the latest version, specify latest for version . For example, to include a package from the latest Snowpark library in Snowflake, use the following: Snowflake supports using Snowpark version 0.9.0 or later in a Scala stored procedure. Note, however, that these versions have\nlimitations. For example, versions prior to 1.1.0 do not support the use of transactions in a stored procedure."
        },
        {
            "name": "HANDLER   =   ' fully_qualified_method_name '",
            "description": "Use the fully qualified name of the method or function for the stored procedure. This is typically in the following form: where: corresponds to the package containing the object or class:"
        },
        {
            "name": "SECURE",
            "description": "Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   }",
            "description": "Specifies that the procedure persists for only the duration of the session in which you created it.\nA temporary procedure is dropped at the end of the session. Default: No value. If a procedure is not declared as TEMPORARY , it is permanent. You cannot create temporary procedures that have the same name as\na procedure that already exists in the schema. Note that creating a temporary procedure does not require the CREATE PROCEDURE privilege on the schema in which the object is created. For more information about creating temporary procedures, see Temporary procedures ."
        },
        {
            "name": "[   [   NOT   ]   NULL   ]",
            "description": "Specifies whether the stored procedure can return NULL values or must return only NON-NULL values. The default is NULL (i.e. the stored procedure can return NULL)."
        },
        {
            "name": "CALLED   ON   NULL   INPUT   or   .   {   RETURNS   NULL   ON   NULL   INPUT   |   STRICT   }",
            "description": "Specifies the behavior of the stored procedure when called with null inputs. In contrast to system-defined functions, which\nalways return null when any input is null, stored procedures can handle null inputs, returning non-null values even when an\ninput is null: CALLED ON NULL INPUT will always call the stored procedure with null inputs. It is up to the procedure to handle such\nvalues appropriately. RETURNS NULL ON NULL INPUT (or its synonym STRICT ) will not call the stored procedure if any input is null,\nso the statements inside the stored procedure will not be executed. Instead, a null value will always be returned. Note that\nthe procedure might still return null for non-null inputs. Default: CALLED ON NULL INPUT"
        },
        {
            "name": "VOLATILE   |   IMMUTABLE",
            "description": "Deprecated Attention These keywords are deprecated for stored procedures. These keywords are not intended to apply to stored procedures. In a\nfuture release, these keywords will be removed from the documentation."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the stored procedure, which is displayed in the DESCRIPTION column in the SHOW PROCEDURES output. Default: stored procedure"
        },
        {
            "name": "EXECUTE   AS   OWNER   or   .   EXECUTE   AS   CALLER   or   .   EXECUTE   AS   RESTRICTED   CALLER",
            "description": "Preview Feature — Open Restricted caller’s rights ( EXECUTE AS RESTRICTED CALLER ) is a preview feature available to all accounts. Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with\nthe privileges of the caller (a “caller’s rights” stored procedure): If you execute CREATE PROCEDURE … EXECUTE AS OWNER, then the procedure will execute as an owner’s rights procedure. If you execute the statement CREATE PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a\ncaller’s rights procedure. If you execute the statement CREATE PROCEDURE … EXECUTE AS RESTRICTED CALLER, then in the future the procedure will execute as a\ncaller’s rights procedure, but might not be able to run with all of the caller’s privileges. For more information, see Restricted caller’s rights . If EXECUTE AS ... isn’t specified, the procedure runs as an owner’s rights stored procedure. Owner’s rights stored\nprocedures have less access to the caller’s environment (for example, the caller’s session variables), and Snowflake defaults to this\nhigher level of privacy and security. For more information, see Understanding caller’s rights and owner’s rights stored procedures . Default: OWNER"
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access privileges from the original procedure when a new procedure is created using CREATE OR REPLACE PROCEDURE. The parameter copies all privileges, except OWNERSHIP, from the existing procedure to the new procedure. The new procedure will\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE PROCEDURE\nstatement owns the new procedure. Note: The SHOW GRANTS output for the replacement procedure lists the grantee for the copied privileges as the\nrole that executed the CREATE PROCEDURE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE PROCEDURE command (i.e. within the same transaction)."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [,   ' stage_path_and_file_name_to_read '   ...]   )",
            "description": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the\nstored procedure’s handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the procedure_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. If you specify both the IMPORTS and TARGET_PATH clauses, the file name in the TARGET_PATH clause must\nbe different from each file name in the IMPORTS clause, even if the files are in different subdirectories or different\nstages. The generated JAR file remains until you explicitly delete it, even if you drop the procedure. When you drop the procedure you should\nseparately remove the JAR file because the JAR is no longer needed to support the procedure. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this procedure to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nprocedure’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE PROCEDURE command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [,   ' stage_path_and_file_name_to_read '   ...]   )",
            "description": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If your stored procedure’s code will be on a stage, you must also include a path to the module file your code is in. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nprocedure’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code\nwhen making requests of an external network, such as an external REST API."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE PROCEDURE command’s EXTERNAL_ACCESS_INTEGRATIONS parameter This parameter’s value is a comma-separated list of assignment expressions with the following parts: secret_name as the name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. For more information, including an example, refer to Using the external access integration in a function or procedure ."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [,   ' stage_path_and_file_name_to_read '   ...]   )",
            "description": "The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that\nyour stored procedure depends on: If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside\nthe stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the\nstored procedure’s handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages."
        },
        {
            "name": "TARGET_PATH   =   stage_path_and_file_name_to_write",
            "description": "Specifies the location to which Snowflake should write the JAR file containing the result of compiling the handler source code specified\nin the procedure_definition . If this clause is included, Snowflake writes the resulting JAR file to the stage location specified by the clause’s value. If this\nclause is omitted, Snowflake re-compiles the source code each time the code is needed. In that case, the JAR file is not stored\npermanently, and the user does not need to clean up the JAR file. Snowflake returns an error if the TARGET_PATH matches an existing file; you cannot use TARGET_PATH to overwrite an\nexisting file. If you specify both the IMPORTS and TARGET_PATH clauses, the file name in the TARGET_PATH clause must\nbe different from each file name in the IMPORTS clause, even if the files are in different subdirectories or different\nstages. The generated JAR file remains until you explicitly delete it, even if you drop the procedure. When you drop the procedure you should\nseparately remove the JAR file because the JAR is no longer needed to support the procedure. For example, the following TARGET_PATH example would result in a myhandler.jar file generated and copied to the handlers stage. When you drop this procedure to remove it, you’ll also need to remove its handler JAR file, such as by executing the REMOVE command ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-warehouse",
    "title": "CREATE WAREHOUSE",
    "description": "Creates a new virtual warehouse in the system.",
    "syntax": "CREATE [ OR REPLACE ] WAREHOUSE [ IF NOT EXISTS ] <name>\n       [ [ WITH ] objectProperties ]\n       [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n       [ objectParams ]\n\nobjectProperties ::=\n  WAREHOUSE_TYPE = { STANDARD | 'SNOWPARK-OPTIMIZED' }\n  WAREHOUSE_SIZE = { XSMALL | SMALL | MEDIUM | LARGE | XLARGE | XXLARGE | XXXLARGE | X4LARGE | X5LARGE | X6LARGE }\n  RESOURCE_CONSTRAINT = { STANDARD_GEN_1 | STANDARD_GEN_2 | MEMORY_1X | MEMORY_1X_x86 | MEMORY_16X | MEMORY_16X_x86 | MEMORY_64X | MEMORY_64X_x86 }\n  MAX_CLUSTER_COUNT = <num>\n  MIN_CLUSTER_COUNT = <num>\n  SCALING_POLICY = { STANDARD | ECONOMY }\n  AUTO_SUSPEND = { <num> | NULL }\n  AUTO_RESUME = { TRUE | FALSE }\n  INITIALLY_SUSPENDED = { TRUE | FALSE }\n  RESOURCE_MONITOR = <monitor_name>\n  COMMENT = '<string_literal>'\n  ENABLE_QUERY_ACCELERATION = { TRUE | FALSE }\n  QUERY_ACCELERATION_MAX_SCALE_FACTOR = <num>\n\nobjectParams ::=\n  MAX_CONCURRENCY_LEVEL = <num>\n  STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = <num>\n  STATEMENT_TIMEOUT_IN_SECONDS = <num>",
    "examples": [
        {
            "code": "CREATE OR REPLACE WAREHOUSE my_wh WITH WAREHOUSE_SIZE='X-LARGE';"
        },
        {
            "code": "CREATE OR REPLACE WAREHOUSE my_wh WAREHOUSE_SIZE=LARGE INITIALLY_SUSPENDED=TRUE;"
        },
        {
            "code": "CREATE WAREHOUSE so_warehouse WITH\n  WAREHOUSE_TYPE = 'SNOWPARK-OPTIMIZED'\n  WAREHOUSE_SIZE = xlarge\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_x86';"
        },
        {
            "code": "CREATE OR ALTER WAREHOUSE so_warehouse\n  WAREHOUSE_TYPE = 'SNOWPARK_OPTIMIZED'\n  WAREHOUSE_SIZE = 'X-LARGE'\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_X86'\n  AUTO_RESUME = TRUE\n  COMMENT = 'Snowpark warehouse for ingestion';"
        },
        {
            "code": "CREATE OR ALTER WAREHOUSE so_warehouse\n  WAREHOUSE_TYPE = 'SNOWPARK-OPTIMIZED'\n  WAREHOUSE_SIZE = 'X-LARGE'\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_X86'\n  AUTO_RESUME = FALSE\n  COMMENT = 'Snowpark warehouse for ingestion (disabled for auto-resume)';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the virtual warehouse; must be unique for your account. In addition, the identifier must start with an alphabetic character and can’t contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "MAX_CONCURRENCY_LEVEL   =   num",
            "description": "Object parameter that specifies the concurrency level for SQL statements (i.e. queries and DML) executed by a warehouse cluster. For a detailed description of this parameter, see MAX_CONCURRENCY_LEVEL ."
        },
        {
            "name": "STATEMENT_QUEUED_TIMEOUT_IN_SECONDS   =   num",
            "description": "Object parameter that specifies the time, in seconds, a SQL statement (query, DDL, DML, etc.) can be queued on a warehouse before it is\ncanceled by the system. For a detailed description of this parameter, see STATEMENT_QUEUED_TIMEOUT_IN_SECONDS ."
        },
        {
            "name": "STATEMENT_TIMEOUT_IN_SECONDS   =   num",
            "description": "Object parameter that specifies the time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For a detailed description of this parameter, see STATEMENT_TIMEOUT_IN_SECONDS ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-catalog-integration",
    "title": "CREATE CATALOG INTEGRATION",
    "description": "Creates a new catalog integration for Apache Iceberg™ tables\nin the account or replaces an existing catalog integration."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-semantic-view",
    "title": "CREATE SEMANTIC VIEW",
    "description": "Creates a new semantic view in the current/specified schema.",
    "syntax": "CREATE [ OR REPLACE ] SEMANTIC VIEW [ IF NOT EXISTS ] <name>\n  TABLES ( logicalTable [ , ... ] )\n  [ RELATIONSHIPS ( relationshipDef [ , ... ] ) ]\n  [ FACTS ( semanticExpression [ , ... ] ) ]\n  [ DIMENSIONS ( semanticExpression [ , ... ] ) ]\n  [ METRICS ( semanticExpression [ , ... ] ) ]\n  [ COMMENT = '<comment_about_semantic_view>' ]\n  [ COPY GRANTS ]\n\nlogicalTable ::=\n  [ <table_alias> AS ] <table_name>\n  [ PRIMARY KEY ( <primary_key_column_name> [ , ... ] ) ]\n  [\n    UNIQUE ( <unique_column_name> [ , ... ] )\n    [ ... ]\n  ]\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_table>' ]\n\nrelationshipDef ::=\n  [ <relationship_identifier> AS ]\n  <table_alias> ( <column_name> [ , ... ] )\n  REFERENCES\n  <ref_table_alias> [ ( <ref_column_name> [ , ... ] ) ]\n\nsemanticExpression ::=\n  <table_alias>.<dim_fact_or_metric> AS <sql_expr>\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_dim_fact_or_metric>' ]\n\nwindowFunctionMetricDefinition ::=\n  <window_function>( <metric> ) OVER (\n    [ PARTITION BY { <exprs_using_dimensions_or_metrics> | EXCLUDING <dimensions> } ]\n    [ ORDER BY <exprs_using_dimensions_or_metrics> [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ]\n    [ <windowFrameClause> ]\n  )",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] SEMANTIC VIEW [ IF NOT EXISTS ] <name>\n  TABLES ( logicalTable [ , ... ] )\n  [ RELATIONSHIPS ( relationshipDef [ , ... ] ) ]\n  [ FACTS ( semanticExpression [ , ... ] ) ]\n  [ DIMENSIONS ( semanticExpression [ , ... ] ) ]\n  [ METRICS ( semanticExpression [ , ... ] ) ]\n  [ COMMENT = '<comment_about_semantic_view>' ]\n  [ COPY GRANTS ]"
        },
        {
            "code": "logicalTable ::=\n  [ <table_alias> AS ] <table_name>\n  [ PRIMARY KEY ( <primary_key_column_name> [ , ... ] ) ]\n  [\n    UNIQUE ( <unique_column_name> [ , ... ] )\n    [ ... ]\n  ]\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_table>' ]"
        },
        {
            "code": "relationshipDef ::=\n  [ <relationship_identifier> AS ]\n  <table_alias> ( <column_name> [ , ... ] )\n  REFERENCES\n  <ref_table_alias> [ ( <ref_column_name> [ , ... ] ) ]"
        },
        {
            "code": "semanticExpression ::=\n  <table_alias>.<dim_fact_or_metric> AS <sql_expr>\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_dim_fact_or_metric>' ]"
        },
        {
            "code": "windowFunctionMetricDefinition ::=\n  <window_function>( <metric> ) OVER (\n    [ PARTITION BY { <exprs_using_dimensions_or_metrics> | EXCLUDING <dimensions> } ]\n    [ ORDER BY <exprs_using_dimensions_or_metrics> [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ]\n    [ <windowFrameClause> ]\n  )"
        },
        {
            "code": "TABLES(\n  ...\n  product_table UNIQUE (service_id)"
        },
        {
            "code": "TABLES(\n  ...\n  product_table UNIQUE (product_area_id, product_id)\n  ..."
        },
        {
            "code": "TABLES(\n  ...\n  product_table UNIQUE (product_area_id, product_id) UNIQUE (service_id)\n  ..."
        },
        {
            "code": "CREATE SEMANTIC VIEW sv\n  ...\n  METRICS (\n    table_1.metric_2 AS SUM(table_1.metric_1) OVER\n      (PARTITION BY EXCLUDING table_l.dimension_1 ORDER BY table_1.dimension_2)\n  )\n  ..."
        },
        {
            "code": "SELECT * FROM SEMANTIC VIEW(\n  sv\n  METRICS (\n    table_1.metric_2\n  )\n  DIMENSIONS (\n    table_1.dimension_1,\n    table_1.dimension_2,\n    table_1.dimension_3\n  );"
        },
        {
            "code": "SUM(table_1.metric_1) OVER (\n  PARTITION BY table_1.dimension_2, table_1.dimension_3\n  ORDER BY table_1.dimension_2\n)"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the name of the semantic view; the name must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' comment_about_semantic_view '",
            "description": "Specifies a comment about the semantic view."
        },
        {
            "name": "COPY   GRANTS",
            "description": "When you specify OR REPLACE to replace an existing semantic view with a new semantic view, you can set this parameter to copy\nany privileges granted on the existing semantic view to the new semantic view. The command copies all privilege grants except OWNERSHIP from the existing semantic view to the new semantic view. The\nrole that executes the CREATE SEMANTIC VIEW statement owns the new view. The new semantic view does not inherit any future grants defined for the object type in the schema. The operation to copy grants occurs atomically with the CREATE SEMANTIC VIEW statement (in other words, within the same\ntransaction). If you omit COPY GRANTS, the new semantic view does not inherit any explicit access privileges granted on the existing\nsemantic view but does inherit any future grants defined for the object type in the schema."
        },
        {
            "name": "table_alias   AS",
            "description": "Specifies an optional alias for the logical table. If you specify an alias, you must use this alias when referring to the logical table in relationships, facts, dimensions,\nand metrics. If you do not specify an alias, you use the unqualified logical table name to refer to the table."
        },
        {
            "name": "table_name",
            "description": "Specifies the name of the logical table."
        },
        {
            "name": "PRIMARY   KEY   (   primary_key_column_name   [   ,   ...   ]   )",
            "description": "Specifies the names of one or more columns in the logical table that serve as the primary key of the table."
        },
        {
            "name": "UNIQUE   (   unique_column_name   [   ,   ...   ]   )",
            "description": "Specifies the name of a column containing a unique value or the names of columns that contain unique combinations of values. For example, if the column service_id contains unique values, specify: If the combination of values in the product_area_id and product_id columns is unique, specify: You can identify multiple columns and multiple combinations of columns as unique in a given logical table: Note If you already identified a column as a primary key column (by using PRIMARY KEY), do not add the UNIQUE clause for that\ncolumn."
        },
        {
            "name": "WITH   SYNONYMS   [   =   ]   (   ' synonym '   [   ,   ...   ]   )",
            "description": "Specifies one or more synonyms for the logical table. Unlike aliases, synonyms are used for informational purposes only. You do\nnot use synonyms to refer to the logical table in relationships, dimensions, metrics, and facts."
        },
        {
            "name": "COMMENT   =   ' comment_about_table '",
            "description": "Specifies a comment about the logical table."
        },
        {
            "name": "relationship_identifier   AS",
            "description": "Specifies an optional identifier for the relationship."
        },
        {
            "name": "table_alias   (   column_name   [   ,   ...   ]   )",
            "description": "Specifies one of the logical tables and one or more of its columns that refers to columns in another logical table."
        },
        {
            "name": "ref_table_alias   [   (   ref_column_name   [   ,   ...   ]   )   ]",
            "description": "Specifies the other logical table and one or more of its columns that are referred to by the first logical table. The columns must be identified as a PRIMARY KEY or UNIQUE in the logical table definition ."
        },
        {
            "name": "table_alias . semantic_expression_name   AS   sql_expr",
            "description": "Specifies a name for a dimension, fact, or metric and the SQL expression for computing that dimension, fact, or metric. See How Snowflake validates semantic views for the rules for defining a valid semantic view."
        },
        {
            "name": "WITH   SYNONYMS   [   =   ]   (   ' synonym '   [   ,   ...   ]   )",
            "description": "Specifies one or more optional synonyms for the dimension, fact, or metric. Note that synonyms are used for informational\npurposes only. You cannot use a synonym to refer to a dimension, fact, or metric in another dimension, fact, or metric."
        },
        {
            "name": "COMMENT   =   ' comment_about_dim_fact_or_metric '",
            "description": "Specifies an optional comment about the dimension, fact, or metric."
        },
        {
            "name": "metric",
            "description": "Specifies a metric expression for this window function. You can specify a metric or any valid metric expression that you can use\nto define a metric in this entity."
        },
        {
            "name": "PARTITION   BY   ...",
            "description": "Groups rows into partitions. You can either partition by a specified set of expressions or by all dimensions (except selected\ndimensions) specified in the query: Groups rows into partitions by SQL expressions. In the SQL expression: Any dimensions in the expression must be accessible from the same entity that defines the window function metric. Any metrics must belong to the same table where this metric is being defined. You cannot specify aggregates, window functions, or subqueries. Groups rows into partitions by all of the dimensions specified in the SEMANTIC_VIEW clause of\nthe query, except for the dimensions specified by dimensions . dimensions must only refer to dimensions that are accessible from the entity that defines the window function\nmetric. For example, suppose that you exclude the dimension table_1.dimension_1 from partitioning: Suppose that you run a query that specifies the dimension table_1.dimension_1 : In the query, the metric table_1.metric_2 is evaluated as: Note how table_1.dimension_1 is excluded from the PARTITION BY clause. Note You cannot use EXCLUDING outside of metric definitions in semantic views. EXCLUDING is not supported in window function\ncalls in any other context."
        },
        {
            "name": "ORDER   BY   exprs_using_dimensions_or_metrics    [   ASC   |   DESC   ]   [   NULLS     FIRST   |   LAST     ]   [,   ...   ]",
            "description": "Orders rows within each partition. In the SQL expression: Any dimensions in the expression must be accessible from the same entity that defines the window function metric. Any metrics must belong to the same table where this metric is being defined. You cannot specify aggregates, window functions, or subqueries."
        },
        {
            "name": "windowFrameClause",
            "description": "See Window function syntax and usage ."
        },
        {
            "name": "PARTITION   BY   exprs_using_dimensions_or_metrics",
            "description": "Groups rows into partitions by SQL expressions. In the SQL expression: Any dimensions in the expression must be accessible from the same entity that defines the window function metric. Any metrics must belong to the same table where this metric is being defined. You cannot specify aggregates, window functions, or subqueries."
        },
        {
            "name": "PARTITION   BY   EXCLUDING   dimensions",
            "description": "Groups rows into partitions by all of the dimensions specified in the SEMANTIC_VIEW clause of\nthe query, except for the dimensions specified by dimensions . dimensions must only refer to dimensions that are accessible from the entity that defines the window function\nmetric. For example, suppose that you exclude the dimension table_1.dimension_1 from partitioning: Suppose that you run a query that specifies the dimension table_1.dimension_1 : In the query, the metric table_1.metric_2 is evaluated as: Note how table_1.dimension_1 is excluded from the PARTITION BY clause. Note You cannot use EXCLUDING outside of metric definitions in semantic views. EXCLUDING is not supported in window function\ncalls in any other context."
        }
    ],
    "usage_notes": "The semantic view must be valid and must follow the rules described in\nHow Snowflake validates semantic views.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-password-policy",
    "title": "CREATE PASSWORD POLICY",
    "description": "Creates a new password policy or replaces an existing password policy.",
    "syntax": "CREATE [ OR REPLACE ] PASSWORD POLICY [ IF NOT EXISTS ] <name>\n  [ PASSWORD_MIN_LENGTH = <integer> ]\n  [ PASSWORD_MAX_LENGTH = <integer> ]\n  [ PASSWORD_MIN_UPPER_CASE_CHARS = <integer> ]\n  [ PASSWORD_MIN_LOWER_CASE_CHARS = <integer> ]\n  [ PASSWORD_MIN_NUMERIC_CHARS = <integer> ]\n  [ PASSWORD_MIN_SPECIAL_CHARS = <integer> ]\n  [ PASSWORD_MIN_AGE_DAYS = <integer> ]\n  [ PASSWORD_MAX_AGE_DAYS = <integer> ]\n  [ PASSWORD_MAX_RETRIES = <integer> ]\n  [ PASSWORD_LOCKOUT_TIME_MINS = <integer> ]\n  [ PASSWORD_HISTORY = <integer> ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE PASSWORD POLICY PASSWORD_POLICY_PROD_1\n    PASSWORD_MIN_LENGTH = 14\n    PASSWORD_MAX_LENGTH = 24\n    PASSWORD_MIN_UPPER_CASE_CHARS = 2\n    PASSWORD_MIN_LOWER_CASE_CHARS = 2\n    PASSWORD_MIN_NUMERIC_CHARS = 2\n    PASSWORD_MIN_SPECIAL_CHARS = 2\n    PASSWORD_MAX_AGE_DAYS = 30\n    PASSWORD_MAX_RETRIES = 3\n    PASSWORD_LOCKOUT_TIME_MINS = 30\n    PASSWORD_HISTORY = 5\n    COMMENT = 'production account password policy';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the password policy; must be unique for your account. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "PASSWORD_MIN_LENGTH   =   integer",
            "description": "Specifies the minimum number of characters the password must contain. Supported range: 8 to 256, inclusive. Default: 14"
        },
        {
            "name": "PASSWORD_MAX_LENGTH   =   integer",
            "description": "Specifies the maximum number of characters the password must contain. This number must be greater than or equal to the sum of PASSWORD_MIN_LENGTH , PASSWORD_MIN_UPPER_CASE_CHARS , and PASSWORD_MIN_LOWER_CASE_CHARS . Supported range: 8 to 256, inclusive. Default: 256"
        },
        {
            "name": "PASSWORD_MIN_UPPER_CASE_CHARS   =   integer",
            "description": "Specifies the minimum number of uppercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1"
        },
        {
            "name": "PASSWORD_MIN_LOWER_CASE_CHARS   =   integer",
            "description": "Specifies the minimum number of lowercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1"
        },
        {
            "name": "PASSWORD_MIN_NUMERIC_CHARS   =   integer",
            "description": "Specifies the minimum number of numeric characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1"
        },
        {
            "name": "PASSWORD_MIN_SPECIAL_CHARS   =   integer",
            "description": "Specifies the minimum number of special characters the password must contain. Supported range: 0 to 256, inclusive. Default: 0"
        },
        {
            "name": "PASSWORD_MIN_AGE_DAYS   =   integer",
            "description": "Specifies the number of days the user must wait before a recently changed password can be changed again. Supported range: 0 to 999, inclusive. Default: 0"
        },
        {
            "name": "PASSWORD_MAX_AGE_DAYS   =   integer",
            "description": "Specifies the maximum number of days before the password must be changed. Supported range: 0 to 999, inclusive. A value of zero (i.e. 0 ) indicates that the password does not need to be changed. Snowflake does not recommend choosing this\nvalue for a default account-level password policy or for any user-level policy. Instead, choose a value that meets your internal\nsecurity guidelines. Default: 90, which means the password must be changed every 90 days. Important This parameter is stateful. For details, see the note in Custom password policy for the account and users ."
        },
        {
            "name": "PASSWORD_MAX_RETRIES   =   integer",
            "description": "Specifies the maximum number of attempts to enter a password before being locked out. Supported range: 1 to 10, inclusive. Default: 5 Important This parameter is stateful. For details, see the note in Custom password policy for the account and users ."
        },
        {
            "name": "PASSWORD_LOCKOUT_TIME_MINS   =   integer",
            "description": "Specifies the number of minutes the user account will be locked after exhausting the designated number of password retries\n(i.e. PASSWORD_MAX_RETRIES ). Supported range: 1 to 999, inclusive. Default: 15 Important This parameter is stateful. For details, see the note in Custom password policy for the account and users ."
        },
        {
            "name": "PASSWORD_HISTORY   =   integer",
            "description": "Specifies the number of the most recent passwords that Snowflake stores. These stored passwords cannot be repeated when a user updates\ntheir password value. The current password value does not count towards the history. When you increase the history value, Snowflake saves the previous values. When you decrease the value, Snowflake saves the stored values up to that value that is set. For example, if the history value is 8 and\nyou change the history value to 3, Snowflake stores the most recent 3 passwords and deletes the 5 older password values from the history. Default: 5 Max: 24"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the password policy."
        }
    ],
    "usage_notes": "If you want to replace an existing password policy and need to see the current definition of the policy, call the\nGET_DDL function or run the DESCRIBE PASSWORD POLICY command.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-masking-policy",
    "title": "CREATE MASKING POLICY",
    "description": "Creates a new masking policy in the current/specified schema or replaces an existing masking policy.",
    "syntax": "CREATE [ OR REPLACE ] MASKING POLICY [ IF NOT EXISTS ] <name> AS\n( <arg_name_to_mask> <arg_type_to_mask> [ , <arg_1> <arg_type_1> ... ] )\nRETURNS <arg_type_to_mask> -> <body>\n[ COMMENT = '<string_literal>' ]\n[ EXEMPT_OTHER_POLICIES = { TRUE | FALSE } ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] MASKING POLICY [ IF NOT EXISTS ] <name> AS\n( <arg_name_to_mask> <arg_type_to_mask> [ , <arg_1> <arg_type_1> ... ] )\nRETURNS <arg_type_to_mask> -> <body>\n[ COMMENT = '<string_literal>' ]\n[ EXEMPT_OTHER_POLICIES = { TRUE | FALSE } ]"
        },
        {
            "code": "CREATE OR REPLACE MASKING POLICY email_mask AS (val string) returns string ->\n  CASE\n    WHEN current_role() IN ('ANALYST') THEN VAL\n    ELSE '*********'\n  END;"
        },
        {
            "code": "case\n  when current_account() in ('<prod_account_identifier>') then val\n  else '*********'\nend;"
        },
        {
            "code": "case\n  when current_role() IN ('ANALYST') then val\n  else NULL\nend;"
        },
        {
            "code": "CASE\n  WHEN current_role() IN ('ANALYST') THEN val\n  ELSE '********'\nEND;"
        },
        {
            "code": "CASE\n  WHEN current_role() IN ('ANALYST') THEN val\n  ELSE sha2(val) -- return hash of the column value\nEND;"
        },
        {
            "code": "CASE\n  WHEN current_role() IN ('ANALYST') THEN val\n  WHEN current_role() IN ('SUPPORT') THEN regexp_replace(val,'.+\\@','*****@') -- leave email domain unmasked\n  ELSE '********'\nEND;"
        },
        {
            "code": "case\n  WHEN current_role() in ('SUPPORT') THEN val\n  else date_from_parts(0001, 01, 01)::timestamp_ntz -- returns 0001-01-01 00:00:00.000\nend;"
        },
        {
            "code": "CASE\n  WHEN current_role() IN ('ANALYST') THEN val\n  ELSE mask_udf(val) -- custom masking function\nEND;"
        },
        {
            "code": "CASE\n   WHEN current_role() IN ('ANALYST') THEN val\n   ELSE OBJECT_INSERT(val, 'USER_IPADDRESS', '****', true)\nEND;"
        },
        {
            "code": "CASE\n  WHEN EXISTS\n    (SELECT role FROM <db>.<schema>.entitlement WHERE mask_method='unmask' AND role = current_role()) THEN val\n  ELSE '********'\nEND;"
        },
        {
            "code": "case\n  when current_role() in ('ANALYST') then DECRYPT(val, $passphrase)\n  else val -- shows encrypted value\nend;"
        },
        {
            "code": "-- Flatten the JSON data\n\ncreate or replace table <table_name> (v variant) as\nselect value::variant\nfrom @<table_name>,\n  table(flatten(input => parse_json($1):stationLocation));\n\n-- JavaScript UDF to mask latitude, longitude, and location data\n\nCREATE OR REPLACE FUNCTION full_location_masking(v variant)\n  RETURNS variant\n  LANGUAGE JAVASCRIPT\n  AS\n  $$\n    if (\"latitude\" in V) {\n      V[\"latitude\"] = \"**latitudeMask**\";\n    }\n    if (\"longitude\" in V) {\n      V[\"longitude\"] = \"**longitudeMask**\";\n    }\n    if (\"location\" in V) {\n      V[\"location\"] = \"**locationMask**\";\n    }\n\n    return V;\n  $$;\n\n  -- Grant UDF usage to ACCOUNTADMIN\n\n  grant ownership on function FULL_LOCATION_MASKING(variant) to role accountadmin;\n\n  -- Create a masking policy using JavaScript UDF\n\n  create or replace masking policy json_location_mask as (val variant) returns variant ->\n    CASE\n      WHEN current_role() IN ('ANALYST') THEN val\n      else full_location_masking(val)\n      -- else object_insert(val, 'latitude', '**locationMask**', true) -- limited to one value at a time\n    END;"
        },
        {
            "code": "create masking policy mask_geo_point as (val geography) returns geography ->\n  case\n    when current_role() IN ('ANALYST') then val\n    else to_geography('POINT(-122.35 37.55)')\n  end;"
        },
        {
            "code": "alter table mydb.myschema.geography modify column b set masking policy mask_geo_point;\nalter session set geography_output_format = 'GeoJSON';\nuse role public;\nselect * from mydb.myschema.geography;"
        },
        {
            "code": "---+--------------------+\n A |         B          |\n---+--------------------+\n 1 | {                  |\n   |   \"coordinates\": [ |\n   |     -122.35,       |\n   |     37.55          |\n   |   ],               |\n   |   \"type\": \"Point\"  |\n   | }                  |\n 2 | {                  |\n   |   \"coordinates\": [ |\n   |     -122.35,       |\n   |     37.55          |\n   |   ],               |\n   |   \"type\": \"Point\"  |\n   | }                  |\n---+--------------------+"
        },
        {
            "code": "alter session set geography_output_format = 'WKT';\nselect * from mydb.myschema.geography;\n\n---+----------------------+\n A |         B            |\n---+----------------------+\n 1 | POINT(-122.35 37.55) |\n 2 | POINT(-122.35 37.55) |\n---+----------------------+"
        },
        {
            "code": "-- Conditional Masking\n\ncreate masking policy email_visibility as\n(email varchar, visibility string) returns varchar ->\n  case\n    when current_role() = 'ADMIN' then email\n    when visibility = 'Public' then email\n    else '***MASKED***'\n  end;"
        },
        {
            "code": "-- Conditional Tokenization\n\ncreate masking policy de_email_visibility as\n (email varchar, visibility string) returns varchar ->\n   case\n     when current_role() = 'ADMIN' and visibility = 'Public' then de_email(email)\n     else email -- sees tokenized data\n   end;"
        },
        {
            "code": "create or replace masking policy governance.policies.email_mask\nas (val string) returns string ->\ncase\n  when current_role() in ('ANALYST') then val\n  when current_role() in ('SUPPORT') then regexp_replace(val,'.+\\@','*****@')\n  else '********'\nend\ncomment = 'specify in row access policy'\nexempt_other_policies = true\n;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the masking policy; must be unique for your schema. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "AS   (   arg_name_to_mask   arg_type_to_mask   [   ,   arg_1   arg_type_1   ...   ]   )",
            "description": "The signature for the masking policy; specifies the input columns and data types to evaluate at query runtime. For more details, see SQL data types reference . The first column and its data type always indicate the column data type values to mask or tokenize in the subsequent\npolicy conditions. Note that you can not specify a virtual column as the first column argument in a conditional masking policy. Specifies the conditional columns and their data types to evaluate to determine whether the policy conditions should mask or tokenize\nthe data in the first column in each row of the query result. If these additional columns and data types are not specified, Snowflake evaluates the policy as a normal masking policy."
        },
        {
            "name": "RETURNS   arg_type_to_mask",
            "description": "The return data type must match the input data type of the first column that is specified as an input column."
        },
        {
            "name": "body",
            "description": "SQL expression that transforms the data in the column designated by arg_name_to_mask . The expression can include Conditional expression functions to represent conditional logic, built-in functions, or UDFs to\ntransform the data. If a UDF or external function is used inside the masking policy body, the policy owner must have USAGE on the UDF or external function.\nThe USAGE privilege on the UDF or external function is not required for the role used to query a column that has a masking policy applied\nto it. If a UDF or external function is used inside the conditional masking policy body, the policy owner must have OWNERSHIP on the UDF or\nexternal function. Users querying a column that has a conditional masking policy applied to it do not need to have USAGE on the UDF or\nexternal function."
        },
        {
            "name": "arg_name_to_mask   arg_type_to_mask",
            "description": "The first column and its data type always indicate the column data type values to mask or tokenize in the subsequent\npolicy conditions. Note that you can not specify a virtual column as the first column argument in a conditional masking policy."
        },
        {
            "name": "[   ,   arg_1   arg_type_1   ...   ]",
            "description": "Specifies the conditional columns and their data types to evaluate to determine whether the policy conditions should mask or tokenize\nthe data in the first column in each row of the query result. If these additional columns and data types are not specified, Snowflake evaluates the policy as a normal masking policy."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the masking policy."
        },
        {
            "name": "EXEMPT_OTHER_POLICIES   =     TRUE   |   FALSE",
            "description": "One of the following depending on the usage: Specifies whether a row access policy or conditional masking policy can reference a column that is already protected by this masking\npolicy. Specifies whether a masking policy assigned to a virtual column overrides the masking policy that the virtual column inherits from the\nVALUE column. When working with external tables, specify this property in the masking policy that protects the VALUE column. Allows a different policy to reference the masked column or allows the masking policy set on a virtual column to override the masking\npolicy the virtual column inherits from the VALUE column in an external table. Does not allow a different policy to reference the masked column or allow the masking policy and does not allow the masking policy the virtual column inherits from the VALUE column in an external table. Note the following: The value of this property in the masking policy cannot change after setting the masking policy on a table or view. To update\nthe value of this property setting, execute a CREATE OR REPLACE MASKING POLICY statement on the masking policy. When the property is set to true it is included in the output of calling the GET_DDL function on the\npolicy."
        },
        {
            "name": "TRUE",
            "description": "Allows a different policy to reference the masked column or allows the masking policy set on a virtual column to override the masking\npolicy the virtual column inherits from the VALUE column in an external table."
        },
        {
            "name": "FALSE",
            "description": "Does not allow a different policy to reference the masked column or allow the masking policy and does not allow the masking policy the virtual column inherits from the VALUE column in an external table."
        }
    ],
    "usage_notes": "If you want to replace an existing masking policy and need to see the current definition of the policy, call the\nGET_DDL function or run the DESCRIBE MASKING POLICY command.\nFor masking policies that include a subquery in the masking policy body, use EXISTS in the\nWHEN branch of the CASE function. For a representative example, refer to the custom entitlement table example in the\nNormal Masking Policy section (in this topic).\nIf the policy body contains a mapping table lookup, create a centralized mapping table and store the mapping table\nin the same database as the protected table. This is particularly important if the body calls the\nIS_DATABASE_ROLE_IN_SESSION function. For details, see the function usage notes.\nA given table or view column can be specified in either a masking policy signature or a row access policy signature. In other words, the\nsame column cannot be specified in both a masking policy signature and a row access policy signature at the same time.\nFor more information, see CREATE ROW ACCESS POLICY.\nA data sharing provider cannot create a masking policy in a reader account.\nIf using a UDF in a masking policy, ensure the data type of the column, UDF, and masking\npolicy match. For more information, see User-defined functions in a masking policy.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\nbody of a masking or row access policy, the function returns the database or schema that contains the protected table, not the database or\nschema in use for the session.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-file-format",
    "title": "CREATE FILE FORMAT",
    "description": "Creates a named file format that describes a set of staged data to access or load into Snowflake tables.",
    "syntax": "CREATE [ OR REPLACE ] [ { TEMP | TEMPORARY | VOLATILE } ] FILE FORMAT [ IF NOT EXISTS ] <name>\n  [ TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM} [ formatTypeOptions ] ]\n  [ COMMENT = '<string_literal>' ]\n\nformatTypeOptions ::=\n-- If TYPE = CSV\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     FILE_EXTENSION = '<string>'\n     PARSE_HEADER = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     ESCAPE = '<character>' | NONE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n     ENCODING = '<string>' | UTF8\n-- If TYPE = JSON\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     TRIM_SPACE = TRUE | FALSE\n     MULTI_LINE = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     FILE_EXTENSION = '<string>'\n     ENABLE_OCTAL = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n-- If TYPE = AVRO\n     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = ORC\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = PARQUET\n     COMPRESSION = AUTO | LZO | SNAPPY | NONE\n     SNAPPY_COMPRESSION = TRUE | FALSE\n     BINARY_AS_TEXT = TRUE | FALSE\n     USE_LOGICAL_TYPE = TRUE | FALSE\n     TRIM_SPACE = TRUE | FALSE\n     USE_VECTORIZED_SCANNER = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = XML\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     PRESERVE_SPACE = TRUE | FALSE\n     STRIP_OUTER_ELEMENT = TRUE | FALSE\n     DISABLE_AUTO_CONVERT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE",
    "examples": [
        {
            "code": "CREATE OR REPLACE FILE FORMAT my_csv_format\n  TYPE = CSV\n  COMMENT = 'my_file_format';"
        },
        {
            "code": "CREATE OR ALTER FILE FORMAT my_csv_format\n  TYPE = CSV\n  FIELD_DELIMITER = '|'\n  SKIP_HEADER = 1\n  NULL_IF = ('NULL', 'null')\n  EMPTY_FIELD_AS_NULL = true\n  COMPRESSION = gzip;"
        },
        {
            "code": "CREATE OR REPLACE FILE FORMAT my_json_format\n  TYPE = JSON;"
        },
        {
            "code": "CREATE OR REPLACE FILE FORMAT my_parquet_format\n  TYPE = PARQUET\n  USE_VECTORIZED_SCANNER = TRUE\n  USE_LOGICAL_TYPE = TRUE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the file format; must be unique for the schema in which the file format is created. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ), Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "{   TEMP   |   TEMPORARY   |   VOLATILE   }",
            "description": "Specifies that the file format persists only for the duration of the session that you created it in.\nA temporary file format is dropped at the end of the session. Default: No value. If a file format is not declared as TEMPORARY , the file format is permanent. If you want to avoid unexpected conflicts, avoid naming temporary file formats after file formats that already exist in the schema. If you created a temporary file format with the same name as another file format in the schema, all queries and operations used on the\nfile format only affect the temporary file format in the session, until you drop the temporary file format. If you drop the file format\nusing a DROP FILE FORMAT command, you drop the temporary file format, and not the file format that already exists in the schema."
        },
        {
            "name": "TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   [   ...   ]",
            "description": "Specifies the format of the input files (for data loading) or output files (for data unloading). Depending on the format type, you can\nspecify additional format-specific options. For more information, see Format Type Options (in this topic). Valid values depend on whether the file format is for loading or unloading data: Any flat, delimited plain text file that uses specific characters such as the following: Separators for fields within records (for example, commas). Separators for records (for example, new line characters). Although the name (CSV) suggests comma-separated values, you can use any valid character as a field separator. Any plain text file containing one or more JSON documents (such as objects or arrays). JSON is a semi-structured file format. The\ndocuments can be comma-separated and optionally enclosed in a big array. A single JSON document can span multiple lines. Note When you load data from files into tables, Snowflake supports either NDJSON (newline delimited JSON)\nstandard format or comma-separated JSON format. When you unload table data to files, Snowflake outputs only to NDJSON format. Binary file in AVRO format. Binary file in ORC format. Binary file in PARQUET format. Plain text file containing XML elements. Preview Feature — Open Available to all accounts. This format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option. For more information about CSV, see Usage Notes in this topic. For more information about JSON and the other semi-structured file formats,\nsee Introduction to Loading Semi-structured Data . For more information about CUSTOM type, see Loading unstructured data with Document AI . Default: CSV"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the file format. Default: No value"
        },
        {
            "name": "CSV  (for loading or unloading)",
            "description": "Any flat, delimited plain text file that uses specific characters such as the following: Separators for fields within records (for example, commas). Separators for records (for example, new line characters). Although the name (CSV) suggests comma-separated values, you can use any valid character as a field separator."
        },
        {
            "name": "JSON  (for loading or unloading)",
            "description": "Any plain text file containing one or more JSON documents (such as objects or arrays). JSON is a semi-structured file format. The\ndocuments can be comma-separated and optionally enclosed in a big array. A single JSON document can span multiple lines. Note When you load data from files into tables, Snowflake supports either NDJSON (newline delimited JSON)\nstandard format or comma-separated JSON format. When you unload table data to files, Snowflake outputs only to NDJSON format."
        },
        {
            "name": "AVRO  (for loading only; you can’t unload data to AVRO format)",
            "description": "Binary file in AVRO format."
        },
        {
            "name": "ORC  (for loading only; you can’t unload data to ORC format)",
            "description": "Binary file in ORC format."
        },
        {
            "name": "PARQUET  (for loading or unloading)",
            "description": "Binary file in PARQUET format."
        },
        {
            "name": "XML  (for loading only; you can’t unload data to XML format)",
            "description": "Plain text file containing XML elements."
        },
        {
            "name": "CUSTOM  (for loading unstructured data only)",
            "description": "Preview Feature — Open Available to all accounts. This format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option."
        }
    ],
    "usage_notes": "Caution\nRecreating a file format (using CREATE OR REPLACE FILE FORMAT) breaks the association between the file format and any external table that\nreferences it. This is because an external table links to a file format using a hidden ID rather than the name of the file format.\nBehind the scenes, the CREATE OR REPLACE syntax drops an object and recreates it with a different hidden ID.\nIf you must recreate a file format after it has been linked to one or more external tables, you must recreate each of the external tables\n(using CREATE OR REPLACE EXTERNAL TABLE) to reestablish the association. Call the GET_DDL function to\nretrieve a DDL statement to recreate each of the external tables.\nConflicting file format values in a SQL statement produce an error. A conflict occurs when the same option is specified multiple times\nwith different values (e.g. ...TYPE = 'CSV' ... TYPE = 'JSON'...).\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-stream",
    "title": "CREATE STREAM",
    "description": "Creates a new stream in the current/specified schema or replaces an existing stream. A stream records data\nmanipulation language (DML) changes made to a table, directory table, dynamic table, external table, or the underlying tables in a view (including\nsecure views). The object for which changes are recorded is called the source object.",
    "syntax": "-- table\nCREATE [ OR REPLACE ] STREAM [IF NOT EXISTS]\n  <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COPY GRANTS ]\n  ON TABLE <table_name>\n  [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> | STREAM => '<name>' } ) ]\n  [ APPEND_ONLY = TRUE | FALSE ]\n  [ SHOW_INITIAL_ROWS = TRUE | FALSE ]\n  [ COMMENT = '<string_literal>' ]\n\n-- Event table\nCREATE [ OR REPLACE ] STREAM [IF NOT EXISTS]\n  <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COPY GRANTS ]\n  ON EVENT TABLE <table_name>\n  [ COMMENT = '<string_literal>' ]\n\n-- External table\nCREATE [ OR REPLACE ] STREAM [IF NOT EXISTS]\n  <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COPY GRANTS ]\n  ON EXTERNAL TABLE <external_table_name>\n  [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> | STREAM => '<name>' } ) ]\n  [ INSERT_ONLY = TRUE ]\n  [ COMMENT = '<string_literal>' ]\n\n-- Directory table\nCREATE [ OR REPLACE ] STREAM [IF NOT EXISTS]\n  <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COPY GRANTS ]\n  ON STAGE <stage_name>\n  [ COMMENT = '<string_literal>' ]\n\n-- Dynamic table\nCREATE [ OR REPLACE ] STREAM [IF NOT EXISTS]\n  <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COPY GRANTS ]\n  ON DYNAMIC TABLE <table_name>\n  [ COMMENT = '<string_literal>' ]\n\n-- View\nCREATE [ OR REPLACE ] STREAM [IF NOT EXISTS]\n  <name>\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ COPY GRANTS ]\n  ON VIEW <view_name>\n  [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> | STREAM => '<name>' } ) ]\n  [ APPEND_ONLY = TRUE | FALSE ]\n  [ SHOW_INITIAL_ROWS = TRUE | FALSE ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE STREAM mystream ON TABLE mytable;"
        },
        {
            "code": "CREATE STREAM mystream ON TABLE mytable BEFORE (TIMESTAMP => TO_TIMESTAMP(40*365*86400));"
        },
        {
            "code": "CREATE STREAM mystream ON TABLE mytable AT (TIMESTAMP => TO_TIMESTAMP_TZ('02/02/2019 01:02:03', 'mm/dd/yyyy hh24:mi:ss'));"
        },
        {
            "code": "CREATE STREAM mystream ON TABLE mytable AT(OFFSET => -60*5);"
        },
        {
            "code": "CREATE STREAM mystream ON TABLE mytable AT(STREAM => 'oldstream');"
        },
        {
            "code": "CREATE OR REPLACE STREAM mystream ON TABLE mytable AT(STREAM => 'mystream');"
        },
        {
            "code": "CREATE STREAM mystream ON TABLE mytable BEFORE(STATEMENT => '8e5d0ca9-005e-44e6-b858-a8f5b37c5726');"
        },
        {
            "code": "CREATE STREAM mystream ON VIEW myview;"
        },
        {
            "code": "-- Create an external table that points to the MY_EXT_STAGE stage.\n-- The external table is partitioned by the date (in YYYY/MM/DD format) in the file path.\nCREATE EXTERNAL TABLE my_ext_table (\n  date_part date as to_date(substr(metadata$filename, 1, 10), 'YYYY/MM/DD'),\n  ts timestamp AS (value:time::timestamp),\n  user_id varchar AS (value:userId::varchar),\n  color varchar AS (value:color::varchar)\n) PARTITION BY (date_part)\n  LOCATION=@my_ext_stage\n  AUTO_REFRESH = false\n  FILE_FORMAT=(TYPE=JSON);\n\n-- Create a stream on the external table\nCREATE STREAM my_ext_table_stream ON EXTERNAL TABLE my_ext_table INSERT_ONLY = TRUE;\n\n-- Execute SHOW streams\n-- The MODE column indicates that the new stream is an INSERT_ONLY stream\nSHOW STREAMS;\n+-------------------------------+------------------------+---------------+-------------+--------------+-----------+------------------------------------+-------+-------+-------------+\n| created_on                    | name                   | database_name | schema_name | owner        | comment   | table_name                         | type  | stale | mode        |\n|-------------------------------+------------------------+---------------+-------------+--------------+-----------+------------------------------------+-------+-------+-------------|\n| 2020-08-02 05:13:20.174 -0800 | MY_EXT_TABLE_STREAM    | MYDB          | PUBLIC      | MYROLE       |           | MYDB.PUBLIC.EXTTABLE_S3_PART       | DELTA | false | INSERT_ONLY |\n+-------------------------------+------------------------+---------------+-------------+--------------+-----------+------------------------------------+-------+-------+-------------+\n\n-- Add a file named '2020/08/05/1408/log-08051409.json' to the stage using the appropriate tool for the cloud storage service.\n\n-- Manually refresh the external table metadata.\nALTER EXTERNAL TABLE my_ext_table REFRESH;\n\n-- Query the external table stream.\n-- The stream indicates that the rows in the added JSON file were recorded in the external table metadata.\nSELECT * FROM my_ext_table_stream;\n+----------------------------------------+------------+-------------------------+---------+-------+-----------------+-------------------+-----------------+---------------------------------------------+\n| VALUE                                  | DATE_PART  | TS                      | USER_ID | COLOR | METADATA$ACTION | METADATA$ISUPDATE | METADATA$ROW_ID | METADATA$FILENAME                           |\n|----------------------------------------+------------+-------------------------+---------+-------+-----------------+-------------------+-----------------+---------------------------------------------|\n| {                                      | 2020-08-05 | 2020-08-05 15:57:01.000 | user25  | green | INSERT          | False             |                 | test/logs/2020/08/05/1408/log-08051409.json |\n|   \"color\": \"green\",                    |            |                         |         |       |                 |                   |                 |                                             |\n|   \"time\": \"2020-08-05 15:57:01-07:00\", |            |                         |         |       |                 |                   |                 |                                             |\n|   \"userId\": \"user25\"                   |            |                         |         |       |                 |                   |                 |                                             |\n| }                                      |            |                         |         |       |                 |                   |                 |                                             |\n| {                                      | 2020-08-05 | 2020-08-05 15:58:02.000 | user56  | brown | INSERT          | False             |                 | test/logs/2020/08/05/1408/log-08051409.json |\n|   \"color\": \"brown\",                    |            |                         |         |       |                 |                   |                 |                                             |\n|   \"time\": \"2020-08-05 15:58:02-07:00\", |            |                         |         |       |                 |                   |                 |                                             |\n|   \"userId\": \"user56\"                   |            |                         |         |       |                 |                   |                 |                                             |\n| }                                      |            |                         |         |       |                 |                   |                 |                                             |\n+----------------------------------------+------------+-------------------------+---------+-------+-----------------+-------------------+-----------------+---------------------------------------------+"
        },
        {
            "code": "CREATE STREAM dirtable_mystage_s ON STAGE mystage;"
        },
        {
            "code": "ALTER STAGE mystage REFRESH;"
        },
        {
            "code": "SELECT * FROM dirtable_mystage_s;\n\n+-------------------+--------+-------------------------------+----------------------------------+----------------------------------+-------------------------------------------------------------------------------------------+-----------------+-------------------+-----------------+\n| RELATIVE_PATH     | SIZE   | LAST_MODIFIED                 | MD5                              | ETAG                             | FILE_URL                                                                                  | METADATA$ACTION | METADATA$ISUPDATE | METADATA$ROW_ID |\n|-------------------+--------+-------------------------------+----------------------------------+----------------------------------+-------------------------------------------------------------------------------------------+-----------------+-------------------+-----------------|\n| file1.csv.gz      |   1048 | 2021-05-14 06:09:08.000 -0700 | c98f600c492c39bef249e2fcc7a4b6fe | c98f600c492c39bef249e2fcc7a4b6fe | https://myaccount.snowflakecomputing.com/api/files/MYDB/MYSCHEMA/MYSTAGE/file1%2ecsv%2egz | INSERT          | False             |                 |\n| file2.csv.gz      |   3495 | 2021-05-14 06:09:09.000 -0700 | 7f1a4f98ef4c7c42a2974504d11b0e20 | 7f1a4f98ef4c7c42a2974504d11b0e20 | https://myaccount.snowflakecomputing.com/api/files/MYDB/MYSCHEMA/MYSTAGE/file2%2ecsv%2egz | INSERT          | False             |                 |\n+-------------------+--------+-------------------------------+----------------------------------+----------------------------------+-------------------------------------------------------------------------------------------+-----------------+-------------------+-----------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the stream; must be unique for the schema in which the stream is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "table_name",
            "description": "String that specifies the identifier (i.e. name) for the table whose changes are tracked by the stream (i.e. the source table). To query a stream, a role must have the SELECT privilege on the underlying table."
        },
        {
            "name": "external_table_name",
            "description": "String that specifies the identifier (i.e. name) for the external table whose changes are tracked by the stream (i.e. the source\nexternal table). To query a stream, a role must have the SELECT privilege on the underlying external table."
        },
        {
            "name": "stage_name",
            "description": "String that specifies the identifier (i.e. name) for the stage whose directory table changes are tracked by the stream (i.e. the\nsource directory table). To query a stream, a role must have the USAGE (external stage) or READ (internal stage) privilege on the underlying\nstage."
        },
        {
            "name": "view_name",
            "description": "String that specifies the identifier (i.e. name) for the source view. The stream tracks DML changes to the underlying tables in\nthe view. For more information about streams on views, see Streams on Views . To query a stream, a role must have the SELECT privilege on the view."
        },
        {
            "name": "Access control :",
            "description": "To query a stream, a role must have the SELECT privilege on the underlying table."
        },
        {
            "name": "Access control :",
            "description": "To query a stream, a role must have the SELECT privilege on the underlying external table."
        },
        {
            "name": "Access control :",
            "description": "To query a stream, a role must have the USAGE (external stage) or READ (internal stage) privilege on the underlying\nstage."
        },
        {
            "name": "Access control :",
            "description": "To query a stream, a role must have the SELECT privilege on the view."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COPY   GRANTS",
            "description": "Specifies to retain the access permissions from the original stream when a new stream is created using any of the following\nCREATE STREAM variants: CREATE OR REPLACE STREAM CREATE STREAM … CLONE The parameter copies all permissions, except OWNERSHIP, from the existing stream to the new stream. By default, the role\nthat executes the CREATE STREAM command owns the new stream. Note If the CREATE STREAM statement references more than one stream (e.g. create or replace stream t1 clone t2; ), the COPY GRANTS clause gives precedence to the stream being replaced. The SHOW GRANTS output for the replacement stream lists the grantee for the copied privileges as the\nrole that executed the CREATE STREAM statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE STREAM command (i.e. within the same transaction). Note This parameter is not supported currently."
        },
        {
            "name": "{   AT   (   {   TIMESTAMP   =>   timestamp   |   OFFSET   =>   time_difference   |   STATEMENT   =>   id   |   STREAM   =>   ' name '   }   )   |   BEFORE   (   {   TIMESTAMP   =>   timestamp   |   OFFSET   =>   time_difference   |   STATEMENT   =>   id   }   )   }",
            "description": "Creates a stream at a specific time/point in the past (using Time Travel ). The AT | BEFORE clause determines the point in the past from which historical data is requested: The AT keyword specifies that the request is inclusive of any changes made by a statement or transaction with a timestamp\nequal to the specified parameter. The STREAM => '<name>' value is special. When provided, the CREATE STREAM statement creates the new stream at the same\noffset as the specified stream. You can also provide this value when recreating an existing stream (using the OR REPLACE keywords) to retain the current offset of the stream after it is recreated. '<name>' is the identifier (i.e. name) for\nthe existing stream whose offset is copied to the new or recreated stream. The new or recreated stream advances the offset, as usual, when the stream is used in a DML transaction. The BEFORE keyword specifies that the request refers to a point immediately preceding the specified parameter. Note If no change tracking data is available on the source object at the point in the past specified in the AT | BEFORE clause, the\nCREATE STREAM statement fails. No stream can be created at a time in the past before change tracking was recorded."
        },
        {
            "name": "APPEND_ONLY   =   TRUE   |   FALSE",
            "description": "Only supported for streams on standard tables or streams on views that query standard tables. Specifies whether this is an append-only stream. Append-only streams track row inserts only. Update and delete operations (including\ntable truncates) are not recorded. For example, if 10 rows are inserted into a table and then 5 of those rows are deleted before the\noffset for an append-only stream is advanced, the stream records 10 rows. This type of stream improves query performance over standard streams and is very useful for extract, load, transform (ELT) and similar\nscenarios that depend exclusively on row inserts. A standard stream joins the deleted and inserted rows in the change set to determine which rows were deleted and which were updated.\nAn append-only stream returns the appended rows only and therefore can be much more performant than a standard stream. For example,\nthe source table can be truncated immediately after the rows in an append-only stream are consumed, and the record deletions do not\ncontribute to the overhead the next time the stream is queried or consumed. FALSE"
        },
        {
            "name": "INSERT_ONLY   =   TRUE   |   FALSE",
            "description": "Required for streams on external tables. Not supported by streams on other objects. Specifies whether this is an insert-only stream. Insert-only streams track row inserts only; they do not record delete operations\nthat remove rows from an inserted set (i.e. no-ops). For example, in-between any two offsets, if File1 is removed from the\ncloud storage location referenced by the external table, and File2 is added, the stream returns records for the rows in File2 only, regardless of whether File1 was added before or within the requested change interval. Unlike\nwhen tracking change data capture (CDC) data for standard tables, access to the historical records for files in cloud storage is\nnot governed by or guaranteed to Snowflake. Overwritten or appended files are essentially handled as new files: The old version of the file is removed from cloud storage, but the\ninsert-only stream does not record the delete operation. The new version of the file is added to cloud storage, and the insert-only\nstream records the rows as inserts. The stream does not record the diff of the old and new file versions. Note that appends may not\ntrigger an automatic refresh of the external table metadata, such as when using Azure AppendBlobs . FALSE"
        },
        {
            "name": "SHOW_INITIAL_ROWS   =   TRUE   |   FALSE",
            "description": "Specifies the records to return the first time the stream is consumed. The stream returns only the rows that existed in the source object at the moment when the stream was created. The\nMETADATA$ISUPDATE column shows a FALSE value in these rows. Subsequently, the stream returns any DML changes to the source object\nsince the most recent offset; that is, the normal stream behavior. This parameter enables initializing any downstream process with the contents of the source object for the stream. FALSE The stream returns any DML changes to the source object since the most recent offset. FALSE"
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "TRUE",
            "description": "The stream returns only the rows that existed in the source object at the moment when the stream was created. The\nMETADATA$ISUPDATE column shows a FALSE value in these rows. Subsequently, the stream returns any DML changes to the source object\nsince the most recent offset; that is, the normal stream behavior. This parameter enables initializing any downstream process with the contents of the source object for the stream."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the stream. Default: No value"
        }
    ],
    "usage_notes": "A stream can be queried multiple times to update multiple objects in the same transaction and it will return the same data.\nThe stream position (i.e. offset) is advanced when the stream is used in a DML statement. The position is updated at the end of the\ntransaction to the beginning timestamp of the transaction. The stream describes change records starting from the current position of the\nstream and ending at the current transactional timestamp.\nTo ensure multiple statements access the same change records in the stream, surround them with an explicit transaction statement\n(BEGIN .. COMMIT). An explicit transaction locks the stream, so that DML updates to\nthe source object are not reported to the stream until the transaction is committed.\nStreams have no Fail-safe period or Time Travel retention period. The metadata in these objects cannot be recovered if a stream is dropped.\nStreams on shared tables:\nThe retention period for a source table is not extended automatically to prevent any streams on the table from becoming stale.\nStandard streams cannot retrieve change data for geospatial data. We recommend creating append-only streams on objects that contain\ngeospatial data.\nStreams on views:\nCreating the first stream on a view using the view owner role (i.e. the role with the OWNERSHIP privilege on the view) enables change\ntracking on the view. If the same role also owns the underlying tables, change tracking is also enabled on the tables. If the role was\nnot granted the OWNERSHIP privilege on both the view and its underlying tables, then change tracking must be enabled manually on the\napplicable objects. For instructions, see Enabling Change Tracking on Views and Underlying Tables.\nDepending on the number of joins in a view, a single change in the underlying tables could result in a large number of changes in the\nstream output.\nAny stream on a given view breaks if the source view or underlying tables are dropped or recreated (using CREATE OR REPLACE VIEW).\nAny streams on a secure view adhere to the secure view constraints.\nIf the owner of a non-secure view (i.e. the role with the OWNERSHIP privilege on the view) changes it to a secure view (using ALTER\nVIEW … SET SECURE), any stream on the view automatically enforces secure view constraints.\nIn addition, the retention period for the underlying tables is not extended automatically to prevent any streams on the secure\nview from becoming stale.\nStreams based on views where the view uses non-deterministic functions can return non-deterministic results.\nFor example, the results of context functions such as CURRENT_DATE,\nand CURRENT_USER are non-deterministic.  The results of data generation functions\nsuch as RANDOM are also non-deterministic.\nIf a view contains a non-deterministic function, then any stream on that view will not be a constant snapshot of the\nfunction’s output. Instead the value in the stream may change when queried.\nWe recommend that you ensure that the non-determinism in the results of a view does not\naffect the correctness of the stream query results.\nFor an example, see Stream on a View That Calls a Non-deterministic SQL Function.\nStreams on directory tables: The METADATA$ROW_ID column values in the stream output are empty.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-network-rule",
    "title": "CREATE NETWORK RULE",
    "description": "Creates a network rule or replaces an existing network rule.",
    "syntax": "CREATE [ OR REPLACE ] NETWORK RULE <name>\n   TYPE = { IPV4 | AWSVPCEID | AZURELINKID | HOST_PORT | PRIVATE_HOST_PORT }\n   VALUE_LIST = ( '<value>' [, '<value>', ... ] )\n   MODE = { INGRESS | INTERNAL_STAGE | EGRESS }\n   [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE NETWORK RULE corporate_network\n  TYPE = AWSVPCEID\n  VALUE_LIST = ('vpce-123abc3420c1931')\n  MODE = INTERNAL_STAGE\n  COMMENT = 'corporate privatelink endpoint';"
        },
        {
            "code": "CREATE NETWORK RULE cloud_network\n  TYPE = IPV4\n  VALUE_LIST = ('47.88.25.32/27')\n  COMMENT ='cloud egress ip range';"
        },
        {
            "code": "CREATE NETWORK RULE external_access_rule\n  TYPE = HOST_PORT\n  MODE = EGRESS\n  VALUE_LIST = ('example.com', 'example.com:443');"
        },
        {
            "code": "CREATE OR REPLACE NETWORK RULE ext_network_access_db.network_rules.azure_sql_private_rule\n  MODE = EGRESS\n  TYPE = PRIVATE_HOST_PORT\n  VALUE_LIST = ('externalaccessdemo.database.windows.net');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the network rule. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TYPE   =   {   IPV4   |   AWSVPCEID   |   AZURELINKID   |   HOST_PORT   |   PRIVATE_HOST_PORT   }",
            "description": "Specifies the type of network identifiers being allowed or blocked. A network rule can have only one type. IPV4 indicates that the network rule will allow or block network traffic based on the IPv4 address of the request origin. AWSVPCEID indicates that the network rule will allow or block network traffic over AWS PrivateLink . AZURELINKID indicates that the network rule will allow or block network traffic over Azure Private Link . HOST_PORT indicates that the network rule will allow outgoing network traffic based on the domain of the request destination. When TYPE = HOST_PORT , the MODE parameter should be set to EGRESS . PRIVATE_HOST_PORT indicates that the network rule allows outgoing network traffic to use private connectivity to an external network location. When TYPE = PRIVATE_HOST_PORT , the MODE parameter must be set to EGRESS ."
        },
        {
            "name": "VALUE_LIST   =   (   ' value '   [,   ' value ',   ...   ]   )",
            "description": "Specifies the network identifiers that will be allowed or blocked. Valid values in the list are determined by the type of network rule: When TYPE = IPV4 , each value must be a valid IPv4 address or range of addresses . When TYPE = AWSVPCEID , each value must be a valid VPCE ID. VPC IDs are not supported. When TYPE = AZURELINKID , each value must be a valid LinkID of an Azure private endpoint . Execute the SYSTEM$GET_PRIVATELINK_AUTHORIZED_ENDPOINTS function to retrieve the LinkID associated with an account. When TYPE = HOST_PORT , each value must resolve to a valid domain. Optionally, it can also include a port or range of ports. In most cases, the valid port range is 1-65535. If you do not specify a port, it defaults to 443. If an external network location supports dynamic ports, you need to specify all possible ports. To allow access to all ports, define the port as 0; for example, example.com:0 . When the value resolves to a domain, you can use a single asterisk as a wildcard character. The asterisk matches only alphanumeric\ncharacters and hyphens ( - ). Wildcards are supported only for a single level of subdomains, as in the following examples: *.google.com snowflake-*.google.com and snowflake*abc.google.com You can allow requests to all outbound endpoints by specifying 0.0.0.0 as the domain, as in the examples below.\nWhen you specify 0.0.0.0 as the domain, you may use only 443 and 80 as port values. Allow access to all endpoints at port 80 Allow access to all endpoints at port 443 Allow access to all endpoints at both port 80 and 443 When TYPE = PRIVATE_HOST_PORT , specify one valid domain. In most cases, the valid port range is 1-65535. If you do not specify a port, it defaults to 443. If an external network location supports dynamic ports, you need to specify all possible ports. To allow access to all ports, define the port as 0; for example, example.com:0 ."
        },
        {
            "name": "MODE   =   {   INGRESS   |   INTERNAL_STAGE   |   EGRESS   }",
            "description": "Specifies what is restricted by the network rule. The behavior of the INGRESS mode depends on the value of the network rule’s TYPE property. If TYPE=IPV4 , by default the network rule controls access to the Snowflake service only. If the account administrator enables the ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES parameter, then MODE=INGRESS and TYPE=IPV4 also protects an AWS internal stage. If TYPE=AWSVPCEID , then the network rule controls access to the Snowflake service only. Allows or blocks requests to an AWS internal stage without restricting access to the Snowflake service. Using this mode requires the\nfollowing: The account administrator must enable the ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES parameter. The TYPE property of the network rule must be AWSVPCEID . Allows Snowflake to send requests to an external destination. Default: INGRESS"
        },
        {
            "name": "INGRESS",
            "description": "The behavior of the INGRESS mode depends on the value of the network rule’s TYPE property. If TYPE=IPV4 , by default the network rule controls access to the Snowflake service only. If the account administrator enables the ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES parameter, then MODE=INGRESS and TYPE=IPV4 also protects an AWS internal stage. If TYPE=AWSVPCEID , then the network rule controls access to the Snowflake service only."
        },
        {
            "name": "INTERNAL_STAGE",
            "description": "Allows or blocks requests to an AWS internal stage without restricting access to the Snowflake service. Using this mode requires the\nfollowing: The account administrator must enable the ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES parameter. The TYPE property of the network rule must be AWSVPCEID ."
        },
        {
            "name": "EGRESS",
            "description": "Allows Snowflake to send requests to an external destination."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the network rule. Default: No value"
        }
    ],
    "usage_notes": "When specifying IP addresses for a network rule, Snowflake supports ranges of IP addresses using Classless Inter-Domain Routing (CIDR) notation.\nFor example, 192.168.1.0/24 represents all IPv4 addresses in the range of 192.168.1.0 to 192.168.1.255.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-security-integration",
    "title": "CREATE SECURITY INTEGRATION",
    "description": "Creates a new security integration in the account or replaces an existing integration. An integration is a Snowflake object that provides\nan interface between Snowflake and a third-party service.",
    "syntax": "CREATE [ OR REPLACE ] SECURITY INTEGRATION [ IF NOT EXISTS ]\n  <name>\n  TYPE = { API_AUTHENTICATION | EXTERNAL_OAUTH | OAUTH | SAML2 | SCIM }\n  ...",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] SECURITY INTEGRATION [ IF NOT EXISTS ]\n  <name>\n  TYPE = { API_AUTHENTICATION | EXTERNAL_OAUTH | OAUTH | SAML2 | SCIM }\n  ..."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-database-role",
    "title": "CREATE DATABASE ROLE",
    "description": "Create a new database role or replace an existing database role in the system.",
    "syntax": "CREATE [ OR REPLACE ] DATABASE ROLE [ IF NOT EXISTS ] <name>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE DATABASE ROLE d1.dr1;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the database role; must be unique in the database in which the role is created. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. If the identifier is not fully qualified in the form of db_name . database_role_name , the command creates the database role\nin the current database for the session. For more details, see Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the database role. Default: No value"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-application-package",
    "title": "CREATE APPLICATION PACKAGE",
    "description": "Creates a new application package that contains the data content and application logic of\nSnowflake Native App. An application package contains the following information about an app:",
    "syntax": "CREATE APPLICATION PACKAGE [ IF NOT EXISTS ] <name>\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , ... ] ) ]\n  [ DISTRIBUTION = { INTERNAL | EXTERNAL } ]\n  [ MULTIPLE_INSTANCES = TRUE ]\n  [ ENABLE_RELEASE_CHANNELS = TRUE ]",
    "examples": [
        {
            "code": "CREATE APPLICATION PACKAGE hello_snowflake_package;"
        },
        {
            "code": "+-----------------------------------------------------------------------+\n| status                                                                |\n|-----------------------------------------------------------------------|\n| Application Package 'hello_snowflake_package' created successfully.   |\n+-----------------------------------------------------------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the application package; must be unique for your account. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the application\npackage, as well as specifying the default Time Travel retention time for all schemas created in the database. For more details, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent databases Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the account level) Note A value of 0 disables Time Travel for the database."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the application package to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for all schemas and tables added to the application package. The default\ncan be overridden at the schema and individual table level. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the application package. Default: No value"
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "DISTRIBUTION   =   {   INTERNAL   |   EXTERNAL   }",
            "description": "Specifies the type of listing a provider can create when using the application package as the data product of a listing. INTERNAL indicates that a provider can only create a private listing within the same organization\nwhere the application package was created. The automated security scan is not performed\nwhen the DISTRIBUTION property is set to INTERNAL. EXTERNAL indicates that a provider can create listings outside the same organization where\nthe application package was created. See Run the automated security scan for information on setting the DISTRIBUTION property and\nthe automated security scan. Note Setting the DISTRIBUTION parameter to EXTERNAL triggers an automated security review for each\nactive version and patch defined in the application package. The following restrictions apply until the automated security review has a status of APPROVED : You cannot set a release directive for a version or patch. You cannot publish a listing for the application package."
        },
        {
            "name": "MULTIPLE_INSTANCES   =   TRUE",
            "description": "Enables the consumer to install multiple instances of an app from the application package. This property cannot be\nset for applications packages that are included in a trial or paid listing. When multiple instances are allowed, consumers can install a maximum of 10 instances of an app in their account. Caution After setting this property to true, it cannot be set to FALSE or unset later."
        },
        {
            "name": "ENABLE_RELEASE_CHANNELS   =   TRUE",
            "description": "Enables release channels for the application package. Caution After setting this property to TRUE , it cannot be set to FALSE or unset later."
        }
    ],
    "usage_notes": "To create an application package, the caller must have the CREATE APPLICATION PACKAGE privilege on the account.\nThere are no restrictions on the types of objects that may reside in the application package or what roles (database or account level)\nthat may own those objects.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter",
    "title": "ALTER",
    "description": "Modifies the metadata of an account-level or database object, or the parameters for a session."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show",
    "title": "SHOW",
    "description": "Lists the existing objects for the specified object type. The output includes metadata for the objects, including:"
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/classification/commands/create-classification",
    "title": "CREATE SNOWFLAKE.ML.CLASSIFICATION",
    "description": "Creates a new classification model or replaces an existing model in the current or specified schema.",
    "syntax": "CREATE [ OR REPLACE ] SNOWFLAKE.ML.CLASSIFICATION [ IF NOT EXISTS ] <model_name> (\n    INPUT_DATA => <input_data>,\n    TARGET_COLNAME => '<target_colname>',\n    [CONFIG_OBJECT => <config_object>],\n)\n[ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n[ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] SNOWFLAKE.ML.CLASSIFICATION [ IF NOT EXISTS ] <model_name> (\n    INPUT_DATA => <input_data>,\n    TARGET_COLNAME => '<target_colname>',\n    [CONFIG_OBJECT => <config_object>],\n)\n[ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n[ COMMENT = '<string_literal>' ]"
        }
    ],
    "parameters": [
        {
            "name": "input_data",
            "description": "A reference to the training data.\nUsing a reference allows the training process, which runs with limited privileges, to use your active role’s\nprivileges to access the data. You can use a reference to a table or a view if your data is already in that form, or\nyou can use a query reference to provide the query to be executed\nto obtain the data. INPUT_DATA must contain the entire training data to be consumed by the classification model. Any columns that are\nnot named in the TARGET_COLNAME arguments are considered training variables (features). The order of the columns in\nthe input data is not important. Feature columns must be STRING, NUMERIC, or BOOLEAN. STRING and BOOLEAN columns are treated as categorical features,\nwhile NUMERIC columns are considered continuous features. To treat a numeric column as categorical, cast it to STRING."
        },
        {
            "name": "target_colname",
            "description": "Name of the column containing the label (target value) for each row in the training data. The target column may be\nBOOLEAN, NUMERIC, or STRING."
        },
        {
            "name": "config_object",
            "description": "An OBJECT whose key-value pairs specify additional training options. Key Type Default Description evaluate BOOLEAN TRUE Whether evaluation metrics should be generated. If TRUE, then additional model is trained for evaluation using the parameters in the evaluation_config . on_error STRING ‘ABORT’ String constant that specifies the error handling method for the model training task. Supported values are: 'ABORT' : Abort the entire training operation if any row results in an error. 'SKIP' : Skip rows that result in an error. The error is shown instead of the results. evaluation_config OBJECT NULL A optional configuration object to specify how out-of-sample evaluation metrics should be generated. Currently, there is only one such option. test_fraction (FLOAT): The fraction of the dataset that should be used as test (evaluation) data. If evaluation configuration is not specified, the default behavior is to try to include a minimum of 500\ninstances of the minority class in the evaluation set and to limit the total test fraction of 20% of the\ndataset. This approach maintains balance in model evaluation and training, particularly for minority classes."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-notebook",
    "title": "CREATE NOTEBOOK",
    "description": "Creates a new Snowflake notebook or replaces an existing notebook.",
    "syntax": "CREATE [ OR REPLACE ] NOTEBOOK [ IF NOT EXISTS ] <name>\n  [ FROM '<source_location>' ]\n  [ MAIN_FILE = '<main_file_name>' ]\n  [ COMMENT = '<string_literal>' ]\n  [ QUERY_WAREHOUSE = <warehouse_to_run_nb_and_sql_queries_in> ]\n  [ IDLE_AUTO_SHUTDOWN_TIME_SECONDS = <number_of_seconds> ]\n  [ WAREHOUSE = <warehouse_to_run_notebook_python_runtime> ]",
    "examples": [
        {
            "code": "CREATE NOTEBOOK mynotebook;"
        },
        {
            "code": "CREATE NOTEBOOK mynotebook\n QUERY_WAREHOUSE = my_warehouse;"
        },
        {
            "code": "CREATE NOTEBOOK mynotebook\n FROM '@my_db.my_schema.my_stage'\n MAIN_FILE = 'my_notebook_file.ipynb'\n QUERY_WAREHOUSE = my_warehouse;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the notebook; must be unique for the schema in which the notebook is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "FROM   ' source_location '",
            "description": "Specifies that the notebook should be created from an .ipynb file in the specified stage location. To create the notebook from a file\non a stage, set source_location to the stage location of the file, and set the MAIN_FILE parameter to the name of the file. If this parameter is not specified, the notebook object is created from a template notebook."
        },
        {
            "name": "MAIN_FILE   =   ' main_file_name '",
            "description": "User-specified identifier for the notebook file name. This is separate from the notebook object name, which is specified in the name parameter. This file must be an ipynb file."
        },
        {
            "name": "QUERY_WAREHOUSE   =   warehouse_name",
            "description": "Specifies the warehouse where SQL queries in the notebook are run.\nThis parameter is optional. However, it is required to run the EXECUTE NOTEBOOK command."
        },
        {
            "name": "IDLE_AUTO_SHUTDOWN_TIME_SECONDS   =   number_of_seconds",
            "description": "Number of seconds of idle time before the notebook is shut down automatically. This parameter is only available for notebooks running\non the Container Runtime. The value must be an integer between 60 and 259200 (72 hours). Default: 3600 seconds"
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Preview Feature — Open The WAREHOUSE parameter is available to all accounts. Warehouse that runs the notebook kernel and python code. If no warehouse is specified when you create a notebook, Snowflake uses the default warehouse defined by the schema lineage parameter\nDEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE. You can set this parameter at the schema, database, or account lineage level to define a preferred warehouse."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-share",
    "title": "CREATE SHARE",
    "description": "Creates a new, empty share. Once the share is created, you can include a database and\nobjects from the database (schemas, tables, and views) in the share using the GRANT <privilege> … TO SHARE command. You can then use\nALTER SHARE to add one or more accounts to the share.",
    "syntax": "CREATE [ OR REPLACE ] SHARE [ IF NOT EXISTS ] <name>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE SHARE sales_s;"
        },
        {
            "code": "+-----------------------------------------+\n| status                                  |\n|-----------------------------------------|\n| Share SALES_S successfully created.     |\n+-----------------------------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the share; must be unique for the account in which the share is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the share. Default: No value"
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-join-policy",
    "title": "CREATE JOIN POLICY",
    "description": "Creates a new join policy in the current/specified schema or replaces an existing\njoin policy.",
    "syntax": "CREATE [ OR REPLACE ] JOIN POLICY [ IF NOT EXISTS ] <name>\n  AS () RETURNS JOIN_CONSTRAINT -> <body>\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE JOIN POLICY jp1 AS ()\n  RETURNS JOIN_CONSTRAINT -> JOIN_CONSTRAINT(JOIN_REQUIRED => TRUE);"
        },
        {
            "code": "CREATE JOIN POLICY jp2 AS ()\n  RETURNS JOIN_CONSTRAINT ->\n    CASE\n      WHEN CURRENT_ROLE() = 'ACCOUNTADMIN'\n        THEN JOIN_CONSTRAINT(JOIN_REQUIRED => FALSE)\n      ELSE JOIN_CONSTRAINT(JOIN_REQUIRED => TRUE)\n    END;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the join policy; must be unique for your schema. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "AS   ()   RETURNS   JOIN_CONSTRAINT",
            "description": "Signature and return type of the policy. The signature does not accept any arguments, and the return type is JOIN_CONSTRAINT, which is an internal data type. All join policies have the same signature and return\ntype."
        },
        {
            "name": "body",
            "description": "SQL expression that determines the restrictions of a join policy. To define the body of the join policy, call the JOIN_CONSTRAINT function, which returns TRUE or FALSE.\nWhen the function returns TRUE, queries are required to use a join to return results. The syntax of the JOIN_CONSTRAINT function is: Where: Specifies whether a join is required in queries when data is selected from tables or views that have\nthe join policy assigned to them. The body of a policy cannot reference user-defined functions, tables, or views. Allowed join columns are specified in the CREATE or ALTER statement for the table or view to which the\npolicy is applied, not in the CREATE JOIN POLICY statement."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the join policy."
        },
        {
            "name": "JOIN_REQUIRED   =>   boolean_expression",
            "description": "Specifies whether a join is required in queries when data is selected from tables or views that have\nthe join policy assigned to them."
        }
    ],
    "usage_notes": "If you want to update an existing join policy and need to see the current body of the policy, run the\nDESCRIBE JOIN POLICY command or GET_DDL function.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration",
    "title": "CREATE STORAGE INTEGRATION",
    "description": "Creates a new storage integration in the account or replaces an existing integration.",
    "syntax": "CREATE [ OR REPLACE ] STORAGE INTEGRATION [IF NOT EXISTS]\n  <name>\n  TYPE = EXTERNAL_STAGE\n  cloudProviderParams\n  ENABLED = { TRUE | FALSE }\n  STORAGE_ALLOWED_LOCATIONS = ('<cloud>://<bucket>/<path>/' [ , '<cloud>://<bucket>/<path>/' ... ] )\n  [ STORAGE_BLOCKED_LOCATIONS = ('<cloud>://<bucket>/<path>/' [ , '<cloud>://<bucket>/<path>/' ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\ncloudProviderParams (for Amazon S3) ::=\n  STORAGE_PROVIDER = 'S3'\n  STORAGE_AWS_ROLE_ARN = '<iam_role>'\n  [ STORAGE_AWS_EXTERNAL_ID = '<external_id>' ]\n  [ STORAGE_AWS_OBJECT_ACL = 'bucket-owner-full-control' ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\ncloudProviderParams (for Google Cloud Storage) ::=\n  STORAGE_PROVIDER = 'GCS'\n\ncloudProviderParams (for Microsoft Azure) ::=\n  STORAGE_PROVIDER = 'AZURE'\n  AZURE_TENANT_ID = '<tenant_id>'\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]",
    "examples": [
        {
            "code": "CREATE STORAGE INTEGRATION s3_int\n  TYPE = EXTERNAL_STAGE\n  STORAGE_PROVIDER = 'S3'\n  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::001234567890:role/myrole'\n  ENABLED = TRUE\n  STORAGE_ALLOWED_LOCATIONS = ('s3://mybucket1/path1/', 's3://mybucket2/path2/');"
        },
        {
            "code": "CREATE STORAGE INTEGRATION gcs_int\n  TYPE = EXTERNAL_STAGE\n  STORAGE_PROVIDER = 'GCS'\n  ENABLED = TRUE\n  STORAGE_ALLOWED_LOCATIONS = ('gcs://mybucket1/path1/', 'gcs://mybucket2/path2/');"
        },
        {
            "code": "CREATE STORAGE INTEGRATION azure_int\n  TYPE = EXTERNAL_STAGE\n  STORAGE_PROVIDER = 'AZURE'\n  ENABLED = TRUE\n  AZURE_TENANT_ID = '<tenant_id>'\n  STORAGE_ALLOWED_LOCATIONS = ('azure://myaccount.blob.core.windows.net/mycontainer/path1/', 'azure://myaccount.blob.core.windows.net/mycontainer/path2/');"
        },
        {
            "code": "CREATE STORAGE INTEGRATION s3_int\n  TYPE = EXTERNAL_STAGE\n  STORAGE_PROVIDER = 'S3'\n  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::001234567890:role/myrole'\n  ENABLED = TRUE\n  STORAGE_ALLOWED_LOCATIONS = ('*')\n  STORAGE_BLOCKED_LOCATIONS = ('s3://mybucket3/path3/', 's3://mybucket4/path4/');"
        },
        {
            "code": "CREATE STORAGE INTEGRATION gcs_int\n  TYPE = EXTERNAL_STAGE\n  STORAGE_PROVIDER = 'GCS'\n  ENABLED = TRUE\n  STORAGE_ALLOWED_LOCATIONS = ('*')\n  STORAGE_BLOCKED_LOCATIONS = ('gcs://mybucket3/path3/', 'gcs://mybucket4/path4/');"
        },
        {
            "code": "CREATE STORAGE INTEGRATION azure_int\n  TYPE = EXTERNAL_STAGE\n  STORAGE_PROVIDER = 'AZURE'\n  ENABLED = TRUE\n  AZURE_TENANT_ID = 'a123b4c5-1234-123a-a12b-1a23b45678c9'\n  STORAGE_ALLOWED_LOCATIONS = ('*')\n  STORAGE_BLOCKED_LOCATIONS = ('azure://myaccount.blob.core.windows.net/mycontainer/path3/', 'azure://myaccount.blob.core.windows.net/mycontainer/path4/');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the integration; must be unique in your account. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "TYPE   =   EXTERNAL_STAGE",
            "description": "Specify the type of integration: EXTERNAL_STAGE : Creates an interface between Snowflake and an external cloud storage location."
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this storage integration is available for usage in stages. TRUE allows users to create new stages that reference this integration. Existing stages that reference this integration\nfunction normally. FALSE prevents users from creating new stages that reference this integration. Existing stages that reference this integration\ncannot access the storage location in the stage definition."
        },
        {
            "name": "STORAGE_ALLOWED_LOCATIONS   =   (   ' cloud_specific_url '   )",
            "description": "Explicitly limits external stages that use the integration to reference one or more storage locations (i.e. S3 bucket, GCS bucket, or\nAzure container). Supports a comma-separated list of URLs for existing buckets and, optionally, paths used to store data files for\nloading/unloading. Alternatively supports the * wildcard, meaning “allow access to all buckets and/or paths”. Amazon S3 STORAGE_ALLOWED_LOCATIONS = ( ' protocol :// bucket / path /' [ , ' protocol :// bucket / path /' ... ] ) protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files (e.g. mybucket ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with\na common string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud\nstorage services. Google Cloud Storage STORAGE_ALLOWED_LOCATIONS = ( 'gcs:// bucket / path /' [ , 'gcs:// bucket / path /' ... ] ) bucket is the name of a GCS bucket that stores your data files (e.g. mybucket ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with\na common string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud\nstorage services. Microsoft Azure STORAGE_ALLOWED_LOCATIONS = ( 'azure:// account .blob.core.windows.net/ container / path /' [ , 'azure:// account .blob.core.windows.net/ container / path /' ... ] ) account is the name of the Azure storage account (e.g. myaccount ). Use the blob.core.windows.net endpoint\nfor all supported types of Azure blob storage accounts, including Data Lake Storage Gen2. container is the name of a Azure blob storage container that stores your data files (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with\na common string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud\nstorage services."
        },
        {
            "name": "STORAGE_BLOCKED_LOCATIONS   =   (   ' cloud_specific_url '   )",
            "description": "Explicitly prohibits external stages that use the integration from referencing one or more storage locations (i.e. S3 buckets or\nGCS buckets). Supports a comma-separated list of URLs for existing storage locations and, optionally, paths used to store data files\nfor loading/unloading. Commonly used when STORAGE_ALLOWED_LOCATIONS is set to the * wildcard, allowing access to all buckets\nin your account except for blocked storage locations and, optionally, paths. Note Make sure to enclose only individual cloud storage location URLs in quotes. If you enclose the entire STORAGE_BLOCKED_LOCATIONS value in quotes, the value is invalid. As a result, the STORAGE_BLOCKED_LOCATIONS parameter setting is ignored when users create stages that reference the storage integration. Amazon S3 STORAGE_BLOCKED_LOCATIONS = ( ' protocol :// bucket / path /' [ , ' protocol :// bucket / path /' ... ] ) protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files (e.g. mybucket ). path is an optional path (or directory ) in the bucket that further limits access to the data files. Google Cloud Storage STORAGE_BLOCKED_LOCATIONS = ( 'gcs:// bucket / path /' [ , 'gcs:// bucket / path /' ... ] ) bucket is the name of a GCS bucket that stores your data files (e.g. mybucket ). path is an optional path (or directory ) in the bucket that further limits access to the data files. Microsoft Azure STORAGE_BLOCKED_LOCATIONS = ( 'azure:// account .blob.core.windows.net/ container / path /' [ , 'azure:// account .blob.core.windows.net/ container / path /' ... ] ) account is the name of the Azure storage account (e.g. myaccount ). container is the name of a Azure blob storage container that stores your data files (e.g. mycontainer ). path is an optional path (or directory ) in the bucket that further limits access to the data files."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the integration. Default: No value"
        },
        {
            "name": "STORAGE_PROVIDER   =   '{   S3   |   S3CHINA   |   S3GOV   }'",
            "description": "Specifies the cloud storage provider that stores your data files: 'S3' : S3 storage in public AWS regions outside of China. 'S3CHINA' : S3 storage in public AWS regions in China. 'S3GOV' : S3 storage in AWS government regions."
        },
        {
            "name": "STORAGE_AWS_ROLE_ARN   =   ' iam_role '",
            "description": "Specifies the Amazon Resource Name (ARN) of the AWS identity and access management (IAM) role that grants privileges on the S3 bucket\ncontaining your data files. For more information, see Configuring secure access to Amazon S3 ."
        },
        {
            "name": "STORAGE_AWS_EXTERNAL_ID   =   ' external_id '",
            "description": "Optionally specifies an external ID that Snowflake uses to establish a trust relationship with AWS.\nYou must specify the same external ID in the trust policy of the IAM role\nthat you configured for this storage integration. For more information,\nsee How to use an external ID when granting access to your AWS resources to a third party . If you don’t specify a value for this parameter,\nSnowflake automatically generates an external ID when you create the storage integration."
        },
        {
            "name": "STORAGE_AWS_OBJECT_ACL   =   'bucket-owner-full-control'",
            "description": "Enables support for AWS access control lists (ACLs) to grant the bucket owner full control. Files created in Amazon S3 buckets from\nunloaded table data are owned by an AWS Identity and Access Management (IAM) role. ACLs support the use case where IAM roles in one\nAWS account are configured to access S3 buckets in one or more other AWS accounts. Without ACL support, users in the bucket-owner\naccounts could not access the data files unloaded to an external (S3) stage using a storage integration. When users unload Snowflake table data to data files in an S3 stage using COPY INTO <location> , the unload\noperation applies an ACL to the unloaded data files. The data files apply the \"s3:x-amz-acl\":\"bucket-owner-full-control\" privilege to the files, granting the S3 bucket owner full control over them."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter, see AWS private connectivity to external stages ."
        },
        {
            "name": "STORAGE_PROVIDER   =   'GCS'",
            "description": "Specifies the cloud storage provider that stores your data files."
        },
        {
            "name": "STORAGE_PROVIDER   =   'AZURE'",
            "description": "Specifies the cloud storage provider that stores your data files."
        },
        {
            "name": "AZURE_TENANT_ID   =   ' tenant_id '",
            "description": "Specifies the ID for your Office 365 tenant that the allowed and blocked storage accounts belong to. A storage integration can\nauthenticate to only one tenant, and so the allowed and blocked storage locations must refer to storage accounts that all belong\nthis tenant. To find your tenant ID, log into the Azure portal and click Azure Active Directory » Properties . The tenant ID\nis displayed in the Tenant ID field."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter,\nsee Azure private connectivity for external stages and Snowpipe automation ."
        }
    ],
    "usage_notes": "Caution\nRecreating a storage integration (using CREATE OR REPLACE STORAGE INTEGRATION) breaks the association between the storage integration\nand any stage that references it. This is because a stage links to a storage integration using a hidden ID rather than the name of the\nstorage integration. Behind the scenes, the CREATE OR REPLACE syntax drops the object and recreates it with a different hidden ID.\nIf you must recreate a storage integration after it has been linked to one or more stages, you must reestablish the association between\neach stage and the storage integration by executing ALTER STAGE stage_name SET STORAGE_INTEGRATION =\nstorage_integration_name, where:\nstage_name is the name of the stage.\nstorage_integration_name is the name of the storage integration.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-clone",
    "title": "CREATE",
    "description": "Creates a copy of an existing object in the system. This command is primarily used for creating\nzero-copy clones of databases, schemas, and tables.\nYou can also use this command to create clones of other schema objects, including\nexternal stages, file formats, sequences, and database roles.",
    "syntax": "CREATE [ OR REPLACE ] { DATABASE | SCHEMA } [ IF NOT EXISTS ] <object_name>\n  CLONE <source_object_name>\n    [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n    [ IGNORE TABLES WITH INSUFFICIENT DATA RETENTION ]\n    [ IGNORE HYBRID TABLES ]\n    [ INCLUDE INTERNAL STAGES ]\n  ...\n\nCREATE [ OR REPLACE ] TABLE [ IF NOT EXISTS ] <object_name>\n  CLONE <source_object_name>\n    [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n  ...\n\nCREATE [ OR REPLACE ] DYNAMIC TABLE <name>\n  CLONE <source_dynamic_table>\n    [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n  [\n    TARGET_LAG = { '<num> { seconds | minutes | hours | days }' | DOWNSTREAM }\n    WAREHOUSE = <warehouse_name>\n  ]\n\nCREATE [ OR REPLACE ] EVENT TABLE <name>\n  CLONE <source_event_table>\n    [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n\nCREATE [ OR REPLACE ] ICEBERG TABLE [ IF NOT EXISTS ] <name>\n  CLONE <source_iceberg_table>\n    [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n    [ COPY GRANTS ]\n    ...\n\nCREATE [ OR REPLACE ] DATABASE ROLE [ IF NOT EXISTS ] <database_role_name>\n  CLONE <source_database_role_name>\n\nCREATE [ OR REPLACE ] { ALERT | FILE FORMAT | SEQUENCE | STAGE | STREAM | TASK }\n  [ IF NOT EXISTS ] <object_name>\n  CLONE <source_object_name>\n  ...",
    "examples": [
        {
            "code": "CREATE DATABASE mytestdb_clone CLONE mytestdb;"
        },
        {
            "code": "CREATE SCHEMA mytestschema_clone CLONE testschema;"
        },
        {
            "code": "CREATE TABLE orders_clone CLONE orders;"
        },
        {
            "code": "CREATE SCHEMA mytestschema_clone_restore CLONE testschema\n  BEFORE (TIMESTAMP => TO_TIMESTAMP(40*365*86400));"
        },
        {
            "code": "CREATE TABLE orders_clone_restore CLONE orders\n  AT (TIMESTAMP => TO_TIMESTAMP_TZ('04/05/2013 01:02:03', 'mm/dd/yyyy hh24:mi:ss'));"
        },
        {
            "code": "CREATE TABLE orders_clone_restore CLONE orders BEFORE (STATEMENT => '8e5d0ca9-005e-44e6-b858-a8f5b37c5726');"
        },
        {
            "code": "CREATE DATABASE restored_db CLONE my_db\n  AT (TIMESTAMP => DATEADD(days, -4, current_timestamp)::timestamp_tz)\n  IGNORE TABLES WITH INSUFFICIENT DATA RETENTION;"
        },
        {
            "code": "CREATE OR REPLACE SCHEMA clone_ht_schema CLONE ht_schema\n  IGNORE HYBRID TABLES;"
        }
    ],
    "parameters": [
        {
            "name": "{   AT   |   BEFORE   }   (   {   TIMESTAMP   =>   timestamp   |   OFFSET   =>   time_difference   |   STATEMENT   =>   id   }   )",
            "description": "The AT | BEFORE clause accepts one of the following parameters: Specifies an exact date and time to use for Time Travel. The value must be explicitly cast to a TIMESTAMP,\nTIMESTAMP_LTZ, TIMESTAMP_NTZ, or TIMESTAMP_TZ data type. If no explicit cast is specified, the timestamp in the AT clause is treated as a timestamp with the UTC time zone (equivalent to\nTIMESTAMP_NTZ). Using the TIMESTAMP data type for an explicit cast may also result in the value being treated as a TIMESTAMP_NTZ\nvalue. For details, see Date & time data types . Specifies the difference in seconds from the current time to use for Time Travel, in the form -N where N can be an integer or arithmetic expression (e.g. -120 is 120 seconds, -30*60 is 1800 seconds or 30 minutes). Specifies the query ID of a statement to use as the reference point for Time Travel. This parameter supports any statement of one of the\nfollowing types: DML (e.g. INSERT, UPDATE, DELETE) TCL (BEGIN, COMMIT transaction) SELECT The query ID must reference a query that has been executed within the last 14 days. If the query ID references a query over 14 days old,\nthe following error is returned: To work around this limitation, use the timestamp for the referenced query."
        },
        {
            "name": "TIMESTAMP   =>   timestamp",
            "description": "Specifies an exact date and time to use for Time Travel. The value must be explicitly cast to a TIMESTAMP,\nTIMESTAMP_LTZ, TIMESTAMP_NTZ, or TIMESTAMP_TZ data type. If no explicit cast is specified, the timestamp in the AT clause is treated as a timestamp with the UTC time zone (equivalent to\nTIMESTAMP_NTZ). Using the TIMESTAMP data type for an explicit cast may also result in the value being treated as a TIMESTAMP_NTZ\nvalue. For details, see Date & time data types ."
        },
        {
            "name": "OFFSET   =>   time_difference",
            "description": "Specifies the difference in seconds from the current time to use for Time Travel, in the form -N where N can be an integer or arithmetic expression (e.g. -120 is 120 seconds, -30*60 is 1800 seconds or 30 minutes)."
        },
        {
            "name": "STATEMENT   =>   id",
            "description": "Specifies the query ID of a statement to use as the reference point for Time Travel. This parameter supports any statement of one of the\nfollowing types: DML (e.g. INSERT, UPDATE, DELETE) TCL (BEGIN, COMMIT transaction) SELECT The query ID must reference a query that has been executed within the last 14 days. If the query ID references a query over 14 days old,\nthe following error is returned: To work around this limitation, use the timestamp for the referenced query."
        },
        {
            "name": "IGNORE   TABLES   WITH   INSUFFICIENT   DATA   RETENTION",
            "description": "Ignore tables that no longer have historical data available in Time Travel to clone. If the time in the past specified in the\nAT | BEFORE clause is beyond the data retention period for any child table in a database or schema, skip the cloning operation\nfor the child table. For more information, see Child Objects and Data Retention Time ."
        },
        {
            "name": "IGNORE   HYBRID   TABLES",
            "description": "Ignore hybrid tables when cloning a database or schema. The cloned database or schema includes other objects but skips hybrid tables.\nFor more information, see Clone databases that contain hybrid tables ."
        },
        {
            "name": "INCLUDE   INTERNAL   STAGES",
            "description": "Include named internal stages when cloning a database or schema. For more information, see the usage notes ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/forecast/commands/create-forecast.html#label-class-forecast-create",
    "title": "CREATE SNOWFLAKE.ML.FORECAST",
    "description": "Creates a new forecast model from the training data you provide or replaces the forecast model of the same name.",
    "syntax": "CREATE [ OR REPLACE ] SNOWFLAKE.ML.FORECAST [ IF NOT EXISTS ] <model_name>(\n  INPUT_DATA => <input_data>,\n  [ SERIES_COLNAME => '<series_colname>', ]\n  TIMESTAMP_COLNAME => '<timestamp_colname>',\n  TARGET_COLNAME => '<target_colname>',\n  [ CONFIG_OBJECT => <config_object> ]\n)\n[ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n[ COMMENT = '<string_literal>' ]\n\nCREATE SNOWFLAKE.ML.FORECAST <name>(\n  '<input_data>', '<series_colname>', '<timestamp_colname>', '<target_colname>'\n);",
    "examples": [
        {
            "code": "CREATE [ OR REPLACE ] SNOWFLAKE.ML.FORECAST [ IF NOT EXISTS ] <model_name>(\n  INPUT_DATA => <input_data>,\n  [ SERIES_COLNAME => '<series_colname>', ]\n  TIMESTAMP_COLNAME => '<timestamp_colname>',\n  TARGET_COLNAME => '<target_colname>',\n  [ CONFIG_OBJECT => <config_object> ]\n)\n[ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n[ COMMENT = '<string_literal>' ]"
        },
        {
            "code": "CREATE SNOWFLAKE.ML.FORECAST <name>(\n  '<input_data>', '<series_colname>', '<timestamp_colname>', '<target_colname>'\n);"
        }
    ],
    "parameters": [
        {
            "name": "model_name",
            "description": "Specifies the identifier for the model; must be unique for the schema in which the model is created. If the model identifier is not fully qualified (in the form of db_name . schema_name . name or schema_name . name ), the command creates the model in the current schema for the session. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters\nunless the entire identifier string is enclosed in double quotes (for example, \"My object\" ). Identifiers enclosed in\ndouble quotes are also case-sensitive. For more details, see Identifier requirements ."
        }
    ],
    "usage_notes": "Replication is supported only for instances\nof the CUSTOM_CLASSIFIER class."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-sequence",
    "title": "CREATE SEQUENCE",
    "description": "Creates a new sequence, which can be used for generating sequential, unique numbers.",
    "syntax": "CREATE [ OR REPLACE ] SEQUENCE [ IF NOT EXISTS ] <name>\n  [ WITH ]\n  [ START [ WITH ] [ = ] <initial_value> ]\n  [ INCREMENT [ BY ] [ = ] <sequence_interval> ]\n  [ { ORDER | NOORDER } ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE OR REPLACE SEQUENCE seq_01 START = 1 INCREMENT = 1;\nCREATE OR REPLACE TABLE sequence_test_table (i INTEGER);"
        },
        {
            "code": "SELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       1 |\n+---------+"
        },
        {
            "code": "SELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       2 |\n+---------+"
        },
        {
            "code": "INSERT INTO sequence_test_table (i) VALUES (seq_01.nextval);"
        },
        {
            "code": "SELECT i FROM sequence_test_table;\n+---+\n| I |\n|---|\n| 3 |\n+---+"
        },
        {
            "code": "CREATE OR REPLACE SEQUENCE seq_5 START = 1 INCREMENT = 5;"
        },
        {
            "code": "SELECT seq_5.nextval a, seq_5.nextval b, seq_5.nextval c, seq_5.nextval d;\n+---+---+----+----+\n| A | B |  C |  D |\n|---+---+----+----|\n| 1 | 6 | 11 | 16 |\n+---+---+----+----+"
        },
        {
            "code": "SELECT seq_5.nextval a, seq_5.nextval b, seq_5.nextval c, seq_5.nextval d;\n+----+----+----+----+\n|  A |  B |  C |  D |\n|----+----+----+----|\n| 36 | 41 | 46 | 51 |\n+----+----+----+----+"
        },
        {
            "code": "CREATE OR REPLACE SEQUENCE seq90;\nCREATE OR REPLACE TABLE sequence_demo (i INTEGER DEFAULT seq90.nextval, dummy SMALLINT);\nINSERT INTO sequence_demo (dummy) VALUES (0);\n\n-- Keep doubling the number of rows:\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;"
        },
        {
            "code": "SELECT i FROM sequence_demo ORDER BY i LIMIT 10;\n+----+\n|  I |\n|----|\n|  1 |\n|  2 |\n|  3 |\n|  4 |\n|  5 |\n|  6 |\n|  7 |\n|  8 |\n|  9 |\n| 10 |\n+----+"
        },
        {
            "code": "SELECT COUNT(i), COUNT(DISTINCT i) FROM sequence_demo;\n+----------+-------------------+\n| COUNT(I) | COUNT(DISTINCT I) |\n|----------+-------------------|\n|     1024 |              1024 |\n+----------+-------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the sequence; must be unique for the schema in which the sequence is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details about identifiers, see Identifier requirements ."
        },
        {
            "name": "START   [   WITH   ]   [   =   ]   initial_value",
            "description": "Specifies the first value returned by the sequence. Supported values are any value that can be represented by a 64-bit two’s\ncomplement integer (from -2^63 to 2^63 - 1 ). Default: 1"
        },
        {
            "name": "INCREMENT   [   BY   ]   [   =   ]   sequence_interval",
            "description": "Specifies the step interval of the sequence: For positive sequence interval n , the next n-1 values are reserved by each sequence call. For negative sequence interval -n , the next n-1 lower values are reserved by each sequence call. Supported values are any non-zero value that can be represented by a 64-bit two’s complement integer. Default: 1"
        },
        {
            "name": "{   ORDER   |   NOORDER   }",
            "description": "Specifies whether or not the values are generated for the sequence in increasing or decreasing order . ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). Default: The NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is set by default."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the sequence. Default: No value"
        }
    ],
    "usage_notes": "The first/initial value for a sequence cannot be changed after the sequence is created.\nA sequence does not necessarily produce a gap-free sequence. Values increase (until the limit is reached) and are unique,\nbut are not necessarily contiguous. For more information, including the upper and lower limits, see Sequence Semantics.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-view",
    "title": "CREATE VIEW",
    "description": "Creates a new view in the current/specified schema, based on a query of one or more existing tables (or any other valid query expression).",
    "syntax": "CREATE [ OR REPLACE ] [ SECURE ] [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE } ] [ RECURSIVE ] VIEW [ IF NOT EXISTS ] <name>\n  [ ( <column_list> ) ]\n  [ <col1> [ WITH ] MASKING POLICY <policy_name> [ USING ( <col1> , <cond_col1> , ... ) ]\n           [ WITH ] PROJECTION POLICY <policy_name>\n           [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ , <col2> [ ... ] ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] JOIN POLICY <policy_name> [ ALLOWED JOIN KEYS ( <col_name> [ , ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  AS <select_statement>",
    "examples": [
        {
            "code": "CREATE VIEW myview COMMENT='Test view' AS SELECT col1, col2 FROM mytable;\n\nSHOW VIEWS;\n\n+---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------+\n| created_on                      | name              | reserved | database_name | schema_name | owner    | comment   | text                                                                     |\n|---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------|\n| Thu, 19 Jan 2017 15:00:37 -0800 | MYVIEW            |          | MYTEST1       | PUBLIC      | SYSADMIN | Test view | CREATE VIEW myview COMMENT='Test view' AS SELECT col1, col2 FROM mytable |\n+---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------+"
        },
        {
            "code": "CREATE OR REPLACE SECURE VIEW myview COMMENT='Test secure view' AS SELECT col1, col2 FROM mytable;\n\nSELECT is_secure FROM information_schema.views WHERE table_name = 'MYVIEW';"
        },
        {
            "code": "CREATE OR REPLACE TABLE employees (title VARCHAR, employee_ID INTEGER, manager_ID INTEGER);"
        },
        {
            "code": "INSERT INTO employees (title, employee_ID, manager_ID) VALUES\n    ('President', 1, NULL),  -- The President has no manager.\n        ('Vice President Engineering', 10, 1),\n            ('Programmer', 100, 10),\n            ('QA Engineer', 101, 10),\n        ('Vice President HR', 20, 1),\n            ('Health Insurance Analyst', 200, 20);"
        },
        {
            "code": "CREATE VIEW employee_hierarchy (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n   WITH RECURSIVE employee_hierarchy_cte (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n      -- Start at the top of the hierarchy ...\n      SELECT title, employee_ID, manager_ID, NULL AS \"MGR_EMP_ID (SHOULD BE SAME)\", 'President' AS \"MGR TITLE\"\n        FROM employees\n        WHERE title = 'President'\n      UNION ALL\n      -- ... and work our way down one level at a time.\n      SELECT employees.title, \n             employees.employee_ID, \n             employees.manager_ID, \n             employee_hierarchy_cte.employee_id AS \"MGR_EMP_ID (SHOULD BE SAME)\", \n             employee_hierarchy_cte.title AS \"MGR TITLE\"\n        FROM employees INNER JOIN employee_hierarchy_cte\n       WHERE employee_hierarchy_cte.employee_ID = employees.manager_ID\n   )\n   SELECT * \n      FROM employee_hierarchy_cte\n);"
        },
        {
            "code": "SELECT * \n    FROM employee_hierarchy \n    ORDER BY employee_ID;\n+----------------------------+-------------+------------+-----------------------------+----------------------------+\n| TITLE                      | EMPLOYEE_ID | MANAGER_ID | MGR_EMP_ID (SHOULD BE SAME) | MGR TITLE                  |\n|----------------------------+-------------+------------+-----------------------------+----------------------------|\n| President                  |           1 |       NULL |                        NULL | President                  |\n| Vice President Engineering |          10 |          1 |                           1 | President                  |\n| Vice President HR          |          20 |          1 |                           1 | President                  |\n| Programmer                 |         100 |         10 |                          10 | Vice President Engineering |\n| QA Engineer                |         101 |         10 |                          10 | Vice President Engineering |\n| Health Insurance Analyst   |         200 |         20 |                          20 | Vice President HR          |\n+----------------------------+-------------+------------+-----------------------------+----------------------------+"
        },
        {
            "code": "CREATE RECURSIVE VIEW employee_hierarchy_02 (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n      -- Start at the top of the hierarchy ...\n      SELECT title, employee_ID, manager_ID, NULL AS \"MGR_EMP_ID (SHOULD BE SAME)\", 'President' AS \"MGR TITLE\"\n        FROM employees\n        WHERE title = 'President'\n      UNION ALL\n      -- ... and work our way down one level at a time.\n      SELECT employees.title, \n             employees.employee_ID, \n             employees.manager_ID, \n             employee_hierarchy_02.employee_id AS \"MGR_EMP_ID (SHOULD BE SAME)\", \n             employee_hierarchy_02.title AS \"MGR TITLE\"\n        FROM employees INNER JOIN employee_hierarchy_02\n        WHERE employee_hierarchy_02.employee_ID = employees.manager_ID\n);"
        },
        {
            "code": "SELECT * \n    FROM employee_hierarchy_02 \n    ORDER BY employee_ID;\n+----------------------------+-------------+------------+-----------------------------+----------------------------+\n| TITLE                      | EMPLOYEE_ID | MANAGER_ID | MGR_EMP_ID (SHOULD BE SAME) | MGR TITLE                  |\n|----------------------------+-------------+------------+-----------------------------+----------------------------|\n| President                  |           1 |       NULL |                        NULL | President                  |\n| Vice President Engineering |          10 |          1 |                           1 | President                  |\n| Vice President HR          |          20 |          1 |                           1 | President                  |\n| Programmer                 |         100 |         10 |                          10 | Vice President Engineering |\n| QA Engineer                |         101 |         10 |                          10 | Vice President Engineering |\n| Health Insurance Analyst   |         200 |         20 |                          20 | Vice President HR          |\n+----------------------------+-------------+------------+-----------------------------+----------------------------+"
        },
        {
            "code": "CREATE OR ALTER TABLE my_table(a INT);"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one)\n  AS SELECT a FROM my_table;"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one)\n  COMMENT = 'fff'\n  CHANGE_TRACKING = true\n  AS SELECT a FROM my_table;"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one COMMENT 'bar')\n  COMMENT = 'foo'\n  AS SELECT a FROM my_table;"
        },
        {
            "code": "CREATE OR ALTER VIEW v2(one COMMENT 'bar')\n  CHANGE_TRACKING = true\n  AS SELECT a FROM my_table;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the view; must be unique for the schema in which the view is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "select_statement",
            "description": "Specifies the query used to create the view. Can be on one or more source tables or any other valid SELECT statement. This\nquery serves as the text/definition for the view and is displayed in the SHOW VIEWS output and the VIEWS Information Schema view."
        },
        {
            "name": "SECURE",
            "description": "Specifies that the view is secure. For more information about secure views, see Working with Secure Views . Default: No value (view is not secure)"
        },
        {
            "name": "{   [   {   LOCAL   |   GLOBAL   }   ]   TEMP   |   TEMPORARY   |   VOLATILE   }",
            "description": "Specifies that the view persists only for the duration of the session that you created it in. A\ntemporary view and all its contents are dropped at the end of the session. The synonyms and abbreviations for TEMPORARY (e.g. GLOBAL TEMPORARY ) are provided for compatibility with other databases\n(e.g. to prevent errors when migrating CREATE VIEW statements). Views created with any of these keywords appear and behave identically to\na view created with the TEMPORARY keyword. Default: No value. If a view is not declared as TEMPORARY , the view is permanent. If you want to avoid unexpected conflicts, avoid naming temporary views after views that already exist in the schema. If you created a temporary view with the same name as another view in the schema, all queries and operations used on the view only affect\nthe temporary view in the session, until you drop the temporary view. If you drop the view, you drop the temporary view, and not the view\nthat already exists in the schema."
        },
        {
            "name": "RECURSIVE",
            "description": "Specifies that the view can refer to itself using recursive syntax without necessarily using a CTE (common table\nexpression). For more information about recursive views in general, and the RECURSIVE keyword in particular,\nsee Recursive Views (Non-materialized Views Only) and the recursive view examples below. Default: No value (view is not recursive, or is recursive only by using a CTE)"
        },
        {
            "name": "column_list",
            "description": "If you want to change the name of a column or add a comment to a column in the new view,\ninclude a column list that specifies the column names and (if needed) comments about\nthe columns. (You do not need to specify the data types of the columns.) If any of the columns in the view are based on expressions (not just simple column names), then you must supply\na column name for each column in the view. For example, the column names are required in the following case: You can specify an optional comment for each column. For example: Comments are particularly helpful when column names are cryptic. To view comments, use DESCRIBE VIEW ."
        },
        {
            "name": "MASKING   POLICY   =   policy_name",
            "description": "Specifies the masking policy to set on a column."
        },
        {
            "name": "USING   (   col_name   ,   cond_col_1   ...   )",
            "description": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal masking policy ."
        },
        {
            "name": "PROJECTION   POLICY   policy_name",
            "description": "Specifies the projection policy to set on a column."
        },
        {
            "name": "CHANGE_TRACKING   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to enable change tracking on the view. TRUE enables change tracking on the view. This setting adds a pair of hidden columns to the source table and begins\nstoring change tracking metadata in the columns. These columns consume a small amount of storage. The change-tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the view."
        },
        {
            "name": "COPY   GRANTS",
            "description": "Retains the access permissions from the original view when a new view is created using the OR REPLACE clause. The parameter copies all privileges, except OWNERSHIP, from the existing view to the new view. The new view does not inherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE VIEW statement owns\nthe new view. If the parameter is not included in the CREATE VIEW statement, then the new view does not inherit any explicit access\nprivileges granted on the original view but does inherit any future grants defined for the object type in the schema. Note that the operation to copy grants occurs atomically with the CREATE VIEW statement (i.e. within the same transaction). Default: No value (grants are not copied)"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the view. Default: No value"
        },
        {
            "name": "ROW   ACCESS   POLICY   policy_name   ON   (   col_name   [   ,   col_name   ...   ]   )",
            "description": "Specifies the row access policy to set on a view."
        },
        {
            "name": "AGGREGATION   POLICY   policy_name   [   ENTITY   KEY   (   col_name   [   ,   col_name   ...   ]   )   ]",
            "description": "Specifies the aggregation policy to set on a view. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the view. For more information, see Implementing entity-level privacy with aggregation policies ."
        },
        {
            "name": "JOIN   POLICY   policy_name   [   ALLOWED   JOIN   KEYS   (   col_name   [   ,   ...   ]   )   ]",
            "description": "Specifies the join policy to set on a view. Use the optional ALLOWED JOIN KEYS parameter to define which columns are allowed to be used as joining columns when\nthis policy is in effect. For more information, see Join policies . This parameter is not supported by the CREATE OR ALTER variant syntax."
        },
        {
            "name": "TAG   (   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]   )",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "WITH   CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the new object with one or more contacts ."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/budget/commands/create-budget",
    "title": "CREATE BUDGET",
    "description": "Fully qualified name: SNOWFLAKE.CORE.BUDGET",
    "syntax": "CREATE [ OR REPLACE ] SNOWFLAKE.CORE.BUDGET [ IF NOT EXISTS ] <name> ()\n  [ [ WITH ] COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE SNOWFLAKE.CORE.BUDGET my_budget();"
        }
    ],
    "parameters": [
        {
            "name": "name :",
            "description": "Specifies the identifier for the budget. The identifier must start with an alphabetic character and cannot contain spaces or\nspecial characters unless the identifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double\nquotes are also case-sensitive. For more details, refer to Identifier requirements ."
        },
        {
            "name": "COMMENT   =   ' string_literal ' :",
            "description": "Specifies a comment for the budget."
        }
    ],
    "usage_notes": "To refer to this class by its unqualified name, include the database and schema of the class in your\nsearch path.\nReplication is supported only for instances\nof the CUSTOM_CLASSIFIER class.\nAn account can contain a maximum of 100 custom budgets."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-session-policy",
    "title": "CREATE SESSION POLICY",
    "description": "Creates a new session policy or replaces an existing session policy.",
    "syntax": "CREATE [OR REPLACE] SESSION POLICY [IF NOT EXISTS] <name>\n  [ SESSION_IDLE_TIMEOUT_MINS = <integer> ]\n  [ SESSION_UI_IDLE_TIMEOUT_MINS = <integer> ]\n  [ ALLOWED_SECONDARY_ROLES = ( [ { 'ALL' | <role_name> [ , <role_name> ... ] } ] ) ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "CREATE SESSION POLICY session_policy_prod_1\n  SESSION_IDLE_TIMEOUT_MINS = 30\n  SESSION_UI_IDLE_TIMEOUT_MINS = 30\n  COMMENT = 'session policy for use in the prod_1 environment'\n;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the session policy; must be unique for your account. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "SESSION_IDLE_TIMEOUT_MINS   =   integer",
            "description": "For Snowflake clients and programmatic clients, the number of minutes in which a session can be idle before users must authenticate to\nSnowflake again. If a value is not specified, Snowflake uses the default value. The number of minutes can be any integer between 5 and 240 , inclusive. Default: 240 (4 hours)"
        },
        {
            "name": "SESSION_UI_IDLE_TIMEOUT_MINS   =   integer",
            "description": "For Snowsight, the number of minutes in which a session can be idle before a user must authenticate to Snowflake again. If a\nvalue is not specified, Snowflake uses the default value. The number of minutes can be any integer between 5 and 240 , inclusive. Default: 240 (4 hours)"
        },
        {
            "name": "ALLOWED_SECONDARY_ROLES   =   (   [   {   'ALL'   |   role_name   [   ,   role_name   ...   ]   }   ]   )",
            "description": "Specifies the secondary roles for a session policy, if any. The possible values for the property are: Disallows secondary roles. Allows all secondary roles. Allows the specified roles as secondary roles. The secondary roles can be user-defined account roles or system roles. Specify the\nrole name as it is stored in Snowflake. For details, see Identifier requirements . Default: ('ALL') . If you do not set the property when you create a new session policy, all secondary roles are allowed."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the session policy."
        },
        {
            "name": "()",
            "description": "Disallows secondary roles."
        },
        {
            "name": "('ALL')",
            "description": "Allows all secondary roles."
        },
        {
            "name": "(   role_name   [   ,   role_name   ...   ]   )",
            "description": "Allows the specified roles as secondary roles. The secondary roles can be user-defined account roles or system roles. Specify the\nrole name as it is stored in Snowflake. For details, see Identifier requirements ."
        }
    ],
    "usage_notes": "If you want to replace an existing session policy and need to see the current definition of the policy, call the\nGET_DDL function or run the DESCRIBE SESSION POLICY command.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They can’t both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
}
]