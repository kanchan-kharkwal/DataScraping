[
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/budget/commands/alter-budget",
    "title": "ALTER BUDGET",
    "description": "Fully qualified name: SNOWFLAKE.CORE.BUDGET",
    "syntax": "ALTER SNOWFLAKE.CORE.BUDGET [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER SNOWFLAKE.CORE.BUDGET [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER SNOWFLAKE.CORE.BUDGET [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER SNOWFLAKE.CORE.BUDGET [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER SNOWFLAKE.CORE.BUDGET [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER SNOWFLAKE.CORE.BUDGET my_budget SET TAG dept = 'finance';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) of the budget. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more budget properties to be set. Sets the comment of the budget. This can also be done using the COMMENT command. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties and/or parameters to unset for the budget, which resets them to the defaults: COMMENT TAG tag_name [ , tag_name ... ]"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Sets the comment of the budget. This can also be done using the COMMENT command."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "You can only modify the properties for a custom budget.\nTo refer to this class by its unqualified name, include the database and schema of the class in your\nsearch path."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-dataset-add-version",
    "title": "ALTER DATASET … ADD VERSION",
    "description": "Adds a version to a dataset. When you add a version, you can specify properties such as partitioning, comments, or custom metadata.",
    "syntax": "ALTER DATASET <name> ADD VERSION <version_name>\n  FROM <select_statement>\n  [ PARTITION BY <string_expr> ]\n  [ COMMENT = <string_literal> ]\n  [ METADATA = <json_string_literal> ]",
    "examples": [
        {
            "code": "ALTER DATASET abc\nADD VERSION 'v1' FROM (\n    SELECT seq4() as ID, uniform(1, 10, random(721)) as PART\n    FROM TABLE(GENERATOR(ROWCOUNT => 100000)) v)\nPARTITION BY PART\nCOMMENT = 'Initial version'\nMETADATA = '{\"source\":\"some_table\",\"created_by\":\"analyst1\"}';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "The name of the dataset that you’re altering."
        },
        {
            "name": "ADD   VERSION   version_name",
            "description": "The name of the new dataset version that you’re creating."
        },
        {
            "name": "FROM   select_statement",
            "description": "The SQL statement that defines the data for the new dataset version."
        },
        {
            "name": "PARTITION   BY   string_expr",
            "description": "The partitioning expression for the new dataset version."
        },
        {
            "name": "COMMENT   =   string_literal",
            "description": "A comment for the new dataset version."
        },
        {
            "name": "METADATA   =   json_string_literal",
            "description": "A JSON string containing metadata for the new dataset version.\nThe following is an example of a JSON string."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-password-policy",
    "title": "ALTER PASSWORD POLICY",
    "description": "Modifies the properties for an existing password policy.",
    "syntax": "ALTER PASSWORD POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER PASSWORD POLICY [ IF EXISTS ] <name> SET [ PASSWORD_MIN_LENGTH = <integer> ]\n                                               [ PASSWORD_MAX_LENGTH = <integer> ]\n                                               [ PASSWORD_MIN_UPPER_CASE_CHARS = <integer> ]\n                                               [ PASSWORD_MIN_LOWER_CASE_CHARS = <integer> ]\n                                               [ PASSWORD_MIN_NUMERIC_CHARS = <integer> ]\n                                               [ PASSWORD_MIN_SPECIAL_CHARS = <integer> ]\n                                               [ PASSWORD_MIN_AGE_DAYS = <integer> ]\n                                               [ PASSWORD_MAX_AGE_DAYS = <integer> ]\n                                               [ PASSWORD_MAX_RETRIES = <integer> ]\n                                               [ PASSWORD_LOCKOUT_TIME_MINS = <integer> ]\n                                               [ PASSWORD_HISTORY = <integer> ]\n                                               [ COMMENT = '<string_literal>' ]\n\nALTER PASSWORD POLICY [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PASSWORD POLICY [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PASSWORD POLICY [ IF EXISTS ] <name> UNSET [ PASSWORD_MIN_LENGTH ]\n                                                 [ PASSWORD_MAX_LENGTH ]\n                                                 [ PASSWORD_MIN_UPPER_CASE_CHARS ]\n                                                 [ PASSWORD_MIN_LOWER_CASE_CHARS ]\n                                                 [ PASSWORD_MIN_NUMERIC_CHARS ]\n                                                 [ PASSWORD_MIN_SPECIAL_CHARS ]\n                                                 [ PASSWORD_MIN_AGE_DAYS ]\n                                                 [ PASSWORD_MAX_AGE_DAYS ]\n                                                 [ PASSWORD_MAX_RETRIES ]\n                                                 [ PASSWORD_LOCKOUT_TIME_MINS ]\n                                                 [ PASSWORD_HISTORY ]\n                                                 [ COMMENT ]",
    "examples": [
        {
            "code": "DESC PASSWORD POLICY password_policy_prod_1;\n\nALTER PASSWORD POLICY password_policy_prod_1 SET PASSWORD_MAX_RETRIES = 3;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the password policy; must be unique for your account. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the session policy; must be unique for your account. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more parameters to set for the password policy separated by blank spaces, commas, or new lines. Specifies the minimum number of characters the password must contain. Supported range: 8 to 256, inclusive. Default: 14 Specifies the maximum number of characters the password must contain. This number must be greater than or equal to the sum of PASSWORD_MIN_LENGTH , PASSWORD_MIN_UPPER_CASE_CHARS , and PASSWORD_MIN_LOWER_CASE_CHARS . Supported range: 8 to 256, inclusive. Default: 256 Specifies the minimum number of uppercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1 Specifies the minimum number of lowercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1 Specifies the minimum number of numeric characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1 Specifies the minimum number of special characters the password must contain. Supported range: 0 to 256, inclusive. Default: 0 Specifies the number of days the user must wait before a recently changed password can be changed again. Supported range: 0 to 999, inclusive. Default: 0 Specifies the maximum number of days before the password must be changed. Supported range: 0 to 999, inclusive. A value of zero (i.e. 0 ) indicates that the password does not need to be changed. Snowflake does not recommend choosing this\nvalue for a default account-level password policy or for any user-level policy. Instead, choose a value that meets your internal\nsecurity guidelines. Default: 90, which means the password must be changed every 90 days. Important This parameter is stateful. For details, see the note in Custom password policy for the account and users . Specifies the maximum number of attempts to enter a password before being locked out. Supported range: 1 to 10, inclusive. Default: 5 Important This parameter is stateful. For details, see the note in Custom password policy for the account and users . Specifies the number of minutes the user account will be locked after exhausting the designated number of password retries\n(i.e. PASSWORD_MAX_RETRIES ). Supported range: 1 to 999, inclusive. Default: 15 Important This parameter is stateful. For details, see the note in Custom password policy for the account and users . Specifies the number of the most recent passwords that Snowflake stores. These stored passwords cannot be repeated when a user updates\ntheir password value. The current password value does not count towards the history. When you increase the history value, Snowflake saves the previous values. When you decrease the value, Snowflake saves the stored values up to that value that is set. For example, if the history value is 8 and\nyou change the history value to 3, Snowflake stores the most recent 3 password values and deletes the 5 older password values from the\nhistory. Default: 5 Max: 24 Adds a comment or overwrites an existing comment for the password policy. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more parameters to unset for the password policy, which resets them to the system defaults. You can reset multiple properties with a single ALTER statement. Each property must be separated by a comma. When\nresetting a property, specify only the name. Specifying a value for the property will return an error."
        },
        {
            "name": "PASSWORD_MIN_LENGTH   =   integer",
            "description": "Specifies the minimum number of characters the password must contain. Supported range: 8 to 256, inclusive. Default: 14"
        },
        {
            "name": "PASSWORD_MAX_LENGTH   =   integer",
            "description": "Specifies the maximum number of characters the password must contain. This number must be greater than or equal to the sum of PASSWORD_MIN_LENGTH , PASSWORD_MIN_UPPER_CASE_CHARS , and PASSWORD_MIN_LOWER_CASE_CHARS . Supported range: 8 to 256, inclusive. Default: 256"
        },
        {
            "name": "PASSWORD_MIN_UPPER_CASE_CHARS   =   integer",
            "description": "Specifies the minimum number of uppercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1"
        },
        {
            "name": "PASSWORD_MIN_LOWER_CASE_CHARS   =   integer",
            "description": "Specifies the minimum number of lowercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1"
        },
        {
            "name": "PASSWORD_MIN_NUMERIC_CHARS   =   integer",
            "description": "Specifies the minimum number of numeric characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1"
        },
        {
            "name": "PASSWORD_MIN_SPECIAL_CHARS   =   integer",
            "description": "Specifies the minimum number of special characters the password must contain. Supported range: 0 to 256, inclusive. Default: 0"
        },
        {
            "name": "PASSWORD_MIN_AGE_DAYS   =   integer",
            "description": "Specifies the number of days the user must wait before a recently changed password can be changed again. Supported range: 0 to 999, inclusive. Default: 0"
        },
        {
            "name": "PASSWORD_MAX_AGE_DAYS   =   integer",
            "description": "Specifies the maximum number of days before the password must be changed. Supported range: 0 to 999, inclusive. A value of zero (i.e. 0 ) indicates that the password does not need to be changed. Snowflake does not recommend choosing this\nvalue for a default account-level password policy or for any user-level policy. Instead, choose a value that meets your internal\nsecurity guidelines. Default: 90, which means the password must be changed every 90 days. Important This parameter is stateful. For details, see the note in Custom password policy for the account and users ."
        },
        {
            "name": "PASSWORD_MAX_RETRIES   =   integer",
            "description": "Specifies the maximum number of attempts to enter a password before being locked out. Supported range: 1 to 10, inclusive. Default: 5 Important This parameter is stateful. For details, see the note in Custom password policy for the account and users ."
        },
        {
            "name": "PASSWORD_LOCKOUT_TIME_MINS   =   integer",
            "description": "Specifies the number of minutes the user account will be locked after exhausting the designated number of password retries\n(i.e. PASSWORD_MAX_RETRIES ). Supported range: 1 to 999, inclusive. Default: 15 Important This parameter is stateful. For details, see the note in Custom password policy for the account and users ."
        },
        {
            "name": "PASSWORD_HISTORY   =   integer",
            "description": "Specifies the number of the most recent passwords that Snowflake stores. These stored passwords cannot be repeated when a user updates\ntheir password value. The current password value does not count towards the history. When you increase the history value, Snowflake saves the previous values. When you decrease the value, Snowflake saves the stored values up to that value that is set. For example, if the history value is 8 and\nyou change the history value to 3, Snowflake stores the most recent 3 password values and deletes the 5 older password values from the\nhistory. Default: 5 Max: 24"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the password policy."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "Before executing this command, run the DESCRIBE PASSWORD POLICY command to determine the attribute values of the policy.\nIf you want to update an existing password policy and need to see the current definition of the policy, call the\nGET_DDL function or run the DESCRIBE PASSWORD POLICY command.\nMoving a password policy to a managed access schema is prohibited unless the password policy owner (i.e. the role that has the\nOWNERSHIP privilege on the password policy) also owns the target schema. For more information, see\nOverview of Access Control Privileges."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-database-role",
    "title": "ALTER DATABASE ROLE",
    "description": "Modifies the properties for an existing database role.",
    "syntax": "ALTER DATABASE ROLE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER DATABASE ROLE [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER DATABASE ROLE [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER DATABASE ROLE [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER DATABASE ROLE [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER DATABASE ROLE d1.dr1 RENAME TO d1.dbr2;"
        },
        {
            "code": "ALTER DATABASE ROLE d1.dbr2 SET COMMENT = 'New comment for database role';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) for the database role; must be unique in the database in which the role is created. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. If the identifier is not fully qualified in the form of db_name . database_role_name , the command looks for the database role\nin the current database for the session."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the database role; must be unique for your account. For more details, see Identifier requirements . Note that when specifying the fully-qualified name of the database role, you cannot specify a different database. The name of\nthe database, db_name , must remain the same. Only the database_role_name can change during a rename operation."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the database role: Adds a comment or overwrites an existing comment for the database role. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the database role, which resets them to the defaults. COMMENT TAG tag_name [ , tag_name ... ]"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the database role."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-model",
    "title": "ALTER MODEL",
    "description": "Modifies the properties for an existing model, including its name, tags, default version, or comment.",
    "syntax": "ALTER MODEL [ IF EXISTS ] <name> SET\n  [ COMMENT = '<string_literal>' ]\n  [ DEFAULT_VERSION = '<version_name>']\n\nALTER MODEL [ IF EXISTS ] <model_name> SET TAG <tag_name> = '<tag_value>'\n\nALTER MODEL [ IF EXISTS ] <model_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER MODEL [ IF EXISTS ] <model_name> VERSION <version_name> SET ALIAS = '<alias_name>'\n\nALTER MODEL [ IF EXISTS ] <model_name> VERSION <version_or_alias_name> UNSET ALIAS\n\nALTER MODEL <model_name> RENAME TO <new_name>",
    "examples": [
        {
            "code": "ALTER MODEL [ IF EXISTS ] <name> SET\n  [ COMMENT = '<string_literal>' ]\n  [ DEFAULT_VERSION = '<version_name>']\n\nALTER MODEL [ IF EXISTS ] <model_name> SET TAG <tag_name> = '<tag_value>'\n\nALTER MODEL [ IF EXISTS ] <model_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER MODEL [ IF EXISTS ] <model_name> VERSION <version_name> SET ALIAS = '<alias_name>'\n\nALTER MODEL [ IF EXISTS ] <model_name> VERSION <version_or_alias_name> UNSET ALIAS\n\nALTER MODEL <model_name> RENAME TO <new_name>"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (i.e. name) of the model. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more model properties to be set. Sets the comment of the model. This can also be done using the COMMENT command. Sets the default version of the model (the version that methods are invoked on when calling a method directly on the\nmodel). The version name is an identifier . The system alias DEFAULT refers to the default version. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Sets alias_name as an alias of the version. An alias is an alternative name that can be easily reassigned.\nThe alias can be used most places where the version name can be used. A version can have at most one alias. The alias name is an identifier . It must be unique in the model and may\nnot duplicate the system alias names, which are: DEFAULT refers to the default version of the model. FIRST refers to the oldest version of the model by creation time. LAST refers to the newest version of the model by creation time."
        },
        {
            "name": "UNSET   TAG   tag_name   [   ,   tag_name   ...   ]",
            "description": "Specifies one or more tags to be unset on the model."
        },
        {
            "name": "UNSET   ALIAS",
            "description": "Removes the alias from this model version, if it has one. The system aliases DEFAULT, FIRST, and LAST cannot be removed.\nYou may specify the version by its name or by alias."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Renames the specified model with a new identifier that is not currently used by any other models in the schema. For more details about identifiers, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When a model is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Sets the comment of the model. This can also be done using the COMMENT command."
        },
        {
            "name": "DEFAULT_VERSION   =   ' version_name '",
            "description": "Sets the default version of the model (the version that methods are invoked on when calling a method directly on the\nmodel). The version name is an identifier . The system alias DEFAULT refers to the default version."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "ALIAS   =   ' alias_name '",
            "description": "Sets alias_name as an alias of the version. An alias is an alternative name that can be easily reassigned.\nThe alias can be used most places where the version name can be used. A version can have at most one alias. The alias name is an identifier . It must be unique in the model and may\nnot duplicate the system alias names, which are: DEFAULT refers to the default version of the model. FIRST refers to the oldest version of the model by creation time. LAST refers to the newest version of the model by creation time."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-compute-pool",
    "title": "ALTER COMPUTE POOL",
    "description": "Modifies the properties of an existing\ncompute pool.",
    "syntax": "ALTER COMPUTE POOL [ IF EXISTS ] <name> { SUSPEND | RESUME }\n\nALTER COMPUTE POOL [ IF EXISTS ] <name> STOP ALL;\n\nALTER COMPUTE POOL [ IF EXISTS ] <name> SET [ MIN_NODES = <num> ]\n                                            [ MAX_NODES = <num> ]\n                                            [ AUTO_RESUME = { TRUE | FALSE } ]\n                                            [ AUTO_SUSPEND_SECS = <num> ]\n                                            [ COMMENT = '<string_literal>' ]\n                                            [ TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ]\n\nALTER COMPUTE POOL [ IF EXISTS ] <name> UNSET { AUTO_SUSPEND_SECS |\n                                                AUTO_RESUME       |\n                                                COMMENT\n                                              }\n                                              [ , ... ]",
    "examples": [
        {
            "code": "ALTER COMPUTE POOL tutorial_compute_pool SET\n  MAX_NODES = 5\n  AUTO_RESUME = FALSE"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the compute pool to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "{   SUSPEND   |   RESUME   }",
            "description": "Suspends a compute pool or resumes a previously suspended compute pool. When you suspend a compute pool, Snowflake suspends all services in that compute pool,\nbut the jobs continue to run until they reach a terminal state (DONE or FAILED), after which the compute pool nodes are released."
        },
        {
            "name": "STOP   ALL",
            "description": "Drops all services and cancels jobs executing in the compute pool. Snowflake then removes all the containers from the\ncompute pool."
        },
        {
            "name": "SET   ...",
            "description": "Sets one or more specified properties or parameters for the compute pool: Specifies the minimum number of compute pool nodes. Specifies the maximum number of compute pool nodes. Specifies whether to automatically resume a compute pool when a service or job is submitted to it. If AUTO_RESUME is FALSE,\nyou need to explicitly resume the compute pool (using ALTER COMPUTE POOL <name> RESUME) before you can start a service or\njob on the compute pool. Number of seconds of inactivity after which you want Snowflake to automatically suspend the compute pool. Inactivity means\nno services and no jobs running on any node in the compute pool. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Specifies a comment for the compute pool."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties and/or parameters to unset for the compute pool, which resets them to the defaults (see CREATE COMPUTE POOL ): AUTO_SUSPEND_SECS AUTO_RESUME COMMENT"
        },
        {
            "name": "MIN_NODES   =   num",
            "description": "Specifies the minimum number of compute pool nodes."
        },
        {
            "name": "MAX_NODES   =   num",
            "description": "Specifies the maximum number of compute pool nodes."
        },
        {
            "name": "AUTO_RESUME   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically resume a compute pool when a service or job is submitted to it. If AUTO_RESUME is FALSE,\nyou need to explicitly resume the compute pool (using ALTER COMPUTE POOL <name> RESUME) before you can start a service or\njob on the compute pool."
        },
        {
            "name": "AUTO_SUSPEND_SECS   =   num",
            "description": "Number of seconds of inactivity after which you want Snowflake to automatically suspend the compute pool. Inactivity means\nno services and no jobs running on any node in the compute pool."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the compute pool."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-file-format",
    "title": "ALTER FILE FORMAT",
    "description": "Modifies the properties for an existing file format object. Currently the only actions that are supported are renaming the file format, changing\nthe file format options (based on the type), and adding/changing a comment. To make any other changes, you must drop the file format and then\nrecreate it.",
    "syntax": "ALTER FILE FORMAT [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER FILE FORMAT [ IF EXISTS ] <name> SET { [ formatTypeOptions ] [ COMMENT = '<string_literal>' ] }\n\nformatTypeOptions ::=\n-- If TYPE = CSV\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     FILE_EXTENSION = '<string>'\n     PARSE_HEADER = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     ESCAPE = '<character>' | NONE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n     ENCODING = '<string>' | UTF8\n-- If TYPE = JSON\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     TRIM_SPACE = TRUE | FALSE\n     MULTI_LINE = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     FILE_EXTENSION = '<string>'\n     ENABLE_OCTAL = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n-- If TYPE = AVRO\n     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = ORC\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = PARQUET\n     COMPRESSION = AUTO | LZO | SNAPPY | NONE\n     SNAPPY_COMPRESSION = TRUE | FALSE\n     BINARY_AS_TEXT = TRUE | FALSE\n     USE_LOGICAL_TYPE = TRUE | FALSE\n     TRIM_SPACE = TRUE | FALSE\n     USE_VECTORIZED_SCANNER = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = XML\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     PRESERVE_SPACE = TRUE | FALSE\n     STRIP_OUTER_ELEMENT = TRUE | FALSE\n     DISABLE_AUTO_CONVERT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE",
    "examples": [
        {
            "code": "ALTER FILE FORMAT IF EXISTS my_format RENAME TO my_new_format;"
        },
        {
            "code": "ALTER FILE FORMAT my_format SET FIELD_DELIMITER=',';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the file format to alter. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the file format; must be unique for the schema. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the options/properties to set for the file format: Modifies the format-specific options for the file format. For more details, see Format Type Options (in this topic). Adds a comment or overwrites the existing comment for the file format."
        },
        {
            "name": "FILE_FORMAT   =   (   ...   )",
            "description": "Modifies the format-specific options for the file format. For more details, see Format Type Options (in this topic)."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the file format."
        }
    ],
    "usage_notes": "ALTER FILE FORMAT does not support the following actions:\nChanging the type (CSV, JSON, etc.) for the file format.\nUnsetting any format options (i.e. resetting the options to the defaults for the type).\nUnsetting (i.e. removing) a comment.\nTo make any of these changes, you must recreate the file format."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-application-package",
    "title": "ALTER APPLICATION PACKAGE",
    "description": "Modifies the properties of an existing application package in the Native Apps Framework.",
    "syntax": "ALTER APPLICATION PACKAGE [ IF EXISTS ] <name> SET\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COMMENT = <string-literal> ]\n  [ DISTRIBUTION = { INTERNAL | EXTERNAL } ]\n  [ MULTIPLE_INSTANCES = TRUE ]\n  [ ENABLE_RELEASE_CHANNELS = TRUE ]\n  [ SET LISTING_AUTO_REFRESH = { TRUE | FALSE } ]\n\nALTER APPLICATION PACKAGE [ IF EXISTS ] <name> UNSET\n  [ DATA_RETENTION_TIME_IN_DAYS ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS ]\n  [ DEFAULT_DDL_COLLATION ]\n  [ COMMENT ]\n  [ DISTRIBUTION ]\n\nALTER APPLICATION PACKAGE <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER APPLICATION PACKAGE <name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER APPLICATION PACKAGE hello_snowflake_package SET\n  COMMENT = 'Altered the Hello Snowflake app.';"
        },
        {
            "code": "+-------------------------------------------+\n| status                                    |\n|-------------------------------------------|\n| Statement executed successfully.          |\n+-------------------------------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the application package to alter. If the identifier contains\nspaces, special characters, or mixed-case characters, the entire string must be enclosed\nin double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the application package (separated by blank spaces, commas, or new lines): Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the\ndefault Time Travel retention time for all schemas created in the database. The value you can specify depends on the Snowflake Edition you are using: Standard Edition: 0 or 1 Enterprise Edition (or higher): 0 to 90 Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the database\nto prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS . Specifies a default collation specification for: Any new columns added to existing tables in the database. All columns in new tables added to the database. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION . Specifies a default collation specification for: Any new columns added to existing tables in the database. All columns in new tables added to the database. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION . Adds a comment or overwrites an existing comment for the database. Specifies the type of listing a provider can create when using the application package as the data product of a listing. INTERNAL indicates that a provider can only create a private listing within the same organization\nwhere the application package was created. The automated security scan is not performed\nwhen the DISTRIBUTION property is set to INTERNAL. EXTERNAL indicates that a provider can create listings outside the same organization where\nthe application package was created. See Run the automated security scan for information on setting the DISTRIBUTION property and\nthe automated security scan. Note Setting the DISTRIBUTION paramater to EXTERNAL triggers an automated security review for each\nactive version and patch defined in the application package. The following restrictions apply until the automated security review has a status of APPROVED : You cannot set a release directive for a version or patch. You cannot publish a listing for the application package. When set to TRUE, initiates replication to all remote regions when there is a change to the release directive of the application package. When a release directive changes, the application package does not wait for the Cross-Cloud Auto-Fulfillment schedule. Enables the consumer to install multiple instances of an app from the application package. This property cannot be\nset for applications packages that are included in a trial or paid listing. When multiple instances are allowed, consumers can install a maximum of 10 instances of an app in their account. Caution After setting this property to true, it cannot be set to FALSE or unset later."
        },
        {
            "name": "ENABLE_RELEASE_CHANNELS   =   TRUE",
            "description": "Enables release channels for the application package. Caution After setting this property to TRUE , it cannot be set to FALSE or unset later."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties and/or parameters to unset for the application package, which resets\nthem to the defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS EXTERNAL_VOLUME CATALOG DEFAULT_DDL_COLLATION TAG tag_name [ , tag_name ... ] COMMENT You can reset multiple properties/parameters with a single ALTER statement; however, each property/parameter must be separated by a\ncomma. When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   num",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the\ndefault Time Travel retention time for all schemas created in the database. The value you can specify depends on the Snowflake Edition you are using: Standard Edition: 0 or 1 Enterprise Edition (or higher): 0 to 90"
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the database\nto prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for: Any new columns added to existing tables in the database. All columns in new tables added to the database. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for: Any new columns added to existing tables in the database. All columns in new tables added to the database. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the database."
        },
        {
            "name": "DISTRIBUTION   =   {   INTERNAL   |   EXTERNAL   }",
            "description": "Specifies the type of listing a provider can create when using the application package as the data product of a listing. INTERNAL indicates that a provider can only create a private listing within the same organization\nwhere the application package was created. The automated security scan is not performed\nwhen the DISTRIBUTION property is set to INTERNAL. EXTERNAL indicates that a provider can create listings outside the same organization where\nthe application package was created. See Run the automated security scan for information on setting the DISTRIBUTION property and\nthe automated security scan. Note Setting the DISTRIBUTION paramater to EXTERNAL triggers an automated security review for each\nactive version and patch defined in the application package. The following restrictions apply until the automated security review has a status of APPROVED : You cannot set a release directive for a version or patch. You cannot publish a listing for the application package."
        },
        {
            "name": "LISTING_AUTO_REFRESH   =     TRUE   |   FALSE",
            "description": "When set to TRUE, initiates replication to all remote regions when there is a change to the release directive of the application package. When a release directive changes, the application package does not wait for the Cross-Cloud Auto-Fulfillment schedule."
        },
        {
            "name": "MULTIPLE_INSTANCES   =   TRUE",
            "description": "Enables the consumer to install multiple instances of an app from the application package. This property cannot be\nset for applications packages that are included in a trial or paid listing. When multiple instances are allowed, consumers can install a maximum of 10 instances of an app in their account. Caution After setting this property to true, it cannot be set to FALSE or unset later."
        }
    ],
    "usage_notes": "If you do not specify the values for the optional properties, the command uses the values specified in the\nmanifest file of the app.\nIf you specify values for the properties in the command and in the manifest file of the app, the values specified in the command take precedence.\nIf two versions are active (e.g. if the current version has not finished rolling out), adding a new version results in an error.\nNew versions are added with a default patch number of 0.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-model-modify-version",
    "title": "ALTER MODEL … MODIFY VERSION",
    "description": "Modifies a version of a model, changing the version’s comment or metadata.",
    "syntax": "ALTER MODEL [ IF EXISTS ] <name> MODIFY VERSION <version_or_alias_name> SET\n  [ COMMENT = '<string_literal>' ]\n  [ METADATA = '<json_metadata>']",
    "examples": [
        {
            "code": "ALTER MODEL [ IF EXISTS ] <name> MODIFY VERSION <version_or_alias_name> SET\n  [ COMMENT = '<string_literal>' ]\n  [ METADATA = '<json_metadata>']"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier of the model. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "version_or_alias_name",
            "description": "Specifies the identifier of the version, either its version name or its alias. Version names that contain spaces or\nthat are case sensitive must be enclosed in double quotes. For information on identifier syntax, see Identifier requirements . Aliases must be valid identifiers without double quotes. See Usage Notes for more information on aliases."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more model version properties to be set. Sets the comment of the version. Sets the metadata of the version. Metadata is a JSON object that stores key-value pairs of your choosing."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Sets the comment of the version."
        },
        {
            "name": "METADATA   =   ' json_metadata '",
            "description": "Sets the metadata of the version. Metadata is a JSON object that stores key-value pairs of your choosing."
        }
    ],
    "usage_notes": "Aliases are alternative names for model versions. In addition to aliases you create, the following three system aliases are available.\nDEFAULT refers to the default version of the model.\nFIRST refers to the oldest version of the model by creation time.\nLAST refers to the newest version of the model by creation time."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-schema",
    "title": "ALTER SCHEMA",
    "description": "Modifies the properties for an existing schema, including renaming the schema or swapping it with another schema, and changing the Time Travel\ndata retention period (if you are using Snowflake Enterprise Edition or higher).",
    "syntax": "ALTER SCHEMA [ IF EXISTS ] <name> RENAME TO <new_schema_name>\n\nALTER SCHEMA [ IF EXISTS ] <name> SWAP WITH <target_schema_name>\n\nALTER SCHEMA [ IF EXISTS ] <name> SET {\n                                      [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n                                      [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n                                      [ EXTERNAL_VOLUME = <external_volume_name> ]\n                                      [ CATALOG = <catalog_integration_name> ]\n                                      [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n                                      [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n                                      [ DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU = '<compute_pool_name>' ]\n                                      [ DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU = '<compute_pool_name>' ]\n                                      [ LOG_LEVEL = '<log_level>' ]\n                                      [ TRACE_LEVEL = '<trace_level>' ]\n                                      [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n                                      [ CLASSIFICATION_PROFILE = '<classification_profile>' ]\n                                      [ COMMENT = '<string_literal>' ]\n                                      [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>' ]\n                                      [ REPLICABLE_WITH_FAILOVER_GROUPS = { 'YES' | 'NO' } ]\n                                      [ BASE_LOCATION_PREFIX = '<string>']\n                                      [ DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE = '<warehouse_name>']\n                                      [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n                                      }\n\nALTER SCHEMA [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER SCHEMA [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER SCHEMA [ IF EXISTS ] <name> UNSET {\n                                        DATA_RETENTION_TIME_IN_DAYS         |\n                                        MAX_DATA_EXTENSION_TIME_IN_DAYS     |\n                                        EXTERNAL_VOLUME                     |\n                                        CATALOG                             |\n                                        REPLACE_INVALID_CHARACTERS          |\n                                        DEFAULT_DDL_COLLATION               |\n                                        LOG_LEVEL                           |\n                                        TRACE_LEVEL                         |\n                                        STORAGE_SERIALIZATION_POLICY        |\n                                        COMMENT                             |\n                                        CATALOG_SYNC                        |\n                                        REPLICABLE_WITH_FAILOVER_GROUPS     |\n                                        BASE_LOCATION_PREFIX                |\n                                        DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE|\n                                        CONTACT <purpose>\n                                        }\n                                        [ , ... ]\n\nALTER SCHEMA [ IF EXISTS ] <name> { ENABLE | DISABLE } MANAGED ACCESS",
    "examples": [
        {
            "code": "ALTER SCHEMA IF EXISTS schema1 RENAME TO schema2;"
        },
        {
            "code": "ALTER SCHEMA schema2 ENABLE MANAGED ACCESS;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the schema to alter. If the identifier contains spaces, special characters, or mixed-case characters, the entire\nstring must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_schema_name",
            "description": "Specifies the new identifier for the schema; must be unique for the database. For more details, see Identifier requirements . You can move the object to a different database while optionally renaming the schema. To do so, specify a qualified new_schema_name value that includes the new database name in the form db_name . new_schema_name . Note The destination database must already exist. In addition, a schema with the same name cannot already exist in the new location;\notherwise, the statement returns an error. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SWAP   WITH   target_schema_name",
            "description": "Swaps all objects (tables, views, etc.) and metadata, including identifiers, between the two specified schemas. Also swaps all access control\nprivileges granted on the schemas and objects they contain. SWAP WITH essentially performs a rename of both schemas as a single operation."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the schema (separated by blank spaces, commas, or new lines): Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the schema, as well as specifying the\ndefault Time Travel retention time for all tables created in the schema. The value you can specify depends on the Snowflake Edition you are using: Standard Edition: 0 or 1 Enterprise Edition (or higher): 0 to 90 Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the schema\nto prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS . Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME . Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG . Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE Specifies a default collation specification for: Any new columns added to existing tables in the schema. All columns in new tables added to the schema. Setting the parameter does not change the collation specification for any existing columns. For more details about the parameter, see DEFAULT_DDL_COLLATION . CPU compute pool name that overrides the default CPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks . GPU compute pool name that overrides the default GPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks . Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing . Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing . Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts . Associates the schema with a classification profile so that sensitive data in the schema is automatically classified . Adds a comment or overwrites an existing comment for the schema. Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the schema with an external catalog in your Snowflake Open Catalog account.\nFor more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value Specifies if this schema is eligible for replication.\nYou can set this property to NO to prevent individual schemas\nwithin a database from being replicated. For more information about this parameter, see Schema-level replication for failover groups . Default: 'YES' Specifies the default warehouse to use when you create a notebook using SQL. Specifies a prefix for Snowflake to use in the write path for Snowflake-managed Apache Iceberg™ tables.\nFor more information,\nsee data and metadata directories for Iceberg tables and BASE_LOCATION_PREFIX in the Snowflake Parameters topic. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties and/or parameters to unset for the database, which resets them to the defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS EXTERNAL_VOLUME CATALOG REPLACE_INVALID_CHARACTERS DEFAULT_DDL_COLLATION TAG tag_name [ , tag_name ... ] LOG_LEVEL TRACE_LEVEL STORAGE_SERIALIZATION_POLICY COMMENT CATALOG_SYNC REPLICABLE_WITH_FAILOVER_GROUPS BASE_LOCATION_PREFIX DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE CONTACT purpose You can reset multiple properties/parameters with a single ALTER statement; however, each property/parameter must be separated by a\ncomma. When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "{   ENABLE   |   DISABLE   }   MANAGED   ACCESS",
            "description": "Enable managed access for a schema, or disable to convert a managed access schema to a regular schema. Managed access schemas centralize\nprivilege management with the schema owner. In regular schemas, the owner of an object (i.e. the role that has the OWNERSHIP privilege on the object) can grant further privileges on\ntheir objects to other roles. In managed access schemas, the schema owner manages all privilege grants, including future grants , on objects in the schema. Object owners retain the OWNERSHIP privileges\non the objects; however, only the schema owner can manage privilege grants on the objects."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the schema, as well as specifying the\ndefault Time Travel retention time for all tables created in the schema. The value you can specify depends on the Snowflake Edition you are using: Standard Edition: 0 or 1 Enterprise Edition (or higher): 0 to 90"
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the schema\nto prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "EXTERNAL_VOLUME   =   external_volume_name",
            "description": "Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME ."
        },
        {
            "name": "CATALOG   =   catalog_integration_name",
            "description": "Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG ."
        },
        {
            "name": "REPLACE_INVALID_CHARACTERS   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for: Any new columns added to existing tables in the schema. All columns in new tables added to the schema. Setting the parameter does not change the collation specification for any existing columns. For more details about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU   =   compute_pool_name",
            "description": "CPU compute pool name that overrides the default CPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks ."
        },
        {
            "name": "DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU   =   compute_pool_name",
            "description": "GPU compute pool name that overrides the default GPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "STORAGE_SERIALIZATION_POLICY   =   {   COMPATIBLE   |   OPTIMIZED   }",
            "description": "Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "CLASSIFICATION_PROFILE   =   ' classification_profile '",
            "description": "Associates the schema with a classification profile so that sensitive data in the schema is automatically classified ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the schema."
        },
        {
            "name": "CATALOG_SYNC   =   ' snowflake_open_catalog_integration_name '",
            "description": "Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the schema with an external catalog in your Snowflake Open Catalog account.\nFor more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value"
        },
        {
            "name": "REPLICABLE_WITH_FAILOVER_GROUPS   =   {   'YES'   |   'NO'   }",
            "description": "Specifies if this schema is eligible for replication.\nYou can set this property to NO to prevent individual schemas\nwithin a database from being replicated. For more information about this parameter, see Schema-level replication for failover groups . Default: 'YES'"
        },
        {
            "name": "DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE   =   ' warehouse_name '",
            "description": "Specifies the default warehouse to use when you create a notebook using SQL."
        },
        {
            "name": "BASE_LOCATION_PREFIX   =   ' string '",
            "description": "Specifies a prefix for Snowflake to use in the write path for Snowflake-managed Apache Iceberg™ tables.\nFor more information,\nsee data and metadata directories for Iceberg tables and BASE_LOCATION_PREFIX in the Snowflake Parameters topic. Default: No value"
        }
    ],
    "usage_notes": "To rename a schema, the role used to perform the operation must have the CREATE SCHEMA privilege on the database for the schema and OWNERSHIP\nprivileges on the schema.\nTo swap two schemas, the role used to perform the operation must have OWNERSHIP privileges on both schemas.\nTo convert a regular schema to a managed access schema:\nThe schema owner (i.e. the role that has the OWNERSHIP privileges on the schema) must also have the global MANAGE GRANTS privilege. The\nMANAGE GRANTS privilege is required because another role with this privilege could have defined future grants on objects of a specified\ntype in the schema. After a regular schema becomes a managed access schema, the schema owner could revoke the future grants without\nunderstanding why a role with the MANAGE GRANTS privilege granted them.\nAll open future grants must be revoked using REVOKE <privileges> … FROM ROLE with the FUTURE keyword.\nAfter a regular schema is converted to a managed access schema, all privileges previously granted on individual objects are retained; however,\nthe object owners cannot grant further privileges on those objects.\nTo convert a managed access schema to a regular schema, the schema owner must also have the global MANAGE GRANTS privilege only if the\ncurrent schema has future privilege grants defined.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-function-dmf",
    "title": "ALTER FUNCTION (DMF)",
    "description": "Modifies the properties of an existing data metric function (DMF).",
    "syntax": "ALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET COMMENT = '<string_literal>'\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET COMMENT\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET COMMENT = '<string_literal>'\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET COMMENT\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( TABLE(  <arg_data_type> [ , ... ] ) [ , TABLE( <arg_data_type> [ , ... ] ) ] )\n  UNSET TAG <tag_name> [ , <tag_name> ... ]"
        },
        {
            "code": "ALTER FUNCTION governance.dmfs.count_positive_numbers(\n TABLE(\n   NUMBER,\n   NUMBER,\n   NUMBER\n))\nSET SECURE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the DMF to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "TABLE(   arg_data_type   [   ,   ...   ]   )   [   ,   TABLE(   <arg_data_type>   [   ,   ...   ]   )   ]",
            "description": "Specifies the data type of the column arguments for the DMF. The data types are necessary because DMFs support name\noverloading, where two DMFs in the same schema can have the same name. The data types of the arguments are used to identify the DMF you\nwant to alter."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the DMF; the combination of the identifier and existing argument data types must be unique for the\nschema. For more information, see Identifier requirements . Note When specifying the new name for the UDF, don’t specify argument data types or parentheses; specify only the new name. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the DMF: Specifies whether a function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures . Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the function, which resets them to the defaults. SECURE COMMENT TAG tag_name [ , tag_name ... ]"
        },
        {
            "name": "SECURE",
            "description": "Specifies whether a function is secure. For more information, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "If you want to update an existing data metric function and need to see the current definition of the function, run the\nDESCRIBE FUNCTION (DMF) command or call the GET_DDL function.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-row-access-policy",
    "title": "ALTER ROW ACCESS POLICY",
    "description": "Modifies the properties for an existing row access policy, including renaming the policy or replacing the policy rules.",
    "syntax": "ALTER ROW ACCESS POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER ROW ACCESS POLICY [ IF EXISTS ] <name> SET BODY -> <expression_on_arg_name>\n\nALTER ROW ACCESS POLICY [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER ROW ACCESS POLICY [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER ROW ACCESS POLICY [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER ROW ACCESS POLICY [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "DESC ROW ACCESS POLICY rap_table_employee_info;"
        },
        {
            "code": "+-------------------------+-------------+-------------+------+\n| name                    | signature   | return_type | body |\n+-------------------------+-------------+-------------+------+\n| rap_table_employee_info | (V VARCHAR) | BOOLEAN     | true |\n+-------------------------+-------------+-------------+------+"
        },
        {
            "code": "ALTER ROW ACCESS POLICY rap_table_employee_info SET BODY -> false;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the row access policy; must be unique in the parent schema of the policy. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the row access policy; must be unique for your schema. The new identifier cannot be used if the\nidentifier is already in place for a different row access policy. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the row access policy: SQL expression that filters the data. The expression can include Conditional expression functions to represent conditional logic, built-in functions, or UDFs to\ntransform the data. If a UDF or external function is used inside the row access policy body, the policy owner must have OWNERSHIP on the UDF or external\nfunction. Users querying a database object that has a row access policy applied to it do not need to have USAGE on the UDF or external\nfunction. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites the existing comment for the masking policy. Default: No value Specifies one or more properties and/or parameters to unset for the masking policy, which resets them to the defaults: TAG tag_name [ , tag_name ... ] COMMENT When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "BODY   ->   expression_on_arg_name",
            "description": "SQL expression that filters the data. The expression can include Conditional expression functions to represent conditional logic, built-in functions, or UDFs to\ntransform the data. If a UDF or external function is used inside the row access policy body, the policy owner must have OWNERSHIP on the UDF or external\nfunction. Users querying a database object that has a row access policy applied to it do not need to have USAGE on the UDF or external\nfunction."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the masking policy. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties and/or parameters to unset for the masking policy, which resets them to the defaults: TAG tag_name [ , tag_name ... ] COMMENT When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        }
    ],
    "usage_notes": "If you want to update an existing row access policy and need to see the current definition of the policy, call the\nGET_DDL function or run the DESCRIBE ROW ACCESS POLICY command.\nYou cannot change the policy signature (i.e. argument name or input/output data type). Similarly, using\nCREATE OR REPLACE ROW ACCESS POLICY is not supported if the policy is attached to a table or view. If you need to change the\nsignature, execute a DROP ROW ACCESS POLICY statement on the policy and create a new row access policy.\nBefore executing an ALTER statement, you can execute a DESCRIBE ROW ACCESS POLICY statement to determine the\nargument name to use for updating the policy.\nIncluding one or more subqueries in the policy body may cause errors. When possible, limit the\nnumber of subqueries, limit the number of JOIN operations, and simplify WHERE clause conditions.\nIf the policy body contains a mapping table lookup, create a centralized mapping table and store the mapping table\nin the same database as the protected table. This is particularly important if the body calls the\nIS_DATABASE_ROLE_IN_SESSION function. For details, see the function usage notes.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-snapshot",
    "title": "ALTER SNAPSHOT",
    "description": "Modifies the properties of an existing snapshot.",
    "syntax": "ALTER SNAPSHOT [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'",
    "examples": [
        {
            "code": "ALTER SNAPSHOT example_snapshot SET COMMENT = 'sample comment.';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the snapshot to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Sets one or more specified properties or parameters for the snapshot: Specifies a comment for the snapshot."
        },
        {
            "name": "COMMENT   =   string-literal",
            "description": "Specifies a comment for the snapshot."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-failover-group",
    "title": "ALTER FAILOVER GROUP",
    "description": "Modifies the properties for an existing failover group.",
    "syntax": "ALTER FAILOVER GROUP [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> SET\n  [ OBJECT_TYPES = <object_type> [ , <object_type> , ... ] ]\n  [ ALLOWED_DATABASES = <db_name> [ , <db_name> , ... ] ]\n  [ ALLOWED_SHARES = <share_name> [ , <share_name> , ... ] ]\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> SET\n  OBJECT_TYPES = INTEGRATIONS [ , <object_type> , ... ]\n  ALLOWED_INTEGRATION_TYPES = <integration_type_name> [ , <integration_type_name> ... ]\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> SET\n  COMMENT = '<string_literal>'\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> SET\n  REPLICATION_SCHEDULE = '{ <num> MINUTE | USING CRON <expr> <time_zone> }'\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> SET\n  ERROR_INTEGRATION = <integration_name>\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> SET\n  TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> UNSET\n  { COMMENT | REPLICATION_SCHEDULE | ERROR_INTEGRATION } [ , ... ]\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> UNSET\n  TAG <tag_name> [ , <tag_name> ... ]\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  ADD <db_name> [ , <db_name> ,  ... ] TO ALLOWED_DATABASES\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  MOVE DATABASES <db_name> [ , <db_name> ,  ... ] TO FAILOVER GROUP <move_to_fg_name>\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  REMOVE <db_name> [ , <db_name> ,  ... ] FROM ALLOWED_DATABASES\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  ADD <share_name> [ , <share_name> ,  ... ] TO ALLOWED_SHARES\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  MOVE SHARES <share_name> [ , <share_name> ,  ... ] TO FAILOVER GROUP <move_to_fg_name>\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  REMOVE <share_name> [ , <share_name> ,  ... ] FROM ALLOWED_SHARES\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  ADD <org_name>.<target_account_name> [ , <org_name>.<target_account_name> ,  ... ] TO ALLOWED_ACCOUNTS\n  [ IGNORE EDITION CHECK ]\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name>\n  REMOVE <org_name>.<target_account_name> [ , <org_name>.<target_account_name> ,  ... ] FROM ALLOWED_ACCOUNTS\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> REFRESH\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> PRIMARY\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> SUSPEND [ IMMEDIATE ]\n\nALTER FAILOVER GROUP [ IF EXISTS ] <name> RESUME",
    "examples": [
        {
            "code": "ALTER FAILOVER GROUP myfg ADD myorg.myaccount3 TO ALLOWED_ACCOUNTS;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg SET\n  OBJECT_TYPES = USERS, ROLES, WAREHOUSES, RESOURCE MONITORS, DATABASES\n  ALLOWED_DATABASES = db1;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg\n  ADD db2, db3 TO ALLOWED_DATABASES;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg\n  MOVE DATABASES db3 TO FAILOVER GROUP myfg2;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg3 SET\n  OBJECT_TYPES = DATABASES, SHARES;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg\n  MOVE DATABASES db2 TO FAILOVER GROUP myfg3;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg\n  SET ALLOWED_DATABASES = NULL;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg\n  REMOVE databases FROM OBJECT_TYPES;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg\n  SET REPLICATION_SCHEDULE = '15 MINUTE';"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg REFRESH;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg PRIMARY;"
        },
        {
            "code": "ALTER FAILOVER GROUP myfg SUSPEND;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the failover group."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the failover group. The new identifier cannot be used if the identifier is already in place for a\ndifferent replication or failover group. For more details, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies properties to set for the failover group (separated by blank spaces, commas, or new lines). Reset the list of object types for which you are enabling replication and failover from the source account to target\naccount(s). Note For database and share objects: If DATABASES or SHARES are included in the OBJECT_TYPES list, and remain in the OBJECT_TYPES list after\nthe list is reset, the respective allowed objects list (ALLOWED_DATABASES or ALLOWED_SHARES) remains\nunchanged. If the OBJECT_TYPES list is reset to add or remove DATABASES, the ALLOWED_DATABASES list is set to NULL. If the OBJECT_TYPES list is reset to add or remove SHARES, the ALLOWED_SHARES list is set to NULL. Use the ADD, MOVE, and REMOVE clauses to modify the list of allowed database or share objects. The following object types are supported: All account-level parameters. This includes account parameters and parameters that can be set for your account . Add database objects to the list of object types. If database objects were already included in the list of specified object\ntypes, the ALLOWED_DATABASES list remains unchanged. To modify the list of databases, use the\nADD, MOVE, or REMOVE clauses. Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set. All network policies in the source account. All resource monitors in the source account. All roles in the source account. Replicating roles implicitly includes all grants for object types included in the failover group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated. Add share objects to the list of object types. If share objects were already included in the list of specified object types, the ALLOWED_SHARES list remains unchanged. To modify the list of shares, use the ADD, MOVE, or REMOVE clauses. All users in the source account. All warehouses in the source account. Note If you replicate users and roles, programmatic access tokens for users are replicated automatically. Specifies the database or list of databases for which you are enabling replication and failover from the source account to the target\naccount. In order for you to set this parameter, the OBJECT_TYPES list must include DATABASES . Specifies the identifier for the database. Specifies the share or list of shares for which you are enabling replication and failover from the source account to the target account.\nIn order for you to set this parameter, the OBJECT_TYPES list must include SHARES . Specifies the identifier for the share. Note If the ALLOWED_DATABASES or ALLOWED_SHARES lists are modified, any objects that were previously in the list and removed\nwill be dropped in any target account with a linked secondary failover group when the next refresh operation occurs. Type(s) of integrations for which you are enabling replication and failover from the source account to the target account. This property requires that the OBJECT_TYPES list include INTEGRATIONS to set this parameter. The following integration types are supported: Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES . Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations . Specifies storage integrations. Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) . Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication . Adds a comment or overwrites an existing comment for the failover group. NULL Specifies the schedule for refreshing secondary failover groups. Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs. NULL Specifies the name of the notification integration to use to email/push notifications when refresh errors occur for the failover\ngroup. For more details, see Error notifications for replication and failover groups . Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "ADD   db_name   [   ,   db_name   ,    ...   ]   TO   ALLOWED_DATABASES",
            "description": "Specifies a comma-separated list of additional databases to enable for replication and failover. To add databases,\nDATABASES must be included in the list of specified object types. If the list of object types does not already include DATABASES, you must\nadd it. Specifies the identifier for the database."
        },
        {
            "name": "MOVE   DATABASES   db_name   [   ,   db_name   ,    ...   ]   TO   FAILOVER   GROUP   move_to_fg_name",
            "description": "Specifies a comma-separated list of databases to move from one failover group to another failover group. The failover group the databases\nare being moved to must include DATABASES in the list of specified object types. Specifies the identifier for the database. Specifies the identifier for the failover group the databases are being moved to."
        },
        {
            "name": "REMOVE   db_name   [   ,   db_name   ,    ...   ]   FROM   ALLOWED_DATABASES",
            "description": "Specifies a comma-separated list of databases to remove from the list of databases enabled for replication and failover. Note When you remove a database from a primary failover group, the database is dropped in any target account with a linked secondary\nfailover group when the next refresh operation occurs. To avoid dropping databases in the target account, you can drop the secondary failover group before the next time the modified\nprimary failover group is replicated to the target account. When you drop the secondary failover group, read-only secondary\ndatabases that were included in the group become standalone read-write databases in the target account."
        },
        {
            "name": "ADD   share_name   [   ,   share_name   ,    ...   ]   TO   ALLOWED_SHARES",
            "description": "Specifies a comma-separated list of additional shares to enable for replication and failover. To add shares, SHARES must be included in\nthe list of specified object types. If the list of object types does not already include SHARES, you must add it. Specifies the identifier for the share."
        },
        {
            "name": "MOVE   SHARES   share_name   [   ,   share_name   ,    ...   ]   TO   FAILOVER   GROUP   move_to_fg_name",
            "description": "Specifies a comma-separated list of shares to move from one failover group to another failover group. The failover group the shares\nare being moved to must include SHARES in the list of specified object types. Specifies the identifier for the share. Specifies the identifier for the failover group the shares are being moved to."
        },
        {
            "name": "REMOVE   share_name   [   ,   share_name   ,    ...   ]   FROM   ALLOWED_SHARES",
            "description": "Specifies a comma-separated list of shares to remove from the list of shares enabled for replication and failover. Note When you remove a share from a primary failover group, the share is dropped in any target account with a secondary\nfailover group when the next refresh operation occurs."
        },
        {
            "name": "ADD   org_name . target_account_name   [   ,   org_name . target_account_name   ,    ...   ]   TO   ALLOWED_ACCOUNTS",
            "description": "Specifies a comma-separated list of target accounts to add to the primary failover group to enable replication and failover of\nspecified objects in the source account to the target account. Secondary failover groups in the target accounts in this list\ncan be promoted to serve as the primary failover group in case of failover. Name of your Snowflake organization. Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "REMOVE   org_name . target_account_name   [   ,   org_name . target_account_name   ,    ...   ]   FROM   ALLOWED_ACCOUNTS",
            "description": "Specifies a comma-separated list of target accounts to remove from the primary failover group to disable replication\nof specified objects in the source account to the target account.\nRemoving a target account disables failover from the current account to this target account. Name of your Snowflake organization. Target account to which you are disabling replication of the specified objects."
        },
        {
            "name": "new_name",
            "description": "Specifies the new identifier for the failover group. The new identifier cannot be used if the identifier is already in place for a\ndifferent replication or failover group. For more details, see Identifier requirements ."
        },
        {
            "name": "OBJECT_TYPES   =   object_type   [   ,   object_type   ,   ...   ]",
            "description": "Reset the list of object types for which you are enabling replication and failover from the source account to target\naccount(s). Note For database and share objects: If DATABASES or SHARES are included in the OBJECT_TYPES list, and remain in the OBJECT_TYPES list after\nthe list is reset, the respective allowed objects list (ALLOWED_DATABASES or ALLOWED_SHARES) remains\nunchanged. If the OBJECT_TYPES list is reset to add or remove DATABASES, the ALLOWED_DATABASES list is set to NULL. If the OBJECT_TYPES list is reset to add or remove SHARES, the ALLOWED_SHARES list is set to NULL. Use the ADD, MOVE, and REMOVE clauses to modify the list of allowed database or share objects. The following object types are supported: All account-level parameters. This includes account parameters and parameters that can be set for your account . Add database objects to the list of object types. If database objects were already included in the list of specified object\ntypes, the ALLOWED_DATABASES list remains unchanged. To modify the list of databases, use the\nADD, MOVE, or REMOVE clauses. Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set. All network policies in the source account. All resource monitors in the source account. All roles in the source account. Replicating roles implicitly includes all grants for object types included in the failover group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated. Add share objects to the list of object types. If share objects were already included in the list of specified object types, the ALLOWED_SHARES list remains unchanged. To modify the list of shares, use the ADD, MOVE, or REMOVE clauses. All users in the source account. All warehouses in the source account. Note If you replicate users and roles, programmatic access tokens for users are replicated automatically."
        },
        {
            "name": "ALLOWED_DATABASES   =   db_name   [   ,   db_name   ,   ...   ]",
            "description": "Specifies the database or list of databases for which you are enabling replication and failover from the source account to the target\naccount. In order for you to set this parameter, the OBJECT_TYPES list must include DATABASES . Specifies the identifier for the database."
        },
        {
            "name": "ALLOWED_SHARES   =   share_name   [   ,   share_name   ,   ...   ]",
            "description": "Specifies the share or list of shares for which you are enabling replication and failover from the source account to the target account.\nIn order for you to set this parameter, the OBJECT_TYPES list must include SHARES . Specifies the identifier for the share."
        },
        {
            "name": "ACCOUNT PARAMETERS :",
            "description": "All account-level parameters. This includes account parameters and parameters that can be set for your account ."
        },
        {
            "name": "DATABASES :",
            "description": "Add database objects to the list of object types. If database objects were already included in the list of specified object\ntypes, the ALLOWED_DATABASES list remains unchanged. To modify the list of databases, use the\nADD, MOVE, or REMOVE clauses."
        },
        {
            "name": "INTEGRATIONS :",
            "description": "Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set."
        },
        {
            "name": "NETWORK POLICIES :",
            "description": "All network policies in the source account."
        },
        {
            "name": "RESOURCE MONITORS :",
            "description": "All resource monitors in the source account."
        },
        {
            "name": "ROLES :",
            "description": "All roles in the source account. Replicating roles implicitly includes all grants for object types included in the failover group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated."
        },
        {
            "name": "SHARES :",
            "description": "Add share objects to the list of object types. If share objects were already included in the list of specified object types, the ALLOWED_SHARES list remains unchanged. To modify the list of shares, use the ADD, MOVE, or REMOVE clauses."
        },
        {
            "name": "USERS :",
            "description": "All users in the source account."
        },
        {
            "name": "WAREHOUSES :",
            "description": "All warehouses in the source account."
        },
        {
            "name": "db_name",
            "description": "Specifies the identifier for the database."
        },
        {
            "name": "share_name",
            "description": "Specifies the identifier for the share."
        },
        {
            "name": "ALLOWED_INTEGRATION_TYPES   =   integration_type_name   [   ,   integration_type_name   ,   ...   ]",
            "description": "Type(s) of integrations for which you are enabling replication and failover from the source account to the target account. This property requires that the OBJECT_TYPES list include INTEGRATIONS to set this parameter. The following integration types are supported: Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES . Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations . Specifies storage integrations. Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) . Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the failover group. NULL"
        },
        {
            "name": "REPLICATION_SCHEDULE   ...",
            "description": "Specifies the schedule for refreshing secondary failover groups. Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs. NULL"
        },
        {
            "name": "ERROR_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the notification integration to use to email/push notifications when refresh errors occur for the failover\ngroup. For more details, see Error notifications for replication and failover groups ."
        },
        {
            "name": "SECURITY INTEGRATIONS :",
            "description": "Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES ."
        },
        {
            "name": "API INTEGRATIONS :",
            "description": "Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations ."
        },
        {
            "name": "STORAGE INTEGRATIONS :",
            "description": "Specifies storage integrations."
        },
        {
            "name": "EXTERNAL ACCESS INTEGRATIONS :",
            "description": "Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) ."
        },
        {
            "name": "NOTIFICATION INTEGRATIONS :",
            "description": "Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "Default :",
            "description": "NULL"
        },
        {
            "name": "USING   CRON   expr   time_zone",
            "description": "Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates."
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/ n",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run)."
        },
        {
            "name": "num   MINUTE",
            "description": "Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs."
        },
        {
            "name": "Default :",
            "description": "NULL"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "db_name",
            "description": "Specifies the identifier for the database."
        },
        {
            "name": "db_name",
            "description": "Specifies the identifier for the database."
        },
        {
            "name": "move_to_fg_name",
            "description": "Specifies the identifier for the failover group the databases are being moved to."
        },
        {
            "name": "share_name",
            "description": "Specifies the identifier for the share."
        },
        {
            "name": "share_name",
            "description": "Specifies the identifier for the share."
        },
        {
            "name": "move_to_fg_name",
            "description": "Specifies the identifier for the failover group the shares are being moved to."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "target_account_name",
            "description": "Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "target_account_name",
            "description": "Target account to which you are disabling replication of the specified objects."
        },
        {
            "name": "IGNORE   EDITION   CHECK",
            "description": "Allows replicating objects to accounts in the following scenario: The primary failover group is in a Business Critical (or higher) account and a signed business associate agreement is in place to\nstore PHI data in the account per HIPAA and HITRUST regulations. However, no such agreement is in place\nfor one or more of the accounts approved for replication, regardless if they are Business Critical (or higher) accounts. This scenario is prohibited by default."
        },
        {
            "name": "name",
            "description": "Specifies the identifier for the failover group."
        },
        {
            "name": "REFRESH",
            "description": "Refreshes the objects in the target (current) account from the source account."
        },
        {
            "name": "PRIMARY",
            "description": "Promote a secondary failover group and its specified objects in the target (current) account to primary (in case of\nfailover)."
        },
        {
            "name": "SUSPEND   [   IMMEDIATE   ]",
            "description": "Suspend the scheduled refresh of the secondary failover group (if the primary failover group has scheduled refreshes using the REPLICATION_SCHEDULE property). The optional IMMEDIATE keyword cancels a scheduled refresh operation that is currently in progress for the secondary failover group\n(if there is one). Note that there might be a slight delay between the time that the statement returns and the time that the cancellation\nof the refresh operation is finished."
        },
        {
            "name": "RESUME",
            "description": "Resume scheduled refresh of the secondary failover group (if the primary failover group has scheduled refreshes using the REPLICATION_SCHEDULE property)."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties to unset for the failover group, which resets them to the defaults: COMMENT REPLICATION_SCHEDULE ERROR_INTEGRATION TAG tag_name [ , tag_name ... ] You can reset multiple properties with a single ALTER statement; however, each property must be separated by\na comma. Also, when resetting a property, you only specify the name; no value is required."
        }
    ],
    "usage_notes": "The following minimal privileges are required:\nTo refresh a secondary failover group using ALTER FAILOVER GROUP … REFRESH, the active, primary role must have either the OWNERSHIP or\nREPLICATE privilege on the failover group.\nTo fail over a secondary failover group using ALTER FAILOVER GROUP … PRIMARY, a role must have either the OWNERSHIP or FAILOVER\nprivilege on the failover group.\nTo make any other changes to the failover group, only a role with the OWNERSHIP privilege on the group can execute this SQL command.\nTo add a database to a failover group, the active role must have the MONITOR privilege on the database.\nTo add a share to a failover group, the active role must have the OWNERSHIP privilege on the share.\nIdentifiers for failover groups and replication groups in an account must be unique.\nObjects other than databases and shares must be in the same failover group.\nA database can only be added to one failover group.\nInbound shares (shares from providers) cannot be added to a replication or failover group.\nPromoting a secondary failover group to primary (in case of failover) fails if a refresh is in progress.\nIf a refresh is in progress when the replication schedule is updated, the refresh continues until completion and the next refresh will\nuse the new schedule.\nOn failover, scheduled refreshes on all secondary failover groups are suspended. ALTER FAILOVER GROUP ... RESUME must be executed\non each secondary to resume automatic refreshes.\nTo move databases or shares from one failover group (the move-from group) to another failover group (the move-to group):\nBoth groups must be of the same type: FAILOVER GROUP.\nIf the last database in the move-from group is moved to another group, the allowed_databases property for the move-from group\nis set to NULL. The same behavior applies to shares.\nIf the move-to group does not have the object type that is being moved (databases or shares) in the object_types\nlist, it must be explicitly added to the move-to group before you move the objects.\nIf database or share objects are removed from a primary failover group (by using the REMOVE parameter or SET parameter to\nmodify the ALLOWED_DATABASES or ALLOWED_SHARES lists), those objects are dropped in any target account when the next\nrefresh operation occurs.\nTo avoid dropping these objects in the target account, you can drop the secondary failover group before the next time the modified\nprimary failover group is replicated to the target account.\nTo retrieve the list of accounts in your organization that are enabled for replication, use the\nSHOW REPLICATION ACCOUNTS command.\nTo retrieve the list of failover groups in your organization, use SHOW FAILOVER GROUPS.\nAutomatically scheduled refresh operations are executed using the role with the OWNERSHIP\nprivilege on the group. If a scheduled refresh operation fails due to insufficient privileges, grant the required privileges\nto the role with the OWNERSHIP privilege on the group.\nThe ALTER FAILOVER GROUP … SUSPEND IMMEDIATE command doesn’t cancel an in-progress refresh operation if it was manually triggered.\nFor information, see Cancel an in-progress refresh operation that wasn’t automatically scheduled.\nCanceling an in-progress refresh operation that is in the SECONDARY_DOWNLOADING_METADATA or SECONDARY_DOWNLOADING_DATA phase might\nresult in an inconsistent state on the target account. For more information see View the current phase of an in-progress refresh operation.\nIf you create a replication or failover group with a tag or modify a replication or failover group by setting a tag on it,\ntag inheritance does not apply to any objects that you specify in the replication or failover group.\nTag inheritance is only applicable to objects with a parent-child relationship, such\ndatabase, schema, and table. There are no child objects of replication or failover groups.\nYou cannot set a tag or modify a tag on a secondary replication or failover group because these objects are read\nonly.\nWhen you refresh a secondary replication or failover group, any tags that are set on the primary group are then set on\nthe secondary group.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-notebook",
    "title": "ALTER NOTEBOOK",
    "description": "Modifies the properties of an existing notebook.",
    "syntax": "ALTER NOTEBOOK [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER NOTEBOOK [ IF EXISTS ] <name> SET\n  [ COMMENT = '<string_literal>' ]\n  [ QUERY_WAREHOUSE = <warehouse_to_run_nb_and_sql_queries_in> ]\n  [ IDLE_AUTO_SHUTDOWN_TIME_SECONDS = <number_of_seconds> ]\n  [ SECRETS = ('<secret_variable_name>' = <secret_name>) [ , ... ] ]",
    "examples": [
        {
            "code": "ALTER NOTEBOOK my_notebook RENAME TO notebook_v2;"
        },
        {
            "code": "ALTER NOTEBOOK my_notebook UNSET QUERY_WAREHOUSE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the notebook to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Changes the name of the notebook to new_name . The new identifier must be unique for the schema. For more details about identifiers, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. You cannot use the RENAME TO parameter to move a private notebook to a different database. If you want to move your notebook to another\ndatabase to make the notebook available to others (for example, for shared use), create a new notebook in a different database,\nusing your private notebook as a template. See Making a private notebook available for general use . When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET   ...",
            "description": "Sets one or more specified properties or parameters for the notebook:"
        },
        {
            "name": "QUERY_WAREHOUSE   =   warehouse_name",
            "description": "Specifies the warehouse where SQL queries in the notebook are run.\nThis parameter is optional. However, it is required to run the EXECUTE NOTEBOOK command."
        },
        {
            "name": "IDLE_AUTO_SHUTDOWN_TIME_SECONDS   =   number_of_seconds",
            "description": "Number of seconds of idle time before the notebook is shut down automatically. This parameter is available for notebooks running\non both Warehouse and Container Runtime. The value must be an integer between 60 and 259200 (72 hours). Default: 3600 seconds"
        },
        {
            "name": "SECRETS   =   '( secret_variable_name '   =   secret_name   [   ,   ...   ])",
            "description": "Sets secret variables for the notebook. secret_variable_name - The variable that will be used in the notebook cell when retrieving information from the secret. secret_name - The name of the Snowflake secret."
        },
        {
            "name": "UNSET   ...",
            "description": "Unsets one or more specified properties or parameters for the notebook, which resets the properties to the defaults: QUERY_WAREHOUSE COMMENT To unset multiple properties or parameters with a single ALTER statement, separate each property or parameter with a comma. When unsetting a property or parameter, specify only the property or parameter name (unless the syntax above indicates that you\nshould specify the value). Specifying the value returns an error."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-projection-policy",
    "title": "ALTER PROJECTION POLICY",
    "description": "Replaces the existing projection policy rules with new rules or a new comment and allows the\nrenaming of a projection policy.",
    "syntax": "ALTER PROJECTION POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER PROJECTION POLICY [ IF EXISTS ] <name> SET BODY -> <expression>\n\nALTER PROJECTION POLICY <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROJECTION POLICY <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROJECTION POLICY [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER PROJECTION POLICY [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER PROJECTION POLICY mypolicy RENAME TO proj_policy_acctnumber;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the projection policy to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the projection policy; must be unique for your schema. The new identifier cannot be used if the\nidentifier is already in place for a different projection policy. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the projection policy: SQL expression that determines whether to project the column. The expression can contain CASE and other logic statements, but must call the PROJECTION_CONSTRAINT function: ALLOW => { TRUE | FALSE } - TRUE allows the column to be projected. FALSE prevents the column from being projected, with the behavior\nspecified by ENFORCEMENT. FALSE affects only columns that appear in the final results table. ENFORCEMENT => ' enforcement_style ' - If ALLOW=FALSE, specifies what should happen if a query includes a protected column.\nSupported values: FAIL - The query will fail if a protected column is included in the outermost query. NULLIFY - All rows in the protected column return the value NULL. Default: FAIL Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites the existing comment for the projection policy. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties and/or parameters to unset, by resetting them to their defaults, for the projection policy: TAG tag_name [ , tag_name ... ] COMMENT When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "BODY   ->   expression",
            "description": "SQL expression that determines whether to project the column. The expression can contain CASE and other logic statements, but must call the PROJECTION_CONSTRAINT function: ALLOW => { TRUE | FALSE } - TRUE allows the column to be projected. FALSE prevents the column from being projected, with the behavior\nspecified by ENFORCEMENT. FALSE affects only columns that appear in the final results table. ENFORCEMENT => ' enforcement_style ' - If ALLOW=FALSE, specifies what should happen if a query includes a protected column.\nSupported values: FAIL - The query will fail if a protected column is included in the outermost query. NULLIFY - All rows in the protected column return the value NULL. Default: FAIL"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the projection policy. Default: No value"
        }
    ],
    "usage_notes": "If you want to update an existing projection policy and need to see the current definition of the policy, run the\nDESCRIBE PROJECTION POLICY command or GET_DDL function.\nMoving a projection policy to a managed access schema\n(using the ALTER PROJECTION POLICY … RENAME TO syntax) is prohibited unless the projection policy owner\n(i.e. the role that has the OWNERSHIP privilege on the projection policy) also owns the target schema.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-service",
    "title": "ALTER SERVICE",
    "description": "Modifies Snowpark Container Services service\nconfiguration, upgrades the code for the service, and allows you to suspend or resume a service. You can:",
    "syntax": "ALTER SERVICE [ IF EXISTS ] <name> { SUSPEND | RESUME }\n\nALTER SERVICE [ IF EXISTS ] <name>\n  {\n     fromSpecification\n     | fromSpecificationTemplate\n  }\n\nALTER SERVICE [IF EXISTS] <service_name> RESTORE VOLUME <volume_name>\n                                                 INSTANCES <comma_separated_instance_ids>\n                                                 FROM SNAPSHOT <snapshot_name>\n\nALTER SERVICE [ IF EXISTS ] <name> SET [ MIN_INSTANCES = <num> ]\n                                       [ MAX_INSTANCES = <num> ]\n                                       [ LOG_LEVEL = '<log_level>' ]\n                                       [ AUTO_SUSPEND_SECS = <num> ]\n                                       [ MIN_READY_INSTANCES = <num> ]\n                                       [ QUERY_WAREHOUSE = <warehouse_name> ]\n                                       [ AUTO_RESUME = { TRUE | FALSE } ]\n                                       [ EXTERNAL_ACCESS_INTEGRATIONS = ( <EAI_name> [ , ... ] ) ]\n                                       [ COMMENT = '<string_literal>' ]\n\n\n\nALTER SERVICE [ IF EXISTS ] <name> UNSET { MIN_INSTANCES                |\n                                           AUTO_SUSPEND_SECS            |\n                                           MAX_INSTANCES                |\n                                           LOG_LEVEL                    |\n                                           MIN_READY_INSTANCES          |\n                                           QUERY_WAREHOUSE              |\n                                           AUTO_RESUME                  |\n                                           EXTERNAL_ACCESS_INTEGRATIONS |\n                                           COMMENT\n                                         }\n                                         [ , ... ]\n\nALTER SERVICE [ IF EXISTS ] <name> SET [ TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]]\n\nfromSpecification ::=\n  {\n    FROM SPECIFICATION_FILE = '<yaml_file_path>' -- for native app service.\n    | FROM @<stage> SPECIFICATION_FILE = '<yaml_file_path>' -- for non-native app service.\n    | FROM SPECIFICATION <specification_text>\n  }\n\nfromSpecificationTemplate ::=\n  {\n    FROM SPECIFICATION_TEMPLATE_FILE = '<yaml_file_path>' -- for native app service.\n    | FROM @<stage> SPECIFICATION_TEMPLATE_FILE = '<yaml_file_path>' -- for non-native app service.\n    | FROM SPECIFICATION_TEMPLATE <specification_text>\n  }\n  USING ( <key> => <value> [ , <key> => <value> [ , ... ] ]  )",
    "examples": [
        {
            "code": "ALTER SERVICE echo_service SUSPEND;"
        },
        {
            "code": "ALTER SERVICE echo_service SET MIN_INSTANCES=3 MAX_INSTANCES=5;"
        },
        {
            "code": "ALTER SERVICE example_service\n  RESTORE VOLUME \"myvolume\"\n  INSTANCES 0,2\n  FROM SNAPSHOT my_snapshot;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the service to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "{   SUSPEND   |   RESUME   }",
            "description": "Specifies whether to suspend or resume the service. When you suspend a service, Snowflake shuts down and deletes the containers. If you later resume a suspended service,Snowflake\nrecreates the containers. That is, Snowflake takes the image from your repository and starts the containers. Note that, Snowflake deploys the same image version; it is not a service update operation. When you invoke a suspended service using either a service function or invoking the public endpoint (ingress), Snowflake\nautomatically resumes the service."
        },
        {
            "name": "FROM   ...",
            "description": "Identifies the specification or\nthe template specification for the service. Using a service specification You can either define the specification either inline or in a separate file . Specifies the file containing the service specification or the service specification inline. If your service specification is in a file, use SPECIFICATION_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file. Using a service specification template You can either define the template specification either inline or in a separate file . Specifies the file containing the service specification template or the service specification template inline. If your service specification template is in a file, use SPECIFICATION_TEMPLATE_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file. When using template specification, you should also include the USING parameter. Specifies the template variables and the values of those variables. key is the name of the template variable. The template variable name can optionally be enclosed in double quotes\n( \" ). value is the value to assign to the variable in the template. String values must be enclosed in ' or $$ . The value must either be alphanumeric or valid JSON. Use a comma between each key-value pair."
        },
        {
            "name": "RESTORE   VOLUME   volume_name   INSTANCES   comma_separated_instance_ids   FROM   SNAPSHOT   snapshot_name",
            "description": "Restores the snapshot snapshot_name on the existing block storage volume volume_name for the instances comma_separated_instance_ids . Snapshots can only be taken for block storage volumes (and not for local, memory, or stage volumes). Volume names are case-sensitive. Therefore, double quotes should always be used to match the corresponding name in the service specification."
        },
        {
            "name": "SET   ...",
            "description": "Sets one or more specified properties or parameters for the service: Specifies the minimum number of service instances. Specifies the maximum number of service instances. Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested.\nCurrently, LOG_LEVEL is supported only for platform events , Changing LOG_LEVEL for container logs is not supported. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing . Specifies the number of seconds of inactivity (service is idle) after which Snowflake automatically suspends the service. When AUTO_SUSPEND_SECS is 0 (default), Snowflake does not auto-suspend the service. You can configure this value to 300 seconds or more to enable auto-suspension. For more information, see Suspending a service . Preview Feature — Open Configuring the automatic suspension of a Snowpark Container Services service using the AUTO_SUSPEND_SECS property is a preview feature . Specifies the minimum service instances that must be ready for Snowflake to consider the service ready to process requests.  For more information, see Scaling services . Warehouse to use if a service container connects to Snowflake to execute a query but does not explicitly specify a warehouse to use. Specifies whether to automatically resume a service when user performs one of the following actions that depend on the service: Executing a query is that uses a service function . Sending a request to the public endpoint exposed by the service ( ingress ). If AUTO_RESUME is FALSE, you need to explicitly resume the service (using ALTER SERVICE … RESUME). Default: TRUE. Specifies the names of the external access integrations that allow your service to access external sites.\nSnowflake replaces all the existing EAIs with those specified in this parameter.\nThe names in this list are case-sensitive. For more information, see Configuring network egress .\nNote that this changes the allowed network access for all running instances of the service. You don’t need to explicitly suspend and resume the service. Specifies a comment for the compute pool. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties and/or parameters to unset for the service, which resets them to the defaults (see CREATE SERVICE ): MIN_INSTANCES MAX_INSTANCES AUTO_SUSPEND_SECS Preview Feature — Open Configuring the automatic suspension of a Snowpark Container Services service using the AUTO_SUSPEND_SECS property is a preview feature . MIN_READY_INSTANCES QUERY_WAREHOUSE AUTO_RESUME EXTERNAL_ACCESS_INTEGRATIONS COMMENT"
        },
        {
            "name": "SPECIFICATION_FILE   =   ' yaml_file_path '   or   .   @ stage   SPECIFICATION_FILE   =   ' yaml_file_path '   or   .   SPECIFICATION   specification_text",
            "description": "Specifies the file containing the service specification or the service specification inline. If your service specification is in a file, use SPECIFICATION_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file."
        },
        {
            "name": "SPECIFICATION_TEMPLATE_FILE   =   ' yaml_file_path '   or   .   @ stage   SPECIFICATION_TEMPLATE_FILE   =   ' yaml_file_path '   or   .   SPECIFICATION_TEMPLATE   specification_text",
            "description": "Specifies the file containing the service specification template or the service specification template inline. If your service specification template is in a file, use SPECIFICATION_TEMPLATE_FILE. For services created in a Snowflake Native App, omit @ stage , and specify a path relative to the app root directory. For services created in other contexts, specify the Snowflake internal stage and path to the service specification file. When using template specification, you should also include the USING parameter."
        },
        {
            "name": "USING   (   key   =>   value   [   ,   key   =>   value   [   ,   ...   ]   ]    )",
            "description": "Specifies the template variables and the values of those variables. key is the name of the template variable. The template variable name can optionally be enclosed in double quotes\n( \" ). value is the value to assign to the variable in the template. String values must be enclosed in ' or $$ . The value must either be alphanumeric or valid JSON. Use a comma between each key-value pair."
        },
        {
            "name": "MIN_INSTANCES   =   num",
            "description": "Specifies the minimum number of service instances."
        },
        {
            "name": "MAX_INSTANCES   =   num",
            "description": "Specifies the maximum number of service instances."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested.\nCurrently, LOG_LEVEL is supported only for platform events , Changing LOG_LEVEL for container logs is not supported. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "AUTO_SUSPEND_SECS   =   num",
            "description": "Specifies the number of seconds of inactivity (service is idle) after which Snowflake automatically suspends the service. When AUTO_SUSPEND_SECS is 0 (default), Snowflake does not auto-suspend the service. You can configure this value to 300 seconds or more to enable auto-suspension. For more information, see Suspending a service . Preview Feature — Open Configuring the automatic suspension of a Snowpark Container Services service using the AUTO_SUSPEND_SECS property is a preview feature ."
        },
        {
            "name": "MIN_READY_INSTANCES   =   num",
            "description": "Specifies the minimum service instances that must be ready for Snowflake to consider the service ready to process requests.  For more information, see Scaling services ."
        },
        {
            "name": "QUERY_WAREHOUSE   =   warehouse_name",
            "description": "Warehouse to use if a service container connects to Snowflake to execute a query but does not explicitly specify a warehouse to use."
        },
        {
            "name": "AUTO_RESUME   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically resume a service when user performs one of the following actions that depend on the service: Executing a query is that uses a service function . Sending a request to the public endpoint exposed by the service ( ingress ). If AUTO_RESUME is FALSE, you need to explicitly resume the service (using ALTER SERVICE … RESUME). Default: TRUE."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   EAI_name   [   ,   ...   ]   )",
            "description": "Specifies the names of the external access integrations that allow your service to access external sites.\nSnowflake replaces all the existing EAIs with those specified in this parameter.\nThe names in this list are case-sensitive. For more information, see Configuring network egress .\nNote that this changes the allowed network access for all running instances of the service. You don’t need to explicitly suspend and resume the service."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the compute pool."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-cortex-search",
    "title": "ALTER CORTEX SEARCH SERVICE",
    "description": "Suspends, resumes, or modifies the properties of an existing Cortex Search service.",
    "syntax": "ALTER CORTEX SEARCH SERVICE [ IF EXISTS ] <name>\n  { SUSPEND | RESUME } [ { INDEXING | SERVING } ]\n\nALTER CORTEX SEARCH SERVICE [ IF EXISTS ] <name> REFRESH\n\nALTER CORTEX SEARCH SERVICE [ IF EXISTS ] <name> SET\n  [ TARGET_LAG = { '<num> { seconds | minutes | hours | days }' } ]\n  [ WAREHOUSE = <warehouse_name> ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER CORTEX SEARCH SERVICE [ IF EXISTS ] <name> UNSET\n  [ PRIMARY KEY ]",
    "examples": [
        {
            "code": "ALTER CORTEX SEARCH SERVICE mysvc SET WAREHOUSE = my_new_wh;"
        },
        {
            "code": "ALTER CORTEX SEARCH SERVICE mysvc SET COMMENT = 'new_comment';"
        },
        {
            "code": "ALTER CORTEX SEARCH SERVICE mysvc SET TARGET_LAG = '1 hour';"
        },
        {
            "code": "ALTER CORTEX SEARCH SERVICE mysvc SET PRIMARY KEY = (region, agent_id);"
        },
        {
            "code": "ALTER CORTEX SEARCH SERVICE mysvc UNSET PRIMARY KEY;"
        },
        {
            "code": "ALTER CORTEX SEARCH SERVICE mysvc SUSPEND SERVING;"
        },
        {
            "code": "ALTER CORTEX SEARCH SERVICE mysvc REFRESH;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the Cortex Search service to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "{   SUSPEND   |   RESUME   }   ...",
            "description": "Suspends or resumes the indexing, serving or both for a Cortex Search service. You can specify one of the following keywords to indicate\nwhich layer to suspend or resume: The target that indicates the indexing layer of the Cortex Search Service. For more details, see Usage Notes . The target that indicates the serving layer of the Cortex Search Service. For more details, see Usage Notes . If you do not specify either keyword, both the indexing and serving layers are suspended or resumed. The OPERATE privilege is required to suspend or resume a Cortex Search service."
        },
        {
            "name": "REFRESH",
            "description": "Triggers a manual refresh of the Cortex Search Service. The indexing service immediately checks for changes to the source data and processes\nany new or changed rows."
        },
        {
            "name": "SET   ...",
            "description": "Sets one or more specified properties or parameters to set for the Cortex Search service: Specifies the maximum amount of time that the Cortex Search service content should lag behind updates to the base tables specified in the source query. Specifies the warehouse to use for running the source query, building the search index, and keeping it refreshed per the TARGET_LAG target. Adds a comment or overwrites an existing comment for the search service. Modifies the set of primary key columns for the Cortex Search service."
        },
        {
            "name": "UNSET   ...",
            "description": "Unsets one or more specified properties or parameters for the Cortex Search service. Removes any primary key columns that were previously set for the Cortex Search service."
        },
        {
            "name": "INDEXING",
            "description": "The target that indicates the indexing layer of the Cortex Search Service. For more details, see Usage Notes ."
        },
        {
            "name": "SERVING",
            "description": "The target that indicates the serving layer of the Cortex Search Service. For more details, see Usage Notes . If you do not specify either keyword, both the indexing and serving layers are suspended or resumed. The OPERATE privilege is required to suspend or resume a Cortex Search service."
        },
        {
            "name": "TARGET_LAG   =   ' num   {   seconds   |   minutes   |   hours   |   days   }'",
            "description": "Specifies the maximum amount of time that the Cortex Search service content should lag behind updates to the base tables specified in the source query."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Specifies the warehouse to use for running the source query, building the search index, and keeping it refreshed per the TARGET_LAG target."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the search service."
        },
        {
            "name": "PRIMARY   KEY   =   ( column_name ,   column_name ,   ...)",
            "description": "Modifies the set of primary key columns for the Cortex Search service."
        },
        {
            "name": "PRIMARY   KEY",
            "description": "Removes any primary key columns that were previously set for the Cortex Search service."
        }
    ],
    "usage_notes": "Attention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nINDEXING is the target that indicates the indexing layer of the Cortex Search Service. When in the RUNNING state, changes in base tables\nreferenced by the service’s source query will prompt refreshes of the materialized data stored as part of the search index. These\nrefreshes incur cost in the form of warehouse compute and vector embeddings. When in the SUSPENDED state, changes in base tables will\nnot trigger refreshes, nor will they be reflected in the queryable data of the Cortex Search Service.\nSERVING is the target that indicates the serving layer of the Cortex Search Service. This target must be in the RUNNING state for the\nservice to be queryable. When in the suspended state, the Cortex Search Service will not incur billing\nin the form of Cortex Search’s serving costs.\nFor detailed cost considerations, see Cost considerations.\nThe INDEXING and SERVING layers of the Cortex Search Service can be managed independently. For instance, if SERVING is in the running\nstate while INDEXING is suspended, you can still query the service. However, the service will not reflect any changes in the base data\nregardless of the TARGET_LAG until INDEXING is resumed and a refresh is completed successfully.\nConversely, if INDEXING is running while SERVING is suspended, the index will continue to refresh. When SERVING is resumed, the loaded\nindex that becomes queryable will reflect the most up-to-date source data.\nWhen neither the SERVING nor INDEXING keywords are specified, both targets will be impacted by the specified action.\nWhen you manually refresh a Cortex Search Service, the service immediately checks for changes in its source data and updates the\nindex as needed. Trigger a manual refresh of your Cortex Search Service when you need the most up-to-date results possible — for\nexample, when you have just added or updated important documents and want these to be available immediately to users.  You can\nalso use a manual refresh to ensure that results are always current at specific times, such as at the start of business.\nYou can trigger a manual refresh of a Cortex Search Service using the ALTER CORTEX SEARCH SERVICE … REFRESH command or in Snowsight.\nAltering the primary key columns of a Cortex Search Service affects only future refreshes.\nThat is, changes to primary keys go into effect after the next change to the source data."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-application-package-version",
    "title": "ALTER APPLICATION PACKAGE … VERSION",
    "description": "Modifies the versioning of an existing application package in the Snowflake Native App Framework.",
    "syntax": "ALTER APPLICATION PACKAGE <name> ADD VERSION [ <version_identifier> ]\n  USING <path_to_version_directory> [ LABEL = '<display_label>' ]\n\nALTER APPLICATION PACKAGE <name> DROP VERSION <version_identifier>\n\nALTER APPLICATION PACKAGE <name> ADD PATCH [<patch_number>] FOR VERSION [<version_identifier>]\n  USING <path_to_version_directory> [ LABEL = '<display_label>' ]",
    "examples": [
        {
            "code": "ALTER APPLICATION PACKAGE hello_snowflake_package\n  ADD VERSION v1_1\n  USING '@hello_snowflake_code.core.hello_snowflake_stage';"
        },
        {
            "code": "+---------------------------------------------------------------------------------------+---------+-------+\n| status                                                                                | version | patch |\n|---------------------------------------------------------------------------------------+---------+-------|\n| Version 'v1_1' of application package 'hello_snowflake_package' created successfully. | v1_1    |     0 |\n+---------------------------------------------------------------------------------------+---------+-------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the application package to alter. If the identifier contains\nspaces, special characters, or mixed-case characters, the entire string must be enclosed\nin double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "ADD   VERSION   [   version_identifier   ]   USING   path_to_version_directory",
            "description": "Adds a version or patch using the application files located in the path to a stage location specified by path_to_version_directory . You can specify an identifier for this version using version_identifier . If you do\nnot specify a version_identifier in the manifest file, you must specify a version_identifier as part of this command. If you specify version_identifier as part of this command, it takes precedence over version_identifier specified\nin the manifest file."
        },
        {
            "name": "[   LABEL   =   ' display_label '   ]",
            "description": "You can use the LABEL clause to specify a label for this new version. This label is displayed\nto the consumer. If you omit the LABEL clause, the label specified in the manifest.yml file is used."
        },
        {
            "name": "DROP   VERSION   version_identifier",
            "description": "Drops the version with the specified version name. Drops a version with the specified version identifier. A version may only be dropped when\nthere are no release directives that are referring to it. Dropping is an asynchronous\nprocess and completes when all application instances have successfully upgraded from the\nolder version and no longer have code running on the dropping version. Use the APPLICATION_STATE view view to monitor\nthe state of the application instances. Use the SHOW VERSIONS IN APPLICATION PACKAGE command to monitor the\nstatus of the dropped version."
        },
        {
            "name": "ADD   PATCH   patch_number   FOR   VERSION   version_identifier   .   USING   path_to_version_directory   [   LABEL   =   ' display_label '   ]",
            "description": "Adds a patch for the specified version ( version_identifier ) using the application files located in the specified path to a\nstage location ( path_to_version_directory ). You can use the LABEL clause to specify a label for this new patch. This label is displayed to the consumer. If you omit the LABEL\nclause, the label specified in the manifest.yml file is used."
        }
    ],
    "usage_notes": "Version identifiers have a maximum limit of 30 characters.\nA single version can have up to 130 patches.\nModifying the version requires a role with the OWNERSHIP privilege on the application or the global MANAGE VERSIONS privilege.\nIf you do not specify the values for the optional properties, the command uses the values specified in the\napplication manifest file.\nIf you specify values for the properties in the command and in the application manifest file, the values\nspecified in the command take precedence.\nIf two versions are active, for example, if the current version has not finished rolling out, adding\na new version results in an error."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-view",
    "title": "ALTER VIEW",
    "description": "Modifies the properties for an existing view. Currently the only supported operations are:",
    "syntax": "ALTER VIEW [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER VIEW [ IF EXISTS ] <name> SET\n  [ SECURE ]\n  [ CHANGE_TRACKING =  { TRUE | FALSE } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER VIEW [ IF EXISTS ] <name> UNSET\n  [ SECURE ]\n  [ CONTACT <purpose> ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER VIEW <name> dataMetricFunctionAction\n\nALTER VIEW [ IF EXISTS ] <name> dataGovnPolicyTagAction\n\ndataMetricFunctionAction ::=\n\n    SET DATA_METRIC_SCHEDULE = {\n        '<num> MINUTE'\n      | 'USING CRON <expr> <time_zone>'\n      | 'TRIGGER_ON_CHANGES'\n    }\n\n  | UNSET DATA_METRIC_SCHEDULE\n\n  | { ADD | DROP } DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] )\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) ]\n\n  | MODIFY DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME }\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME } ]\n\ndataGovnPolicyTagAction ::=\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n  |\n  {\n      SET AGGREGATION POLICY <policy_name>\n        [ ENTITY KEY ( <col_name> [, ... ] ) ]\n        [ FORCE ]\n    | UNSET AGGREGATION POLICY\n  }\n  |\n  {\n      SET JOIN POLICY <policy_name>\n        [ FORCE ]\n    | UNSET JOIN POLICY\n  }\n  |\n  ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n    [ [ WITH ] MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] ]\n    [ [ WITH ] PROJECTION POLICY <policy_name> ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>'\n          [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] [ FORCE ]\n      | UNSET MASKING POLICY\n  }\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET PROJECTION POLICY <policy_name>\n          [ FORCE ]\n      | UNSET PROJECTION POLICY\n  }\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> SET TAG\n      <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n      , [ COLUMN ] <col2_name> SET TAG\n          <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n                   , [ COLUMN ] <col2_name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER VIEW view1 RENAME TO view2;"
        },
        {
            "code": "ALTER VIEW view1 SET SECURE;"
        },
        {
            "code": "ALTER VIEW view1 UNSET SECURE;"
        },
        {
            "code": "-- single column\n\nALTER VIEW user_info_v MODIFY COLUMN ssn_number SET MASKING POLICY ssn_mask_v;\n\n-- multiple columns\n\nALTER VIEW user_info_v MODIFY\n  COLUMN ssn_number SET MASKING POLICY ssn_mask_v,\n  COLUMN dob SET MASKING POLICY dob_mask_v\n  ;"
        },
        {
            "code": "-- single column\n\nALTER VIEW user_info_v MODIFY COLUMN ssn_number UNSET MASKING POLICY;\n\n-- multiple columns\n\nALTER VIEW user_info_v MODIFY\n  COLUMN ssn_number UNSET MASKING POLICY,\n  COLUMN dob UNSET MASKING POLICY\n  ;"
        },
        {
            "code": "ALTER VIEW v1\n  ADD ROW ACCESS POLICY rap_v1 ON (empl_id);"
        },
        {
            "code": "ALTER VIEW v1\n  DROP ROW ACCESS POLICY rap_v1;"
        },
        {
            "code": "ALTER VIEW v1\n  DROP ROW ACCESS POLICY rap_v1_version_1,\n  ADD ROW ACCESS POLICY rap_v1_version_2 ON (empl_id);"
        },
        {
            "code": "ALTER VIEW join_view\n  SET JOIN POLICY jp1;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the view to alter. If the identifier contains spaces or special characters, the entire string must be enclosed\nin double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the view; must be unique for the schema. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the property to set for the view: Specifies a view as secure. Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the view, and cascades the setting to all underlying tables. FALSE disables change tracking on the view, and cascades the setting to all underlying tables. Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts . Adds a comment or overwrites an existing comment for the view. Note You must set each view property individually."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the property to unset for the view, which resets it to the default: SECURE CONTACT purpose COMMENT When resetting a property, specify only the name; specifying a value for the property will return an error. Note You must reset each view property individually."
        },
        {
            "name": "SECURE",
            "description": "Specifies a view as secure."
        },
        {
            "name": "CHANGE_TRACKING   =     TRUE   |   FALSE",
            "description": "Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the view, and cascades the setting to all underlying tables. FALSE disables change tracking on the view, and cascades the setting to all underlying tables."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the view."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-privacy-policy",
    "title": "ALTER PRIVACY POLICY",
    "description": "Modifies the properties of an existing privacy policy.",
    "syntax": "ALTER PRIVACY POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER PRIVACY POLICY [ IF EXISTS ] <name> SET BODY -> <expression>\n\nALTER PRIVACY POLICY <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PRIVACY POLICY <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PRIVACY POLICY [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER PRIVACY POLICY [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "-- Modify the body of privacy policy \"my_priv_policy\" so it always returns a\n-- budget named \"analysts\"\nALTER PRIVACY POLICY my_priv_policy SET BODY ->\n  PRIVACY_BUDGET(BUDGET_NAME => 'analysts');\n\n-- Set budget limit to 50 and max budget per aggregate to 0.1\n-- budget window is not mentioned so it is reset to its default value\nALTER PRIVACY POLICY users_policy SET BODY ->\n  privacy_budget(budget_name=>'analysts', budget_limit=>50, max_budget_per_aggregate=>0.1);"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the privacy policy to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the privacy policy; must be unique for your schema. The new identifier cannot be used if the\nidentifier is already in place for a different privacy policy. For more information, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the privacy policy: Specifies a new body for the policy. The SQL expression of the body calls two functions to control the return value of the policy:\nNO_PRIVACY_POLICY and PRIVACY_BUDGET. When a query is executed against a table that has been assigned the\npolicy, Snowflake evaluates the conditions of the body to call the appropriate function and return a value. This return value determines\nwhich privacy budget, if any, is associated with the query against the privacy-protected table. The expression can use context functions such as CURRENT_ROLE or INVOKER_ROLE to associate a user or group of users with a privacy budget. If you use a CASE block in the body’s expression, it must include an ELSE statement that\ncalls either NO_PRIVACY_POLICY or PRIVACY_BUDGET. Every user must either be associated with a privacy budget or have unrestricted access to\nthe privacy-protected table. If a user should not have any access to a privacy-protected table or view, revoke SELECT privileges rather than\ntrying to define this in the privacy policy. Use the body’s expression to call the NO_PRIVACY_POLICY function when you want a query to have unrestricted access to the table or view to which the privacy policy is assigned. Use the body’s expression to call the PRIVACY_BUDGET function when you want to return a privacy budget from the policy. The\nexpression can contain conditions that allow the policy to return different privacy budgets for different queries based on factors like\nthe user who is executing the query. In cross-account collaboration, privacy budgets are automatically namespaced by the account identifier of the consumer account, which\nprevents two different consumer accounts from sharing the same privacy budget even if the name of the privacy budget is the same. Using\nthe CURRENT_ACCOUNT function to concatenate the name of the account with the name of the privacy budget\ncan help distinguish between privacy budgets. For example, you could call the function as follows: PRIVACY_BUDGET(BUDGET_NAME => 'external_budget.' || CURRENT_ACCOUNT()) . The signature of the PRIVACY_BUDGET function is: Privacy budget arguments: Resolves to the name of a privacy budget. Snowflake creates the privacy budget automatically when its name is\nspecified in the body of the privacy policy. A decimal number > 0 that specifies the budget limit for this privacy policy.\nThis controls the total amount of privacy loss allowed. Adjusting this value\nchanges how many total differentially private aggregates can be calculated\nagainst tables protected by this privacy budget during the refresh period. When a query is run that would\ncause the cumulative privacy loss to exceed this number, the query will fail.\nAs a rough estimate, a budget\nlimit of 233 with MAX_BUDGET_PER_AGGREGATE=1 permits about 1000 aggregates\nper refresh period. Default: 233.0 Specifies how much privacy budget is used for each aggregate function in a\nquery. Adjusting this value changes the amount of noise added to each aggregate\nquery, as well as the number of aggregates that can be calculated before the budget limit is reached. As an example, the query select count(*), avg(a) ... has two aggregates: count(*) and avg(a) . Specify a decimal value > 0. Default: 0.5 How often the privacy budget is refreshed, that is, has its cumulative privacy loss reset to 0. Valid values: Daily : Refreshed every day at 12:00 AM UTC Weekly : Refreshed every Sunday at 12:00 AM UTC Monthly : Refreshed on the first day of the calendar month at 12:00 AM UTC Yearly : Refreshed on January 1 at 12:00 AM UTC Never : Privacy budget is never refreshed. Default: Weekly Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites the existing comment for the privacy policy. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties and/or parameters to unset, by resetting them to their defaults, for the privacy policy: TAG tag_name [ , tag_name ... ] COMMENT When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "BODY   ->   expression",
            "description": "Specifies a new body for the policy. The SQL expression of the body calls two functions to control the return value of the policy:\nNO_PRIVACY_POLICY and PRIVACY_BUDGET. When a query is executed against a table that has been assigned the\npolicy, Snowflake evaluates the conditions of the body to call the appropriate function and return a value. This return value determines\nwhich privacy budget, if any, is associated with the query against the privacy-protected table. The expression can use context functions such as CURRENT_ROLE or INVOKER_ROLE to associate a user or group of users with a privacy budget. If you use a CASE block in the body’s expression, it must include an ELSE statement that\ncalls either NO_PRIVACY_POLICY or PRIVACY_BUDGET. Every user must either be associated with a privacy budget or have unrestricted access to\nthe privacy-protected table. If a user should not have any access to a privacy-protected table or view, revoke SELECT privileges rather than\ntrying to define this in the privacy policy. Use the body’s expression to call the NO_PRIVACY_POLICY function when you want a query to have unrestricted access to the table or view to which the privacy policy is assigned. Use the body’s expression to call the PRIVACY_BUDGET function when you want to return a privacy budget from the policy. The\nexpression can contain conditions that allow the policy to return different privacy budgets for different queries based on factors like\nthe user who is executing the query. In cross-account collaboration, privacy budgets are automatically namespaced by the account identifier of the consumer account, which\nprevents two different consumer accounts from sharing the same privacy budget even if the name of the privacy budget is the same. Using\nthe CURRENT_ACCOUNT function to concatenate the name of the account with the name of the privacy budget\ncan help distinguish between privacy budgets. For example, you could call the function as follows: PRIVACY_BUDGET(BUDGET_NAME => 'external_budget.' || CURRENT_ACCOUNT()) . The signature of the PRIVACY_BUDGET function is: Privacy budget arguments: Resolves to the name of a privacy budget. Snowflake creates the privacy budget automatically when its name is\nspecified in the body of the privacy policy. A decimal number > 0 that specifies the budget limit for this privacy policy.\nThis controls the total amount of privacy loss allowed. Adjusting this value\nchanges how many total differentially private aggregates can be calculated\nagainst tables protected by this privacy budget during the refresh period. When a query is run that would\ncause the cumulative privacy loss to exceed this number, the query will fail.\nAs a rough estimate, a budget\nlimit of 233 with MAX_BUDGET_PER_AGGREGATE=1 permits about 1000 aggregates\nper refresh period. Default: 233.0 Specifies how much privacy budget is used for each aggregate function in a\nquery. Adjusting this value changes the amount of noise added to each aggregate\nquery, as well as the number of aggregates that can be calculated before the budget limit is reached. As an example, the query select count(*), avg(a) ... has two aggregates: count(*) and avg(a) . Specify a decimal value > 0. Default: 0.5 How often the privacy budget is refreshed, that is, has its cumulative privacy loss reset to 0. Valid values: Daily : Refreshed every day at 12:00 AM UTC Weekly : Refreshed every Sunday at 12:00 AM UTC Monthly : Refreshed on the first day of the calendar month at 12:00 AM UTC Yearly : Refreshed on January 1 at 12:00 AM UTC Never : Privacy budget is never refreshed. Default: Weekly"
        },
        {
            "name": "NO_PRIVACY_POLICY",
            "description": "Use the body’s expression to call the NO_PRIVACY_POLICY function when you want a query to have unrestricted access to the table or view to which the privacy policy is assigned."
        },
        {
            "name": "PRIVACY_BUDGET",
            "description": "Use the body’s expression to call the PRIVACY_BUDGET function when you want to return a privacy budget from the policy. The\nexpression can contain conditions that allow the policy to return different privacy budgets for different queries based on factors like\nthe user who is executing the query. In cross-account collaboration, privacy budgets are automatically namespaced by the account identifier of the consumer account, which\nprevents two different consumer accounts from sharing the same privacy budget even if the name of the privacy budget is the same. Using\nthe CURRENT_ACCOUNT function to concatenate the name of the account with the name of the privacy budget\ncan help distinguish between privacy budgets. For example, you could call the function as follows: PRIVACY_BUDGET(BUDGET_NAME => 'external_budget.' || CURRENT_ACCOUNT()) . The signature of the PRIVACY_BUDGET function is: Privacy budget arguments: Resolves to the name of a privacy budget. Snowflake creates the privacy budget automatically when its name is\nspecified in the body of the privacy policy. A decimal number > 0 that specifies the budget limit for this privacy policy.\nThis controls the total amount of privacy loss allowed. Adjusting this value\nchanges how many total differentially private aggregates can be calculated\nagainst tables protected by this privacy budget during the refresh period. When a query is run that would\ncause the cumulative privacy loss to exceed this number, the query will fail.\nAs a rough estimate, a budget\nlimit of 233 with MAX_BUDGET_PER_AGGREGATE=1 permits about 1000 aggregates\nper refresh period. Default: 233.0 Specifies how much privacy budget is used for each aggregate function in a\nquery. Adjusting this value changes the amount of noise added to each aggregate\nquery, as well as the number of aggregates that can be calculated before the budget limit is reached. As an example, the query select count(*), avg(a) ... has two aggregates: count(*) and avg(a) . Specify a decimal value > 0. Default: 0.5 How often the privacy budget is refreshed, that is, has its cumulative privacy loss reset to 0. Valid values: Daily : Refreshed every day at 12:00 AM UTC Weekly : Refreshed every Sunday at 12:00 AM UTC Monthly : Refreshed on the first day of the calendar month at 12:00 AM UTC Yearly : Refreshed on January 1 at 12:00 AM UTC Never : Privacy budget is never refreshed. Default: Weekly"
        },
        {
            "name": "BUDGET_NAME   =>   expression",
            "description": "Resolves to the name of a privacy budget. Snowflake creates the privacy budget automatically when its name is\nspecified in the body of the privacy policy."
        },
        {
            "name": "BUDGET_LIMIT   =>   decimal",
            "description": "A decimal number > 0 that specifies the budget limit for this privacy policy.\nThis controls the total amount of privacy loss allowed. Adjusting this value\nchanges how many total differentially private aggregates can be calculated\nagainst tables protected by this privacy budget during the refresh period. When a query is run that would\ncause the cumulative privacy loss to exceed this number, the query will fail.\nAs a rough estimate, a budget\nlimit of 233 with MAX_BUDGET_PER_AGGREGATE=1 permits about 1000 aggregates\nper refresh period. Default: 233.0"
        },
        {
            "name": "MAX_BUDGET_PER_AGGREGATE   =>   decimal",
            "description": "Specifies how much privacy budget is used for each aggregate function in a\nquery. Adjusting this value changes the amount of noise added to each aggregate\nquery, as well as the number of aggregates that can be calculated before the budget limit is reached. As an example, the query select count(*), avg(a) ... has two aggregates: count(*) and avg(a) . Specify a decimal value > 0. Default: 0.5"
        },
        {
            "name": "BUDGET_WINDOW   =>   string",
            "description": "How often the privacy budget is refreshed, that is, has its cumulative privacy loss reset to 0. Valid values: Daily : Refreshed every day at 12:00 AM UTC Weekly : Refreshed every Sunday at 12:00 AM UTC Monthly : Refreshed on the first day of the calendar month at 12:00 AM UTC Yearly : Refreshed on January 1 at 12:00 AM UTC Never : Privacy budget is never refreshed. Default: Weekly"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the privacy policy. Default: No value"
        }
    ],
    "usage_notes": "If you want to update an existing privacy policy and need to see the current definition of the policy, run the\nDESCRIBE PRIVACY POLICY command. You can also use the GET_DDL function to obtain the full definition\nof the privacy policy, including its body.\nMoving a privacy policy to a managed access schema\n(using the ALTER PRIVACY POLICY … RENAME TO syntax) is prohibited unless the privacy policy owner\n(that is, the role that has the OWNERSHIP privilege on the privacy policy) also owns the target schema.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-streamlit",
    "title": "ALTER STREAMLIT",
    "description": "Modifies the properties of an existing Streamlit object.",
    "syntax": "ALTER STREAMLIT [ IF EXISTS ] <name> SET\n  [ ROOT_LOCATION = '<stage_path_and_root_directory>' ]\n  [ MAIN_FILE = '<path_to_main_file>']\n  [ QUERY_WAREHOUSE = <warehouse_name> ]\n  [ COMMENT = '<string_literal>']\n  [ TITLE = '<app_title>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , ... ] ) ]\n\nALTER STREAMLIT [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER STREAMLIT <name> COMMIT\n\nALTER STREAMLIT <name> PUSH [ TO <git_branch_uri> ]\n  [ { GIT_CREDENTIALS = <snowflake_secret> | USERNAME = <git_username> PASSWORD = <git_password> } NAME = <git_author_name> EMAIL = <git_author_email> ]\n  [ COMMENT = <git_push_comment> ]\n\nALTER STREAMLIT <name> ABORT\n\nALTER STREAMLIT <name> PULL",
    "examples": [
        {
            "code": "ALTER STREAMLIT [ IF EXISTS ] <name> SET\n  [ ROOT_LOCATION = '<stage_path_and_root_directory>' ]\n  [ MAIN_FILE = '<path_to_main_file>']\n  [ QUERY_WAREHOUSE = <warehouse_name> ]\n  [ COMMENT = '<string_literal>']\n  [ TITLE = '<app_title>' ]\n  [ IMPORTS = ( '<stage_path_and_file_name_to_read>' [ , ... ] ) ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , ... ] ) ]\n\nALTER STREAMLIT [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER STREAMLIT <name> COMMIT\n\nALTER STREAMLIT <name> PUSH [ TO <git_branch_uri> ]\n  [ { GIT_CREDENTIALS = <snowflake_secret> | USERNAME = <git_username> PASSWORD = <git_password> } NAME = <git_author_name> EMAIL = <git_author_email> ]\n  [ COMMENT = <git_push_comment> ]\n\nALTER STREAMLIT <name> ABORT\n\nALTER STREAMLIT <name> PULL"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the Streamlit object. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the property to set for the Streamlit object: ROOT_LOCATION = ' stage_path_and_root_directory ' Important ROOT_LOCATION is a legacy parameter. Snowflake recommends using FROM source_location . For Streamlit apps created using ROOT_LOCATION, multi-file editing and Git integration are not supported. Specifies the root stage name and prefix containing the Streamlit Python files, media files and environment.yml file. This parameter must point to a single directory inside a named internal stage. Specifies the filename of the Streamlit Python application. This filename is relative to the value of ROOT_LOCATION . Specifies the warehouse where SQL queries issued by the Streamlit application are run. Adds a comment or overwrites an existing comment for the Streamlit object. Adds a title for the Streamlit app to display in Snowsight. The location (stage), path, and name of the file(s) to import. The names of external access integrations needed in order for the\nStreamlit app code to access external networks."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the Streamlit object; must be unique for the schema. For more details about identifiers, see Identifier requirements ."
        },
        {
            "name": "PUSH",
            "description": "Pushes the latest committed changes to the Git repo, using the branch stored in the base version if TO git_branch_uri is not specified. If the base version is not based on a Git branch, throws an error. Preview Feature — Open Available to all accounts. Pushes committed changes to the specified branch. Specifies the Snowflake secret containing the credentials to use for authenticating with the repository. Specifies a Git username. Specifies a Git password. Specifies the name of the git author to use. Specifies a valid e-mail address to use as the git author’s name. Specifies a comment to include in the git push."
        },
        {
            "name": "ABORT",
            "description": "Removes an existing version and deletes its files. Preview Feature — Open Available to all accounts."
        },
        {
            "name": "PULL",
            "description": "Pulls latest changes. Preview Feature — Open Available to all accounts."
        },
        {
            "name": "MAIN_FILE   =   ' path_to_main_file_in_root_directory '",
            "description": "Specifies the filename of the Streamlit Python application. This filename is relative to the value of ROOT_LOCATION ."
        },
        {
            "name": "QUERY_WAREHOUSE   =   warehouse_name",
            "description": "Specifies the warehouse where SQL queries issued by the Streamlit application are run."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the Streamlit object."
        },
        {
            "name": "TITLE   =   ' app_title '",
            "description": "Adds a title for the Streamlit app to display in Snowsight."
        },
        {
            "name": "IMPORTS   =   (   ' stage_path_and_file_name_to_read '   [   ,   ...   ]   )",
            "description": "The location (stage), path, and name of the file(s) to import."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for the\nStreamlit app code to access external networks."
        },
        {
            "name": "TO   git_branch_uri",
            "description": "Pushes committed changes to the specified branch."
        },
        {
            "name": "GIT_CREDENTIALS   =   snowflake_secret",
            "description": "Specifies the Snowflake secret containing the credentials to use for authenticating with the repository."
        },
        {
            "name": "USERNAME   =   git_username",
            "description": "Specifies a Git username."
        },
        {
            "name": "PASSWORD   =   git_password",
            "description": "Specifies a Git password."
        },
        {
            "name": "NAME   =   git_author_name",
            "description": "Specifies the name of the git author to use."
        },
        {
            "name": "EMAIL   =   git_author_email",
            "description": "Specifies a valid e-mail address to use as the git author’s name."
        },
        {
            "name": "COMMENT   =   git_push_comment",
            "description": "Specifies a comment to include in the git push."
        }
    ],
    "usage_notes": "If you run the ALTER STREAMLIT command while the Streamlit app is running, the app does not reflect the changes\nthat you made.\nIf you want your changes reflected in the app, you must reload or reboot the app.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-model-drop-version",
    "title": "ALTER MODEL … DROP VERSION",
    "description": "Removes a version from the specified machine learning model.",
    "syntax": "ALTER MODEL [ IF EXISTS ] <name> DROP VERSION <version_name>",
    "examples": [
        {
            "code": "ALTER MODEL [ IF EXISTS ] <name> DROP VERSION <version_name>"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier of the model. If the identifier contains spaces, special characters, or mixed-case\ncharacters, the entire identifier must be enclosed in double quotes. Identifiers enclosed in double quotes are also\ncase-sensitive. For information on identifier syntax, see Identifier requirements . If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "version_name",
            "description": "Specifies the identifier of the version to be removed. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        }
    ],
    "usage_notes": "Aliases are alternative names for model versions. In addition to aliases you create, the following three system aliases are available.\nDEFAULT refers to the default version of the model.\nFIRST refers to the oldest version of the model by creation time.\nLAST refers to the newest version of the model by creation time.\nWhen you drop the first or last model version, the corresponding system alias, FIRST or LAST, adjusts to point\nto the new first or last alias.\nYou cannot drop the default version of a model. Change the default to a different version, if there is one, using\nALTER MODEL … SET DEFAULT VERSION, then drop the unneeded version. If there is no other version to\nselect as the default, because the model has only one version, drop the entire model."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc",
    "title": "DESCRIBE",
    "description": "Describes the details for the specified object."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-catalog-integration",
    "title": "ALTER CATALOG INTEGRATION",
    "description": "Modifies the properties of an existing catalog integration.",
    "syntax": "ALTER CATALOG INTEGRATION [ IF EXISTS ] <name> SET\n  REST_AUTHENTICATION = (\n    restAuthenticationParams\n  )\n  [ REFRESH_INTERVAL_SECONDS = <value> ]\n  [ COMMENT = '<string_literal>' ]\n\nrestAuthenticationParams (for OAuth) ::=\n\n  OAUTH_CLIENT_SECRET = '<oauth_client_secret>'\n\nrestAuthenticationParams (for Bearer token) ::=\n\n  BEARER_TOKEN = '<bearer_token>'",
    "examples": [
        {
            "code": "ALTER CATALOG INTEGRATION myCatalogIntegration SET REFRESH_INTERVAL_SECONDS = 30;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the catalog integration to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Sets one or more specified properties or parameters to set for the catalog integration: Specifies the number of seconds that Snowflake waits between attempts to poll the external Iceberg catalog for metadata updates\nfor automated refresh . For Delta-based tables, specifies the number of seconds that Snowflake waits between attempts to poll your external cloud storage for\nnew metadata. Values: 30 to 86400, inclusive Default: 30 seconds String (literal) that specifies a comment for the integration. Default: No value"
        },
        {
            "name": "REFRESH_INTERVAL_SECONDS   =   value",
            "description": "Specifies the number of seconds that Snowflake waits between attempts to poll the external Iceberg catalog for metadata updates\nfor automated refresh . For Delta-based tables, specifies the number of seconds that Snowflake waits between attempts to poll your external cloud storage for\nnew metadata. Values: 30 to 86400, inclusive Default: 30 seconds"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the integration. Default: No value"
        },
        {
            "name": "OAUTH_CLIENT_SECRET   =   oauth_client_secret",
            "description": "Your OAuth2 client secret."
        },
        {
            "name": "BEARER_TOKEN   =   bearer_token",
            "description": "The bearer token for your identity provider. You can alternatively specify a personal access token (PAT)."
        },
        {
            "name": "OAUTH_CLIENT_SECRET   =   oauth_client_secret",
            "description": "Your OAuth2 client secret."
        },
        {
            "name": "BEARER_TOKEN   =   bearer_token",
            "description": "The bearer token for your identity provider. You can alternatively specify a personal access token (PAT)."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-storage-integration",
    "title": "ALTER STORAGE INTEGRATION",
    "description": "Modifies the properties for an existing storage integration.",
    "syntax": "ALTER [ STORAGE ] INTEGRATION [ IF EXISTS ] <name> SET\n  [ cloudProviderParams ]\n  [ ENABLED = { TRUE | FALSE } ]\n  [ STORAGE_ALLOWED_LOCATIONS = ('<cloud>://<bucket>/<path>/' [ , '<cloud>://<bucket>/<path>/' ... ] ) ]\n  [ STORAGE_BLOCKED_LOCATIONS = ('<cloud>://<bucket>/<path>/' [ , '<cloud>://<bucket>/<path>/' ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER [ STORAGE ] INTEGRATION [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER [ STORAGE ] INTEGRATION <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER [ STORAGE ] INTEGRATION [ IF EXISTS ] <name>  UNSET {\n                                                          ENABLED                   |\n                                                          STORAGE_BLOCKED_LOCATIONS |\n                                                          COMMENT\n                                                          }\n                                                          [ , ... ]\n\ncloudProviderParams (for Amazon S3) ::=\n  STORAGE_AWS_ROLE_ARN = '<iam_role>'\n  [ STORAGE_AWS_OBJECT_ACL = 'bucket-owner-full-control' ]\n  [ STORAGE_AWS_EXTERNAL_ID = '<external_id>' ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\ncloudProviderParams (for Microsoft Azure) ::=\n  AZURE_TENANT_ID = '<tenant_id>'\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]",
    "examples": [
        {
            "code": "ALTER STORAGE INTEGRATION myint SET ENABLED = TRUE;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the integration to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the table (separated by blank spaces, commas, or new lines): Specifies whether this storage integration is available for usage in stages. TRUE allows users to create new stages that reference this integration. Existing stages that reference this integration function\nnormally. FALSE prevents users from creating new stages that reference this integration. Existing stages that reference this integration\ncannot access the storage location in the stage definition. Explicitly limits external stages that use the integration to reference one or more storage locations (Amazon S3, Google Cloud Storage, or\nMicrosoft Azure). Supports a comma-separated list of URLs for existing buckets and, optionally, paths used to store data files for\nloading/unloading. Alternatively supports the * wildcard, meaning “allow access to all buckets and/or paths”. Amazon S3 STORAGE_ALLOWED_LOCATIONS = ( ' protocol :// bucket / path /' [ , ' protocol :// bucket / path /' ... ] ) protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files (e.g. mybucket ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Google Cloud Storage STORAGE_ALLOWED_LOCATIONS = ( 'gcs:// bucket / path /' [ , 'gcs:// bucket / path /' ... ] ) bucket is the name of a GCS bucket that stores your data files (e.g. mybucket ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Microsoft Azure STORAGE_ALLOWED_LOCATIONS = ( 'azure:// account .blob.core.windows.net/ container / path /' [ , 'azure:// account .blob.core.windows.net/ container / path /' ... ] ) account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all supported\ntypes of Azure blob storage accounts, including Data Lake Storage Gen2. container is the name of the Azure container that stores your data files (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Explicitly prohibits external stages that use the integration from referencing one or more storage locations (Amazon S3, Google Cloud Storage,\nMicrosoft Azure). Supports a comma-separated list of URLs for existing storage locations and, optionally, paths used to store data files for\nloading/unloading. Commonly used when STORAGE_ALLOWED_LOCATIONS is set to the * wildcard, allowing access to all buckets in your account except for blocked storage locations and, optionally, paths. Note Make sure to enclose only individual cloud storage location URLs in quotes. If you enclose the entire STORAGE_BLOCKED_LOCATIONS value in quotes, the value is invalid. As a result, the STORAGE_BLOCKED_LOCATIONS parameter\nsetting is ignored when users create stages that reference the storage integration. Amazon S3 STORAGE_BLOCKED_LOCATIONS = ( ' protocol :// bucket / path /' [ , ' protocol :// bucket / path /' ... ] ) protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files (e.g. mybucket ). path is an optional path (or directory ) in the bucket that further limits access to data files. Google Cloud Storage STORAGE_BLOCKED_LOCATIONS = ( 'gcs:// bucket / path /' [ , 'gcs:// bucket / path /' ... ] ) bucket is the name of a GCS bucket that stores your data files (e.g. mybucket ). path is an optional path (or directory ) in the bucket that further limits access to data files. Microsoft Azure STORAGE_BLOCKED_LOCATIONS = ( 'azure:// account .blob.core.windows.net/ container / path /' [ , 'azure:// account .blob.core.windows.net/ container / path /' ... ] ) account is the name of the Azure account (e.g. myaccount ). container is the name of the Azure container that stores your data files (e.g. mycontainer ). path is an optional path (or directory ) in the container that further limits access to data files. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . String (literal) that specifies a comment for the integration."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties/parameters to unset for the storage integration, which resets them back to their defaults: ENABLED STORAGE_BLOCKED_LOCATIONS TAG tag_name [ , tag_name ... ] COMMENT"
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this storage integration is available for usage in stages. TRUE allows users to create new stages that reference this integration. Existing stages that reference this integration function\nnormally. FALSE prevents users from creating new stages that reference this integration. Existing stages that reference this integration\ncannot access the storage location in the stage definition."
        },
        {
            "name": "STORAGE_ALLOWED_LOCATIONS   =   (   ' cloud_specific_url '   )",
            "description": "Explicitly limits external stages that use the integration to reference one or more storage locations (Amazon S3, Google Cloud Storage, or\nMicrosoft Azure). Supports a comma-separated list of URLs for existing buckets and, optionally, paths used to store data files for\nloading/unloading. Alternatively supports the * wildcard, meaning “allow access to all buckets and/or paths”. Amazon S3 STORAGE_ALLOWED_LOCATIONS = ( ' protocol :// bucket / path /' [ , ' protocol :// bucket / path /' ... ] ) protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files (e.g. mybucket ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Google Cloud Storage STORAGE_ALLOWED_LOCATIONS = ( 'gcs:// bucket / path /' [ , 'gcs:// bucket / path /' ... ] ) bucket is the name of a GCS bucket that stores your data files (e.g. mybucket ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Microsoft Azure STORAGE_ALLOWED_LOCATIONS = ( 'azure:// account .blob.core.windows.net/ container / path /' [ , 'azure:// account .blob.core.windows.net/ container / path /' ... ] ) account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all supported\ntypes of Azure blob storage accounts, including Data Lake Storage Gen2. container is the name of the Azure container that stores your data files (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits access to a set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "STORAGE_BLOCKED_LOCATIONS   =   (   ' cloud_specific_url '   )",
            "description": "Explicitly prohibits external stages that use the integration from referencing one or more storage locations (Amazon S3, Google Cloud Storage,\nMicrosoft Azure). Supports a comma-separated list of URLs for existing storage locations and, optionally, paths used to store data files for\nloading/unloading. Commonly used when STORAGE_ALLOWED_LOCATIONS is set to the * wildcard, allowing access to all buckets in your account except for blocked storage locations and, optionally, paths. Note Make sure to enclose only individual cloud storage location URLs in quotes. If you enclose the entire STORAGE_BLOCKED_LOCATIONS value in quotes, the value is invalid. As a result, the STORAGE_BLOCKED_LOCATIONS parameter\nsetting is ignored when users create stages that reference the storage integration. Amazon S3 STORAGE_BLOCKED_LOCATIONS = ( ' protocol :// bucket / path /' [ , ' protocol :// bucket / path /' ... ] ) protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files (e.g. mybucket ). path is an optional path (or directory ) in the bucket that further limits access to data files. Google Cloud Storage STORAGE_BLOCKED_LOCATIONS = ( 'gcs:// bucket / path /' [ , 'gcs:// bucket / path /' ... ] ) bucket is the name of a GCS bucket that stores your data files (e.g. mybucket ). path is an optional path (or directory ) in the bucket that further limits access to data files. Microsoft Azure STORAGE_BLOCKED_LOCATIONS = ( 'azure:// account .blob.core.windows.net/ container / path /' [ , 'azure:// account .blob.core.windows.net/ container / path /' ... ] ) account is the name of the Azure account (e.g. myaccount ). container is the name of the Azure container that stores your data files (e.g. mycontainer ). path is an optional path (or directory ) in the container that further limits access to data files."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the integration."
        },
        {
            "name": "STORAGE_AWS_ROLE_ARN   =   ' iam_role '",
            "description": "Specifies the Amazon Resource Name (ARN) of the AWS identity and access management (IAM) role that grants privileges on the S3 bucket\ncontaining your data files. For more information, see Configuring secure access to Amazon S3 ."
        },
        {
            "name": "STORAGE_AWS_OBJECT_ACL   =   'bucket-owner-full-control'",
            "description": "Enables support for AWS access control lists (ACLs) to grant the S3 bucket owner full control. Files created in Amazon S3 buckets from\nunloaded table data are owned by an AWS Identity and Access Management (IAM) role. ACLs support the use case where IAM roles in one AWS\naccount are configured to access S3 buckets in one or more other AWS accounts. Without ACL support, users in the bucket-owner accounts\ncould not access the data files unloaded to an external (S3) stage using a storage integration. When users unload Snowflake table data to data files in an S3 stage using COPY INTO <location> , the unload operation\napplies an ACL to the unloaded data files. The data files apply the \"s3:x-amz-acl\":\"bucket-owner-full-control\" privilege to the files,\ngranting the S3 bucket owner full control over them."
        },
        {
            "name": "STORAGE_AWS_EXTERNAL_ID   =   ' external_id '",
            "description": "Specifies an external ID that Snowflake uses to establish a trust relationship with AWS.\nYou must specify the same external ID in the trust policy of the IAM role\nthat you configured for this storage integration. For more information,\nsee How to use an external ID when granting access to your AWS resources to a third party ."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter, see AWS private connectivity to external stages ."
        },
        {
            "name": "AZURE_TENANT_ID   =   ' tenant_id '",
            "description": "Specifies the ID for your Office 365 tenant that the allowed and blocked storage accounts belong to. A storage integration can authenticate\nto only one tenant, and so the allowed and blocked storage locations must refer to storage accounts that all belong this tenant. To find your tenant ID, log into the Azure portal and click Azure Active Directory » Properties . The tenant ID is\ndisplayed in the Tenant ID field."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter,\nsee Azure private connectivity for external stages and Snowpipe automation ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-task",
    "title": "ALTER TASK",
    "description": "Modifies the properties for an existing task.",
    "syntax": "ALTER TASK [ IF EXISTS ] <name> RESUME | SUSPEND\n\nALTER TASK [ IF EXISTS ] <name> REMOVE AFTER <string> [ , <string> , ... ]\n  | ADD AFTER <string> [ , <string> , ... ]\n\nALTER TASK [ IF EXISTS ] <name> SET\n  [ { WAREHOUSE = <string> }\n    | { USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = <string> } ]\n  [ SCHEDULE = { '<num> { HOURS | MINUTES | SECONDS }'\n               | 'USING CRON <expr> <time_zone>' } ]\n  [ CONFIG = <configuration_string> ]\n  [ ALLOW_OVERLAPPING_EXECUTION = TRUE | FALSE ]\n  [ USER_TASK_TIMEOUT_MS = <num> ]\n  [ SUSPEND_TASK_AFTER_NUM_FAILURES = <num> ]\n  [ ERROR_INTEGRATION = <integration_name> ]\n  [ SUCCESS_INTEGRATION = <integration_name> ]\n  [ LOG_LEVEL = '<log_level>' ]\n  [ COMMENT = <string> ]\n  [ <session_parameter> = <value>\n    [ , <session_parameter> = <value> ... ] ]\n  [ TASK_AUTO_RETRY_ATTEMPTS = <num> ]\n  [ USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS = <num> ]\n  [ TARGET_COMPLETION_INTERVAL = '<num> { HOURS | MINUTES | SECONDS }' ]\n  [ SERVERLESS_TASK_MIN_STATEMENT_SIZE= 'XSMALL | SMALL\n    | MEDIUM | LARGE | XLARGE | XXLARGE' ]\n  [ SERVERLESS_TASK_MAX_STATEMENT_SIZE= 'XSMALL | SMALL\n    | MEDIUM | LARGE | XLARGE | XXLARGE' ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\n\nALTER TASK [ IF EXISTS ] <name> UNSET\n  [ WAREHOUSE ]\n  [ SCHEDULE ]\n  [ CONFIG ]\n  [ ALLOW_OVERLAPPING_EXECUTION ]\n  [ USER_TASK_TIMEOUT_MS ]\n  [ SUSPEND_TASK_AFTER_NUM_FAILURES ]\n  [ LOG_LEVEL ]\n  [ COMMENT ]\n  [ <session_parameter> [ , <session_parameter> ... ] ]\n  [ TARGET_COMPLETION_INTERVAL ]\n  [ SERVERLESS_TASK_MIN_STATEMENT_SIZE ]\n  [ SERVERLESS_TASK_MAX_STATEMENT_SIZE ]\n  [ CONTACT <purpose> ]\n  [ , ... ]\n\nALTER TASK [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>'\n  [ , <tag_name> = '<tag_value>' ... ]\n\nALTER TASK [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER TASK [ IF EXISTS ] <name> SET FINALIZE = <string>\n\nALTER TASK [ IF EXISTS ] <name> UNSET FINALIZE\n\nALTER TASK [ IF EXISTS ] <name> MODIFY AS <sql>\n\nALTER TASK [ IF EXISTS ] <name> MODIFY WHEN <boolean_expr>\n\nALTER TASK [ IF EXISTS ] <name> REMOVE WHEN",
    "examples": [
        {
            "code": "ALTER TASK mytask RESUME;"
        },
        {
            "code": "ALTER TASK mytask UNSET WAREHOUSE;\n\nALTER TASK mytask SET USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL';"
        },
        {
            "code": "ALTER TASK mytask SET TIMEZONE = 'America/Los_Angeles', CLIENT_TIMESTAMP_TYPE_MAPPING = TIMESTAMP_LTZ;"
        },
        {
            "code": "ALTER TASK mytask SET SCHEDULE = 'USING CRON */3 * * * * UTC';"
        },
        {
            "code": "ALTER TASK mytask REMOVE AFTER pred_task1, pred_task2;\n\nALTER TASK mytask ADD AFTER pred_task3;"
        },
        {
            "code": "ALTER TASK mytask MODIFY AS SELECT CURRENT_VERSION();"
        },
        {
            "code": "ALTER TASK mytask MODIFY WHEN SYSTEM$STREAM_HAS_DATA('MYSTREAM');"
        },
        {
            "code": "ALTER TASK task_with_config SET\n      CONFIG=$${\"output_directory\": \"/temp/prod_directory/\", \"environment\": \"prod\"}$$;"
        },
        {
            "code": "ALTER TASK task_with_config UNSET CONFIG;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the task to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "RESUME   |   SUSPEND",
            "description": "Specifies the action to perform on the task: RESUME brings a suspended task to the ‘Started’ state. Note that accounts are currently limited to a maximum of 30000 started\ntasks. Before resuming the root task of your Task Graph, resume all child tasks. To recursively resume the root task’s child\ntasks, use SYSTEM$TASK_DEPENDENTS_ENABLE . SUSPEND puts the task into a ‘Suspended’ state. If the task schedule is set to an interval of ( number { HOURS | MINUTES | SECONDS } ), the base interval time for the schedule is reset to the current time the task is resumed. The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 MINUTES is set and\nthe task is resumed at 9:03 AM, then the task runs at 9:13 AM, 9:23 AM, and so on. Note that we only guarantee that tasks don’t execute before their set interval occurs. In the current example, the task could first run at 9:14 AM, but won’t run at 9:12 AM."
        },
        {
            "name": "REMOVE   AFTER   string   [   ,   string   ,   ...   ]",
            "description": "Specifies the names of one or more current predecessor tasks for this child task in a task graph . When all predecessors for a child task are removed, then the former child task becomes either a standalone task or a root task, depending on\nwhether other tasks identify this former child task as their predecessor. If the former child task becomes a root task, this task is suspended\nby default and must be resumed manually."
        },
        {
            "name": "ADD   AFTER   string   [   ,   string   ,   ...   ]",
            "description": "Specifies the names of one or more existing tasks to add as predecessors for this child task in a task graph .\nEach child task in a task graph runs when all predecessor tasks finish their runs successfully. For more information, see the description\nof the AFTER parameter in CREATE TASK . Each child task is limited to 100 predecessor tasks."
        },
        {
            "name": "SET   ...",
            "description": "Specifies either or both of the following: One (or more) properties to set for the task (separated by blank spaces, commas, or new lines). For more details about the properties you\ncan set, see CREATE TASK . A comma-separated list of session parameters to set for the session when the task runs. A task supports all session parameters. For the\ncomplete list, see Parameters . Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties and/or session parameters to unset for the task, which resets them to the defaults. You can reset multiple properties/parameters with a single ALTER statement; however, each property/parameter must be separated by a\ncomma. When resetting a property/parameter, specify only the name; specifying a value for the property/parameter will return an error. To detach a contact from the task, specify UNSET CONTACT purpose ."
        },
        {
            "name": "sql",
            "description": "Specifies the SQL code to execute when the task runs: Single SQL statement Call to a stored procedure Procedural logic using Snowflake Scripting Note that currently, Snowsight and the Classic Console do not support creating or modifying tasks to use Snowflake Scripting.\nInstead, use SnowSQL or another command-line client. Note Verify that the SQL code you reference in a task executes as expected before you create the task. Tasks are intended to\nautomate SQL code that has already been tested thoroughly."
        },
        {
            "name": "WHEN   boolean_expr",
            "description": "Specifies a Boolean SQL expression. When a task is triggered, it validates the conditions of the expression to determine whether to\nexecute. If the conditions of the expression are not met, then the task skips the current run. Any tasks that identify this task\nas a predecessor also do not run. Validating the conditions of the WHEN expression does not require a virtual warehouse. The validation is instead processed in the cloud\nservices layer. A nominal charge accrues each time a task evaluates its WHEN condition and does not run. The charges accumulate each time\nthe task is triggered until it runs. At that time, the charge is converted to Snowflake credits and added to the compute resource usage\nfor the task run. Generally the compute time to validate the condition is insignificant compared to task execution time. As a best practice, align\nscheduled and actual task runs as closely as possible. Avoid task schedules that are wildly out of synch with actual task runs. For\nexample, if data is inserted into a table with a stream roughly every 24 hours, don’t schedule a task that checks for stream data every\nminute. The charge to validate the WHEN expression with each run is generally insignificant, but the charges are cumulative. Note that daily consumption of cloud services that falls below the 10% quota of the daily usage of the compute resources accumulates no cloud services charges. Currently, the following functions are supported for evaluation in the SQL expression: Indicates whether a specified stream contains change tracking data. Used to run a triggered task if no schedule is defined for the\ntask. You can also use this to skip the current task run if the stream contains no change data. If the result is FALSE , then the task does not run. Retrieves the return value for the predecessor task in a task graph.\nUsed to decide whether the task should run based on the returned result."
        },
        {
            "name": "REMOVE   WHEN",
            "description": "Remove the WHEN condition that you have specified."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "SYSTEM$STREAM_HAS_DATA",
            "description": "Indicates whether a specified stream contains change tracking data. Used to run a triggered task if no schedule is defined for the\ntask. You can also use this to skip the current task run if the stream contains no change data. If the result is FALSE , then the task does not run."
        },
        {
            "name": "SYSTEM$GET_PREDECESSOR_RETURN_VALUE",
            "description": "Retrieves the return value for the predecessor task in a task graph.\nUsed to decide whether the task should run based on the returned result."
        }
    ],
    "usage_notes": "Resuming or suspending a task (using ALTER TASK … RESUME or ALTER TASK … SUSPEND, respectively) requires either the OWNERSHIP or\nOPERATE privilege on the task.\nWhen a task is resumed, Snowflake verifies that the role with the OWNERSHIP privilege on the task also has the USAGE privilege on the\nwarehouse assigned to the task, as well as the global EXECUTE TASK privilege; if not, an error is produced.\nOnly account administrators (users with the ACCOUNTADMIN role) can grant the EXECUTE TASK privilege to a role. For ease of use, we recommend\ncreating a custom role (e.g. TASKADMIN) and assigning the EXECUTE TASK privilege to this role. Any role that can grant privileges\n(e.g. SECURITYADMIN or any role with the MANAGE GRANTS privilege) can then grant this custom role to any task owner role to allow altering\ntheir own tasks. For instructions for creating custom roles and role hierarchies, see Configuring access control.\nOnly the task owner (i.e. the role with the OWNERSHIP privilege on the task) can set or unset properties on a task.\nWhen altering configuration you cannot update individual key-value pairs. Rather you must supply the entire replacement json string\nrepresenting the complete configuration.\nA standalone task must be suspended before it can be modified.\nThe root task in a task graph must be suspended before any\ntask in the task graph is modified, a child task is suspended or resumed, or a child task is added (using ALTER TASK … AFTER).\nA task graph is limited to a maximum of 1000 tasks total (including the root task) in either a resumed or suspended state.\nTo recursively resume all dependent tasks tied to a root task in a task graph, query the SYSTEM$TASK_DEPENDENTS_ENABLE\nfunction rather than enabling each task individually (using ALTER TASK … RESUME).\nBy default, a DML statement executed without explicitly starting a transaction is automatically committed on success or rolled back on failure\nat the end of the statement. This behavior is called autocommit and is controlled with the AUTOCOMMIT parameter. This parameter\nmust be set to TRUE. If the AUTOCOMMIT parameter is set to FALSE at the account level, then set the parameter to TRUE for the\nindividual task (using ALTER TASK … SET AUTOCOMMIT = TRUE).\nWhen a task is suspended, any current run of the task (i.e. a run with an EXECUTING state in the TASK_HISTORY\noutput) is completed. To abort the run of the specified task, execute the SYSTEM$USER_TASK_CANCEL_ONGOING_EXECUTIONS\nfunction.\nThe compute resources for individual runs of a task are either managed by Snowflake (i.e. the serverless compute model) or a\nuser-specified virtual warehouse. To convert a task that relies on a warehouse to the serverless compute model, unset the\nWAREHOUSE.\nIf a task fails with an unexpected error, you can receive a notification about the error.\nFor more information about configuring task error notifications, see Enabling notifications for tasks.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nRegarding the finalizer task:\nWhen you SET FINALIZE = <root task name>, this function configures a normal task to be a finalizer task associated with the\ngiven root task.\nWhen you UNSET FINALIZE, a finalizer task changes to a normal standalone task with no schedule or predecessor.\nSET FINALIZE conflicts with SET SCHEDULE and ADD AFTER. A task with an existing schedule or predecessor will\nalso fail the SET FINALIZE query.\nTo alter the root task’s defined finalizer task, first use UNSET FINALIZE to unset the finalizer task and then use\nSET FINALIZE = <root task name> to update the root task’s finalizer task.\nThe root task must be suspended before the finalizer task is modified, set, or unset."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-tag",
    "title": "ALTER TAG",
    "description": "Modifies the properties for an existing tag, including renaming the tag and setting a masking policy on a tag.",
    "syntax": "ALTER TAG [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER TAG [ IF EXISTS ] <name> { ADD | DROP } ALLOWED_VALUES '<val_1>' [ , '<val_2>' [ , ... ] ]\n\nALTER TAG [ IF EXISTS ] <name> SET\n  [ PROPAGATE = { ON_DEPENDENCY_AND_DATA_MOVEMENT | ON_DEPENDENCY | ON_DATA_MOVEMENT }\n    [ ON_CONFLICT = { '<string>' | ALLOWED_VALUES_SEQUENCE } ] ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER TAG [ IF EXISTS ] <name> UNSET { ALLOWED_VALUES | PROPAGATE | ON_CONFLICT | COMMENT }\n\nALTER TAG [ IF EXISTS ] <name> SET MASKING POLICY\n  <masking_policy_name> [ , MASKING POLICY <masking_policy_2_name> , ... ] [ FORCE ]\n\nALTER TAG [ IF EXISTS ] <name> UNSET MASKING POLICY <masking_policy_name> [ , MASKING POLICY <masking_policy_2_name> , ... ]",
    "examples": [
        {
            "code": "ALTER TAG cost_center RENAME TO cost_center_na;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the tag. Assign the tag string value on an object using either a CREATE <object> statement or an ALTER <object> statement. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. “My object”). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements"
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the tag; must be unique for your schema. The new identifier cannot be used if the identifier is already\nin place for a different tag. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "ALLOWED_VALUES   ' val_1 '   [   ,   ' val_2 '   [   ,   ...   ]   ]",
            "description": "Specifies a comma-separated list of the possible string values that can be assigned to the tag when the tag is set on an object using the corresponding CREATE <object> or ALTER <object> command. The maximum number of tag values in this list is 300. If a tag is configured to automatically propagate to target objects, the order of values in the allowed list can affect how conflicts are\nresolved. For more information, see Tag propagation conflicts . Default: NULL (all string values are allowed, including an empty string value (that is, ' ' ))."
        },
        {
            "name": "PROPAGATE   =   {   ON_DEPENDENCY_AND_DATA_MOVEMENT   |   ON_DEPENDENCY   |   ON_DATA_MOVEMENT   }",
            "description": "Enterprise Edition Feature This parameter requires Enterprise Edition or higher. To inquire about upgrading, please contact Snowflake Support . Specifies that the tag will be automatically propagated from source objects to target\nobjects. You can configure the tag to propagate when there is an object dependency , data movement , or both. Changes to this parameter do not automatically propagate to target objects. These changes have no effect on tags that were previously\napplied to target objects as part of tag propagation. Possible values are: Propagates the tag when there is an object dependency or data movement. Propagates the tag for object dependencies, but not for data movement. Propagates the tag when there is data movement, but not for object dependencies."
        },
        {
            "name": "ON_CONFLICT   =   {   ' string '   |   ALLOWED_VALUES_SEQUENCE   }",
            "description": "Enterprise Edition Feature This parameter requires Enterprise Edition or higher. To inquire about upgrading, please contact Snowflake Support . Specifies what happens when there is a conflict between the values of propagated tags . If you don’t set this parameter and there is a conflict, the value of the tag is set to the string CONFLICT . Changes to this parameter do not automatically propagate to target objects. These changes have no effect on tags that were previously\napplied to target objects as part of tag propagation. Possible values are: When there is a conflict, the value of the tag is set to the specified string. The order of the values in the ALLOWED_VALUES property of the tag determines which value is used when there is a conflict.\nFor example, suppose you created a tag with the following statement: If there is a conflict, then the value of my_tag will be blue because it comes before red in the allowed values list. Default: Set the value of the tag to CONFLICT ."
        },
        {
            "name": "MASKING   POLICY   masking_policy_name   [   ,   MASKING   POLICY   masking_policy_2_name   ,   ...   ]",
            "description": "Specifies a comma-separated list of masking policies that can be assigned to the tag."
        },
        {
            "name": "FORCE",
            "description": "Replaces a masking policy that is currently set on a tag with a different masking policy in a single statement. Note that using the FORCE keyword replaces the masking policy when a policy of the same data type is\nalready set on the tag. If a masking policy is not currently set on the tag, specifying this keyword has no effect. For details, see Replace a masking policy on a tag ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the tag. Default: No value"
        },
        {
            "name": "UNSET",
            "description": "Specifies one (or more) properties and/or parameters to unset for the tag, which resets them to the defaults: ALLOWED_VALUES PROPAGATE ON_CONFLICT COMMENT"
        },
        {
            "name": "ON_DEPENDENCY_AND_DATA_MOVEMENT",
            "description": "Propagates the tag when there is an object dependency or data movement."
        },
        {
            "name": "ON_DEPENDENCY",
            "description": "Propagates the tag for object dependencies, but not for data movement."
        },
        {
            "name": "ON_DATA_MOVEMENT",
            "description": "Propagates the tag when there is data movement, but not for object dependencies."
        },
        {
            "name": "' string '",
            "description": "When there is a conflict, the value of the tag is set to the specified string."
        },
        {
            "name": "ALLOWED_VALUES_SEQUENCE",
            "description": "The order of the values in the ALLOWED_VALUES property of the tag determines which value is used when there is a conflict.\nFor example, suppose you created a tag with the following statement: If there is a conflict, then the value of my_tag will be blue because it comes before red in the allowed values list."
        }
    ],
    "usage_notes": "For more information on tag DDL authorization, see required privileges.\nRegarding assigning one or more masking policies to a tag:\nA tag can have only one masking policy per data type.\nFor example, a tag can have one policy for the STRING data type, one policy for the NUMBER data type, and so on.\nIf a masking policy already protects a column and the tag with a masking policy is set on the same column, the masking policy\ndirectly assigned to the column takes precedence over the masking policy assigned to the tag.\nA tag cannot be dropped if a masking policy is assigned to the tag, nor can the masking policy be\ndropped if the masking policy is assigned to a tag.\nRegarding replication, particularly with tag-based masking policies, see\npolicy replication considerations.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-warehouse",
    "title": "ALTER WAREHOUSE",
    "description": "Suspends or resumes a virtual warehouse,\nor aborts all queries (and other SQL statements) for a warehouse. Can also be used to rename or\nset/unset the properties for a warehouse.",
    "syntax": "ALTER WAREHOUSE [ IF EXISTS ] [ <name> ] { SUSPEND | RESUME [ IF SUSPENDED ] }\n\nALTER WAREHOUSE [ IF EXISTS ] [ <name> ] ABORT ALL QUERIES\n\nALTER WAREHOUSE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER WAREHOUSE [ IF EXISTS ] <name> SET [ objectProperties ]\n                                         [ objectParams ]\n\nALTER WAREHOUSE [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER WAREHOUSE [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER WAREHOUSE [ IF EXISTS ] <name> UNSET { <property_name> | <param_name> } [ , ... ]\n\nobjectProperties ::=\n  WAREHOUSE_TYPE = { STANDARD | 'SNOWPARK-OPTIMIZED' }\n  WAREHOUSE_SIZE = { XSMALL | SMALL | MEDIUM | LARGE | XLARGE | XXLARGE | XXXLARGE | X4LARGE | X5LARGE | X6LARGE }\n  RESOURCE_CONSTRAINT = { STANDARD_GEN_1 | STANDARD_GEN_2 | MEMORY_1X | MEMORY_1X_x86 | MEMORY_16X | MEMORY_16X_x86 | MEMORY_64X | MEMORY_64X_x86 }\n  WAIT_FOR_COMPLETION = { TRUE | FALSE }\n  MAX_CLUSTER_COUNT = <num>\n  MIN_CLUSTER_COUNT = <num>\n  SCALING_POLICY = { STANDARD | ECONOMY }\n  AUTO_SUSPEND = { <num> | NULL }\n  AUTO_RESUME = { TRUE | FALSE }\n  RESOURCE_MONITOR = <monitor_name>\n  COMMENT = '<string_literal>'\n  ENABLE_QUERY_ACCELERATION = { TRUE | FALSE }\n  QUERY_ACCELERATION_MAX_SCALE_FACTOR = <num>\n\nobjectParams ::=\n  MAX_CONCURRENCY_LEVEL = <num>\n  STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = <num>\n  STATEMENT_TIMEOUT_IN_SECONDS = <num>",
    "examples": [
        {
            "code": "ALTER WAREHOUSE IF EXISTS wh1 RENAME TO wh2;"
        },
        {
            "code": "ALTER WAREHOUSE my_wh RESUME;\n\nALTER WAREHOUSE my_wh SET warehouse_size=MEDIUM;"
        },
        {
            "code": "ALTER WAREHOUSE so_warehouse SET\n  RESOURCE_CONSTRAINT = 'MEMORY_16X_x86';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the warehouse to alter. If the identifier contains spaces or special characters, the entire string must be enclosed\nin double quotes. Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements . Note A warehouse identifier is required or optional depending on the following: When resuming/suspending a warehouse or aborting queries for a warehouse, if a warehouse is currently in use for the session, the identifier\ncan be omitted. When renaming a warehouse or performing any other operations on a warehouse, the identifier must be specified."
        },
        {
            "name": "{   SUSPEND   |   RESUME   [   IF   SUSPENDED   ]   }",
            "description": "Specifies the action to perform on the warehouse: SUSPEND removes all compute nodes from a warehouse and puts the warehouse into a ‘Suspended’ state. RESUME [ IF SUSPENDED ] brings a suspended warehouse to a usable ‘Running’ state by provisioning compute resources. The optional IF SUSPENDED clause specifies whether the ALTER WAREHOUSE command completes successfully when resuming a warehouse that\nis already running: If omitted, the command fails and returns an error if the warehouse is already running. If specified, the command completes successfully regardless of whether the warehouse is running."
        },
        {
            "name": "ABORT   ALL   QUERIES",
            "description": "Aborts all the queries currently running or queued on the warehouse."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies a new identifier for the warehouse; must be unique for your account. For more details, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the warehouse (separated by blank spaces, commas, or new lines): Specifies the warehouse type. STANDARD , 'STANDARD' 'SNOWPARK-OPTIMIZED' STANDARD Note To use a value that contains a hyphen ( 'SNOWPARK-OPTIMIZED' ), you must enclose the value in single quotes, as shown. Specifies the size of the virtual warehouse. The size determines the amount of compute resources in each cluster and, therefore,\nthe number of credits consumed while the warehouse is running. For more information see Resizing a warehouse . XSMALL , 'X-SMALL' SMALL MEDIUM LARGE XLARGE , 'X-LARGE' XXLARGE , X2LARGE , '2X-LARGE' XXXLARGE , X3LARGE , '3X-LARGE' X4LARGE , '4X-LARGE' X5LARGE , '5X-LARGE' X6LARGE , '6X-LARGE' XSMALL Note The default size for Snowpark-optimized warehouses is MEDIUM. X5LARGE and X6LARGE sizes for Snowpark-optimized warehouses are only supported with the MEMORY_16X resource constraint. X5LARGE and X6LARGE sizes aren’t supported for standard warehouses that use the STANDARD_GEN_2 resource constraint. To use a value that contains a hyphen (for example, '2X-LARGE' ), you must enclose the value in single quotes, as shown. To block the immediate return of the ALTER WAREHOUSE command until the resize is complete, add the WAIT_FOR_COMPLETION parameter. The upper limit for the MAX_CLUSTER_COUNT property depends on the warehouse size. When you change WAREHOUSE_SIZE\nto a value higher than MEDIUM , you might need to reduce MAX_CLUSTER_COUNT at the same time. For the upper limit\non MAX_CLUSTER_COUNT for each warehouse size, see Upper limit on number of clusters for a multi-cluster warehouse . Larger warehouse sizes 5X-Large and 6X-Large are generally available in all Amazon Web Services (AWS) and Microsoft Azure regions. Larger warehouse sizes are in preview in US Government regions (requires FIPS support on ARM). Preview Feature — Open The 1 TB resource constraints (MEMORY_64X and MEMORY_64X_x86) are available as a preview feature.\nThe 1 TB constraints are  available only on the Amazon Web Services (AWS) cloud platform. All other MEMORY_* resource constraint sizes are generally available and are available for all cloud platforms. Specifies the memory and CPU architecture for Snowpark-optimized warehouses ,\nor generation 1 or generation 2 capabilities for standard warehouses . The following table includes the valid values for the property, available memory, CPU architecture, and the minimum warehouse\nsize required for the resource_constraint setting.\nFor more information about regions and cloud service providers where generation 2 standard warehouses\nare available, see Snowflake generation 2 standard warehouses . Value Memory (up to) CPU architecture Min warehouse size required Max warehouse size STANDARD_GEN_1 16 GB Standard XSMALL X6LARGE STANDARD_GEN_2 16 GB Standard (generation 2) XSMALL X4LARGE MEMORY_1X 16 GB Standard XSMALL X4LARGE MEMORY_1X_x86 16 GB x86 XSMALL X4LARGE MEMORY_16X 256 GB Standard MEDIUM X6LARGE MEMORY_16X_x86 256 GB x86 MEDIUM X4LARGE MEMORY_64X 1 TB Standard LARGE X4LARGE MEMORY_64X_x86 1 TB x86 LARGE X4LARGE MEMORY_16X for Snowpark-optimized warehouses. For standard warehouses, the default depends on\nGen2 support for your cloud service provider region and whether your organization was created after\nGen2 support became available in that region. For more information, see Default value for the RESOURCE_CONSTRAINT for standard warehouses . When resizing a warehouse, you can use this parameter to block the return of the ALTER WAREHOUSE command until the resize has finished\nprovisioning all its compute resources. Blocking the return of the command when resizing to a larger warehouse serves to notify you\nthat your compute resources have been fully provisioned and the warehouse is now ready to execute queries using all the new resources. TRUE : The ALTER WAREHOUSE command will block until the warehouse resize completes. FALSE : The ALTER WAREHOUSE command returns immediately, before the warehouse resize completes. FALSE Note The value of this parameter isn’t persisted and must be set to TRUE on every execution if you want the warehouse resizing to\ncomplete before this command returns. If set to TRUE and you abort the ALTER WAREHOUSE command, only the waiting is aborted and the warehouse resize will go\nthrough. To resize the warehouse back to its original size, you will need to execute another ALTER WAREHOUSE command. This parameter must be used with the WAREHOUSE_SIZE parameter, otherwise an exception will be thrown. Specifies the maximum number of clusters for a multi-cluster warehouse. For a single-cluster warehouse, this value is always 1 . 1 to an upper limit that varies depending on warehouse size. Note that specifying a value greater than 1 indicates the warehouse is a multi-cluster warehouse; however, the value can\nonly be set to a higher value in Snowflake Enterprise Edition (or higher). The upper limit for the MAX_CLUSTER_COUNT property depends on the warehouse size. When you change WAREHOUSE_SIZE\nto a value higher than MEDIUM, you might need to reduce MAX_CLUSTER_COUNT at the same time. For the upper limit\non MAX_CLUSTER_COUNT for each warehouse size, see Upper limit on number of clusters for a multi-cluster warehouse . For more information about multi-cluster warehouses, see Multi-cluster warehouses . 1 (single-cluster warehouse) Tip For Snowflake Enterprise Edition (or higher), we recommend always setting the value greater than 1 to help maintain\nhigh-availability and optimal performance of the (multi-cluster) warehouse. This also helps ensure continuity in the unlikely event\nthat a cluster fails. Specifies the minimum number of clusters for a multi-cluster warehouse. 1 to the value of MAX_CLUSTER_COUNT. The upper limit for MAX_CLUSTER_COUNT varies depending on the warehouse size. MIN_CLUSTER_COUNT must be equal to or less than MAX_CLUSTER_COUNT: If both parameters are equal, the warehouse runs in Maximized mode. If MIN_CLUSTER_COUNT is less than MAX_CLUSTER_COUNT, the warehouse runs in Auto-scale mode. For more information, including the upper limit for each warehouse size, see Multi-cluster warehouses . 1 Object parameter that specifies the policy for automatically starting and shutting down clusters in a multi-cluster warehouse\nrunning in Auto-scale mode. For a detailed description of this parameter, see Setting the scaling policy for a multi-cluster warehouse . Specifies the number of seconds of inactivity after which a warehouse is automatically suspended. Any integer 0 or greater, or NULL : The background process that suspends a warehouse runs approximately every 30 seconds and therefore, the setting for\nthis property isn’t intended for enabling precise control over warehouse suspension. Setting a value less than 30, or a value that isn’t a multiple of 30, is allowed but might not result in the expected\nbehavior due to the 30 second poll interval for warehouse suspension. Setting a 0 or NULL value means the warehouse never suspends. 600 (the warehouse suspends automatically after 10 minutes of inactivity) Important Setting AUTO_SUSPEND to 0 or NULL is not recommended, unless your query workloads require a continually running\nwarehouse. Note that this can result in significant consumption of credits (and corresponding charges), particularly for larger warehouses. For more details, see Warehouse considerations . Specifies whether to automatically resume a warehouse when a SQL statement (for example, query) is submitted to it. If FALSE , the warehouse\nonly starts again when explicitly resumed using ALTER WAREHOUSE or through the Snowflake web interface. TRUE : The warehouse resumes when a new query is submitted. FALSE : The warehouse only resumes when explicitly resumed using ALTER WAREHOUSE or through the Snowflake web interface. TRUE (the warehouse resumes automatically when a SQL statement is submitted to it) Not applicable when altering a warehouse Specifies the identifier of a resource monitor that is explicitly assigned to the warehouse. When a resource monitor is explicitly assigned\nto a warehouse, the monitor controls the monthly credits used by the warehouse (and all other warehouses to which the monitor is assigned). Any existing resource monitor. For more details, see Working with resource monitors . No value (no resource monitor assigned to the warehouse) Tip To view all resource monitors and their identifiers, use the SHOW RESOURCE MONITORS command. Adds a comment or overwrites an existing comment for the warehouse. Object parameter that specifies the concurrency level for SQL statements (i.e. queries and DML) executed by a warehouse cluster. When\nthe level is reached: For a single-cluster warehouse or a multi-cluster warehouse (in Maximized mode), additional statements are queued until resources\nare available. For a multi-cluster warehouse (in Auto-scale mode), additional clusters are started. This parameter can be used in conjunction with STATEMENT_QUEUED_TIMEOUT_IN_SECONDS to ensure a warehouse is never backlogged. For a detailed description of this parameter, see MAX_CONCURRENCY_LEVEL . Object parameter that specifies the time, in seconds, a SQL statement (query, DDL, DML, etc.) can be queued on a warehouse before it is\ncanceled by the system. This parameter can be used in conjunction with MAX_CONCURRENCY_LEVEL to ensure a warehouse is never backlogged. For a detailed description of this parameter, see STATEMENT_QUEUED_TIMEOUT_IN_SECONDS . Object parameter that specifies the time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For a detailed description of this parameter, see STATEMENT_TIMEOUT_IN_SECONDS . Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Specifies whether to enable the query acceleration service for queries that rely on\nthis warehouse for compute resources. Enterprise Edition Feature Query acceleration service requires Enterprise Edition (or higher).\nTo inquire about upgrading, please contact Snowflake Support . TRUE Enables Query Acceleration FALSE Disables Query Acceleration FALSE : Query Acceleration is disabled Specifies the maximum scale factor for leasing compute resources for query acceleration.  The scale factor is used as a multiplier based\non warehouse size . Setting the QUERY_ACCELERATION_MAX_SCALE_FACTOR to 0 eliminates the limit and allows queries to lease as many resources as necessary and\nas available to service the query. Regardless of the QUERY_ACCELERATION_MAX_SCALE_FACTOR value, the amount of available compute resources for query acceleration is bound by\nthe available resources in the service and the number of other concurrent requests. For more details, refer to Adjusting the scale factor . 0 to 100 8"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties and/or parameters to unset for the database, which resets them to the defaults: property_name param_name TAG tag_name [ , tag_name ... ] You can reset multiple properties/parameters with a single ALTER statement; however, each property/parameter must be separated by\na comma. Also, when resetting a property/parameter, you only specify the name; no value is required. Note UNSET can be used to unset all the properties and parameters for a warehouse, except WAREHOUSE_SIZE , which can only\nbe changed using SET ."
        },
        {
            "name": "WAREHOUSE_TYPE   =   {   STANDARD   |   'SNOWPARK-OPTIMIZED'   }",
            "description": "Specifies the warehouse type. STANDARD , 'STANDARD' 'SNOWPARK-OPTIMIZED' STANDARD Note To use a value that contains a hyphen ( 'SNOWPARK-OPTIMIZED' ), you must enclose the value in single quotes, as shown."
        },
        {
            "name": "Valid values :",
            "description": "STANDARD , 'STANDARD' 'SNOWPARK-OPTIMIZED'"
        },
        {
            "name": "Default :",
            "description": "STANDARD"
        },
        {
            "name": "WAREHOUSE_SIZE   =   string_constant",
            "description": "Specifies the size of the virtual warehouse. The size determines the amount of compute resources in each cluster and, therefore,\nthe number of credits consumed while the warehouse is running. For more information see Resizing a warehouse . XSMALL , 'X-SMALL' SMALL MEDIUM LARGE XLARGE , 'X-LARGE' XXLARGE , X2LARGE , '2X-LARGE' XXXLARGE , X3LARGE , '3X-LARGE' X4LARGE , '4X-LARGE' X5LARGE , '5X-LARGE' X6LARGE , '6X-LARGE' XSMALL Note The default size for Snowpark-optimized warehouses is MEDIUM. X5LARGE and X6LARGE sizes for Snowpark-optimized warehouses are only supported with the MEMORY_16X resource constraint. X5LARGE and X6LARGE sizes aren’t supported for standard warehouses that use the STANDARD_GEN_2 resource constraint. To use a value that contains a hyphen (for example, '2X-LARGE' ), you must enclose the value in single quotes, as shown. To block the immediate return of the ALTER WAREHOUSE command until the resize is complete, add the WAIT_FOR_COMPLETION parameter. The upper limit for the MAX_CLUSTER_COUNT property depends on the warehouse size. When you change WAREHOUSE_SIZE\nto a value higher than MEDIUM , you might need to reduce MAX_CLUSTER_COUNT at the same time. For the upper limit\non MAX_CLUSTER_COUNT for each warehouse size, see Upper limit on number of clusters for a multi-cluster warehouse . Larger warehouse sizes 5X-Large and 6X-Large are generally available in all Amazon Web Services (AWS) and Microsoft Azure regions. Larger warehouse sizes are in preview in US Government regions (requires FIPS support on ARM)."
        },
        {
            "name": "Valid values :",
            "description": "XSMALL , 'X-SMALL' SMALL MEDIUM LARGE XLARGE , 'X-LARGE' XXLARGE , X2LARGE , '2X-LARGE' XXXLARGE , X3LARGE , '3X-LARGE' X4LARGE , '4X-LARGE' X5LARGE , '5X-LARGE' X6LARGE , '6X-LARGE'"
        },
        {
            "name": "Default :",
            "description": "XSMALL"
        },
        {
            "name": "RESOURCE_CONSTRAINT   =   {   STANDARD_GEN_1   |   STANDARD_GEN_2   |   MEMORY_1X|   MEMORY_1X_x86   |   MEMORY_16X   |   MEMORY_16X_x86   |   MEMORY_64X   |   MEMORY_64X_x86   }",
            "description": "Preview Feature — Open The 1 TB resource constraints (MEMORY_64X and MEMORY_64X_x86) are available as a preview feature.\nThe 1 TB constraints are  available only on the Amazon Web Services (AWS) cloud platform. All other MEMORY_* resource constraint sizes are generally available and are available for all cloud platforms. Specifies the memory and CPU architecture for Snowpark-optimized warehouses ,\nor generation 1 or generation 2 capabilities for standard warehouses . The following table includes the valid values for the property, available memory, CPU architecture, and the minimum warehouse\nsize required for the resource_constraint setting.\nFor more information about regions and cloud service providers where generation 2 standard warehouses\nare available, see Snowflake generation 2 standard warehouses . Value Memory (up to) CPU architecture Min warehouse size required Max warehouse size STANDARD_GEN_1 16 GB Standard XSMALL X6LARGE STANDARD_GEN_2 16 GB Standard (generation 2) XSMALL X4LARGE MEMORY_1X 16 GB Standard XSMALL X4LARGE MEMORY_1X_x86 16 GB x86 XSMALL X4LARGE MEMORY_16X 256 GB Standard MEDIUM X6LARGE MEMORY_16X_x86 256 GB x86 MEDIUM X4LARGE MEMORY_64X 1 TB Standard LARGE X4LARGE MEMORY_64X_x86 1 TB x86 LARGE X4LARGE MEMORY_16X for Snowpark-optimized warehouses. For standard warehouses, the default depends on\nGen2 support for your cloud service provider region and whether your organization was created after\nGen2 support became available in that region. For more information, see Default value for the RESOURCE_CONSTRAINT for standard warehouses ."
        },
        {
            "name": "Valid values :",
            "description": ""
        },
        {
            "name": "Default value :",
            "description": "MEMORY_16X for Snowpark-optimized warehouses. For standard warehouses, the default depends on\nGen2 support for your cloud service provider region and whether your organization was created after\nGen2 support became available in that region. For more information, see Default value for the RESOURCE_CONSTRAINT for standard warehouses ."
        },
        {
            "name": "WAIT_FOR_COMPLETION   =   {   TRUE   |   FALSE   }",
            "description": "When resizing a warehouse, you can use this parameter to block the return of the ALTER WAREHOUSE command until the resize has finished\nprovisioning all its compute resources. Blocking the return of the command when resizing to a larger warehouse serves to notify you\nthat your compute resources have been fully provisioned and the warehouse is now ready to execute queries using all the new resources. TRUE : The ALTER WAREHOUSE command will block until the warehouse resize completes. FALSE : The ALTER WAREHOUSE command returns immediately, before the warehouse resize completes. FALSE Note The value of this parameter isn’t persisted and must be set to TRUE on every execution if you want the warehouse resizing to\ncomplete before this command returns. If set to TRUE and you abort the ALTER WAREHOUSE command, only the waiting is aborted and the warehouse resize will go\nthrough. To resize the warehouse back to its original size, you will need to execute another ALTER WAREHOUSE command. This parameter must be used with the WAREHOUSE_SIZE parameter, otherwise an exception will be thrown."
        },
        {
            "name": "Valid values :",
            "description": "TRUE : The ALTER WAREHOUSE command will block until the warehouse resize completes. FALSE : The ALTER WAREHOUSE command returns immediately, before the warehouse resize completes."
        },
        {
            "name": "Default :",
            "description": "FALSE"
        },
        {
            "name": "MAX_CLUSTER_COUNT   =   num",
            "description": "Specifies the maximum number of clusters for a multi-cluster warehouse. For a single-cluster warehouse, this value is always 1 . 1 to an upper limit that varies depending on warehouse size. Note that specifying a value greater than 1 indicates the warehouse is a multi-cluster warehouse; however, the value can\nonly be set to a higher value in Snowflake Enterprise Edition (or higher). The upper limit for the MAX_CLUSTER_COUNT property depends on the warehouse size. When you change WAREHOUSE_SIZE\nto a value higher than MEDIUM, you might need to reduce MAX_CLUSTER_COUNT at the same time. For the upper limit\non MAX_CLUSTER_COUNT for each warehouse size, see Upper limit on number of clusters for a multi-cluster warehouse . For more information about multi-cluster warehouses, see Multi-cluster warehouses . 1 (single-cluster warehouse) Tip For Snowflake Enterprise Edition (or higher), we recommend always setting the value greater than 1 to help maintain\nhigh-availability and optimal performance of the (multi-cluster) warehouse. This also helps ensure continuity in the unlikely event\nthat a cluster fails."
        },
        {
            "name": "Valid values :",
            "description": "1 to an upper limit that varies depending on warehouse size. Note that specifying a value greater than 1 indicates the warehouse is a multi-cluster warehouse; however, the value can\nonly be set to a higher value in Snowflake Enterprise Edition (or higher). The upper limit for the MAX_CLUSTER_COUNT property depends on the warehouse size. When you change WAREHOUSE_SIZE\nto a value higher than MEDIUM, you might need to reduce MAX_CLUSTER_COUNT at the same time. For the upper limit\non MAX_CLUSTER_COUNT for each warehouse size, see Upper limit on number of clusters for a multi-cluster warehouse . For more information about multi-cluster warehouses, see Multi-cluster warehouses ."
        },
        {
            "name": "Default :",
            "description": "1 (single-cluster warehouse)"
        },
        {
            "name": "MIN_CLUSTER_COUNT   =   num",
            "description": "Specifies the minimum number of clusters for a multi-cluster warehouse. 1 to the value of MAX_CLUSTER_COUNT. The upper limit for MAX_CLUSTER_COUNT varies depending on the warehouse size. MIN_CLUSTER_COUNT must be equal to or less than MAX_CLUSTER_COUNT: If both parameters are equal, the warehouse runs in Maximized mode. If MIN_CLUSTER_COUNT is less than MAX_CLUSTER_COUNT, the warehouse runs in Auto-scale mode. For more information, including the upper limit for each warehouse size, see Multi-cluster warehouses . 1"
        },
        {
            "name": "Valid values :",
            "description": "1 to the value of MAX_CLUSTER_COUNT. The upper limit for MAX_CLUSTER_COUNT varies depending on the warehouse size. MIN_CLUSTER_COUNT must be equal to or less than MAX_CLUSTER_COUNT: If both parameters are equal, the warehouse runs in Maximized mode. If MIN_CLUSTER_COUNT is less than MAX_CLUSTER_COUNT, the warehouse runs in Auto-scale mode. For more information, including the upper limit for each warehouse size, see Multi-cluster warehouses ."
        },
        {
            "name": "Default :",
            "description": "1"
        },
        {
            "name": "SCALING_POLICY   =   {   STANDARD   |   ECONOMY   }",
            "description": "Object parameter that specifies the policy for automatically starting and shutting down clusters in a multi-cluster warehouse\nrunning in Auto-scale mode. For a detailed description of this parameter, see Setting the scaling policy for a multi-cluster warehouse ."
        },
        {
            "name": "AUTO_SUSPEND   =   {   num   |   NULL   }",
            "description": "Specifies the number of seconds of inactivity after which a warehouse is automatically suspended. Any integer 0 or greater, or NULL : The background process that suspends a warehouse runs approximately every 30 seconds and therefore, the setting for\nthis property isn’t intended for enabling precise control over warehouse suspension. Setting a value less than 30, or a value that isn’t a multiple of 30, is allowed but might not result in the expected\nbehavior due to the 30 second poll interval for warehouse suspension. Setting a 0 or NULL value means the warehouse never suspends. 600 (the warehouse suspends automatically after 10 minutes of inactivity) Important Setting AUTO_SUSPEND to 0 or NULL is not recommended, unless your query workloads require a continually running\nwarehouse. Note that this can result in significant consumption of credits (and corresponding charges), particularly for larger warehouses. For more details, see Warehouse considerations ."
        },
        {
            "name": "Valid values :",
            "description": "Any integer 0 or greater, or NULL : The background process that suspends a warehouse runs approximately every 30 seconds and therefore, the setting for\nthis property isn’t intended for enabling precise control over warehouse suspension. Setting a value less than 30, or a value that isn’t a multiple of 30, is allowed but might not result in the expected\nbehavior due to the 30 second poll interval for warehouse suspension. Setting a 0 or NULL value means the warehouse never suspends."
        },
        {
            "name": "Default :",
            "description": "600 (the warehouse suspends automatically after 10 minutes of inactivity)"
        },
        {
            "name": "AUTO_RESUME   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to automatically resume a warehouse when a SQL statement (for example, query) is submitted to it. If FALSE , the warehouse\nonly starts again when explicitly resumed using ALTER WAREHOUSE or through the Snowflake web interface. TRUE : The warehouse resumes when a new query is submitted. FALSE : The warehouse only resumes when explicitly resumed using ALTER WAREHOUSE or through the Snowflake web interface. TRUE (the warehouse resumes automatically when a SQL statement is submitted to it)"
        },
        {
            "name": "Valid values :",
            "description": "TRUE : The warehouse resumes when a new query is submitted. FALSE : The warehouse only resumes when explicitly resumed using ALTER WAREHOUSE or through the Snowflake web interface."
        },
        {
            "name": "Default :",
            "description": "TRUE (the warehouse resumes automatically when a SQL statement is submitted to it)"
        },
        {
            "name": "INITIALLY_SUSPENDED   =   {   TRUE   |   FALSE   }",
            "description": "Not applicable when altering a warehouse"
        },
        {
            "name": "RESOURCE_MONITOR   =   monitor_name",
            "description": "Specifies the identifier of a resource monitor that is explicitly assigned to the warehouse. When a resource monitor is explicitly assigned\nto a warehouse, the monitor controls the monthly credits used by the warehouse (and all other warehouses to which the monitor is assigned). Any existing resource monitor. For more details, see Working with resource monitors . No value (no resource monitor assigned to the warehouse) Tip To view all resource monitors and their identifiers, use the SHOW RESOURCE MONITORS command."
        },
        {
            "name": "Valid values :",
            "description": "Any existing resource monitor. For more details, see Working with resource monitors ."
        },
        {
            "name": "Default :",
            "description": "No value (no resource monitor assigned to the warehouse)"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the warehouse."
        },
        {
            "name": "MAX_CONCURRENCY_LEVEL   =   num",
            "description": "Object parameter that specifies the concurrency level for SQL statements (i.e. queries and DML) executed by a warehouse cluster. When\nthe level is reached: For a single-cluster warehouse or a multi-cluster warehouse (in Maximized mode), additional statements are queued until resources\nare available. For a multi-cluster warehouse (in Auto-scale mode), additional clusters are started. This parameter can be used in conjunction with STATEMENT_QUEUED_TIMEOUT_IN_SECONDS to ensure a warehouse is never backlogged. For a detailed description of this parameter, see MAX_CONCURRENCY_LEVEL ."
        },
        {
            "name": "STATEMENT_QUEUED_TIMEOUT_IN_SECONDS   =   num",
            "description": "Object parameter that specifies the time, in seconds, a SQL statement (query, DDL, DML, etc.) can be queued on a warehouse before it is\ncanceled by the system. This parameter can be used in conjunction with MAX_CONCURRENCY_LEVEL to ensure a warehouse is never backlogged. For a detailed description of this parameter, see STATEMENT_QUEUED_TIMEOUT_IN_SECONDS ."
        },
        {
            "name": "STATEMENT_TIMEOUT_IN_SECONDS   =   num",
            "description": "Object parameter that specifies the time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For a detailed description of this parameter, see STATEMENT_TIMEOUT_IN_SECONDS ."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "ENABLE_QUERY_ACCELERATION   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to enable the query acceleration service for queries that rely on\nthis warehouse for compute resources. Enterprise Edition Feature Query acceleration service requires Enterprise Edition (or higher).\nTo inquire about upgrading, please contact Snowflake Support . TRUE Enables Query Acceleration FALSE Disables Query Acceleration FALSE : Query Acceleration is disabled"
        },
        {
            "name": "Valid values :",
            "description": "TRUE Enables Query Acceleration FALSE Disables Query Acceleration"
        },
        {
            "name": "Default :",
            "description": "FALSE : Query Acceleration is disabled"
        },
        {
            "name": "QUERY_ACCELERATION_MAX_SCALE_FACTOR   =   num",
            "description": "Specifies the maximum scale factor for leasing compute resources for query acceleration.  The scale factor is used as a multiplier based\non warehouse size . Setting the QUERY_ACCELERATION_MAX_SCALE_FACTOR to 0 eliminates the limit and allows queries to lease as many resources as necessary and\nas available to service the query. Regardless of the QUERY_ACCELERATION_MAX_SCALE_FACTOR value, the amount of available compute resources for query acceleration is bound by\nthe available resources in the service and the number of other concurrent requests. For more details, refer to Adjusting the scale factor . 0 to 100 8"
        },
        {
            "name": "Valid values :",
            "description": "0 to 100"
        },
        {
            "name": "Default :",
            "description": "8"
        }
    ],
    "usage_notes": "A warehouse does not need to be suspended to set or change any of its properties, except for type.\nTo change the warehouse type, the warehouse must be in the suspended state. Execute the following statement to suspend\na warehouse:\nWhen the warehouse size is changed, the change doesn’t impact any statements, including queries, that are currently executing. Once the\nstatements complete, and the compute resources are fully provisioned, the new size is used for all subsequent statements.\nSuspending a warehouse does not abort any queries being processed by the warehouse at the time it is suspended. Instead, the\nwarehouse completes the queries, then shuts down the compute resources used to process the queries. During this time period, the warehouse\nis in quiescing mode. When all the compute resources are shut down, the warehouse’s status changes to Suspended.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nResuming a Snowpark-optimized virtual warehouse may take longer than standard warehouses.\nSnowpark-optimized warehouses don’t support Query Acceleration.\nSpecifying the IF EXISTS clause requires the role in use or a role in the active role hierarchy to have the appropriate\nwarehouse privileges on the warehouse."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-join-policy",
    "title": "ALTER JOIN POLICY",
    "description": "Replaces the existing rules or comment for a join policy. Also allows you to rename a join policy.",
    "syntax": "ALTER JOIN POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER JOIN POLICY [ IF EXISTS ] <name> SET BODY -> <expression>\n\nALTER JOIN POLICY <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER JOIN POLICY <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER JOIN POLICY [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER JOIN POLICY [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER JOIN POLICY jp3 SET BODY -> JOIN_CONSTRAINT(JOIN_REQUIRED => FALSE);"
        },
        {
            "code": "ALTER JOIN POLICY my_join_policy RENAME TO my_join_policy_2;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the join policy to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the join policy; must be unique for your schema. The new identifier cannot be used if the\nidentifier is already in place for a different join policy. For more information, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the join policy: SQL expression that determines the restrictions of a join policy. To define the body of the join policy, call the JOIN_CONSTRAINT function, which returns TRUE or FALSE.\nWhen the function returns TRUE, queries are required to use a join to return results. The syntax of the JOIN_CONSTRAINT function is: Where: Specifies whether a join is required in queries when data is selected from tables or views that have\nthe join policy assigned to them. The body of a policy cannot reference user-defined functions, tables, or views. Allowed join columns are specified in the CREATE or ALTER statement for the table or view to which the\npolicy is applied, not in the CREATE JOIN POLICY statement. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites the existing comment for the join policy. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties and/or parameters to unset, by resetting them to their defaults, for the join policy: TAG tag_name [ , tag_name ... ] COMMENT When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "BODY   ->   expression",
            "description": "SQL expression that determines the restrictions of a join policy. To define the body of the join policy, call the JOIN_CONSTRAINT function, which returns TRUE or FALSE.\nWhen the function returns TRUE, queries are required to use a join to return results. The syntax of the JOIN_CONSTRAINT function is: Where: Specifies whether a join is required in queries when data is selected from tables or views that have\nthe join policy assigned to them. The body of a policy cannot reference user-defined functions, tables, or views. Allowed join columns are specified in the CREATE or ALTER statement for the table or view to which the\npolicy is applied, not in the CREATE JOIN POLICY statement."
        },
        {
            "name": "JOIN_REQUIRED   =>   boolean_expression",
            "description": "Specifies whether a join is required in queries when data is selected from tables or views that have\nthe join policy assigned to them."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the join policy. Default: No value"
        }
    ],
    "usage_notes": "If you want to update an existing join policy and need to see the current body of the policy, run the\nDESCRIBE JOIN POLICY command. You can also use the GET_DDL function to obtain the full definition of the join policy, including its body.\nMoving a join policy to a managed access schema\n(using the ALTER JOIN POLICY … RENAME TO syntax) is prohibited unless the join policy owner\n(that is, the role that has the OWNERSHIP privilege on the join policy) also owns the target schema.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-network-policy",
    "title": "ALTER NETWORK POLICY",
    "description": "Modifies the properties for an existing network policy. Currently, the only supported actions are changing the IP addresses that are allowed or\ndenied access to the account and adding/overwriting/removing a comment for a network policy.",
    "syntax": "ALTER NETWORK POLICY [ IF EXISTS ] <name> SET {\n    [ ALLOWED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n    [ BLOCKED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n    [ ALLOWED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' ... ] ) ]\n    [ BLOCKED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' ... ] ) ]\n    [ COMMENT = '<string_literal>' ] }\n\nALTER NETWORK POLICY [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER NETWORK POLICY <name> ADD { ALLOWED_NETWORK_RULE_LIST = '<network_rule>' | BLOCKED_NETWORK_RULE_LIST = '<network_rule>' }\n\nALTER NETWORK POLICY <name> REMOVE { ALLOWED_NETWORK_RULE_LIST = '<network_rule>' | BLOCKED_NETWORK_RULE_LIST = '<network_rule>' }\n\nALTER NETWORK POLICY <name>  RENAME TO <new_name>\n\nALTER NETWORK POLICY <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER NETWORK POLICY <name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER NETWORK POLICY [ IF EXISTS ] <name> SET {\n    [ ALLOWED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n    [ BLOCKED_NETWORK_RULE_LIST = ( '<network_rule>' [ , '<network_rule>' , ... ] ) ]\n    [ ALLOWED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' ... ] ) ]\n    [ BLOCKED_IP_LIST = ( [ '<ip_address>' ] [ , '<ip_address>' ... ] ) ]\n    [ COMMENT = '<string_literal>' ] }\n\nALTER NETWORK POLICY [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER NETWORK POLICY <name> ADD { ALLOWED_NETWORK_RULE_LIST = '<network_rule>' | BLOCKED_NETWORK_RULE_LIST = '<network_rule>' }\n\nALTER NETWORK POLICY <name> REMOVE { ALLOWED_NETWORK_RULE_LIST = '<network_rule>' | BLOCKED_NETWORK_RULE_LIST = '<network_rule>' }\n\nALTER NETWORK POLICY <name>  RENAME TO <new_name>\n\nALTER NETWORK POLICY <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER NETWORK POLICY <name> UNSET TAG <tag_name> [ , <tag_name> ... ]"
        },
        {
            "code": "DESC NETWORK POLICY allow_access_policy;"
        },
        {
            "code": "+---------------------------+-------------------+\n| name                      | value             |\n|---------------------------+-------------------|\n| ALLOWED_NETWORK_RULE_LIST | allow_access_rule |\n+---------------------------+-------------------+"
        },
        {
            "code": "ALTER NETWORK POLICY IF EXISTS allow_access_policy SET\n  BLOCKED_NETWORK_RULE_LIST = ('block_access_rule');\nDESC NETWORK POLICY allow_access_policy;"
        },
        {
            "code": "+---------------------------+-------------------+\n| name                      | value             |\n|---------------------------+-------------------|\n| ALLOWED_NETWORK_RULE_LIST | ALLOW_ACCESS_RULE |\n| BLOCKED_NETWORK_RULE_LIST | BLOCK_ACCESS_RULE |\n+---------------------------+-------------------+"
        },
        {
            "code": "ALTER NETWORK POLICY allow_access_policy RENAME TO limit_access_policy;"
        },
        {
            "code": "ALTER NETWORK POLICY limit_access_policy SET COMMENT = 'No_Lists_See_Rules';\nSHOW NETWORK POLICIES;"
        },
        {
            "code": "+-------------------------------+---------------------+--------------------+----------------------------+----------------------------+----------------------------------+----------------------------------+\n| created on                    | name                | comment            | entries_in_allowed_ip_list | entries_in_blocked_ip_list | entries_in_allowed_network_rules | entries_in_blocked_network_rules |\n|-------------------------------+------------------------------------------|----------------------------|----------------------------|----------------------------------|----------------------------------|\n|...                            |                     |                    |                            |                            |                                  |                                  |\n|-------------------------------+------------------------------------------|----------------------------|----------------------------|----------------------------------|----------------------------------|\n| 2024-12-04 10:33:19.853 -0800 | LIMIT_ACCESS_POLICY | NO_LISTS_SEE_RULES |                           0|                           0|                                 1|                                 1|\n|-------------------------------+------------------------------------------|----------------------------|----------------------------|----------------------------------|----------------------------------|\n|...                            |                     |                    |                            |                            |                                  |                                  |\n+-------------------------------+---------------------+--------------------+----------------------------+----------------------------+----------------------------------+----------------------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the network policy to alter. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the parameter to set for the network policy: Specifies a list of network rules that contain the network identifiers that are allowed access to\nSnowflake. There is no limit on the number of network rules in the list. Replaces existing network rules in the allowed list. To add network rules without replacing existing ones, use the ALTER NETWORK POLICY ... ADD command. Specifies a list of network rules that contain the network identifiers that are denied access to Snowflake. There is no limit on the\nnumber of network rules in the list. Replaces existing network rules in the blocked list. To add network rules without replacing existing ones, use the ALTER NETWORK POLICY ... ADD command. Specifies a list of IPv4 addresses that are allowed access to your Snowflake account. This is referred to as the allowed list . Snowflake recommends using network rules in conjunction with network policies rather than using this property. Use the ALLOWED_NETWORK_RULE_LIST property to specify network rules that contain IPv4 addresses. If you are not yet using network rules, specify at least one IPv4 address or CIDR block range to allow access to your Snowflake\naccount. Additionally, if you are not using network rules and this property is specified with an empty list, no IPv4 addresses are\nallowed to access your Snowflake account. Specifies a list of IPv4 addresses that are denied access to your Snowflake account. This is referred to as the blocked list .\nTo unset this parameter, specify a different CIDR block range, a series of IPv4 addresses, or a single IPv4 address. Snowflake recommends using network rules in conjunction with network policies rather than using this parameter. Use the BLOCKED_NETWORK_RULE_LIST property to specify network rules that contain IPv4 addresses. To block public access, use a network rule and add the network rule to the BLOCKED_NETWORK_RULE_LIST property. The result is\nthat only IP addresses that use private connectivity, such as AWS PrivateLink, can access your Snowflake account. Default: No value; no IP addresses in ALLOWED_IP_LIST property are blocked. Adds a comment or overwrites an existing comment for the network policy. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the network policy, which resets them to the defaults: COMMENT , which removes the comment, if one exists, for the network policy. TAG tag_name [ , tag_name ... ]"
        },
        {
            "name": "ALLOWED_NETWORK_RULE_LIST   =   (   ' network_rule '   [   ,   ' network_rule '   ,   ...   ]   )",
            "description": "Specifies a list of network rules that contain the network identifiers that are allowed access to\nSnowflake. There is no limit on the number of network rules in the list. Replaces existing network rules in the allowed list. To add network rules without replacing existing ones, use the ALTER NETWORK POLICY ... ADD command."
        },
        {
            "name": "BLOCKED_NETWORK_RULE_LIST   =   (   ' network_rule '   [   ,   ' network_rule '   ,   ...   ]   )",
            "description": "Specifies a list of network rules that contain the network identifiers that are denied access to Snowflake. There is no limit on the\nnumber of network rules in the list. Replaces existing network rules in the blocked list. To add network rules without replacing existing ones, use the ALTER NETWORK POLICY ... ADD command."
        },
        {
            "name": "ALLOWED_IP_LIST   =   (   [   ip_address   ]   [   ,   ip_address   ,   ...   ]   )",
            "description": "Specifies a list of IPv4 addresses that are allowed access to your Snowflake account. This is referred to as the allowed list . Snowflake recommends using network rules in conjunction with network policies rather than using this property. Use the ALLOWED_NETWORK_RULE_LIST property to specify network rules that contain IPv4 addresses. If you are not yet using network rules, specify at least one IPv4 address or CIDR block range to allow access to your Snowflake\naccount. Additionally, if you are not using network rules and this property is specified with an empty list, no IPv4 addresses are\nallowed to access your Snowflake account."
        },
        {
            "name": "BLOCKED_IP_LIST   =   (   [   ip_address   ]   [   ,   ip_address   ,   ...   ]   )",
            "description": "Specifies a list of IPv4 addresses that are denied access to your Snowflake account. This is referred to as the blocked list .\nTo unset this parameter, specify a different CIDR block range, a series of IPv4 addresses, or a single IPv4 address. Snowflake recommends using network rules in conjunction with network policies rather than using this parameter. Use the BLOCKED_NETWORK_RULE_LIST property to specify network rules that contain IPv4 addresses. To block public access, use a network rule and add the network rule to the BLOCKED_NETWORK_RULE_LIST property. The result is\nthat only IP addresses that use private connectivity, such as AWS PrivateLink, can access your Snowflake account. Default: No value; no IP addresses in ALLOWED_IP_LIST property are blocked."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the network policy."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "ADD   {   ALLOWED_NETWORK_RULE_LIST   =   ' network_rule '   |   BLOCKED_NETWORK_RULE_LIST   =   ' network_rule '   }",
            "description": "Adds a network rule to the allowed or blocked list of the network policy without removing existing ones."
        },
        {
            "name": "REMOVE   {   ALLOWED_NETWORK_RULE_LIST   =   ' network_rule '   |   BLOCKED_NETWORK_RULE_LIST   =   ' network_rule '   }",
            "description": "Removes a network rule from the allowed or blocked list of the network policy."
        },
        {
            "name": "RENAME   TO   ...",
            "description": "Specifies a new name for the existing network policy."
        }
    ],
    "usage_notes": "Do not modify a network policy to have empty ALLOWED_IP_LIST and BLOCKED_IP_LIST properties. Use network rules in\nconjunction with the network policy to manage access to your Snowflake account.\nThe SET action for the allowed/blocked lists is not additive (that is, it removes all IP addresses in the existing lists\nfor the network policy and replaces them with the specified lists).\nAs a result, to make additions to the existing lists, you must specify the new IP addresses and replicate the existing lists.\nEach ip_address can cover a range of addresses using Classless Inter-Domain Routing (CIDR) notation:\nip_address[/optional_prefix_length]\nFor example:\n192.168.1.0/24\nWhen a network policy includes values for both ALLOWED_IP_LIST and BLOCKED_IP_LIST, Snowflake applies the blocked list\nfirst.\nDo not add 0.0.0.0/0 to BLOCKED_IP_LIST. Because Snowflake applies the blocked list first, this would block your own\naccess. Additionally, in order to block all IP addresses except a select list, you only need to add IP addresses to ALLOWED_IP_LIST.\nSnowflake automatically blocks all IP addresses not included in the allowed list.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-procedure",
    "title": "ALTER PROCEDURE",
    "description": "Modifies the properties for an existing stored procedure. If you need to make any changes not supported here, use DROP PROCEDURE\ninstead and then recreate the stored procedure.",
    "syntax": "ALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = '<integration_name>' [ , '<integration_name>' ... ] ]\n  [ SECRETS = '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = '<integration_name>' [ , '<integration_name>' ... ] ]\n  [ SECRETS = '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = '<integration_name>' [ , '<integration_name>' ... ] ]\n  [ SECRETS = '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ AUTO_EVENT_LOGGING = '<option>' ]\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET COMMENT\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PROCEDURE [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) EXECUTE AS { OWNER | CALLER | RESTRICTED CALLER }",
    "examples": [
        {
            "code": "ALTER PROCEDURE IF EXISTS procedure1(FLOAT) RENAME TO procedure2;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the stored procedure to alter. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "arg_data_type   [   ,   ...   ]",
            "description": "Specifies the data type of the argument(s) for the stored procedure, if it has arguments. The argument types are required because stored\nprocedures support name overloading (i.e. two stored procedures in the same schema can have the same name) and the argument types are used to\nidentify the procedure you wish to alter."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the stored procedure; the combination of the identifier and existing argument data types must be unique for\nthe schema. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the stored procedure. (For Snowflake Scripting stored procedures only) Controls whether additional Snowflake Scripting log messages and trace events are\ningested automatically into the event table . For information about the options, see AUTO_EVENT_LOGGING . Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing . Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing . The names of external access integrations needed in order for this\nprocedure’s handler code to access external networks. An external access integration contains network rules and secrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview . Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameter’s value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an external access integration’s ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integration’s name must, in turn, be specified as a value of this CREATE PROCEDURE call’s\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. Adds a comment or overwrites the existing comment for the stored procedure. The value you specify is displayed in the DESCRIPTION column in the output for SHOW PROCEDURES . Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the stored procedure, which resets them to the defaults. Currently, the only properties you can unset are: COMMENT , which removes the comment, if any, for the procedure. TAG tag_name [ , tag_name ... ]"
        },
        {
            "name": "EXECUTE   AS   {   OWNER   |   CALLER   |   RESTRICTED   CALLER   }",
            "description": "Preview Feature — Open Restricted caller’s rights ( EXECUTE AS RESTRICTED CALLER ) is a preview feature available to all accounts. Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with\nthe privileges of the caller (a “caller’s rights” stored procedure): If you execute ALTER PROCEDURE … EXECUTE AS OWNER, then in the future the procedure will execute as an owner’s rights procedure. If you execute the statement ALTER PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a\ncaller’s rights procedure. If you execute the statement ALTER PROCEDURE … EXECUTE AS RESTRICTED CALLER, then in the future the procedure will execute as a\ncaller’s rights procedure, but might not be able to run with all of the caller’s privileges. For more information, see Restricted caller’s rights . If EXECUTE AS ... isn’t specified, the procedure runs as an owner’s rights stored procedure. Owner’s rights stored\nprocedures have less access to the caller’s environment (for example, the caller’s session variables), and Snowflake defaults to this\nhigher level of privacy and security. For more information, see Understanding caller’s rights and owner’s rights stored procedures . Default: EXECUTE AS OWNER"
        },
        {
            "name": "AUTO_EVENT_LOGGING   =   ' option '",
            "description": "(For Snowflake Scripting stored procedures only) Controls whether additional Snowflake Scripting log messages and trace events are\ningested automatically into the event table . For information about the options, see AUTO_EVENT_LOGGING ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nprocedure’s handler code to access external networks. An external access integration contains network rules and secrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview ."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameter’s value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an external access integration’s ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integration’s name must, in turn, be specified as a value of this CREATE PROCEDURE call’s\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the stored procedure. The value you specify is displayed in the DESCRIPTION column in the output for SHOW PROCEDURES ."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-notification-integration",
    "title": "ALTER NOTIFICATION INTEGRATION",
    "description": "Modifies the properties for an existing notification integration."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-packages-policy",
    "title": "ALTER PACKAGES POLICY",
    "description": "Modifies the properties for an existing packages policy.",
    "syntax": "ALTER PACKAGES POLICY [ IF EXISTS ] <name> SET\n  [ ALLOWLIST = ( [ '<packageSpec>' ] [ , '<packageSpec>' ... ] ) ]\n  [ BLOCKLIST = ( [ '<packageSpec>' ] [ , '<packageSpec>' ... ] ) ]\n  [ ADDITIONAL_CREATION_BLOCKLIST = ( [ '<packageSpec>' ] [ , '<packageSpec>' ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER PACKAGES POLICY [ IF EXISTS ] <name> UNSET\n  [ ALLOWLIST ]\n  [ BLOCKLIST ]\n  [ ADDITIONAL_CREATION_BLOCKLIST ]\n  [ COMMENT ]",
    "examples": [
        {
            "code": "ALTER PACKAGES POLICY packages_policy_prod_1 SET ALLOWLIST = ('pandas==1.2.3');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the packages policy to alter. If the identifier contains spaces or special characters,\nthe entire string must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties to set for the packages policy. Specifies a list of package specs that are allowed. Default: ('*') (i.e. allow all packages). Specifies a list of package specs that are blocked. To unset this parameter, specify an empty list. Default: () (i.e. do not block any packages). Specifies a list of package specs that are blocked at creation time. To unset this parameter, specify an empty list.\nIf the ADDITIONAL_CREATION_BLOCKLIST is set, it is appended to the basic BLOCKLIST at the creation time.\nFor temporary UDFs and anonymous stored procedures, the ADDITIONAL_CREATION_BLOCKLIST is appended to the basic BLOCKLIST at both creation and execution time. Default: () (i.e. do not block any packages). Adds a comment or overwrites an existing comment for the packages policy."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties to unset for the packages policy, which resets them to the defaults: ALLOWLIST BLOCKLIST ADDITIONAL_CREATION_BLOCKLIST COMMENT You can reset multiple properties with a single ALTER statement; however, each property must be separated by a comma. When resetting\na property, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "ALLOWLIST   =   (   [   ' packageSpec '   ]   [   ,   ' packageSpec '   ...   ]   )",
            "description": "Specifies a list of package specs that are allowed. Default: ('*') (i.e. allow all packages)."
        },
        {
            "name": "BLOCKLIST   =   (   [   ' packageSpec '   ]   [   ,   ' packageSpec '   ...   ]   )",
            "description": "Specifies a list of package specs that are blocked. To unset this parameter, specify an empty list. Default: () (i.e. do not block any packages)."
        },
        {
            "name": "ADDITIONAL_CREATION_BLOCKLIST   =   (   [   ' packageSpec '   ]   [   ,   ' packageSpec '   ...   ]   )",
            "description": "Specifies a list of package specs that are blocked at creation time. To unset this parameter, specify an empty list.\nIf the ADDITIONAL_CREATION_BLOCKLIST is set, it is appended to the basic BLOCKLIST at the creation time.\nFor temporary UDFs and anonymous stored procedures, the ADDITIONAL_CREATION_BLOCKLIST is appended to the basic BLOCKLIST at both creation and execution time. Default: () (i.e. do not block any packages)."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the packages policy."
        }
    ],
    "usage_notes": "If you want to update an existing packages policy and need to see the current definition of the policy, call the\nGET_DDL function or run\nthe DESCRIBE PACKAGES POLICY command.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-model-add-version",
    "title": "ALTER MODEL … ADD VERSION",
    "description": "Adds a new version to an existing model from an existing model version. Versions are the actual model code that contains\nmethods that can be called to perform inference and other functions.",
    "syntax": "ALTER MODEL [ IF EXISTS ] <name> ADD VERSION <version_name>\n  FROM MODEL <source_model_name> [ VERSION <source_version_name> ]",
    "examples": [
        {
            "code": "ALTER MODEL [ IF EXISTS ] <name> ADD VERSION <version_name>\n  FROM MODEL <source_model_name> [ VERSION <source_version_name> ]"
        },
        {
            "code": "ALTER MODEL [ IF EXISTS ] <name> ADD VERSION <version_name> FROM internalStage"
        },
        {
            "code": "internalStage ::=\n    @[<namespace>.]<int_stage_name>[/<path>]\n| @[<namespace>.]%<table_name>[/<path>]\n| @~[/<path>]"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier of the model. If the identifier contains spaces, special characters, or mixed-case\ncharacters, the entire identifier must be enclosed in double quotes. Identifiers enclosed in double quotes are also\ncase-sensitive. For information on identifier syntax, see Identifier requirements ."
        },
        {
            "name": "ADD   VERSION   version_name",
            "description": "Specifies the identifier of the version, which must be unique within the model. If the identifier contains\nspaces, special characters, or mixed-case characters, the entire identifier must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For information on identifier syntax, see Identifier requirements ."
        },
        {
            "name": "FROM   MODEL   source_model_name   [   VERSION   source_version_or_alias_name   ]",
            "description": "Specifies the name of the model from which the version will be obtained. To obtain a specific version of that model, specify the VERSION source_version_or_alias_name clause. If\nyou omit this clause, the command obtains the default version of the source model."
        },
        {
            "name": "FROM   internalStage",
            "description": "Specifies the internal stage that contains the version’s files."
        },
        {
            "name": "Required if not using FROM internalStage variant",
            "description": "Specifies the name of the model from which the version will be obtained. To obtain a specific version of that model, specify the VERSION source_version_or_alias_name clause. If\nyou omit this clause, the command obtains the default version of the source model."
        },
        {
            "name": "Required if  using FROM internalStage variant",
            "description": "Specifies the internal stage that contains the version’s files."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-table",
    "title": "ALTER TABLE",
    "description": "Modifies the properties, columns, or constraints for an existing table.",
    "syntax": "ALTER TABLE [ IF EXISTS ] <name> RENAME TO <new_table_name>\n\nALTER TABLE [ IF EXISTS ] <name> SWAP WITH <target_table_name>\n\nALTER TABLE [ IF EXISTS ] <name> { clusteringAction | tableColumnAction | constraintAction  }\n\nALTER TABLE [ IF EXISTS ] <name> dataMetricFunctionAction\n\nALTER TABLE [ IF EXISTS ] <name> dataGovnPolicyTagAction\n\nALTER TABLE [ IF EXISTS ] <name> extTableColumnAction\n\nALTER TABLE [ IF EXISTS ] <name> searchOptimizationAction\n\nALTER TABLE [ IF EXISTS ] <name> SET\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE  } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER TABLE [ IF EXISTS ] <name> UNSET {\n                                       DATA_RETENTION_TIME_IN_DAYS         |\n                                       MAX_DATA_EXTENSION_TIME_IN_DAYS     |\n                                       CHANGE_TRACKING                     |\n                                       DEFAULT_DDL_COLLATION               |\n                                       ENABLE_SCHEMA_EVOLUTION             |\n                                       CONTACT <purpose>                   |\n                                       COMMENT                             |\n                                       }\n                                       [ , ... ]\n\nclusteringAction ::=\n  {\n     CLUSTER BY ( <expr> [ , <expr> , ... ] )\n     /* RECLUSTER is deprecated */\n   | RECLUSTER [ MAX_SIZE = <budget_in_bytes> ] [ WHERE <condition> ]\n     /* { SUSPEND | RESUME } RECLUSTER is valid action */\n   | { SUSPEND | RESUME } RECLUSTER\n   | DROP CLUSTERING KEY\n  }\n\ntableColumnAction ::=\n  {\n     ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n        [\n           {\n              DEFAULT <default_value>\n              | { AUTOINCREMENT | IDENTITY }\n                 /* AUTOINCREMENT (or IDENTITY) is supported only for           */\n                 /* columns with numeric data types (NUMBER, INT, FLOAT, etc.). */\n                 /* Also, if the table is not empty (that is, if the table contains */\n                 /* any rows), only DEFAULT can be altered.                     */\n                 [\n                    {\n                       ( <start_num> , <step_num> )\n                       | START <num> INCREMENT <num>\n                    }\n                 ]\n                 [  { ORDER | NOORDER } ]\n           }\n        ]\n        [ inlineConstraint ]\n        [ COLLATE '<collation_specification>' ]\n\n   | RENAME COLUMN <col_name> TO <new_col_name>\n\n   | ALTER | MODIFY [ ( ]\n                            [ COLUMN ] <col1_name> DROP DEFAULT\n                          , [ COLUMN ] <col1_name> SET DEFAULT <seq_name>.NEXTVAL\n                          , [ COLUMN ] <col1_name> { [ SET ] NOT NULL | DROP NOT NULL }\n                          , [ COLUMN ] <col1_name> [ [ SET DATA ] TYPE ] <type>\n                          , [ COLUMN ] <col1_name> COMMENT '<string>'\n                          , [ COLUMN ] <col1_name> UNSET COMMENT\n                        [ , [ COLUMN ] <col2_name> ... ]\n                        [ , ... ]\n                    [ ) ]\n\n   | DROP [ COLUMN ] [ IF EXISTS ] <col1_name> [, <col2_name> ... ]\n  }\n\n  inlineConstraint ::=\n    [ NOT NULL ]\n    [ CONSTRAINT <constraint_name> ]\n    { UNIQUE | PRIMARY KEY | { [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ] } }\n    [ <constraint_properties> ]\n\ndataMetricFunctionAction ::=\n\n    SET DATA_METRIC_SCHEDULE = {\n        '<num> MINUTE'\n      | 'USING CRON <expr> <time_zone>'\n      | 'TRIGGER_ON_CHANGES'\n    }\n\n  | UNSET DATA_METRIC_SCHEDULE\n\n  | { ADD | DROP } DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] )\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) ]\n\n  | MODIFY DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME }\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME } ]\n\ndataGovnPolicyTagAction ::=\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n  |\n  {\n      SET AGGREGATION POLICY <policy_name>\n        [ ENTITY KEY ( <col_name> [, ... ] ) ]\n        [ FORCE ]\n    | UNSET AGGREGATION POLICY\n  }\n  |\n  {\n      SET JOIN POLICY <policy_name>\n        [ FORCE ]\n    | UNSET JOIN POLICY\n  }\n  |\n  ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n    [ [ WITH ] MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] ]\n    [ [ WITH ] PROJECTION POLICY <policy_name> ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>'\n          [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] [ FORCE ]\n      | UNSET MASKING POLICY\n  }\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET PROJECTION POLICY <policy_name>\n          [ FORCE ]\n      | UNSET PROJECTION POLICY\n  }\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> SET TAG\n      <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n      , [ COLUMN ] <col2_name> SET TAG\n          <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n                   , [ COLUMN ] <col2_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nextTableColumnAction ::=\n  {\n     ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type> AS ( <expr> )\n\n   | RENAME COLUMN <col_name> TO <new_col_name>\n\n   | DROP [ COLUMN ] [ IF EXISTS ] <col1_name> [, <col2_name> ... ]\n  }\n\nconstraintAction ::=\n  {\n     ADD outoflineConstraint\n   | RENAME CONSTRAINT <constraint_name> TO <new_constraint_name>\n   | { ALTER | MODIFY } { CONSTRAINT <constraint_name> | PRIMARY KEY | UNIQUE | FOREIGN KEY } ( <col_name> [ , ... ] )\n                         [ [ NOT ] ENFORCED ] [ VALIDATE | NOVALIDATE ] [ RELY | NORELY ]\n   | DROP { CONSTRAINT <constraint_name> | PRIMARY KEY | UNIQUE | FOREIGN KEY } ( <col_name> [ , ... ] )\n                         [ CASCADE | RESTRICT ]\n  }\n\n  outoflineConstraint ::=\n    [ CONSTRAINT <constraint_name> ]\n    {\n       UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n     | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n     | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n                          REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n    }\n    [ <constraint_properties> ]\n\nsearchOptimizationAction ::=\n  {\n     ADD SEARCH OPTIMIZATION [\n       ON <search_method_with_target> [ , <search_method_with_target> ... ]\n     ]\n\n   | DROP SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n  }",
    "examples": [
        {
            "code": "CREATE OR REPLACE TABLE t1(a1 number);"
        },
        {
            "code": "SHOW TABLES LIKE 't1';"
        },
        {
            "code": "+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+\n| created_on                    | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | change_tracking | is_external | enable_schema_evolution | owner_role_type | is_event | budget |\n|-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------|\n| 2023-10-19 10:37:04.858 -0700 | T1   | TESTDB        | MY_SCHEMA   | TABLE |         |            |    0 |     0 | PUBLIC | 1              | OFF             | N           | N                       | ROLE            | N        | NULL   |\n+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+"
        },
        {
            "code": "ALTER TABLE t1 RENAME TO tt1;"
        },
        {
            "code": "SHOW TABLES LIKE 'tt1';"
        },
        {
            "code": "+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+\n| created_on                    | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | change_tracking | is_external | enable_schema_evolution | owner_role_type | is_event | budget |\n|-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------|\n| 2023-10-19 10:37:04.858 -0700 | TT1  | TESTDB        | MY_SCHEMA   | TABLE |         |            |    0 |     0 | PUBLIC | 1              | OFF             | N           | N                       | ROLE            | N        | NULL   |\n+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE t1(a1 NUMBER, a2 VARCHAR, a3 DATE);\nCREATE OR REPLACE TABLE t2(b1 VARCHAR);"
        },
        {
            "code": "DESC TABLE t1;"
        },
        {
            "code": "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | DATE              | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "DESC TABLE t2;"
        },
        {
            "code": "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "ALTER TABLE t1 SWAP WITH t2;"
        },
        {
            "code": "DESC TABLE t1;"
        },
        {
            "code": "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "DESC TABLE t2;"
        },
        {
            "code": "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | DATE              | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE t1(a1 NUMBER);"
        },
        {
            "code": "DESC TABLE t1;"
        },
        {
            "code": "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "ALTER TABLE t1 ADD COLUMN a2 NUMBER;"
        },
        {
            "code": "ALTER TABLE t1 ADD COLUMN a3 NUMBER NOT NULL;"
        },
        {
            "code": "ALTER TABLE t1 ADD COLUMN a4 NUMBER DEFAULT 0 NOT NULL;"
        },
        {
            "code": "ALTER TABLE t1 ADD COLUMN a5 VARCHAR COLLATE 'en_US';"
        },
        {
            "code": "DESC TABLE t1;"
        },
        {
            "code": "+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type                              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0)                      | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0)                      | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A5   | VARCHAR(16777216) COLLATE 'en_us' | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "ALTER TABLE t1 ADD COLUMN IF NOT EXISTS a2 NUMBER;"
        },
        {
            "code": "DESC TABLE t1;"
        },
        {
            "code": "+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type                              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0)                      | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0)                      | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A5   | VARCHAR(16777216) COLLATE 'en_us' | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "ALTER TABLE t1 RENAME COLUMN a1 TO b1;"
        },
        {
            "code": "DESC TABLE t1;"
        },
        {
            "code": "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0) | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0) | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "ALTER TABLE t1 DROP COLUMN a2;"
        },
        {
            "code": "DESC TABLE t1;"
        },
        {
            "code": "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0) | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0) | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "ALTER TABLE t1 DROP COLUMN IF EXISTS a2;"
        },
        {
            "code": "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0) | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0) | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        },
        {
            "code": "CREATE EXTERNAL TABLE exttable1\n  LOCATION=@mystage/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET)\n  ;"
        },
        {
            "code": "DESC EXTERNAL TABLE exttable1;"
        },
        {
            "code": "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+"
        },
        {
            "code": "ALTER TABLE exttable1 ADD COLUMN a1 VARCHAR AS (value:a1::VARCHAR);"
        },
        {
            "code": "DESC EXTERNAL TABLE exttable1;"
        },
        {
            "code": "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n| A1        | VARCHAR(16777216) | VIRTUAL   | Y     | NULL    | N           | N          | NULL  | TO_CHAR(GET(VALUE, 'a1'))                                | NULL                  |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+"
        },
        {
            "code": "ALTER TABLE exttable1 RENAME COLUMN a1 TO b1;"
        },
        {
            "code": "DESC EXTERNAL TABLE exttable1;"
        },
        {
            "code": "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n| B1        | VARCHAR(16777216) | VIRTUAL   | Y     | NULL    | N           | N          | NULL  | TO_CHAR(GET(VALUE, 'a1'))                                | NULL                  |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+"
        },
        {
            "code": "ALTER TABLE exttable1 DROP COLUMN b1;"
        },
        {
            "code": "DESC EXTERNAL TABLE exttable1;"
        },
        {
            "code": "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE T1 (id NUMBER, date TIMESTAMP_NTZ, name STRING) CLUSTER BY (id, date);"
        },
        {
            "code": "SHOW TABLES LIKE 'T1';"
        },
        {
            "code": "---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (ID,DATE)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+"
        },
        {
            "code": "ALTER TABLE t1 CLUSTER BY (date, id);"
        },
        {
            "code": "SHOW TABLES LIKE 'T1';"
        },
        {
            "code": "---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (DATE,ID)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+"
        },
        {
            "code": "ALTER TABLE t1 ADD ROW ACCESS POLICY rap_t1 ON (empl_id);"
        },
        {
            "code": "ALTER TABLE t1 ADD ROW ACCESS POLICY rap_test2 ON (cost, item);"
        },
        {
            "code": "ALTER TABLE t1 DROP ROW ACCESS POLICY rap_v1;"
        },
        {
            "code": "alter table t1\n  drop row access policy rap_t1_version_1,\n  add row access policy rap_t1_version_2 on (empl_id);"
        },
        {
            "code": "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = '5 MINUTE';"
        },
        {
            "code": "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'USING CRON 0 8 * * * UTC';"
        },
        {
            "code": "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'USING CRON 0 8 * * MON,TUE,WED,THU,FRI UTC';"
        },
        {
            "code": "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'USING CRON 0 6,12,18 * * * UTC';"
        },
        {
            "code": "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES';"
        },
        {
            "code": "ALTER TABLE join_table_2\n  SET JOIN POLICY jp1 ALLOWED JOIN KEYS (col1);"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case sensitive."
        },
        {
            "name": "RENAME   TO   new_table_name",
            "description": "Renames the specified table with a new identifier that is not currently used by any other tables in the schema. For more information about table identifiers, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object (table, column, etc.) is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SWAP   WITH   target_table_name",
            "description": "Swap renames two tables in a single transaction. Note that swapping a permanent or transient table with a temporary table, which persists only for the duration of the user session in which\nit was created, is not allowed. This restriction prevents a naming conflict that could occur when a temporary table is swapped with a permanent\nor transient table, and an existing permanent or transient table has the same name as the temporary table. To swap a permanent or transient\ntable with a temporary table, use three ALTER TABLE ... RENAME TO statements: Rename  table a to c , b to a , and then c to b ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the table (separated by blank spaces, commas, or new lines): Object-level parameter that modifies the retention period for the table for Time Travel. For more information, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Note A value of 0 effectively disables Time Travel for the table. Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS . Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the table. This option adds several hidden columns to the source table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the table. FALSE disables change tracking on the table. Associated hidden columns are dropped from the table. Specifies a default collation specification for any new columns added to the table. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION . Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support . The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER , ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false. Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts . Adds a comment or overwrites the existing comment for the table."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Object-level parameter that modifies the retention period for the table for Time Travel. For more information, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Note A value of 0 effectively disables Time Travel for the table."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "CHANGE_TRACKING   =     TRUE   |   FALSE",
            "description": "Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the table. This option adds several hidden columns to the source table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the table. FALSE disables change tracking on the table. Associated hidden columns are dropped from the table."
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for any new columns added to the table. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "ENABLE_SCHEMA_EVOLUTION   =   {   TRUE   |   FALSE   }",
            "description": "Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support . The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER , ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the table."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties/parameters to unset for the table, which resets them back to their defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS CHANGE_TRACKING DEFAULT_DDL_COLLATION ENABLE_SCHEMA_EVOLUTION CONTACT purpose COMMENT"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-listing",
    "title": "ALTER LISTING",
    "description": "Modifies the properties of a listings with an inline YAML manifest, or from a file located in a stage location.",
    "syntax": "ALTER LISTING [ IF EXISTS ] <name> [ { PUBLISH | UNPUBLISH | REVIEW } ]\n\nALTER LISTING [ IF EXISTS ] <name> AS '<yaml_manifest_string>'\n  [ PUBLISH={ TRUE | FALSE } ]\n  [ REVIEW= { TRUE | FALSE } ]\n  [ COMMENT = '<string>' ]\n\nALTER LISTING <name> ADD VERSION [ [ IF NOT EXISTS ] <version_name> ]\n  FROM <yaml_manifest_stage_location>\n  [ COMMENT = '<string>' ]\n\nALTER LISTING [ IF EXISTS ] <name> RENAME TO <new_name>;\n\nALTER LISTING [ IF EXISTS ] <name> SET COMMENT = '<string>'",
    "examples": [
        {
            "code": "ALTER LISTING MYLISTING\nAS\n  $$\n  title: \"MyListing\"\n  subtitle: \"Subtitle for MyListing\"\n  description: \"Description or MyListing\"\n  listing_terms:\n    type: \"STANDARD\"\n  targets:\n    accounts: [\"Org1.Account1\"]\n  usage_examples:\n     - title: \"this is a test sql\"\n       description: \"Simple example\"\n       query: \"select *\"\n  $$"
        },
        {
            "code": "ALTER LISTING MYLISTING REVIEW;"
        },
        {
            "code": "ALTER LISTING MYLISTING PUBLISH;"
        },
        {
            "code": "ALTER LISTING MYLISTING UNPUBLISH;"
        },
        {
            "code": "ALTER LISTING MYLISTING SET COMMENT = 'My listing is ready!';"
        },
        {
            "code": "ALTER LISTING MYLISTING ADD VERSION V3 FROM @dbforstage.public.listingstage/listingmanifests;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (name) for the listing being altered."
        },
        {
            "name": "{   PUBLISH   |   UNPUBLISH   |   REVIEW   }",
            "description": "The action to perform on the listing: PUBLISH Makes a previously undiscoverable listing discoverable. Specifying PUBLISH on a previously published listing has no effect. UNPUBLISH Makes a previously discoverable listing undiscoverable for new consumers.\nExisting consumers can continue to access the data associated with an unpublished listing. Specifying UNPUBLISH on a previously unpublished listing has no effect. See also Unpublish a listing . REVIEW Submits the listing for review."
        },
        {
            "name": "yaml_manifest_string",
            "description": "The YAML manifest for the listing. For manifest parameters, see Listing manifest reference . Manifests are normally provided as dollar quoted strings.\nFor more information, see Dollar-quoted string constants ."
        },
        {
            "name": "ADD   VERSION   version_name",
            "description": "Specifies the unique version identifier for the version being added.\nIf the identifier contains spaces, special characters, or mixed-case characters, the entire identifier must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case sensitive. For information about identifier syntax,\nsee Identifier Requirements ."
        },
        {
            "name": "FROM   ' yaml_manifest_stage_location '",
            "description": "Specifies the path for the internal or Snowflake Git repository clone manifest.yml file. If the changes require Marketplace Ops review, use the REVIEW and PUBLISH operations."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Changes the name of the listing to new_name . Listing names must be unique. The new identifier cannot be used if the identifier is already in use for a different listing."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for an existing listing."
        },
        {
            "name": "PUBLISH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies how the listing should be published. If TRUE, listing is published immediately on listing to Marketplace Ops for review. Default: TRUE."
        },
        {
            "name": "REVIEW   =    {   TRUE   |   FALSE   }",
            "description": "Specifies whether the listing should or should not submitted to Marketplace Ops review. Default: TRUE."
        }
    ],
    "usage_notes": "Listings can be renamed only in DRAFT state.\nWhen setting the live version of the YAML format manifest for a listing, you must use COMMIT to apply the changes, or ABORT to discard the changes."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-account",
    "title": "ALTER ACCOUNT",
    "description": "Modifies an account. The ALTER ACCOUNT command has two purposes:",
    "syntax": "ALTER ACCOUNT SET { [ accountParams ] | [ objectParams ] | [ sessionParams ] }\n\nALTER ACCOUNT UNSET <param_name> [ , ... ]\n\nALTER ACCOUNT SET RESOURCE_MONITOR = <monitor_name>\n\nALTER ACCOUNT ADD ORGANIZATION USER GROUP <group_name>\nALTER ACCOUNT REMOVE ORGANIZATION USER GROUP <group_name>\n\nALTER ACCOUNT SET { AUTHENTICATION | PASSWORD | SESSION } POLICY <policy_name>\n\nALTER ACCOUNT UNSET { AUTHENTICATION | PASSWORD | SESSION } POLICY\n\nALTER ACCOUNT SET FEATURE POLICY <policy_name> FOR ALL APPLICATIONS [ FORCE ]\n\nALTER ACCOUNT UNSET FEATURE POLICY FOR ALL APPLICATIONS\n\nALTER ACCOUNT SET PACKAGES POLICY <policy_name> [ FORCE ]\n\nALTER ACCOUNT UNSET { PACKAGES | PASSWORD | SESSION } POLICY\n\nALTER ACCOUNT SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER ACCOUNT UNSET TAG <tag_name> [ , <tag_name> ... ]\n\naccountParams ::=\n    ALLOW_ID_TOKEN = TRUE | FALSE\n    CLIENT_ENCRYPTION_KEY_SIZE = <integer>\n    CORTEX_ENABLED_CROSS_REGION = { 'DISABLED' | 'ANY_REGION' | '<list_of_regions>' }\n    DISABLE_USER_PRIVILEGE_GRANTS = TRUE | FALSE\n    ENABLE_EGRESS_COST_OPTIMIZER = TRUE | FALSE\n    ENABLE_INTERNAL_STAGES_PRIVATELINK = TRUE | FALSE\n    ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES = TRUE | FALSE\n    ENABLE_NOTEBOOK_CREATION_IN_PERSONAL_DB = TRUE | FALSE\n    EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST = TRUE | FALSE\n    INITIAL_REPLICATION_SIZE_LIMIT_IN_TB = <num>\n    LISTING_AUTO_FULFILLMENT_REPLICATION_REFRESH_SCHEDULE = <schedule>\n    LLM_INFERENCE_PARSE_DOCUMENT_PRESIGNED_URL_EXPIRY_SECONDS = <integer>\n    NETWORK_POLICY = <string>\n    OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST = TRUE | FALSE\n    PERIODIC_DATA_REKEYING = TRUE | FALSE\n    REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_CREATION = TRUE | FALSE\n    REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_OPERATION = TRUE | FALSE\n    SAML_IDENTITY_PROVIDER = <json_object>\n    SSO_LOGIN_PAGE = TRUE | FALSE\n\nobjectParams ::=\n    CORTEX_MODELS_ALLOWLIST = {'<list_of_models>' | 'ALL' | 'NONE'}\n    DATA_RETENTION_TIME_IN_DAYS = <integer>\n    DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE = <warehouse_name>\n    ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR = TRUE | FALSE\n    ENABLE_UNREDACTED_SECURE_OBJECT_ERROR = TRUE | FALSE\n    MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer>\n    EXTERNAL_VOLUME = <external_volume_name>\n    CATALOG = <catalog_integration_name>\n    DEFAULT_DDL_COLLATION = '<collation_specification>'\n    DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU = <compute_pool_name>\n    DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU = <compute_pool_name>\n    MAX_CONCURRENCY_LEVEL = <num>\n    NETWORK_POLICY = <string>\n    PIPE_EXECUTION_PAUSED = TRUE | FALSE\n    PREVENT_UNLOAD_TO_INLINE_URL = TRUE | FALSE\n    PREVENT_UNLOAD_TO_INTERNAL_STAGES = TRUE | FALSE\n    REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n    STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = <num>\n    STATEMENT_TIMEOUT_IN_SECONDS = <num>\n    STORAGE_SERIALIZATION_POLICY = COMPATIBLE | OPTIMIZED\n    CATALOG_SYNC = '<snowflake_open_catalog_integration_name>'\n    BASE_LOCATION_PREFIX = '<string>'\n\nsessionParams ::=\n    ABORT_DETACHED_QUERY = TRUE | FALSE\n    AUTOCOMMIT = TRUE | FALSE\n    BINARY_INPUT_FORMAT = <string>\n    BINARY_OUTPUT_FORMAT = <string>\n    DATE_INPUT_FORMAT = <string>\n    DATE_OUTPUT_FORMAT = <string>\n    DEFAULT_NULL_ORDERING = <string>\n    ERROR_ON_NONDETERMINISTIC_MERGE = TRUE | FALSE\n    ERROR_ON_NONDETERMINISTIC_UPDATE = TRUE | FALSE\n    JSON_INDENT = <num>\n    LOCK_TIMEOUT = <num>\n    QUERY_TAG = <string>\n    ROWS_PER_RESULTSET = <num>\n    S3_STAGE_VPCE_DNS_NAME = <string>\n    SEARCH_PATH = <string>\n    SIMULATED_DATA_SHARING_CONSUMER = <string>\n    STATEMENT_TIMEOUT_IN_SECONDS = <num>\n    STRICT_JSON_OUTPUT = TRUE | FALSE\n    TIMESTAMP_DAY_IS_ALWAYS_24H = TRUE | FALSE\n    TIMESTAMP_INPUT_FORMAT = <string>\n    TIMESTAMP_LTZ_OUTPUT_FORMAT = <string>\n    TIMESTAMP_NTZ_OUTPUT_FORMAT = <string>\n    TIMESTAMP_OUTPUT_FORMAT = <string>\n    TIMESTAMP_TYPE_MAPPING = <string>\n    TIMESTAMP_TZ_OUTPUT_FORMAT = <string>\n    TIMEZONE = <string>\n    TIME_INPUT_FORMAT = <string>\n    TIME_OUTPUT_FORMAT = <string>\n    TRANSACTION_DEFAULT_ISOLATION_LEVEL = <string>\n    TWO_DIGIT_CENTURY_START = <num>\n    UNSUPPORTED_DDL_ACTION = <string>\n    USE_CACHED_RESULT = TRUE | FALSE\n    WEEK_OF_YEAR_POLICY = <num>\n    WEEK_START = <num>\n\nALTER ACCOUNT <name> SET IS_ORG_ADMIN = { TRUE | FALSE }\n\nALTER ACCOUNT <name> RENAME TO <new_name> [ SAVE_OLD_URL = { TRUE | FALSE } ]\n\nALTER ACCOUNT <name> DROP OLD URL\n\nALTER ACCOUNT <name> DROP OLD ORGANIZATION URL",
    "examples": [
        {
            "code": "ALTER ACCOUNT SET NETWORK_POLICY = mypolicy;"
        },
        {
            "code": "ALTER ACCOUNT SET DISABLE_USER_PRIVILEGE_GRANTS = TRUE;"
        },
        {
            "code": "ALTER ACCOUNT UNSET NETWORK_POLICY;"
        },
        {
            "code": "ALTER ACCOUNT SET PACKAGES POLICY packages_policy_prod_1 FORCE;"
        },
        {
            "code": "ALTER ACCOUNT UNSET PACKAGES POLICY;"
        }
    ],
    "parameters": [
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) account, session, and object parameters to set for your account (separated by blank spaces, commas, or new lines): Account parameters cannot be changed by any other users. Session and object parameters set at the account level serve only as defaults and can be changed by other users. For descriptions of the parameters you can set for your account, see Parameters ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) account, session, and object parameters to unset for your account, which resets them to the system defaults. You can reset multiple properties with a single ALTER statement; however, each property must be separated by a comma. When resetting a\nproperty, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "SET   RESOURCE_MONITOR   resource_monitor_name",
            "description": "Special parameter that specifies the name of the resource monitor used to control all virtual warehouses created in the account. Important Setting a resource monitor at the account level does not impact any of the Snowflake-provided warehouses that Snowflake uses\nfor Snowpipe, automatic reclustering, or materialized views. The credits consumed by these warehouses do not count towards the\ncredit quota for an account-level resource monitor. For more details, see Working with resource monitors ."
        },
        {
            "name": "ADD   ORGANIZATION   USER   GROUP   group_name",
            "description": "Preview Feature — Open Available to all accounts. Imports an organization user group into the account. Organization users in the group are added to the\naccount as user objects."
        },
        {
            "name": "REMOVE   ORGANIZATION   USER   GROUP   group_name",
            "description": "Preview Feature — Open Available to all accounts. Removes an organization user group from the account."
        },
        {
            "name": "SET   {   AUTHENTICATION   |   PASSWORD   |   SESSION   }   POLICY   policy_name",
            "description": "Specifies one of the following policies for the account: Authentication policy Password policy Session policy"
        },
        {
            "name": "SET   FEATURE   POLICY   policy_name   FOR   ALL   APPLICATIONS   [   FORCE   ]",
            "description": "Specifies the feature policy to set for the account. If a feature policy\nis already set on the current account, you can use FORCE to set the feature policy\nwithout having to unset the feature policy first."
        },
        {
            "name": "UNSET   FEATURE   POLICY   FOR   ALL   APPLICATIONS",
            "description": "Unsets the feature policy for the account."
        },
        {
            "name": "SET   PACKAGES   POLICY   policy_name   [   FORCE   ]",
            "description": "Specifies the packages policy to set for the account.\nIf a packages policy is already set on the current account, you can use FORCE to set the packages policy\nwithout having to unset the packages policy first."
        },
        {
            "name": "UNSET   {   PACKAGES   |   PASSWORD   |   SESSION   }   POLICY",
            "description": "Unsets the packages policy, password policy, or session policy for the account."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "name",
            "description": "Specifies the name of the account that is being modified."
        },
        {
            "name": "SET",
            "description": "Specifies an account property to set for the account. Sets an account property that determines whether the ORGADMIN role is enabled in the account. Note Using the ORGADMIN role in a regular account is being phased out. Organization administrators should use the organization account to complete organization-level tasks. To enable the ORGADMIN role for an account, specify SET IS_ORG_ADMIN = TRUE . You cannot set the property to FALSE from the current account. As a workaround, enable the role in a different account,\nand then switch to that account before executing the ALTER ACCOUNT command. By default, the ORGADMIN role can be enabled in a maximum of 8 accounts. If your organization requires more accounts with the ORGADMIN\nrole, contact Snowflake Support ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Changes the name of an account to the specified name. Organization administrators cannot rename an account while they are logged in to it, so they must log in to a different account before\nexecuting the ALTER ACCOUNT command. If your organization consists of a single account that needs to be renamed, contact Snowflake Support . Optional parameter used in conjunction with RENAME TO that preserves the account URL used to\naccess Snowflake prior to renaming. By default, Snowflake saves the original URL, which means you can access the account with either\nthe old URL or the URL that contains the new account name. When set to FALSE , you must use the new URL to access the account. TRUE"
        },
        {
            "name": "DROP   OLD   URL",
            "description": "Removes the original account URL of an account that was renamed. Once the old URL is dropped, you must access the account with the URL that\ncontains the new account name. If an account has an old account URL because it was moved to another organization, had its organization renamed, or was part of an\norganization that was merged, use the ALTER ACCOUNT … DROP OLD ORGANIZATION URL instead."
        },
        {
            "name": "IS_ORG_ADMIN   =   {   TRUE   |   FALSE   }",
            "description": "Sets an account property that determines whether the ORGADMIN role is enabled in the account. Note Using the ORGADMIN role in a regular account is being phased out. Organization administrators should use the organization account to complete organization-level tasks. To enable the ORGADMIN role for an account, specify SET IS_ORG_ADMIN = TRUE . You cannot set the property to FALSE from the current account. As a workaround, enable the role in a different account,\nand then switch to that account before executing the ALTER ACCOUNT command. By default, the ORGADMIN role can be enabled in a maximum of 8 accounts. If your organization requires more accounts with the ORGADMIN\nrole, contact Snowflake Support ."
        },
        {
            "name": "SAVE_OLD_URL   =   {   TRUE   |   FALSE   }",
            "description": "Optional parameter used in conjunction with RENAME TO that preserves the account URL used to\naccess Snowflake prior to renaming. By default, Snowflake saves the original URL, which means you can access the account with either\nthe old URL or the URL that contains the new account name. When set to FALSE , you must use the new URL to access the account. TRUE"
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "DROP   OLD   ORGANIZATION   URL",
            "description": "Removes the original account URL of an account after one of the following occurs: Account moved to another organization Account had its organization renamed. Account was part of an organization that was merged with another organization. If an account has an old account URL because the account, not the organization, was renamed, use the ALTER ACCOUNT … DROP OLD URL\ncommand instead."
        }
    ],
    "usage_notes": "Account parameters can be set only at the account level.\nSession and object parameters that are set using this command serve only as defaults:\nUser parameters can be overridden at the individual user level.\nSession parameters can be overridden at the individual user and session level.\nObject parameters can be overridden at the individual object level.\nSetting a resource monitor at the account level controls the credit usage for all virtual warehouses created in the account, but does not impact\nthe credit usage for any of the Snowflake-provided warehouses. For more details, see Working with resource monitors.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-feature-policy",
    "title": "ALTER FEATURE POLICY",
    "description": "Alters or renames a feature policy.",
    "syntax": "ALTER FEATURE POLICY [ IF EXISTS ] <name> SET\n  [ BLOCKED_OBJECT_TYPES_FOR_CREATION = ( [ <type> [ , <type>  ... ] ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FEATURE POLICY [ IF EXISTS ] <name> UNSET\n  [ BLOCKED_OBJECT_TYPES_FOR_CREATION ]\n  [ COMMENT ]\n\nALTER FEATURE POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER FEATURE POLICY [ IF EXISTS ] <name> SET  TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FEATURE POLICY [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , ... ]",
    "examples": [
        {
            "code": "ALTER FEATURE POLICY block_create_db_policy SET\n  BLOCKED_OBJECT_TYPES_FOR_CREATION = (DATABASES, TASKS);"
        },
        {
            "code": "ALTER FEATURE POLICY block_create_db_policy RENAME TO block_create_db_task_policy;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the feature policy to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET",
            "description": "Specifies one (or more) properties to set for the feature policy. Specifies the objects that an app is prohibit from creating. Possible values are: COMPUTE_POOLS DATABASES TASKS WAREHOUSES String (literal) that specifies a comment for the feature policy."
        },
        {
            "name": "BLOCKED_OBJECT_TYPES_FOR_CREATION   =   (   type   [   ,   type   ...   ]   )",
            "description": "Specifies the objects that an app is prohibit from creating. Possible values are: COMPUTE_POOLS DATABASES TASKS WAREHOUSES"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the feature policy."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "If a previous policy had been applied to the account or an app an error is return, unless the you\nspecify the FORCE option to force the replacement of the existing policy.\nWhen a feature policy is unbound from an app, the account level policy takes effect, if it exists."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-iceberg-table",
    "title": "ALTER ICEBERG TABLE",
    "description": "Modifies properties such as clustering options and tags for an existing Apache Iceberg™ table.",
    "syntax": "ALTER ICEBERG TABLE [ IF EXISTS ] <table_name> { clusteringAction | tableColumnAction }\n\nALTER ICEBERG TABLE [ IF EXISTS ] <table_name> SET\n  [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n  [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>']\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nALTER ICEBERG TABLE [ IF EXISTS ] <table_name> UNSET\n  [ REPLACE_INVALID_CHARACTERS ]\n  [ CONTACT <purpose> ]\n\nALTER ICEBERG TABLE [ IF EXISTS ] dataGovnPolicyTagAction\n\nALTER ICEBERG TABLE [ IF EXISTS ] <table_name> searchOptimizationAction\n\nclusteringAction ::=\n  {\n     CLUSTER BY ( <expr> [ , <expr> , ... ] )\n     /* { SUSPEND | RESUME } RECLUSTER is valid action */\n   | { SUSPEND | RESUME } RECLUSTER\n   | DROP CLUSTERING KEY\n  }\n\ntableColumnAction ::=\n  {\n     ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n        [ inlineConstraint ]\n        [ COLLATE '<collation_specification>' ]\n\n   | RENAME COLUMN <col_name> TO <new_col_name>\n\n   | ALTER | MODIFY [ ( ]\n                          , [ COLUMN ] <col1_name> { [ SET ] NOT NULL | DROP NOT NULL }\n                          , [ COLUMN ] <col1_name> [ [ SET DATA ] TYPE ] <type>\n                          , [ COLUMN ] <col1_name> COMMENT '<string>'\n                          , [ COLUMN ] <col1_name> UNSET COMMENT\n                        [ , [ COLUMN ] <col2_name> ... ]\n                        [ , ... ]\n                    [ ) ]\n\n   | DROP [ COLUMN ] [ IF EXISTS ] <col1_name> [, <col2_name> ... ]\n  }\n\n  inlineConstraint ::=\n    [ NOT NULL ]\n    [ CONSTRAINT <constraint_name> ]\n    { UNIQUE | PRIMARY KEY | { [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ] } }\n    [ <constraint_properties> ]\n\ndataGovnPolicyTagAction ::=\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n  |\n  {\n      SET AGGREGATION POLICY <policy_name>\n        [ ENTITY KEY ( <col_name> [, ... ] ) ]\n        [ FORCE ]\n    | UNSET AGGREGATION POLICY\n  }\n  |\n  {\n      SET JOIN POLICY <policy_name>\n        [ FORCE ]\n    | UNSET JOIN POLICY\n  }\n  |\n  ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n    [ [ WITH ] MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] ]\n    [ [ WITH ] PROJECTION POLICY <policy_name> ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>'\n          [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] [ FORCE ]\n      | UNSET MASKING POLICY\n  }\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET PROJECTION POLICY <policy_name>\n          [ FORCE ]\n      | UNSET PROJECTION POLICY\n  }\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> SET TAG\n      <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n      , [ COLUMN ] <col2_name> SET TAG\n          <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n                   , [ COLUMN ] <col2_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nsearchOptimizationAction ::=\n  {\n     ADD SEARCH OPTIMIZATION [\n       ON <search_method_with_target> [ , <search_method_with_target> ... ]\n     ]\n\n   | DROP SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n  }",
    "examples": [
        {
            "code": "ALTER ICEBERG TABLE my_iceberg_table SET TAG my_tag = 'customer';"
        },
        {
            "code": "ALTER ICEBERG TABLE my_iceberg_table SET AUTO_REFRESH = TRUE;"
        },
        {
            "code": "ALTER ICEBERG TABLE my_iceberg_table ADD SEARCH OPTIMIZATION ON SUBSTRING(C6);\n\nALTER ICEBERG TABLE my_iceberg_table DROP SEARCH OPTIMIZATION ON EQUALITY(C7, C8);"
        }
    ],
    "parameters": [
        {
            "name": "table_name",
            "description": "Identifier for the table to modify. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the external table (separated by blank spaces, commas, or new lines): Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results.\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. If not specified, the Iceberg table defaults to the parameter value for the schema, database, or account.\nThe schema takes precedence over the database, and the database takes precedence over the account. Default: FALSE Specifies the name of a catalog integration configured for Snowflake Open Catalog . Snowflake syncs\nthe table with an external catalog in your Snowflake Open Catalog account. For more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Specifies the retention period for a Snowflake-managed table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical\ndata in the table. For more information, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table. Specifies whether Snowflake should automatically poll the external Iceberg catalog associated with the table for metadata updates\nwhen you use automated refresh . For Delta-based tables, Snowflake polls your external cloud storage for updates. If no value is specified for the REFRESH_INTERVAL_SECONDS parameter on the catalog integration, Snowflake uses a default\nrefresh interval of 30 seconds. Default: FALSE Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "UNSET",
            "description": "Currently, you can only unset the following parameters with this command: REPLACE_INVALID_CHARACTERS CATALOG_SYNC CONTACT purpose"
        },
        {
            "name": "REPLACE_INVALID_CHARACTERS   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results.\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. If not specified, the Iceberg table defaults to the parameter value for the schema, database, or account.\nThe schema takes precedence over the database, and the database takes precedence over the account. Default: FALSE"
        },
        {
            "name": "CATALOG_SYNC   =   ' snowflake_open_catalog_integration_name '",
            "description": "Specifies the name of a catalog integration configured for Snowflake Open Catalog . Snowflake syncs\nthe table with an external catalog in your Snowflake Open Catalog account. For more information about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC ."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Specifies the retention period for a Snowflake-managed table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical\ndata in the table. For more information, see Understanding & using Time Travel . For a detailed description of this object-level parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
        },
        {
            "name": "AUTO_REFRESH   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether Snowflake should automatically poll the external Iceberg catalog associated with the table for metadata updates\nwhen you use automated refresh . For Delta-based tables, Snowflake polls your external cloud storage for updates. If no value is specified for the REFRESH_INTERVAL_SECONDS parameter on the catalog integration, Snowflake uses a default\nrefresh interval of 30 seconds. Default: FALSE"
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        }
    ],
    "usage_notes": "Only the table owner (that is, the role with the OWNERSHIP privilege on the table) or higher can execute this command.\nClustering is only supported for tables that use Snowflake as the Iceberg catalog. To add clustering to an Iceberg table,\nyou must also have the USAGE or OWNERSHIP privileges on the schema and database that contain the table.\nYou can use data metric functions with Iceberg tables by executing an ALTER TABLE command. For more information, see\nUse data metric functions to perform data quality checks.\nFor more information about using search optimization with Iceberg tables, including limitations, see\nSupport for Apache Iceberg™ tables in the search optimization documentation.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nTo troubleshooting issues with altering the CATALOG_SYNC parameter, see You can’t alter an Iceberg table when specifying the CATALOG_SYNC parameter"
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-dataset-drop-version",
    "title": "ALTER DATASET … DROP VERSION",
    "description": "Drops a dataset version.",
    "syntax": "ALTER DATASET [ IF EXISTS ] <name> DROP VERSION <version_name>",
    "examples": [
        {
            "code": "ALTER DATASET my_dataset\nDROP VERSION 'v1';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "The name of the dataset that you’re dropping."
        },
        {
            "name": "DROP   VERSION   version_name",
            "description": "The name of the dataset version that you’re dropping."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-stage",
    "title": "ALTER STAGE",
    "description": "Modifies the properties for an existing named internal or external stage.",
    "syntax": "ALTER STAGE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER STAGE [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER STAGE <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\n-- Internal stage\nALTER STAGE [ IF EXISTS ] <name> SET\n  [ FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM } [ formatTypeOptions ] } ) ]\n  { [ COMMENT = '<string_literal>' ] }\n\n-- External stage\nALTER STAGE [ IF EXISTS ] <name> SET {\n    [ externalStageParams ]\n    [ FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML | CUSTOM } [ formatTypeOptions ] } ) ]\n    [ COMMENT = '<string_literal>' ]\n    }\n\nexternalStageParams (for Amazon S3) ::=\n  URL = '<protocol>://<bucket>[/<path>/]'\n  [ AWS_ACCESS_POINT_ARN = '<string>' ]\n  [ { STORAGE_INTEGRATION = <integration_name> } | { CREDENTIALS = ( {  { AWS_KEY_ID = '<string>' AWS_SECRET_KEY = '<string>' [ AWS_TOKEN = '<string>' ] } | AWS_ROLE = '<string>'  } ) } ]\n  [ ENCRYPTION = ( [ TYPE = 'AWS_CSE' ] MASTER_KEY = '<string>'\n                   | TYPE = 'AWS_SSE_S3'\n                   | TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = '<string>' ]\n                   | TYPE = 'NONE' ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\nexternalStageParams (for Google Cloud Storage) ::=\n  [ URL = 'gcs://<bucket>[/<path>/]' ]\n  [ STORAGE_INTEGRATION = <integration_name> } ]\n  [ ENCRYPTION = (   TYPE = 'GCS_SSE_KMS' [ KMS_KEY_ID = '<string>' ]\n                   | TYPE = 'NONE' ) ]\n\nexternalStageParams (for Microsoft Azure) ::=\n  [ URL = 'azure://<account>.blob.core.windows.net/<container>[/<path>/]' ]\n  [ { STORAGE_INTEGRATION = <integration_name> } | { CREDENTIALS = ( [ AZURE_SAS_TOKEN = '<string>' ] ) } ]\n  [ ENCRYPTION = (   TYPE = 'AZURE_CSE' [ MASTER_KEY = '<string>' ]\n                   | TYPE = 'NONE' ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\nformatTypeOptions ::=\n-- If TYPE = CSV\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     FILE_EXTENSION = '<string>'\n     PARSE_HEADER = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     ESCAPE = '<character>' | NONE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n     ENCODING = '<string>' | UTF8\n-- If TYPE = JSON\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     DATE_FORMAT = '<string>' | AUTO\n     TIME_FORMAT = '<string>' | AUTO\n     TIMESTAMP_FORMAT = '<string>' | AUTO\n     BINARY_FORMAT = HEX | BASE64 | UTF8\n     TRIM_SPACE = TRUE | FALSE\n     MULTI_LINE = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n     FILE_EXTENSION = '<string>'\n     ENABLE_OCTAL = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE\n-- If TYPE = AVRO\n     COMPRESSION = AUTO | GZIP | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = ORC\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = PARQUET\n     COMPRESSION = AUTO | LZO | SNAPPY | NONE\n     SNAPPY_COMPRESSION = TRUE | FALSE\n     BINARY_AS_TEXT = TRUE | FALSE\n     USE_LOGICAL_TYPE = TRUE | FALSE\n     TRIM_SPACE = TRUE | FALSE\n     USE_VECTORIZED_SCANNER = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ] )\n-- If TYPE = XML\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     IGNORE_UTF8_ERRORS = TRUE | FALSE\n     PRESERVE_SPACE = TRUE | FALSE\n     STRIP_OUTER_ELEMENT = TRUE | FALSE\n     DISABLE_AUTO_CONVERT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     SKIP_BYTE_ORDER_MARK = TRUE | FALSE",
    "examples": [
        {
            "code": "ALTER STAGE my_int_stage RENAME TO new_int_stage;"
        },
        {
            "code": "ALTER STAGE my_ext_stage\nSET URL='s3://loading/files/new/'\nCOPY_OPTIONS = (ON_ERROR='skip_file');"
        },
        {
            "code": "ALTER STAGE my_ext_stage SET STORAGE_INTEGRATION = myint;"
        },
        {
            "code": "ALTER STAGE my_ext_stage SET CREDENTIALS=(AWS_KEY_ID='d4c3b2a1' AWS_SECRET_KEY='z9y8x7w6');"
        },
        {
            "code": "ALTER STAGE my_ext_stage3 SET ENCRYPTION=(TYPE='AWS_SSE_S3');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the stage to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double  quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the stage; must be unique for the schema. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the options/properties to set for the stage: Modifies the cloud-specific URL, storage integration or credentials, and/or encryption for the external stage. For more details, see External Stage Parameters (in this topic). Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites an existing comment for the stage."
        },
        {
            "name": "URL   =   '   ...   '  ,  .   STORAGE_INTEGRATION   =   ...  ,  .   CREDENTIALS   =   (   ...   )  ,  .   ENCRYPTION   =   (   ...   )",
            "description": "Modifies the cloud-specific URL, storage integration or credentials, and/or encryption for the external stage. For more details, see External Stage Parameters (in this topic)."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the stage."
        },
        {
            "name": "FILE_FORMAT   =   (   FORMAT_NAME   =   ' file_format_name '   )   or   .   FILE_FORMAT   =   (   TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   |   CUSTOM   [   ...   ]   )",
            "description": "Modifies the file format for the stage, which can be either: Specifies an existing file format object to use for the stage. The specified file format object determines the format type (CSV, JSON, etc.)\nand other format options for data files. Note that no additional format options are specified in the string. Instead, the named file format object defines the other file format\noptions used for loading/unloading data. For more information, see CREATE FILE FORMAT . Specifies the file format type for the stage: Loading data from a stage (using COPY INTO <table> ) accommodates all of the supported file format types. Unloading data into a stage (using COPY INTO <location> ) accommodates CSV, JSON, or PARQUET. If a file format type is specified, additional format-specific options can be modified. For more details, see Format Type Options (in this topic). The CUSTOM format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option. Note FORMAT_NAME and TYPE are mutually exclusive; you can only specify one or the other for a stage."
        },
        {
            "name": "FORMAT_NAME   =   file_format_name",
            "description": "Specifies an existing file format object to use for the stage. The specified file format object determines the format type (CSV, JSON, etc.)\nand other format options for data files. Note that no additional format options are specified in the string. Instead, the named file format object defines the other file format\noptions used for loading/unloading data. For more information, see CREATE FILE FORMAT ."
        },
        {
            "name": "TYPE   =   CSV   |   JSON   |   AVRO   |   ORC   |   PARQUET   |   XML   |   CUSTOM   [   ...   ]",
            "description": "Specifies the file format type for the stage: Loading data from a stage (using COPY INTO <table> ) accommodates all of the supported file format types. Unloading data into a stage (using COPY INTO <location> ) accommodates CSV, JSON, or PARQUET. If a file format type is specified, additional format-specific options can be modified. For more details, see Format Type Options (in this topic). The CUSTOM format type specifies that the underlying stage holds unstructured data and can only be used with the FILE_PROCESSOR copy option."
        },
        {
            "name": "URL   =   ' cloud_specific_url '",
            "description": "If a stage does not have a URL, it is an internal stage Warning Modifying the URL parameter of a stage can break the following functionality for objects that rely on the stage: Pipe objects that leverage cloud messaging to trigger data loads (i.e. where AUTO_INGEST = TRUE ). External tables that leverage cloud messaging to trigger metadata refreshes (i.e. where AUTO_REFRESH = TRUE ). Amazon S3 Modifies the URL for the external location (existing S3 bucket) used to store data files for loading/unloading, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . Accessing cloud storage in a government region using a storage integration is limited to Snowflake\naccounts hosted in the same government region. Similarly, if you need to access cloud storage in a region in China, you can use a storage integration only from a Snowflake\naccount hosted in the same region in China. In these cases, use the CREDENTIALS parameter in the CREATE STAGE command (rather than using a storage\nintegration) to provide the credentials for authentication. bucket is the name of the S3 bucket or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the AWS_ACCESS_POINT_ARN parameter. path is an optional case-sensitive path for files in the cloud storage location (files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage URL . Google Cloud Storage Modifies the URL for the external location (existing GCS bucket) used to store data files for loading/unloading, where: bucket is the name of the GCS bucket. path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files to load. Paths are alternatively called prefixes or folders by different cloud storage\nservices. Microsoft Azure Modifies the URL for the external location (existing Azure container) used to store data files for loading, where: account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all\nsupported types of Azure blob storage accounts, including Data Lake Storage Gen2. container is the name of the Azure container (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files to load. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name   or   .   CREDENTIALS   =   (   cloud_specific_credentials   )",
            "description": "Required only if the Amazon S3, Google Cloud Storage, or Microsoft Azure is private; not required for public buckets/containers Amazon S3 Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a Snowflake\nidentity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using the CREDENTIALS\nparameter when creating stages or loading data. Modifies the security credentials for connecting to AWS and accessing the private S3 bucket where the files to load/unload are staged. For\nmore information, see Configuring secure access to Amazon S3 . The credentials you specify depend on whether you associated the Snowflake access permissions for the bucket with an AWS IAM\n(Identity & Access Management) user or role: IAM user: IAM credentials are required. Temporary (aka “scoped”) credentials are generated by AWS Security Token Service (STS) and\nconsist of three components: AWS_KEY_ID AWS_SECRET_KEY AWS_TOKEN All three are required to access a private bucket. After a designated period of time, temporary credentials expire and can no\nlonger be used. You must then generate a new set of valid temporary credentials. Important The COPY command also allows permanent (aka “long-term”) credentials to be used; however, for security reasons, Snowflake does not recommend using them. If you must use permanent credentials, Snowflake recommends periodically generating new permanent credentials for\nexternal stages. IAM role: Omit the security credentials and access keys and, instead, identify the role using AWS_ROLE and specify the AWS\nrole ARN (Amazon Resource Name). Important The ability to use an AWS IAM role to access a private S3 bucket to load or unload data is now deprecated (i.e. support will be removed\nin a future release, TBD). We highly recommend modifying any existing S3 stages that use this feature to instead reference storage\nintegration objects. For instructions, see Option 1: Configuring a Snowflake storage integration to access Amazon S3 . Google Cloud Storage Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a Snowflake\nidentity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Microsoft Azure Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a Snowflake\nidentity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using the CREDENTIALS\nparameter when creating stages or loading data. Modifies the SAS (shared access signature) token for connecting to Azure and accessing the private container where the files containing\nloaded data are staged. Credentials are generated by Azure."
        },
        {
            "name": "ENCRYPTION   =   (   cloud_specific_encryption   )",
            "description": "Required only for loading from/unloading into encrypted files; not required if storage location and files are unencrypted Modifies the encryption settings used to decrypt encrypted files in the storage location and extract data. Modifies the encryption settings used to encrypt files unloaded to the storage location. Amazon S3 ENCRYPTION = ( [ TYPE = 'AWS_CSE' ] MASTER_KEY = ' string ' | TYPE = 'AWS_SSE_S3' | TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = ' string ' ] | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: AWS_CSE : Client-side encryption (requires a MASTER_KEY value). Currently, the client-side master key you provide can only be a symmetric key. Note that, when a MASTER_KEY value is provided, Snowflake assumes TYPE = AWS_CSE (i.e. when a MASTER_KEY value is\nprovided, TYPE is not required). AWS_SSE_S3 : Server-side encryption that requires no additional encryption settings. AWS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information about the encryption types, see the AWS documentation for client-side encryption or server-side encryption . NONE : No encryption. Specifies the client-side master key used to encrypt the files in the bucket. The master key must be a 128-bit or 256-bit key in\nBase64-encoded form. Optionally specifies the ID for the AWS KMS-managed key used to encrypt files unloaded into the bucket. If no value is provided,\nyour default KMS key ID is used to encrypt files on unload. Note that this value is ignored for data loading. Default: NONE Google Cloud Storage ENCRYPTION = ( TYPE = 'GCS_SSE_KMS' [ KMS_KEY_ID = ' string ' ] | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information, see the Google Cloud documentation: https://cloud.google.com/storage/docs/encryption/customer-managed-keys https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys NONE : No encryption. Optionally specifies the ID for the Cloud KMS-managed key that is used to encrypt files unloaded into the bucket. If no value\nis provided, your default KMS key ID set on the bucket is used to encrypt files on unload. Note that this value is ignored for data loading. The load operation should succeed if the service account has sufficient permissions\nto decrypt data in the bucket. Default: NONE Microsoft Azure ENCRYPTION = ( TYPE = 'AZURE_CSE' MASTER_KEY = ' string ' | TYPE = 'NONE' ) Specifies the encryption type used. Possible values are: AZURE_CSE : Client-side encryption (requires a MASTER_KEY value). For information, see the Client-side encryption information in\nthe Microsoft Azure documentation. NONE : No encryption. Specifies the client-side master key used to encrypt or decrypt files. The master key must be a 128-bit or 256-bit key in Base64-encoded\nform. Default: NONE"
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use private connectivity for an external stage to harden your\nsecurity posture. If the external stage uses a storage integration, and that integration is configured for private connectivity, set this parameter to\nFALSE. For information about using this parameter, see one of the following: AWS private connectivity to external stages . Azure private connectivity for external stages and Snowpipe automation ."
        },
        {
            "name": "URL   =   ' protocol :// bucket [/ path /]'",
            "description": "Modifies the URL for the external location (existing S3 bucket) used to store data files for loading/unloading, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3china refers to S3 storage in public AWS regions in China. s3gov refers to S3 storage in government regions . Accessing cloud storage in a government region using a storage integration is limited to Snowflake\naccounts hosted in the same government region. Similarly, if you need to access cloud storage in a region in China, you can use a storage integration only from a Snowflake\naccount hosted in the same region in China. In these cases, use the CREDENTIALS parameter in the CREATE STAGE command (rather than using a storage\nintegration) to provide the credentials for authentication. bucket is the name of the S3 bucket or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the AWS_ACCESS_POINT_ARN parameter. path is an optional case-sensitive path for files in the cloud storage location (files have names that begin with\na common string) that limits the set of files. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "AWS_ACCESS_POINT_ARN   =   ' string '",
            "description": "Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage URL ."
        },
        {
            "name": "URL   =   'gcs:// bucket [/ path /]'",
            "description": "Modifies the URL for the external location (existing GCS bucket) used to store data files for loading/unloading, where: bucket is the name of the GCS bucket. path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files to load. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "URL   =   'azure:// account .blob.core.windows.net/ container [/ path /]'",
            "description": "Modifies the URL for the external location (existing Azure container) used to store data files for loading, where: account is the name of the Azure account (e.g. myaccount ). Use the blob.core.windows.net endpoint for all\nsupported types of Azure blob storage accounts, including Data Lake Storage Gen2. container is the name of the Azure container (e.g. mycontainer ). path is an optional case-sensitive path for files in the cloud storage location (i.e. files have names that begin with a\ncommon string) that limits the set of files to load. Paths are alternatively called prefixes or folders by different cloud storage\nservices."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a Snowflake\nidentity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using the CREDENTIALS\nparameter when creating stages or loading data."
        },
        {
            "name": "CREDENTIALS   =   (   AWS_KEY_ID   =   ' string '   AWS_SECRET_KEY   =   ' string '   [   AWS_TOKEN   =   ' string '   ]   )   or   .   CREDENTIALS   =   (   AWS_ROLE   =   ' string '   )",
            "description": "Modifies the security credentials for connecting to AWS and accessing the private S3 bucket where the files to load/unload are staged. For\nmore information, see Configuring secure access to Amazon S3 . The credentials you specify depend on whether you associated the Snowflake access permissions for the bucket with an AWS IAM\n(Identity & Access Management) user or role: IAM user: IAM credentials are required. Temporary (aka “scoped”) credentials are generated by AWS Security Token Service (STS) and\nconsist of three components: AWS_KEY_ID AWS_SECRET_KEY AWS_TOKEN All three are required to access a private bucket. After a designated period of time, temporary credentials expire and can no\nlonger be used. You must then generate a new set of valid temporary credentials. Important The COPY command also allows permanent (aka “long-term”) credentials to be used; however, for security reasons, Snowflake does not recommend using them. If you must use permanent credentials, Snowflake recommends periodically generating new permanent credentials for\nexternal stages. IAM role: Omit the security credentials and access keys and, instead, identify the role using AWS_ROLE and specify the AWS\nrole ARN (Amazon Resource Name). Important The ability to use an AWS IAM role to access a private S3 bucket to load or unload data is now deprecated (i.e. support will be removed\nin a future release, TBD). We highly recommend modifying any existing S3 stages that use this feature to instead reference storage\nintegration objects. For instructions, see Option 1: Configuring a Snowflake storage integration to access Amazon S3 ."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a Snowflake\nidentity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION ."
        },
        {
            "name": "STORAGE_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a Snowflake\nidentity and access management (IAM) entity. For more details, see CREATE STORAGE INTEGRATION . Note We highly recommend the use of storage integrations. This option avoids the need to supply cloud storage credentials using the CREDENTIALS\nparameter when creating stages or loading data."
        },
        {
            "name": "CREDENTIALS   =   (   AZURE_SAS_TOKEN   =   ' string '   )",
            "description": "Modifies the SAS (shared access signature) token for connecting to Azure and accessing the private container where the files containing\nloaded data are staged. Credentials are generated by Azure."
        },
        {
            "name": "Data loading :",
            "description": "Modifies the encryption settings used to decrypt encrypted files in the storage location and extract data."
        },
        {
            "name": "Data unloading :",
            "description": "Modifies the encryption settings used to encrypt files unloaded to the storage location."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: AWS_CSE : Client-side encryption (requires a MASTER_KEY value). Currently, the client-side master key you provide can only be a symmetric key. Note that, when a MASTER_KEY value is provided, Snowflake assumes TYPE = AWS_CSE (i.e. when a MASTER_KEY value is\nprovided, TYPE is not required). AWS_SSE_S3 : Server-side encryption that requires no additional encryption settings. AWS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information about the encryption types, see the AWS documentation for client-side encryption or server-side encryption . NONE : No encryption."
        },
        {
            "name": "MASTER_KEY   =   ' string '  (applies to  AWS_CSE  encryption only)",
            "description": "Specifies the client-side master key used to encrypt the files in the bucket. The master key must be a 128-bit or 256-bit key in\nBase64-encoded form."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  AWS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the AWS KMS-managed key used to encrypt files unloaded into the bucket. If no value is provided,\nyour default KMS key ID is used to encrypt files on unload. Note that this value is ignored for data loading."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption that accepts an optional KMS_KEY_ID value. For more information, see the Google Cloud documentation: https://cloud.google.com/storage/docs/encryption/customer-managed-keys https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys NONE : No encryption."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  GCS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the Cloud KMS-managed key that is used to encrypt files unloaded into the bucket. If no value\nis provided, your default KMS key ID set on the bucket is used to encrypt files on unload. Note that this value is ignored for data loading. The load operation should succeed if the service account has sufficient permissions\nto decrypt data in the bucket."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: AZURE_CSE : Client-side encryption (requires a MASTER_KEY value). For information, see the Client-side encryption information in\nthe Microsoft Azure documentation. NONE : No encryption."
        },
        {
            "name": "MASTER_KEY   =   ' string '  (applies to AZURE_CSE encryption only)",
            "description": "Specifies the client-side master key used to encrypt or decrypt files. The master key must be a 128-bit or 256-bit key in Base64-encoded\nform."
        },
        {
            "name": "ENABLE   =     TRUE   |   FALSE",
            "description": "Specifies whether to add a directory table to the stage. When the value is TRUE, a directory table is added to the stage. Note Setting this parameter to TRUE is not supported for S3-compatible external stages . The metadata for S3-compatible external stages cannot be refreshed automatically. Default: FALSE"
        },
        {
            "name": "REFRESH",
            "description": "Accesses the staged data files referenced in the directory table definition and updates the table metadata: New files in the path are added to the table metadata. Changes to files in the path are updated in the table metadata. Files no longer in the path are removed from the table metadata. You can execute this command each time files are added to the stage, updated, or dropped. This step synchronizes\nthe metadata with the latest set of associated files in the stage definition for the directory table."
        },
        {
            "name": "SUBPATH   =   ' relative-path '",
            "description": "Optionally specify a relative path to refresh the metadata for a specific subset of the data files."
        }
    ],
    "usage_notes": "For external stages that use an S3 access point:\nIf you’re using a storage integration, you must configure the IAM policy for the integration\nto grant permission to your S3 access point. For more information, see Option 1: Configuring a Snowflake storage integration to access Amazon S3.\nMulti-region access points aren’t supported.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-application",
    "title": "ALTER APPLICATION",
    "description": "Modifies the properties of an installed Snowflake Native App. Use ALTER APPLICATION to upgrade an app to a\nspecific version or patch. This command is also used to set other properties for an app.",
    "syntax": "ALTER APPLICATION [ IF EXISTS ] <name> SET\n  [ COMMENT = '<string-literal>' ]\n  [ SHARE_EVENTS_WITH_PROVIDER = { TRUE | FALSE } ]\n  [ DEBUG_MODE = { TRUE | FALSE } ]\n\nALTER APPLICATION [ IF EXISTS ] <name> UNSET\n  [ COMMENT ]\n  [ SHARE_EVENTS_WITH_PROVIDER ]\n  [ DEBUG_MODE ]\n\nALTER APPLICATION [ IF EXISTS ] <name> RENAME TO <new_app_name>\n\nALTER APPLICATION <name> SET FEATURE POLICY <policy_name> [ FORCE ]\n\nALTER APPLICATION <name> UNSET FEATURE POLICY;\n\nALTER APPLICATION <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER APPLICATION <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER APPLICATION <name> SET SHARED TELEMETRY EVENTS ('<event_definition' [ , <event_definition>, ...])\n\nALTER APPLICATION <name> SET AUTHORIZE_TELEMETRY_EVENT_SHARING = { TRUE | FALSE }\n\nALTER APPLICATION <name> UNSET REFERENCES [ ( '<reference_name>' [ , '<reference_alias>' ] ) ]\n\nALTER APPLICATION <name> UPGRADE\n\nALTER APPLICATION <name> UPGRADE USING VERSION <version_name> [ PATCH <patch_num> ]\n\nALTER APPLICATION <name> UPGRADE USING <path_to_stage>",
    "examples": [
        {
            "code": "ALTER APPLICATION [ IF EXISTS ] <name> SET\n  [ COMMENT = '<string-literal>' ]\n  [ SHARE_EVENTS_WITH_PROVIDER = { TRUE | FALSE } ]\n  [ DEBUG_MODE = { TRUE | FALSE } ]\n\nALTER APPLICATION [ IF EXISTS ] <name> UNSET\n  [ COMMENT ]\n  [ SHARE_EVENTS_WITH_PROVIDER ]\n  [ DEBUG_MODE ]\n\nALTER APPLICATION [ IF EXISTS ] <name> RENAME TO <new_app_name>\n\nALTER APPLICATION <name> SET FEATURE POLICY <policy_name> [ FORCE ]\n\nALTER APPLICATION <name> UNSET FEATURE POLICY;\n\nALTER APPLICATION <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER APPLICATION <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER APPLICATION <name> SET SHARED TELEMETRY EVENTS ('<event_definition' [ , <event_definition>, ...])\n\nALTER APPLICATION <name> SET AUTHORIZE_TELEMETRY_EVENT_SHARING = { TRUE | FALSE }\n\nALTER APPLICATION <name> UNSET REFERENCES [ ( '<reference_name>' [ , '<reference_alias>' ] ) ]\n\nALTER APPLICATION <name> UPGRADE\n\nALTER APPLICATION <name> UPGRADE USING VERSION <version_name> [ PATCH <patch_num> ]\n\nALTER APPLICATION <name> UPGRADE USING <path_to_stage>"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the app being altered. If the identifier contains\nspaces, special characters, or mixed-case characters, the entire string must be enclosed\nin double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET",
            "description": "Specifies one (or more) properties to set for the app (separated by blank spaces, commas, or new lines). For more details\nabout the properties you can set, see CREATE APPLICATION . Adds a comment or overwrites an existing comment for the app. Enables or disables debug mode for the installed app. TRUE enables debug mode for the installed app. FALSE disables debug mode for the installed app. You can only set DEBUG_MODE on the app if the following conditions are met: The installed app is in the same account as the application package. The installed app must have been created in development mode. Development mode is installed with an explicit stage, version, or patch. You have OWNERSHIP privileges on the installed app and your role has been granted\nthe DEVELOP privilege on the application package used to create the installed app. Specifies whether to share logs and event data with the provider. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET",
            "description": "Specifies one (or more) properties and/or session parameters to unset for the app, which resets them to the defaults. You can reset multiple properties/parameters with a single ALTER statement; however, each property/parameter must be\nseparated by a comma. When resetting a property/parameter, specify only the name; specifying a value for the\nproperty/parameter will return an error. COMMENT DEBUG_MODE Disables debug mode for the installed app. This clause is semantically the same as setting DEBUG_MODE = FALSE . TAG tag_name [ , tag_name ... ] REFERENCES[ ( ' reference_name ' [, ' reference_alias ' ] ) ] Unsets a persistent reference for an app. If no arguments are passed,\nunsets all persistent references set for the app."
        },
        {
            "name": "RENAME   TO   new_app_name",
            "description": "Specifies a new identifier for the app. This identifier must be unique for\nyour account."
        },
        {
            "name": "SET   FEATURE   POLICY   policy_name   [   FORCE   ]",
            "description": "Specifies the feature policy to apply to the app. If a feature policy is already set on\nthe app, you can use FORCE to set the feature policy without having to unset the\nfeature policy first."
        },
        {
            "name": "UNSET   FEATURE   POLICY",
            "description": "Removes the feature policy from the app. When a feature policy is removed from an app\nthe account-level feature policy, if it exists, is applied."
        },
        {
            "name": "SET   SHARED   TELEMETRY   EVENTS   (   ' event_definition '   [   ,   event_definition ,   ...   ]   )",
            "description": "Specifies the optional event definition to enable for an app."
        },
        {
            "name": "SET   AUTHORIZE_TELEMETRY_EVENT_SHARING   =   {   TRUE   |   FALSE   }",
            "description": "When set to TRUE, enables all required event definitions for an app. However, optional event definitions\nremain disabled. Use the SET SHARED TELEMETRY EVENTS clause to set optional event definitions for an app. Caution After setting this value to TRUE, you cannot reset the value back to FALSE if there are required event\ndefinitions in the app."
        },
        {
            "name": "UNSET   REFERENCES[   (   ' reference_name '   [   ,   ' reference_alias '   ]   )   ]",
            "description": "Removes the specified references from the app."
        },
        {
            "name": "UPGRADE",
            "description": "Upgrades the app if the provider has published a new version or patch for the app. An app is automatically upgraded when the provider sets the release directive of the app. However, this command may be used to\nbegin the upgrade immediately without waiting for automatic upgrade to take place. This command may only be used on apps\nthat were not created in development mode. Apps in development mode are installed from a listing or without specifying a stage\nor version, and are primarily intended to test the upgrade process."
        },
        {
            "name": "UPGRADE   USING   VERSION   version_name   [   PATCH   patch_num   ]",
            "description": "Upgrades the app to the specified version. If patch_num is not specified,\nthe latest patch is used. This command is only valid for apps that were installed by\nspecifying a version and patch."
        },
        {
            "name": "UPGRADE   USING   path_to_stage",
            "description": "Upgrades the app using files on a named stage at the path specified by path_to_stage . This clause applies only if you installed the app from a named stage."
        },
        {
            "name": "COMMENT   =   '{string}'",
            "description": "Adds a comment or overwrites an existing comment for the app."
        },
        {
            "name": "DEBUG_MODE   =   {   TRUE   |   FALSE   }",
            "description": "Enables or disables debug mode for the installed app. TRUE enables debug mode for the installed app. FALSE disables debug mode for the installed app. You can only set DEBUG_MODE on the app if the following conditions are met: The installed app is in the same account as the application package. The installed app must have been created in development mode. Development mode is installed with an explicit stage, version, or patch. You have OWNERSHIP privileges on the installed app and your role has been granted\nthe DEVELOP privilege on the application package used to create the installed app."
        },
        {
            "name": "SHARE_EVENTS_WITH_PROVIDER   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to share logs and event data with the provider."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "If you do not specify values for optional parameters, values for these parameters are taken from the manifest.yml file. If you\nspecify values in both the manifest and when running the command, values specified in the command take precedence.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-contact",
    "title": "ALTER CONTACT",
    "description": "Modifies the properties of an existing contact.",
    "syntax": "ALTER CONTACT [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER CONTACT [ IF EXISTS ] <name> SET\n  [ {\n    USERS = ( <user_name> [ , <user_name> ... ] )\n    | EMAIL_DISTRIBUTION_LIST = '<email>'\n    | URL = '<url>'\n    } ]\n  [ COMMENT = '<string_literal>' ]",
    "examples": [
        {
            "code": "ALTER CONTACT my_contact SET EMAIL_DISTRIBUTION_LIST = 'support@example.com';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the contact to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Changes the name of the contact to new_name . The new identifier must be unique for the schema. For more information about identifiers, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When a contact is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET   ...",
            "description": "Sets one of the following parameters for the contact: Comma-delimited list of Snowflake users who can be contacted, specified by the name of their user objects. A valid email address, which can be a distribution list if you want users to be able to contact more than one individual. A URL that can be used to contact people about an object. A user-defined string. Specifies a comment for the contact."
        },
        {
            "name": "USERS   =   (   user_name   [   ,   user_name   ...   ]   )",
            "description": "Comma-delimited list of Snowflake users who can be contacted, specified by the name of their user objects."
        },
        {
            "name": "EMAIL_DISTRIBUTION_LIST   =   ' email '",
            "description": "A valid email address, which can be a distribution list if you want users to be able to contact more than one individual."
        },
        {
            "name": "URL   =   ' url '",
            "description": "A URL that can be used to contact people about an object."
        },
        {
            "name": "COMMENT   =   '<string_literal>'",
            "description": "A user-defined string. Specifies a comment for the contact."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-external-access-integration",
    "title": "ALTER EXTERNAL ACCESS INTEGRATION",
    "description": "Modifies the properties of an existing external access integration.",
    "syntax": "ALTER EXTERNAL ACCESS INTEGRATION [ IF EXISTS ] <name> SET\n  [ ALLOWED_NETWORK_RULES = (<rule_name> [ , <rule_name> ... ]) ]\n  [ ALLOWED_API_AUTHENTICATION_INTEGRATIONS = ( { <integration_name_1> [, <integration_name_2>, ... ] | none } ) ]\n  [ ALLOWED_AUTHENTICATION_SECRETS = ( { <secret_name> [ , <secret_name> ... ] | all | none } ) ]\n  [ ENABLED = { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  [ TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ] ]\n\nALTER EXTERNAL ACCESS INTEGRATION [ IF EXISTS ] <name> UNSET {\n  ALLOWED_NETWORK_RULES |\n  ALLOWED_API_AUTHENTICATION_INTEGRATIONS |\n  ALLOWED_AUTHENTICATION_SECRETS |\n  COMMENT |\n  TAG <tag_name> }\n  [ , ... ]",
    "examples": [
        {
            "code": "ALTER EXTERNAL ACCESS INTEGRATION IF EXISTS dev_integration\n  SET ALLOWED_AUTHENTICATION_SECRETS = (my_new_secret);"
        },
        {
            "code": "ALTER EXTERNAL ACCESS INTEGRATION IF EXISTS dev_integration_disabled\n  SET ENABLED = FALSE;\n\nALTER EXTERNAL ACCESS INTEGRATION IF EXISTS dev_integration_disabled\n  SET COMMENT = 'Disabled until the end of the Q1.';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the external access integration to alter. If the identifier contains spaces or special characters, the entire string\nmust be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the integration: Specifies the allowed network rules. Only egress rules may be specified. For reference information about network rules, refer to CREATE NETWORK RULE . Specifies the security integrations whose OAuth authorization server issued the secret used by the UDF or procedure. The security\nintegration must be the type used for external API integration. For reference information about security integrations, refer to CREATE SECURITY INTEGRATION (External API Authentication) . Specifies the secrets that a UDF or procedure can use when referring to this integration. For reference information about secrets, refer to CREATE SECRET . Specifies whether this integration is enabled or disabled. If the integration is disabled, any handler code that relies\non it will be unable to reach the external endpoint. The value is case-insensitive. The default is TRUE . Specifies a comment for the external access integration. Default: No value Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the property to unset for the integration, which resets it to the default: ALLOWED_NETWORK_RULES ALLOWED_API_AUTHENTICATION_INTEGRATIONS ALLOWED_AUTHENTICATION_SECRETS COMMENT TAG tag_name You can reset multiple properties/parameters with a single ALTER statement; however, each property/parameter must be separated by a\ncomma. When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "ALLOWED_NETWORK_RULES   =   ( rule_name   [   ,   rule_name   ...   ])",
            "description": "Specifies the allowed network rules. Only egress rules may be specified. For reference information about network rules, refer to CREATE NETWORK RULE ."
        },
        {
            "name": "ALLOWED_API_AUTHENTICATION_INTEGRATIONS   =   (   integration_name_1   [,   integration_name_2 ,   ...   ]   |   none   )",
            "description": "Specifies the security integrations whose OAuth authorization server issued the secret used by the UDF or procedure. The security\nintegration must be the type used for external API integration. For reference information about security integrations, refer to CREATE SECURITY INTEGRATION (External API Authentication) ."
        },
        {
            "name": "ALLOWED_AUTHENTICATION_SECRETS   =   ( secret_name   [   ,   secret_name   ...   ]   |   all   |   none   )",
            "description": "Specifies the secrets that a UDF or procedure can use when referring to this integration. For reference information about secrets, refer to CREATE SECRET ."
        },
        {
            "name": "ENABLED   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether this integration is enabled or disabled. If the integration is disabled, any handler code that relies\non it will be unable to reach the external endpoint. The value is case-insensitive. The default is TRUE ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the external access integration. Default: No value"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-stream",
    "title": "ALTER STREAM",
    "description": "Modifies the properties, columns, or constraints for an existing stream.",
    "syntax": "ALTER STREAM [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER STREAM [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER STREAM <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER STREAM [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER STREAM mystream SET COMMENT = 'New comment for stream';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the stream to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the stream: Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites an existing comment for the stream."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties/parameters to unset for the stream, which resets them back to their defaults: TAG tag_key [ , tag_key ... ] COMMENT"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string '",
            "description": "Adds a comment or overwrites an existing comment for the stream."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-external-volume",
    "title": "ALTER EXTERNAL VOLUME",
    "description": "Modifies the properties for an existing external volume.",
    "syntax": "ALTER EXTERNAL VOLUME [ IF EXISTS ] <name> ADD STORAGE_LOCATION =\n  (\n    NAME = '<storage_location_name>'\n    cloudProviderParams\n  )\n\nALTER EXTERNAL VOLUME [ IF EXISTS ] <name> REMOVE STORAGE_LOCATION '<storage_location_name>'\n\nALTER EXTERNAL VOLUME [ IF EXISTS ] <name> UPDATE\n  STORAGE_LOCATION = '<s3_compatible_storage_location_name>'\n  CREDENTIALS = (\n    AWS_KEY_ID = '<string>'\n    AWS_SECRET_KEY = '<string>'\n  )\n\nALTER EXTERNAL VOLUME [ IF EXISTS ] <name> SET ALLOW_WRITES = { TRUE | FALSE }\n\nALTER EXTERNAL VOLUME [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\ncloudProviderParams (for Amazon S3) ::=\n  STORAGE_PROVIDER = '{ S3 | S3GOV }'\n  STORAGE_AWS_ROLE_ARN = '<iam_role>'\n  STORAGE_BASE_URL = '<protocol>://<bucket>[/<path>/]'\n  [ STORAGE_AWS_ACCESS_POINT_ARN = '<string>' ]\n  [ ENCRYPTION = ( [ TYPE = 'AWS_SSE_S3' ] |\n              [ TYPE = 'AWS_SSE_KMS' [ KMS_KEY_ID = '<string>' ] ] |\n              [ TYPE = 'NONE' ] ) ]\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n\ncloudProviderParams (for Google Cloud Storage) ::=\n  STORAGE_PROVIDER = 'GCS'\n  STORAGE_BASE_URL = 'gcs://<bucket>[/<path>/]'\n  [ ENCRYPTION = ( [ TYPE = 'GCS_SSE_KMS' ] [ KMS_KEY_ID = '<string>' ] |\n              [ TYPE = 'NONE' ] ) ]\n\ncloudProviderParams (for Microsoft Azure) ::=\n  STORAGE_PROVIDER = 'AZURE'\n  AZURE_TENANT_ID = '<tenant_id>'\n  [ USE_PRIVATELINK_ENDPOINT = { TRUE | FALSE } ]\n  STORAGE_BASE_URL = 'azure://<account>.blob.core.windows.net/<container>[/<path>/]'",
    "examples": [
        {
            "code": "ALTER EXTERNAL VOLUME exvol1 REMOVE STORAGE_LOCATION 'my-us-east-1';"
        },
        {
            "code": "ALTER EXTERNAL VOLUME exvol1\n  ADD STORAGE_LOCATION =\n    (\n      NAME = 'my-s3-us-central-2'\n      STORAGE_PROVIDER = 'S3'\n      STORAGE_BASE_URL = 's3://my_bucket_us_central-1/'\n      STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789012:role/myrole'\n    );"
        },
        {
            "code": "ALTER EXTERNAL VOLUME exvol2\n  ADD STORAGE_LOCATION =\n    (\n      NAME = 'my-gcs-europe-west4'\n      STORAGE_PROVIDER = 'GCS'\n      STORAGE_BASE_URL = 'gcs://my_bucket_europe-west4/'\n    );"
        },
        {
            "code": "ALTER EXTERNAL VOLUME exvol3\n  ADD STORAGE_LOCATION =\n    (\n      NAME = 'my-azure-japaneast'\n      STORAGE_PROVIDER = 'AZURE'\n      STORAGE_BASE_URL = 'azure://sfcdev1.blob.core.windows.net/my_container_japaneast/'\n      AZURE_TENANT_ID = 'a9876545-4321-987b-b23c-2kz436789d0'\n    );"
        },
        {
            "code": "ALTER EXTERNAL VOLUME ext_vol_s3_compat UPDATE\n  STORAGE_LOCATION = 'my_s3_compat_storage_location'\n  CREDENTIALS = (\n    AWS_KEY_ID = '4d5e6f...'\n    AWS_SECRET_KEY = '7g8h9i...'\n  );"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the external volume to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "ADD   STORAGE_LOCATION   =   (   NAME   =   ' storage_location_name '   cloudProviderParams   )",
            "description": "Adds a named storage location to the external volume definition.\nTo add multiple storage locations, execute an ALTER EXTERNAL VOLUME\nstatement for each storage location. Note Apache Iceberg™ tables write to and read from the first storage location in the set that is located\nin the same region as your Snowflake account. To view the external volume definition and storage location regions,\nexecute DESCRIBE EXTERNAL VOLUME ."
        },
        {
            "name": "REMOVE   STORAGE_LOCATION   ' storage_location_name '",
            "description": "Removes the specified storage location from the external volume definition. To remove multiple storage locations,\nexecute an ALTER EXTERNAL VOLUME statement for each storage location. Note The ALTER EXTERNAL VOLUME statement fails if you attempt to remove the active storage location used by Iceberg tables in your account."
        },
        {
            "name": "UPDATE   STORAGE_LOCATION   =   ' s3_compatible_storage_location_name '",
            "description": "Updates the specified S3-compatible storage location from the external volume definition."
        },
        {
            "name": "CREDENTIALS   =   (   AWS_KEY_ID   =   ' string '   AWS_SECRET_KEY   =   ' string '   )",
            "description": "Specifies updated security credentials for connecting to and accessing an S3-compatible storage location."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the external volume (separated by blank spaces, commas, or new lines): Specifies whether write operations are allowed for the external volume. TRUE specifies that write operations are allowed. This parameter must be set to TRUE for Snowflake-managed\nIceberg tables. FALSE specifies that write operations aren’t allowed. You can’t change the value of this parameter to FALSE if\nthere are Snowflake-managed Iceberg tables associated with the external volume. String (literal) that specifies a comment for the external volume."
        },
        {
            "name": "ALLOW_WRITES   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether write operations are allowed for the external volume. TRUE specifies that write operations are allowed. This parameter must be set to TRUE for Snowflake-managed\nIceberg tables. FALSE specifies that write operations aren’t allowed. You can’t change the value of this parameter to FALSE if\nthere are Snowflake-managed Iceberg tables associated with the external volume."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the external volume."
        },
        {
            "name": "STORAGE_PROVIDER   =   'S3'",
            "description": "Specifies the cloud storage provider that stores your data files."
        },
        {
            "name": "STORAGE_AWS_ROLE_ARN   =   iam_role",
            "description": "Specifies the Amazon Resource Name (ARN) of the AWS identity and access management (IAM) role that grants privileges on the S3 bucket\ncontaining your data files. For more information, see Configuring secure access to Amazon S3 ."
        },
        {
            "name": "STORAGE_BASE_URL   =   ' protocol :// bucket [/ path /]'",
            "description": "Specifies the base URL for your cloud storage location, where: protocol is one of the following: s3 refers to S3 storage in public AWS regions outside of China. s3gov refers to S3 storage in government regions . bucket is the name of an S3 bucket that stores your data files or the bucket-style alias for an S3 bucket access point. For an S3 access point, you must also specify a value for the STORAGE_AWS_ACCESS_POINT_ARN parameter. path is an optional path that can be used to provide granular control over objects in the bucket."
        },
        {
            "name": "STORAGE_AWS_ACCESS_POINT_ARN   =   ' string '",
            "description": "Specifies the Amazon resource name (ARN) for your S3 access point. Required only when you specify an S3 access point alias\nfor your storage STORAGE_BASE_URL ."
        },
        {
            "name": "ENCRYPTION   =   (   [   TYPE   =   'AWS_SSE_S3'   ]   |   [   TYPE   =   'AWS_SSE_KMS'   [   KMS_KEY_ID   =   ' string '   ]   ]   |   [   TYPE   =   'NONE'   ]   )",
            "description": "Specifies the properties needed to encrypt data on the external volume. Specifies the encryption type used. Possible values are: AWS_SSE_S3 : Server-side encryption using S3-managed encryption keys. For more information, see Using server-side encryption with Amazon S3-managed encryption keys (SSE-S3) . AWS_SSE_KMS : Server-side encryption using keys stored in KMS. For more information, see Using server-side encryption with AWS Key Management Service (SSE-KMS) . NONE : No encryption. Optionally specifies the ID for the AWS KMS-managed key used to encrypt files written to the bucket. If no value is provided, your default KMS key is used to encrypt files for writing data. Note that this value is ignored when reading data."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter, see Private connectivity to external volumes for AWS ."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: AWS_SSE_S3 : Server-side encryption using S3-managed encryption keys. For more information, see Using server-side encryption with Amazon S3-managed encryption keys (SSE-S3) . AWS_SSE_KMS : Server-side encryption using keys stored in KMS. For more information, see Using server-side encryption with AWS Key Management Service (SSE-KMS) . NONE : No encryption."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  AWS_SSE_KMS  encryption only)",
            "description": "Optionally specifies the ID for the AWS KMS-managed key used to encrypt files written to the bucket. If no value is provided, your default KMS key is used to encrypt files for writing data. Note that this value is ignored when reading data."
        },
        {
            "name": "STORAGE_PROVIDER   =   'GCS'",
            "description": "Specifies the cloud storage provider that stores your data files."
        },
        {
            "name": "STORAGE_BASE_URL   =   'gcs:// bucket [/ path /]'",
            "description": "Specifies the base URL for your cloud storage location, where: bucket is the name of a Cloud Storage bucket that stores your data files. path is an optional path that can be used to provide granular control over objects in the bucket."
        },
        {
            "name": "ENCRYPTION   =   (   [   TYPE   =   'GCS_SSE_KMS'   ]   [   KMS_KEY_ID   =   ' string '   ]   |   [   TYPE   =   'NONE'   ]   )",
            "description": "Specifies the properties needed to encrypt data on the external volume. Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption using keys stored in KMS. For more information, see customer-managed encryption keys . NONE : No encryption. Specifies the ID for the Cloud KMS-managed key that is used to encrypt files written to the bucket. This value is ignored when reading data. The read operation should succeed if the service account has sufficient permissions to the data and any specified KMS keys."
        },
        {
            "name": "TYPE   =   ...",
            "description": "Specifies the encryption type used. Possible values are: GCS_SSE_KMS : Server-side encryption using keys stored in KMS. For more information, see customer-managed encryption keys . NONE : No encryption."
        },
        {
            "name": "KMS_KEY_ID   =   ' string '  (applies to  GCS_SSE_KMS  encryption only)",
            "description": "Specifies the ID for the Cloud KMS-managed key that is used to encrypt files written to the bucket. This value is ignored when reading data. The read operation should succeed if the service account has sufficient permissions to the data and any specified KMS keys."
        },
        {
            "name": "STORAGE_PROVIDER   =   'AZURE'",
            "description": "Specifies the cloud storage provider that stores your data files."
        },
        {
            "name": "AZURE_TENANT_ID   =   ' tenant_id '",
            "description": "Specifies the ID for your Office 365 tenant that the allowed and blocked storage accounts belong to.\nAn external volume can authenticate to only one tenant,\nso the allowed and blocked storage locations must refer to storage accounts that all belong to this tenant. To find your tenant ID, log into the Azure portal and click Azure Active Directory » Properties . The tenant ID is\ndisplayed in the Tenant ID field."
        },
        {
            "name": "USE_PRIVATELINK_ENDPOINT   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to use outbound private connectivity to harden your security posture. For information about using this parameter, see Private connectivity to external volumes for Microsoft Azure ."
        },
        {
            "name": "STORAGE_BASE_URL   =   'azure:// account .blob.core.windows.net/ container [/ path /]'",
            "description": "Specifies the base URL for your cloud storage location, where: account is the name of your Azure account; for example, myaccount . container is the name of an Azure container that stores your data files. path is an optional path that can be used to provide granular control over logical directories in the container."
        }
    ],
    "usage_notes": "For S3 external volumes that use an S3 access point:\nYou must configure the IAM policy for the external volume\nto grant permission to your S3 access point. For more information,\nsee Step 1: Create an IAM policy that grants access to your S3 location.\nMulti-region access points aren’t supported.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-organization-profile",
    "title": "ALTER ORGANIZATION PROFILE",
    "description": "Modifies the properties of an organization profile\nusing an inline YAML manifest, or using a YAML manifest file located in a stage location.",
    "syntax": "ALTER ORGANIZATION PROFILE [ IF EXISTS ] <name> AS '<yaml_manifest_string>'\n\nALTER ORGANIZATION PROFILE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER ORGANIZATION PROFILE [ IF EXISTS ] <name> PUBLISH\n\nALTER ORGANIZATION PROFILE <name> ADD VERSION [ [ IF NOT EXISTS ] <version_alias_name> ]\n  FROM @<yaml_manifest_stage_location>\n\nALTER ORGANIZATION PROFILE <name> ADD LIVE VERSION [ [ IF NOT EXISTS ] <version_alias_name> ]\n  FROM LAST\n\nALTER ORGANIZATION PROFILE <name> COMMIT\n\nALTER ORGANIZATION PROFILE <name> ABORT",
    "examples": [
        {
            "code": "ALTER ORGANIZATION PROFILE MYORGPROFILE ADD VERSION V2 FROM @STAGE_PATH_WITH_UPDATED_MANIFEST;"
        },
        {
            "code": "ALTER ORGANIZATION PROFILE MYORGPROFILE PUBLISH;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier (name) for the organization profile being altered. Organization profile names can only contain uppercase characters or numbers, and they must start with an uppercase character."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Changes the name of the organization profile to new_name . The new identifier must be unique within the current organization. The identifier must conform to Snowflake identifier requirements. See Identifier requirements . Additionally, organization profile names can only contain uppercase characters or numbers, and they must start with an uppercase character. Note An organization profile with the same name cannot already exist in the organization;\notherwise, the statement returns an error."
        },
        {
            "name": "PUBLISH",
            "description": "Makes a previously undiscoverable organization profile discoverable."
        },
        {
            "name": "ADD   VERSION   [   [   IF   NOT   EXISTS   ]   version_alias_name   ]",
            "description": "Specifies the unique version identifier for the version being added. If version_alias_name isn’t specified, an alias isn’t created. If the identifier contains spaces, special characters, or mixed-case characters, the entire identifier must be enclosed in double quotes. Identifiers enclosed in double quotes are also case sensitive. The FIRST, LAST, DEFAULT, and LIVE keywords are reserved as version shortcuts and can’t be used. The unique version identifier can’t start with “version$” and can’t contain slashes ( / ). For information about identifier syntax, see Identifier requirements ."
        },
        {
            "name": "ADD   LIVE   VERSION   [   [   IF   NOT   EXISTS   ]   version_alias_name   ]",
            "description": "Adds a new live editable version with the specified name from the last committed version. version_alias_name is optional and if it isn’t specified, an alias isn’t created. If the identifier contains spaces, special characters, or mixed-case characters, the entire identifier must be enclosed in double quotes. Identifiers enclosed in double quotes are also case sensitive. The FIRST, LAST, DEFAULT, and LIVE keywords are reserved as version shortcuts and can’t be used. The unique version identifier can’t start with “version$” and can’t contain slashes ( / ). For information about identifier syntax, see Identifier requirements . Changes made to the files in a live version are not applied to the organization profile until the live version is committed. The properties of an organization profile remain unchanged until the live version is committed."
        },
        {
            "name": "AS   yaml_manifest_string",
            "description": "The YAML manifest for the organization profile. For organizational listing profile manifest fields,\nsee Organization profile manifest reference . Inline manifests are normally provided as dollar-quoted strings.\nFor more information, see Dollar-quoted string constants ."
        },
        {
            "name": "FROM   ' yaml_manifest_stage_location '",
            "description": "Specifies the external stage, internal stage, or Snowflake Git repository clone YAML format manifest stage location."
        },
        {
            "name": "COMMIT",
            "description": "Commits the changes in the organization profile. The live version being committed must contain a valid organization profile manifest file."
        },
        {
            "name": "ABORT",
            "description": "Discards the changes in the organization profile."
        }
    ],
    "usage_notes": "Organization profiles can be renamed only when they are in draft state.\nWhen setting the live version of the YAML format manifest for an organization profile, you must use COMMIT to apply the changes, or ABORT to discard the changes. An organization profile can only have one live version at a time."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-dynamic-table",
    "title": "ALTER DYNAMIC TABLE",
    "description": "Modifies the properties of a dynamic table.",
    "syntax": "ALTER DYNAMIC TABLE [ IF EXISTS ] <name> { SUSPEND | RESUME }\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> SWAP WITH <target_dynamic_table_name>\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> REFRESH [ COPY SESSION ]\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> { clusteringAction }\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> { tableColumnCommentAction }\n\nALTER DYNAMIC TABLE <name> { SET | UNSET } COMMENT = '<string_literal>'\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> dataGovnPolicyTagAction\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> searchOptimizationAction\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> SET\n  [ TARGET_LAG = { '<num> { seconds | minutes | hours | days }'  | DOWNSTREAM } ]\n  [ WAREHOUSE = <warehouse_name> ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ LOG_LEVEL = '<log_level>' ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> UNSET\n  [ DATA_RETENTION_TIME_IN_DAYS ],\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS ],\n  [ DEFAULT_DDL_COLLATION ]\n  [ LOG_LEVEL ]\n  [ CONTACT <purpose> ]\n\nclusteringAction ::=\n  {\n    CLUSTER BY ( <expr> [ , <expr> , ... ] )\n    | { SUSPEND | RESUME } RECLUSTER\n    | DROP CLUSTERING KEY\n  }\n\ntableCommentAction ::=\n  {\n    ALTER | MODIFY [ ( ]\n                           [ COLUMN ] <col1_name> COMMENT '<string>'\n                         , [ COLUMN ] <col1_name> UNSET COMMENT\n                       [ , ... ]\n                   [ ) ]\n  }\n\ndataGovnPolicyTagAction ::=\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n  |\n  {\n    SET AGGREGATION POLICY <policy_name>\n      [ ENTITY KEY ( <col_name> [, ... ] ) ]\n      [ FORCE ]\n  | UNSET AGGREGATION POLICY\n  }\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] [ FORCE ]\n      | UNSET MASKING POLICY\n  }\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> SET TAG\n      <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n      , [ COLUMN ] <col2_name> SET TAG\n          <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET PROJECTION POLICY <policy_name>\n          [ FORCE ]\n      | UNSET PROJECTION POLICY\n}\n|\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n                  , [ COLUMN ] <col2_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n\nsearchOptimizationAction ::=\n  {\n    ADD SEARCH OPTIMIZATION [\n      ON <search_method_with_target> [ , <search_method_with_target> ... ]\n        [ EQUALITY ]\n      ]\n\n    | DROP SEARCH OPTIMIZATION [\n      ON { <search_method_with_target> | <column_name> | <expression_id> }\n        [ EQUALITY ]\n        [ , ... ]\n      ]\n\n    | SUSPEND SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n\n    | RESUME SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n  }",
    "examples": [
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table SET\n  TARGET_LAG = '1 hour';"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table SET TARGET_LAG = DOWNSTREAM;"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table SUSPEND;"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table RESUME;"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table RENAME TO my_updated_dynamic_table;"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table SWAP WITH my_new_dynamic_table;"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table CLUSTER BY (date);"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table DROP CLUSTERING KEY;"
        },
        {
            "code": "ALTER DYNAMIC TABLE my_dynamic_table REFRESH COPY SESSION"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the dynamic table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SUSPEND   |   RESUME",
            "description": "Specifies the action to perform on the dynamic table: SUSPEND suspends refreshes on the dynamic table. If the dynamic table is used\nby other dynamic tables, they are also suspended. RESUME resumes refreshes on the dynamic table. Resume operations cascade\ndownstream to all downstream dynamic tables not manually suspended."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Renames the specified dynamic table with a new identifier that is not currently used by\nany other dynamic tables in the schema. Renaming a dynamic table requires the CREATE DYNAMIC TABLE privilege on the schema for\nthe dynamic table. You can also move the dynamic table to a different database and/or schema while\noptionally renaming the dynamic table. To do so, specify a qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . new_name or schema_name . new_name ,\nrespectively. The following restrictions apply: The destination database and/or schema must already exist. In addition, an object\nwith the same name cannot already exist in the new location; otherwise, the\nstatement returns an error. You can’t move an object to a managed access schema unless the object owner\n(that is, the role that has the OWNERSHIP privilege on the object) also owns the\ntarget schema. When an object (table, column, etc.) is renamed, other objects that reference it\nmust be updated with the new name."
        },
        {
            "name": "SWAP   WITH   target_dynamic_table_name",
            "description": "Swaps two dynamic tables in a single transaction. The role used to perform this\noperation must have OWNERSHIP privileges on both dynamic tables. The following restrictions apply: You can only swap a dynamic table with another dynamic table."
        },
        {
            "name": "REFRESH   [   COPY   SESSION   ]",
            "description": "Specifies that the dynamic table should be manually refreshed. Both user-suspended and auto-suspended dynamic tables can be manually refreshed.\nManually refreshed dynamic tables return MANUAL as the output for refresh_trigger in the DYNAMIC_TABLE_REFRESH_HISTORY function. Note that refreshing a dynamic table also refreshes all upstream dynamic tables as\nof the same data timestamp. For more information, see Alter the warehouse or target lag for dynamic tables . For information on dynamic table refresh status, see DYNAMIC_TABLE_REFRESH_HISTORY . COPY SESSION Runs the refresh operation in a copy of the current session using the current user and\nwarehouse. This only applies to a single manual refresh; it does not permanently update the credentials for the dynamic table.\nUse the GRANT OWNERSHIP command to transfer the ownership for scheduled\nrefreshes. For more information, see Transfer ownership . The primary role is the role that owns the dynamic table and secondary roles will match\nthe DEFAULT_SECONDARY_ROLES property of the user."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the table (separated by blank\nspaces, commas, or new lines): Specifies the target lag for the dynamic table: Specifies the maximum amount of time that the dynamic table’s content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes . If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours . The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B. Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed. Specifies the name of the warehouse that provides the compute resources for\nrefreshing the dynamic table. The owner role of the dynamic table must have the USAGE privilege on this warehouse. Object-level parameter that modifies the retention period for the dynamic table for\nTime Travel. For more details, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this parameter and more information about object\nparameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent dynamic tables 0 or 1 for transient dynamic tables Note A value of 0 effectively disables Time Travel for the dynamic table. Object parameter that specifies the maximum number of days Snowflake can extend the\ndata retention period to prevent streams on the dynamic table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS . Specifies a default collation specification for any new columns added to the dynamic table. Setting this parameter does not change the collation specification for any\nexisting columns. For more information, see DEFAULT_DDL_COLLATION . Specifies the severity level of events for this dynamic table that are\ningested and made available in the active event table. Events at the specified level (and at more severe levels) are\ningested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing . Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "TARGET_LAG   =   {   num   {   seconds   |   minutes   |   hours   |   days   }   |   DOWNSTREAM   }",
            "description": "Specifies the target lag for the dynamic table: Specifies the maximum amount of time that the dynamic table’s content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes . If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours . The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B. Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Specifies the name of the warehouse that provides the compute resources for\nrefreshing the dynamic table. The owner role of the dynamic table must have the USAGE privilege on this warehouse."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Object-level parameter that modifies the retention period for the dynamic table for\nTime Travel. For more details, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this parameter and more information about object\nparameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent dynamic tables 0 or 1 for transient dynamic tables Note A value of 0 effectively disables Time Travel for the dynamic table."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days Snowflake can extend the\ndata retention period to prevent streams on the dynamic table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for any new columns added to the dynamic table. Setting this parameter does not change the collation specification for any\nexisting columns. For more information, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of events for this dynamic table that are\ningested and made available in the active event table. Events at the specified level (and at more severe levels) are\ningested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "' num   seconds   |   minutes   |   hours   |   days'",
            "description": "Specifies the maximum amount of time that the dynamic table’s content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes . If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours . The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B."
        },
        {
            "name": "DOWNSTREAM",
            "description": "Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties/parameters to unset for the dynamic table, which\nresets them back to their defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS DEFAULT_DDL_COLLATION LOG_LEVEL CONTACT purposes"
        }
    ],
    "usage_notes": "To alter a dynamic table, you must be using a role that has OPERATE privilege on that\ndynamic table. For general information, see Privileges to view a dynamic table’s metadata.\nMaking changes to masking policies on a base table causes a reinitialization.\nIf you want to update an existing dynamic table and need to see its current definition,\ncall the GET_DDL function.\nYou can use data metric functions with dynamic tables by executing an ALTER TABLE\ncommand. For more information, see Use data metric functions to perform data quality checks.\nYou cannot use IDENTIFIER() to specify the\nname of the dynamic table to alter. For example, the following statement isn’t supported:\nAfter a reinitialization or full refresh, search indexes on dynamic tables are rebuilt.\nThis process involves dropping the existing indexes and rebuilding them from scratch,\nwhich might incur higher costs. For more information, see Search optimization cost estimation and management.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-network-rule",
    "title": "ALTER NETWORK RULE",
    "description": "Modifies an existing network rule.",
    "syntax": "ALTER NETWORK RULE [ IF EXISTS ] <name> SET\n  VALUE_LIST = ( '<value>'  [ , '<value>', ... ] )\n  [ COMMENT = '<string_literal>' ]\n\nALTER NETWORK RULE [ IF EXISTS ] <name> UNSET { VALUE_LIST | COMMENT }",
    "examples": [
        {
            "code": "ALTER NETWORK RULE [ IF EXISTS ] <name> SET\n  VALUE_LIST = ( '<value>'  [ , '<value>', ... ] )\n  [ COMMENT = '<string_literal>' ]\n\nALTER NETWORK RULE [ IF EXISTS ] <name> UNSET { VALUE_LIST | COMMENT }"
        },
        {
            "code": "value_list = ('0.0.0.0:80');"
        },
        {
            "code": "value_list = ('0.0.0.0:443');"
        },
        {
            "code": "value_list = ('0.0.0.0');"
        },
        {
            "code": "value_list = ('0.0.0.0:80', '0.0.0.0:443');"
        },
        {
            "code": "ALTER NETWORK RULE cloud_network SET VALUE_LIST = ('47.88.25.32/27');"
        },
        {
            "code": "ALTER NETWORK RULE corporate_network SET VALUE_LIST = ('vpce-123abc3420c1931');"
        },
        {
            "code": "ALTER NETWORK RULE external_access_rule SET VALUE_LIST = ('example.com', 'example.com:443');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier of the network rule. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes. Identifiers enclosed in\ndouble quotes are case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the network rule: Replaces the current network identifiers with a new list of identifiers. Using this command is not additive; previously specified\nvalues are removed when you set a new value list. Valid values in the list are determined by the type of network rule: When TYPE = IPV4 , each value must be a valid IPv4 address or range of addresses . When TYPE = AWSVPCEID , each value must be a valid VPCE ID. VPC IDs are not supported. When TYPE = AZURELINKID , each value must be a valid LinkID of an Azure private endpoint . Execute the SYSTEM$GET_PRIVATELINK_AUTHORIZED_ENDPOINTS function to retrieve the LinkID associated with an account. When TYPE = HOST_PORT , each value must resolve to a valid domain. Optionally, it can also include a port or range of ports. In most cases, the valid port range is 1-65535. If you do not specify a port, it defaults to 443. If an external network location supports dynamic ports, you need to specify all possible ports. To allow access to all ports, define the port as 0; for example, example.com:0 . When the value resolves to a domain, you can use a single asterisk as a wildcard character. The asterisk matches only alphanumeric\ncharacters and hyphens ( - ). Wildcards are supported only for a single level of subdomains, as in the following examples: *.google.com snowflake-*.google.com and snowflake*abc.google.com You can allow requests to all outbound endpoints by specifying 0.0.0.0 as the domain, as in the examples below.\nWhen you specify 0.0.0.0 as the domain, you may use only 443 and 80 as port values. Allow access to all endpoints at port 80 Allow access to all endpoints at port 443 Allow access to all endpoints at both port 80 and 443 When TYPE = PRIVATE_HOST_PORT , specify one valid domain. In most cases, the valid port range is 1-65535. If you do not specify a port, it defaults to 443. If an external network location supports dynamic ports, you need to specify all possible ports. To allow access to all ports, define the port as 0; for example, example.com:0 . Adds a comment for the first time or overwrites an existing comment."
        },
        {
            "name": "UNSET   ...",
            "description": "Clears properties of the network rule: Removes all network identifiers from the network rule. Removes the comment that was associated with the network rule."
        },
        {
            "name": "VALUE_LIST   =   (   ' value '    [,   ' value ',   ...]   )",
            "description": "Replaces the current network identifiers with a new list of identifiers. Using this command is not additive; previously specified\nvalues are removed when you set a new value list. Valid values in the list are determined by the type of network rule: When TYPE = IPV4 , each value must be a valid IPv4 address or range of addresses . When TYPE = AWSVPCEID , each value must be a valid VPCE ID. VPC IDs are not supported. When TYPE = AZURELINKID , each value must be a valid LinkID of an Azure private endpoint . Execute the SYSTEM$GET_PRIVATELINK_AUTHORIZED_ENDPOINTS function to retrieve the LinkID associated with an account. When TYPE = HOST_PORT , each value must resolve to a valid domain. Optionally, it can also include a port or range of ports. In most cases, the valid port range is 1-65535. If you do not specify a port, it defaults to 443. If an external network location supports dynamic ports, you need to specify all possible ports. To allow access to all ports, define the port as 0; for example, example.com:0 . When the value resolves to a domain, you can use a single asterisk as a wildcard character. The asterisk matches only alphanumeric\ncharacters and hyphens ( - ). Wildcards are supported only for a single level of subdomains, as in the following examples: *.google.com snowflake-*.google.com and snowflake*abc.google.com You can allow requests to all outbound endpoints by specifying 0.0.0.0 as the domain, as in the examples below.\nWhen you specify 0.0.0.0 as the domain, you may use only 443 and 80 as port values. Allow access to all endpoints at port 80 Allow access to all endpoints at port 443 Allow access to all endpoints at both port 80 and 443 When TYPE = PRIVATE_HOST_PORT , specify one valid domain. In most cases, the valid port range is 1-65535. If you do not specify a port, it defaults to 443. If an external network location supports dynamic ports, you need to specify all possible ports. To allow access to all ports, define the port as 0; for example, example.com:0 ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment for the first time or overwrites an existing comment."
        },
        {
            "name": "VALUE_LIST",
            "description": "Removes all network identifiers from the network rule."
        },
        {
            "name": "COMMENT",
            "description": "Removes the comment that was associated with the network rule."
        }
    ],
    "usage_notes": "When specifying IP addresses for a network rule, Snowflake supports ranges of IP addresses using Classless Inter-Domain Routing (CIDR) notation.\nFor example, 192.168.1.0/24 represents all IPv4 addresses in the range of 192.168.1.0 to 192.168.1.255.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-role",
    "title": "ALTER ROLE",
    "description": "Modifies the properties for an existing custom role. Currently, the only supported\noperations are renaming a role or adding/overwriting/removing a comment for a role.",
    "syntax": "ALTER ROLE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER ROLE [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER ROLE [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER ROLE [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER ROLE [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER ROLE role1 RENAME TO role2;"
        },
        {
            "code": "ALTER ROLE myrole SET COMMENT = 'New comment for role';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the role to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the role; must be unique for your account. For more details, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the role: Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites an existing comment for the role."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the role, which resets them to the defaults. TAG tag_name [ , tag_name ... ] COMMENT"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the role."
        }
    ],
    "usage_notes": "Only the role owner (i.e. the role with the OWNERSHIP privilege on the role), or a higher role, can execute this command.\nTo rename a role (using the RENAME TO new_name parameter) the role that executes this command must also have the global CREATE ROLE\nprivilege.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-git-repository",
    "title": "ALTER GIT REPOSITORY",
    "description": "Modifies the properties of a Snowflake Git repository clone.",
    "syntax": "ALTER GIT REPOSITORY [ IF EXISTS ] <name> SET\n  [ GIT_CREDENTIALS = <secret_name> ]\n  [ API_INTEGRATION = <integration_name> ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER GIT REPOSITORY [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER GIT REPOSITORY [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER GIT REPOSITORY [ IF EXISTS ] <name> UNSET {\n  GIT_CREDENTIALS |\n  COMMENT }\n  [ , ... ]\n\nALTER GIT REPOSITORY [ IF EXISTS ] <name> FETCH",
    "examples": [
        {
            "code": "ALTER GIT REPOSITORY snowflake_extensions FETCH;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the Git repository clone to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the integration: Specifies the secret object containing credentials for authenticating with the remote Git repository. The secret you specify here must be a secret specified by the ALLOWED_AUTHENTICATION_SECRETS parameter of the API integration specified\nfor this Git repository. For reference information about secrets, see CREATE SECRET . Specifies the API integration containing details about how Snowflake should interact with the repository API. For reference information about API integrations, see CREATE API INTEGRATION . Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Specifies a comment. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the property to unset for the integration, which resets it to the default value: GIT_CREDENTIALS COMMENT To unset multiple properties or parameters with a single ALTER statement, separate each property or parameter with a comma. When unsetting a property or parameter, specify only the property or parameter name (unless the syntax above indicates that you\nshould specify the value). Specifying the value returns an error."
        },
        {
            "name": "FETCH",
            "description": "Fetches content from the remote Git repository to the Git repository clone. The content fetched is a full clone that fetches all branches, tags, and commits from the remote repository. The command also prunes\nbranches and commits that were fetched earlier but no longer exist in the remote repository."
        },
        {
            "name": "GIT_CREDENTIALS   =   secret_name",
            "description": "Specifies the secret object containing credentials for authenticating with the remote Git repository. The secret you specify here must be a secret specified by the ALLOWED_AUTHENTICATION_SECRETS parameter of the API integration specified\nfor this Git repository. For reference information about secrets, see CREATE SECRET ."
        },
        {
            "name": "API_INTEGRATION   =   integration_name",
            "description": "Specifies the API integration containing details about how Snowflake should interact with the repository API. For reference information about API integrations, see CREATE API INTEGRATION ."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment. Default: No value"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-connection",
    "title": "ALTER CONNECTION",
    "description": "Modifies the properties for an existing connection.",
    "syntax": "ALTER CONNECTION [ IF EXISTS ] <name> ENABLE FAILOVER TO ACCOUNTS <organization_name>.<account_name> [ , <organization_name>.<account_name> ... ]\n                        [ IGNORE EDITION CHECK ]\n\nALTER CONNECTION [ IF EXISTS ] <name> DISABLE FAILOVER [ TO ACCOUNTS <organization_name>.<account_name> [ , <organization_name>.<account_name> ... ] ]\n\nALTER CONNECTION [ IF EXISTS ] <name> PRIMARY\n\nALTER CONNECTION [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER CONNECTION [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER CONNECTION myconnection ENABLE FAILOVER TO ACCOUNTS myorg.myaccount2, myorg.myaccount3;"
        },
        {
            "code": "ALTER CONNECTION myconnection SET COMMENT = 'New comment for connection';"
        },
        {
            "code": "ALTER CONNECTION myconnection PRIMARY;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the connection to alter."
        },
        {
            "name": "ENABLE   FAILOVER   TO   ACCOUNTS   organization_name . account_name   [   ,   organization_name . account_name   ...   ]",
            "description": "Specifies a comma-separated list of accounts in your organization where a secondary connection for this primary connection can be\npromoted to serve as the primary connection. Include your organization name for each account in the list. Each account in the list must be located in a different region than the account with the primary connection. Otherwise,\nthe command fails."
        },
        {
            "name": "DISABLE   FAILOVER   [   TO   ACCOUNTS   organization_name . account_name   [   ,   organization_name . account_name   ...   ]   ]",
            "description": "Disables failover for this primary connection, meaning no secondary connection for this primary connection can be promoted to serve as the primary\nconnection. To disable failover to selected accounts (rather than to all accounts), specify a comma-delimited list of those accounts."
        },
        {
            "name": "PRIMARY",
            "description": "Promote connection to serve as primary connection."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the connection: Adds a comment or overwrites an existing comment for the connection."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the connection, which resets them to the defaults. Currently, the only property you can unset is COMMENT , which removes the comment, if one exists, for the connection."
        },
        {
            "name": "COMMENT   =   ' string '",
            "description": "Adds a comment or overwrites an existing comment for the connection."
        }
    ],
    "usage_notes": "Only account administrators (users with the ACCOUNTADMIN role) can execute this SQL command.\nIf private connectivity to the Snowflake service is enabled for your Snowflake account, your network administrator must update the\nDNS CNAME record for your connection URL when a connection is promoted to serve as the primary connection. For more information, see\nConfiguring the DNS settings for private connectivity to the Snowflake service.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-dbt-project",
    "title": "ALTER DBT PROJECT",
    "description": "Modifies the properties of an existing dbt project object.",
    "syntax": "ALTER DBT PROJECT [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER DBT PROJECT [ IF EXISTS ] ADD VERSION <version_name_alias>\n  FROM '<source_location>'\n\nALTER DBT PROJECT [ IF EXISTS ] <name> SET\n  [ DEFAULT_ARGS = '<string_literal>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER DBT PROJECT [ IF EXISTS ] <name> UNSET\n  [ DEFAULT_ARGS ]\n  [ COMMENT ]",
    "examples": [
        {
            "code": "-- Update the Git repository object to fetch the latest code\n\nALTER GIT REPOSITORY sales_db.integrations_schema.sales_dbt_git_stage FETCH;\n\n-- Add a new version to the dbt project object based on the updated Git repository object\n\nALTER DBT PROJECT sales_db.dbt_projects_schema.sales_model\n  ADD VERSION\n  FROM '@sales_db.integrations_schema.sales_dbt_git_stage/branches/main/sales_dbt_project';"
        },
        {
            "code": "ALTER DBT PROJECT sales_db.dbt_projects_schema.sales_model\n  SET DEFAULT_ARGS = 'run --target prod';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the dbt project object to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Changes the name of the dbt project object to new_name . The new identifier must be unique for the schema. For more information about identifiers, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "ADD   VERSION   version_name_alias",
            "description": "Creates a new version name alias that corresponds to the version specified as source_location . The version name alias is a string literal. A string that specifies the location of the source files and version for the dbt project from which the version name alias will be created. The dbt project source files can be in any one of the following locations: A Git repository stage , for example: '@my_db.my_schema.my_git_repository_stage/branches/my_branch/path/to/dbt_project_or_projects_parent' For more information about creating a Git repository object in Snowflake that connects a Git repository to a workspace for dbt Projects on Snowflake, see Create a workspace connected to your Git repository . For more information about creating and managing a Git repository object and stage without using a workspace, see Using a Git repository in Snowflake and CREATE GIT REPOSITORY . An existing dbt project stage , for example: 'snow://dbt/my_db.my_schema.my_existing_dbt_project_object/versions/last' The version specifier is required and can be last (as shown in the previous example), first , or the specifier for any existing version in the form version$<num> . For more information, see Versioning for dbt project objects and files . An internal named stage , for example: '@my_db.my_schema.my_internal_named_stage/path/to/dbt_projects_or_projects_parent' Internal user stages and table stages aren’t supported. A workspace for dbt on Snowflake , for example: 'snow://workspace/user$.public.\"my_workspace_name\"/versions/live/path/to/dbt_projects_or_projects_parent' We recommend enclosing the workspace name in double quotes because workspace names are case-sensitive and can contain special characters. The version specifier is required and can be last , first , live , or the specifier for any existing version in the form version$<num> . For more information, see Versioning for dbt project objects and files ."
        },
        {
            "name": "SET   ...",
            "description": "Sets one or more specified properties or parameters to set for the dbt project object: A string that specifies the default dbt command and command line options to use if EXECUTE DBT PROJECT specifies no command. Important Arguments that you explicitly specify in an EXECUTE DBT PROJECT command overwrite any and all DEFAULT_ARGS specified in the DBT PROJECT definition. Adds a comment or overwrites an existing comment for the dbt project object."
        },
        {
            "name": "UNSET   ...",
            "description": "Unsets one or more specified properties or parameters for the dbt project object, which resets the properties to NULL: DEFAULT_ARGS COMMENT To unset multiple properties or parameters with a single ALTER statement, separate each property or parameter with a comma. When unsetting a property or parameter, specify only the property or parameter name (unless the syntax above indicates that you\nshould specify the value). Specifying the value returns an error."
        },
        {
            "name": "FROM   ' source_location '",
            "description": "A string that specifies the location of the source files and version for the dbt project from which the version name alias will be created. The dbt project source files can be in any one of the following locations: A Git repository stage , for example: '@my_db.my_schema.my_git_repository_stage/branches/my_branch/path/to/dbt_project_or_projects_parent' For more information about creating a Git repository object in Snowflake that connects a Git repository to a workspace for dbt Projects on Snowflake, see Create a workspace connected to your Git repository . For more information about creating and managing a Git repository object and stage without using a workspace, see Using a Git repository in Snowflake and CREATE GIT REPOSITORY . An existing dbt project stage , for example: 'snow://dbt/my_db.my_schema.my_existing_dbt_project_object/versions/last' The version specifier is required and can be last (as shown in the previous example), first , or the specifier for any existing version in the form version$<num> . For more information, see Versioning for dbt project objects and files . An internal named stage , for example: '@my_db.my_schema.my_internal_named_stage/path/to/dbt_projects_or_projects_parent' Internal user stages and table stages aren’t supported. A workspace for dbt on Snowflake , for example: 'snow://workspace/user$.public.\"my_workspace_name\"/versions/live/path/to/dbt_projects_or_projects_parent' We recommend enclosing the workspace name in double quotes because workspace names are case-sensitive and can contain special characters. The version specifier is required and can be last , first , live , or the specifier for any existing version in the form version$<num> . For more information, see Versioning for dbt project objects and files ."
        },
        {
            "name": "DEFAULT_ARGS   =   ' string_literal '",
            "description": "A string that specifies the default dbt command and command line options to use if EXECUTE DBT PROJECT specifies no command. Important Arguments that you explicitly specify in an EXECUTE DBT PROJECT command overwrite any and all DEFAULT_ARGS specified in the DBT PROJECT definition."
        },
        {
            "name": "COMMENT   =   edition_num",
            "description": "Adds a comment or overwrites an existing comment for the dbt project object."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-model-monitor",
    "title": "ALTER MODEL MONITOR",
    "description": "Modifies the properties of a model monitor:",
    "syntax": "ALTER MODEL MONITOR [ IF EXISTS ] <monitor_name> { SUSPEND | RESUME }\n\nALTER MODEL MONITOR [ IF EXISTS ] <monitor_name> SET\n   [ BASELINE='<baseline_table_name>' ]\n   [ REFRESH_INTERVAL='<refresh_interval>' ]\n   [ WAREHOUSE=<warehouse_name> ]",
    "examples": [
        {
            "code": "ALTER MODEL MONITOR [ IF EXISTS ] <monitor_name> { SUSPEND | RESUME }\n\nALTER MODEL MONITOR [ IF EXISTS ] <monitor_name> SET\n   [ BASELINE='<baseline_table_name>' ]\n   [ REFRESH_INTERVAL='<refresh_interval>' ]\n   [ WAREHOUSE=<warehouse_name> ]"
        }
    ],
    "parameters": [
        {
            "name": "monitor_name",
            "description": "Specifies the identifier (i.e. name) of the model monitor. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more model monitor properties to be set. Sets the baseline table that the monitor uses. Sets the warehouse that the monitor uses. The interval at which the monitor refreshes its internal state. The value must be a string representing a time period,\nsuch as '1 day' . Supported units include seconds, minutes, hours, days, weeks, months, quarters, and years.\nYou may use singular (“hour”) or plural (“hours”) for the interval name, but may not abbreviate."
        },
        {
            "name": "BASELINE='<baseline_table_name>'",
            "description": "Sets the baseline table that the monitor uses."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Sets the warehouse that the monitor uses."
        },
        {
            "name": "REFRESH_INTERVAL   =   ' refresh_interval '",
            "description": "The interval at which the monitor refreshes its internal state. The value must be a string representing a time period,\nsuch as '1 day' . Supported units include seconds, minutes, hours, days, weeks, months, quarters, and years.\nYou may use singular (“hour”) or plural (“hours”) for the interval name, but may not abbreviate."
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-authentication-policy",
    "title": "ALTER AUTHENTICATION POLICY",
    "description": "Modifies the properties of an authentication policy.",
    "syntax": "ALTER AUTHENTICATION POLICY <name> RENAME TO <new_name>\n\nALTER AUTHENTICATION POLICY [ IF EXISTS ] <name> SET\n  [ AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_AUTHENTICATION_METHODS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ MFA_ENROLLMENT = { REQUIRED | OPTIONAL } ]\n  [ MFA_POLICY= ( ALLOWED_METHODS = ( { 'ALL' | 'PASSKEY' | 'TOTP' | 'DUO' } [ , { 'PASSKEY' | 'TOTP' | 'DUO' } ... ] ) ) ]\n  [ CLIENT_TYPES = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ SECURITY_INTEGRATIONS = ( '<string_literal>' [ , '<string_literal>' , ...  ] ) ]\n  [ PAT_POLICY = ( {list_of_properties} ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER AUTHENTICATION POLICY [ IF EXISTS ] <name> UNSET\n  [ CLIENT_TYPES ]\n  [ AUTHENTICATION_METHODS ]\n  [ SECURITY_INTEGRATIONS ]\n  [ MFA_AUTHENTICATION_METHODS ]\n  [ MFA_ENROLLMENT ]\n  [ MFA_POLICY ]\n  [ PAT_POLICY ]\n  [ COMMENT ]",
    "examples": [
        {
            "code": "ALTER AUTHENTICATION POLICY restrict_client_types_policy SET CLIENT_TYPES = ('SNOWFLAKE_UI', 'SNOWSQL');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the authentication policy to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   ...",
            "description": "Specifies a new name for an existing authentication policy."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties to set for the authentication policy, separated by blank spaces, commas, or new lines. Changes the authentication methods that are allowed during login. This parameter accepts one or more of the following values: Caution Restricting by authentication method can have unintended consequences, such as blocking driver connections or third-party\nintegrations. Allow all authentication methods. Allows SAML2 security integrations . If SAML is\npresent, an SSO login option appears. If SAML is not present, an SSO login option does not appear. Allows users to authenticate using username and password. Allows External OAuth . Allows Key pair authentication . Allows users to authenticate with a programmatic access token . Default: ALL . A list of authentication methods that enforce multi-factor authentication (MFA) during login. Authentication methods not listed in this\nparameter do not prompt for multi-factor authentication. The following authentication methods support MFA: SAML PASSWORD This parameter accepts one or more of the following values: Prompts users for MFA, if they are enrolled in MFA, when authenticating with SAML2 security integrations . Prompts users for MFA, if they are enrolled in MFA, when authenticating with a username and password. Default: ('PASSWORD', 'SAML') . Changes whether a user must enroll in multi-factor authentication. Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI ,\nbecause Snowsight is the only place users can enroll in multi-factor authentication (MFA) . Users can choose whether to enroll in MFA. Default: REQUIRED . Specifies the multi-factor authentication (MFA) methods that users can use as a second factor of authentication. You can specify more\nthan one method. Users can use a passkey, an authenticator app, or Duo as their second factor of authentication. Users can use a passkey as their second factor of authentication. Users can use an authenticator app as their second factor of authentication. Users can use Duo as their second factor of authentication. Changes which clients can authenticate with Snowflake. If a client tries to connect, and the client is not one of the valid CLIENT_TYPES values listed below, then the login attempt fails. If you set MFA_ENROLLMENT to REQUIRED , then you must include SNOWFLAKE_UI in the CLIENT_TYPES list to allow\nusers to enroll in MFA. If you want to exclude SNOWFLAKE_UI from the CLIENT_TYPES list, then you must set MFA_ENROLLMENT to OPTIONAL . The CLIENT_TYPES property of an authentication policy is a best effort method to block user logins based on specific clients. It should not be used as the sole control to establish a security boundary. This property accepts one or more of the following values: Allow all clients to authenticate. Snowsight or Classic Console , the Snowflake web interfaces. Caution If SNOWFLAKE_UI is not included in the CLIENT_TYPES list while MFA_ENROLLMENT is set to REQUIRED , or MFA_ENROLLMENT is unspecified, MFA enrollment doesn’t work. Drivers allow access to Snowflake from applications written in supported languages . For example, the Go , JDBC , .NET drivers, and Snowpipe Streaming . Caution If DRIVERS is not included in the CLIENT_TYPES list, automated ingestion may stop working. A command-line client for connecting to Snowflake and for managing developer-centric workloads and SQL operations. A command-line client for connecting to Snowflake. If a client tries to connect, and the client is not one of the valid CLIENT_TYPES , then the login attempt fails. If CLIENT_TYPES is unset, any client can connect. Default: ALL . Changes the security integrations that the authentication policy is associated with. This parameter has no effect when SAML or OAUTH are not in the AUTHENTICATION_METHODS list. All values in the SECURITY_INTEGRATIONS list must be compatible with the values in the AUTHENTICATION_METHODS list. For\nexample, if SECURITY_INTEGRATIONS contains a SAML security integration, and AUTHENTICATION_METHODS contains OAUTH , then you cannot create the authentication policy. Allow all security integrations. Default: ALL . Specifies the policies for programmatic access tokens . Set this to a\nspace-delimited list of one or more of the following properties and values: Specifies the default expiration time (in days) for a programmatic access token. You can specify a value from 1 to the\nmaximum time (which you can specify by setting MAX_EXPIRY_IN_DAYS). The default expiration time is 15 days. For more information, see Setting the default expiration time . Specifies the maximum number of days that can be set for the expiration time for a programmatic access token. You can specify\na value from 1 to 365. The default maximum expiration time is 365 days. Note If there are existing programmatic access tokens with expiration times that exceed the new maximum expiration time, attempts to\nauthenticate with those tokens will fail. For example, suppose that you generate a programmatic access token named my_token with the expiration time of 7 days. If you\nlater change the maximum expiration time for all tokens to 2 days, authenticating with my_token will fail because the\nexpiration time of the token exceeds the new maximum expiration time. For more information, see Setting the maximum expiration time . Specifies how network policy requirements are handled for programmatic access tokens. By default, a user must be subject to a network policy with one or more network rules to generate or use programmatic access tokens: Service users (with TYPE=SERVICE) must be subject to a network policy to generate and use programmatic access tokens. Human users (with TYPE=PERSON) must be subject to a network policy to use programmatic access tokens. To override this behavior, set this property to one of the following values: The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication. For example: Changes the comment for the authentication policy."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the authentication policy, which resets them to their defaults."
        },
        {
            "name": "AUTHENTICATION_METHODS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "Changes the authentication methods that are allowed during login. This parameter accepts one or more of the following values: Caution Restricting by authentication method can have unintended consequences, such as blocking driver connections or third-party\nintegrations. Allow all authentication methods. Allows SAML2 security integrations . If SAML is\npresent, an SSO login option appears. If SAML is not present, an SSO login option does not appear. Allows users to authenticate using username and password. Allows External OAuth . Allows Key pair authentication . Allows users to authenticate with a programmatic access token . Default: ALL ."
        },
        {
            "name": "MFA_AUTHENTICATION_METHODS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "A list of authentication methods that enforce multi-factor authentication (MFA) during login. Authentication methods not listed in this\nparameter do not prompt for multi-factor authentication. The following authentication methods support MFA: SAML PASSWORD This parameter accepts one or more of the following values: Prompts users for MFA, if they are enrolled in MFA, when authenticating with SAML2 security integrations . Prompts users for MFA, if they are enrolled in MFA, when authenticating with a username and password. Default: ('PASSWORD', 'SAML') ."
        },
        {
            "name": "MFA_ENROLLMENT   =   {   REQUIRED   |   OPTIONAL   }",
            "description": "Changes whether a user must enroll in multi-factor authentication. Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI ,\nbecause Snowsight is the only place users can enroll in multi-factor authentication (MFA) . Users can choose whether to enroll in MFA. Default: REQUIRED ."
        },
        {
            "name": "MFA_POLICY=   (   ALLOWED_METHODS   =   (   {   'ALL'   |   'PASSKEY'   |   'TOTP'   |   'DUO'   }   [   ,   {   'PASSKEY'   |   'TOTP'   |   'DUO'   }   ...   ]   )   )",
            "description": "Specifies the multi-factor authentication (MFA) methods that users can use as a second factor of authentication. You can specify more\nthan one method. Users can use a passkey, an authenticator app, or Duo as their second factor of authentication. Users can use a passkey as their second factor of authentication. Users can use an authenticator app as their second factor of authentication. Users can use Duo as their second factor of authentication."
        },
        {
            "name": "CLIENT_TYPES   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "Changes which clients can authenticate with Snowflake. If a client tries to connect, and the client is not one of the valid CLIENT_TYPES values listed below, then the login attempt fails. If you set MFA_ENROLLMENT to REQUIRED , then you must include SNOWFLAKE_UI in the CLIENT_TYPES list to allow\nusers to enroll in MFA. If you want to exclude SNOWFLAKE_UI from the CLIENT_TYPES list, then you must set MFA_ENROLLMENT to OPTIONAL . The CLIENT_TYPES property of an authentication policy is a best effort method to block user logins based on specific clients. It should not be used as the sole control to establish a security boundary. This property accepts one or more of the following values: Allow all clients to authenticate. Snowsight or Classic Console , the Snowflake web interfaces. Caution If SNOWFLAKE_UI is not included in the CLIENT_TYPES list while MFA_ENROLLMENT is set to REQUIRED , or MFA_ENROLLMENT is unspecified, MFA enrollment doesn’t work. Drivers allow access to Snowflake from applications written in supported languages . For example, the Go , JDBC , .NET drivers, and Snowpipe Streaming . Caution If DRIVERS is not included in the CLIENT_TYPES list, automated ingestion may stop working. A command-line client for connecting to Snowflake and for managing developer-centric workloads and SQL operations. A command-line client for connecting to Snowflake. If a client tries to connect, and the client is not one of the valid CLIENT_TYPES , then the login attempt fails. If CLIENT_TYPES is unset, any client can connect. Default: ALL ."
        },
        {
            "name": "SECURITY_INTEGRATIONS   =   (   ' string_literal '   [   ,   ' string_literal '   ,   ...   ]   )",
            "description": "Changes the security integrations that the authentication policy is associated with. This parameter has no effect when SAML or OAUTH are not in the AUTHENTICATION_METHODS list. All values in the SECURITY_INTEGRATIONS list must be compatible with the values in the AUTHENTICATION_METHODS list. For\nexample, if SECURITY_INTEGRATIONS contains a SAML security integration, and AUTHENTICATION_METHODS contains OAUTH , then you cannot create the authentication policy. Allow all security integrations. Default: ALL ."
        },
        {
            "name": "ALL",
            "description": "Allow all authentication methods."
        },
        {
            "name": "SAML",
            "description": "Allows SAML2 security integrations . If SAML is\npresent, an SSO login option appears. If SAML is not present, an SSO login option does not appear."
        },
        {
            "name": "PASSWORD",
            "description": "Allows users to authenticate using username and password."
        },
        {
            "name": "OAUTH",
            "description": "Allows External OAuth ."
        },
        {
            "name": "KEYPAIR",
            "description": "Allows Key pair authentication ."
        },
        {
            "name": "PROGRAMMATIC_ACCESS_TOKEN",
            "description": "Allows users to authenticate with a programmatic access token ."
        },
        {
            "name": "SAML",
            "description": "Prompts users for MFA, if they are enrolled in MFA, when authenticating with SAML2 security integrations ."
        },
        {
            "name": "PASSWORD",
            "description": "Prompts users for MFA, if they are enrolled in MFA, when authenticating with a username and password."
        },
        {
            "name": "REQUIRED",
            "description": "Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI ,\nbecause Snowsight is the only place users can enroll in multi-factor authentication (MFA) ."
        },
        {
            "name": "OPTIONAL",
            "description": "Users can choose whether to enroll in MFA."
        },
        {
            "name": "ALL",
            "description": "Users can use a passkey, an authenticator app, or Duo as their second factor of authentication."
        },
        {
            "name": "PASSKEY",
            "description": "Users can use a passkey as their second factor of authentication."
        },
        {
            "name": "TOTP",
            "description": "Users can use an authenticator app as their second factor of authentication."
        },
        {
            "name": "DUO",
            "description": "Users can use Duo as their second factor of authentication."
        },
        {
            "name": "ALL",
            "description": "Allow all clients to authenticate."
        },
        {
            "name": "SNOWFLAKE_UI",
            "description": "Snowsight or Classic Console , the Snowflake web interfaces. Caution If SNOWFLAKE_UI is not included in the CLIENT_TYPES list while MFA_ENROLLMENT is set to REQUIRED , or MFA_ENROLLMENT is unspecified, MFA enrollment doesn’t work."
        },
        {
            "name": "DRIVERS",
            "description": "Drivers allow access to Snowflake from applications written in supported languages . For example, the Go , JDBC , .NET drivers, and Snowpipe Streaming . Caution If DRIVERS is not included in the CLIENT_TYPES list, automated ingestion may stop working."
        },
        {
            "name": "SNOWFLAKE_CLI",
            "description": "A command-line client for connecting to Snowflake and for managing developer-centric workloads and SQL operations."
        },
        {
            "name": "SNOWSQL",
            "description": "A command-line client for connecting to Snowflake."
        },
        {
            "name": "ALL",
            "description": "Allow all security integrations."
        },
        {
            "name": "PAT_POLICY   =   (   list_of_properties   )",
            "description": "Specifies the policies for programmatic access tokens . Set this to a\nspace-delimited list of one or more of the following properties and values: Specifies the default expiration time (in days) for a programmatic access token. You can specify a value from 1 to the\nmaximum time (which you can specify by setting MAX_EXPIRY_IN_DAYS). The default expiration time is 15 days. For more information, see Setting the default expiration time . Specifies the maximum number of days that can be set for the expiration time for a programmatic access token. You can specify\na value from 1 to 365. The default maximum expiration time is 365 days. Note If there are existing programmatic access tokens with expiration times that exceed the new maximum expiration time, attempts to\nauthenticate with those tokens will fail. For example, suppose that you generate a programmatic access token named my_token with the expiration time of 7 days. If you\nlater change the maximum expiration time for all tokens to 2 days, authenticating with my_token will fail because the\nexpiration time of the token exceeds the new maximum expiration time. For more information, see Setting the maximum expiration time . Specifies how network policy requirements are handled for programmatic access tokens. By default, a user must be subject to a network policy with one or more network rules to generate or use programmatic access tokens: Service users (with TYPE=SERVICE) must be subject to a network policy to generate and use programmatic access tokens. Human users (with TYPE=PERSON) must be subject to a network policy to use programmatic access tokens. To override this behavior, set this property to one of the following values: The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication. For example:"
        },
        {
            "name": "DEFAULT_EXPIRY_IN_DAYS   =   number_of_days",
            "description": "Specifies the default expiration time (in days) for a programmatic access token. You can specify a value from 1 to the\nmaximum time (which you can specify by setting MAX_EXPIRY_IN_DAYS). The default expiration time is 15 days. For more information, see Setting the default expiration time ."
        },
        {
            "name": "MAX_EXPIRY_IN_DAYS   =   number_of_days",
            "description": "Specifies the maximum number of days that can be set for the expiration time for a programmatic access token. You can specify\na value from 1 to 365. The default maximum expiration time is 365 days. Note If there are existing programmatic access tokens with expiration times that exceed the new maximum expiration time, attempts to\nauthenticate with those tokens will fail. For example, suppose that you generate a programmatic access token named my_token with the expiration time of 7 days. If you\nlater change the maximum expiration time for all tokens to 2 days, authenticating with my_token will fail because the\nexpiration time of the token exceeds the new maximum expiration time. For more information, see Setting the maximum expiration time ."
        },
        {
            "name": "NETWORK_POLICY_EVALUATION   =   {   ENFORCED_REQUIRED   |   ENFORCED_NOT_REQUIRED   |   NOT_ENFORCED   }",
            "description": "Specifies how network policy requirements are handled for programmatic access tokens. By default, a user must be subject to a network policy with one or more network rules to generate or use programmatic access tokens: Service users (with TYPE=SERVICE) must be subject to a network policy to generate and use programmatic access tokens. Human users (with TYPE=PERSON) must be subject to a network policy to use programmatic access tokens. To override this behavior, set this property to one of the following values: The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication. The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication."
        },
        {
            "name": "ENFORCED_REQUIRED  (default behavior)",
            "description": "The user must be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication."
        },
        {
            "name": "ENFORCED_NOT_REQUIRED",
            "description": "The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is enforced during authentication."
        },
        {
            "name": "NOT_ENFORCED",
            "description": "The user does not need to be subject to a network policy to generate and use programmatic access tokens. If the user is subject to a network policy, the network policy is not enforced during authentication."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Changes the comment for the authentication policy."
        }
    ],
    "usage_notes": "If you want to update an existing authentication policy and need to see the definition of the policy, run the\nDESCRIBE AUTHENTICATION POLICY command or GET_DDL function."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-secret",
    "title": "ALTER SECRET",
    "description": "Modifies the properties of an existing secret.",
    "syntax": "ALTER SECRET [ IF EXISTS ] <name> SET [ OAUTH_SCOPES = ( '<scope_1>' [ , '<scope_2>' ... ] ) ]\n                                      [ COMMENT = '<string_literal>' ]\n\nALTER SECRET [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER SECRET [ IF EXISTS ] <name> SET [ OAUTH_REFRESH_TOKEN = '<token>' ]\n                                      [ OAUTH_REFRESH_TOKEN_EXPIRY_TIME = '<string_literal>' ]\n                                      [ COMMENT = '<string_literal>' ]\n\nALTER SECRET [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER SECRET [ IF EXISTS ] <name> SET [ API_AUTHENTICATION = '<cloud_provider_security_integration>' ]\n                                      [ COMMENT = '<string_literal>' ]\n\nALTER SECRET [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER SECRET [ IF EXISTS ] <name> SET [ USERNAME = '<username>' ]\n                                      [ PASSWORD = '<password>' ]\n                                      [ COMMENT = '<string_literal>' ]\n\nALTER SECRET [ IF EXISTS ] <name> UNSET COMMENT\n\nALTER SECRET [ IF EXISTS ] <name> SET [ SECRET_STRING = '<string_literal>' ]\n                                      [ COMMENT = '<string_literal>' ]\n\nALTER SECRET [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER SECRET service_now_creds_pw SET COMMENT = 'production secret for servicenow';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set (separated by blank spaces, commas, or new lines). Specifies a comma-separated list of scopes to use when making a request from the OAuth server by a role with USAGE on the integration\nduring the OAuth client credentials flow. This list must be a subset of the scopes defined in the OAUTH_ALLOWED_SCOPES property of the security integration. If the OAUTH_SCOPES property values are not specified, the secret inherits all of the scopes that are specified in the security\nintegration."
        },
        {
            "name": "OAUTH_SCOPES   =   (   ' scope_1 '   [   ,   ' scope_2 '   ...   ]   )",
            "description": "Specifies a comma-separated list of scopes to use when making a request from the OAuth server by a role with USAGE on the integration\nduring the OAuth client credentials flow. This list must be a subset of the scopes defined in the OAUTH_ALLOWED_SCOPES property of the security integration. If the OAUTH_SCOPES property values are not specified, the secret inherits all of the scopes that are specified in the security\nintegration."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set (separated by blank spaces, commas, or new lines). Specifies that this is secret for use with a cloud provider, such as Amazon Web Services (AWS). Specifies the name value of the Snowflake security integration that connects Snowflake to a cloud provider."
        },
        {
            "name": "TYPE   =   CLOUD_PROVIDER_TOKEN",
            "description": "Specifies that this is secret for use with a cloud provider, such as Amazon Web Services (AWS)."
        },
        {
            "name": "API_AUTHENTICATION   =   ' cloud_provider_security_integration '",
            "description": "Specifies the name value of the Snowflake security integration that connects Snowflake to a cloud provider."
        },
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set (separated by blank spaces, commas, or new lines). Specifies the token as a string that is used to obtain a new access token from the OAuth authorization server when the access token\nexpires. Specifies the timestamp as a string when the OAuth refresh token expires."
        },
        {
            "name": "OAUTH_REFRESH_TOKEN   =   ' token '",
            "description": "Specifies the token as a string that is used to obtain a new access token from the OAuth authorization server when the access token\nexpires."
        },
        {
            "name": "OAUTH_REFRESH_TOKEN_EXPIRY_TIME   =   ' string_literal '",
            "description": "Specifies the timestamp as a string when the OAuth refresh token expires."
        },
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set for the session (separated by blank spaces, commas, or new lines). Specifies the username value to store in the secret. Specify this property value when using a secret for basic authentication (i.e. the secret is TYPE = PASSWORD ). Specifies the password value to store in the secret. Specify this property value when using a secret for basic authentication (i.e. the secret is TYPE = PASSWORD )."
        },
        {
            "name": "USERNAME   =   ' username '",
            "description": "Specifies the username value to store in the secret. Specify this property value when using a secret for basic authentication (i.e. the secret is TYPE = PASSWORD )."
        },
        {
            "name": "PASSWORD   =   ' password '",
            "description": "Specifies the password value to store in the secret. Specify this property value when using a secret for basic authentication (i.e. the secret is TYPE = PASSWORD )."
        },
        {
            "name": "name",
            "description": "String that specifies the identifier (i.e. name) for the secret, must be unique in your schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set (separated by blank spaces, commas, or new lines). Specifies the string to store in the secret. The string can be an API token or a string of sensitive value that can be used in the handler code of a UDF or stored procedure. For\ndetails, see Creating and using an external access integration . You should not use this property to store any kind of OAuth token; use one of the other secret types for your OAuth use cases."
        },
        {
            "name": "SECRET_STRING   =   ' string_literal '",
            "description": "Specifies the string to store in the secret. The string can be an API token or a string of sensitive value that can be used in the handler code of a UDF or stored procedure. For\ndetails, see Creating and using an external access integration . You should not use this property to store any kind of OAuth token; use one of the other secret types for your OAuth use cases."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set for the session (separated by blank spaces, commas, or new lines). String (literal) that specifies a comment for the secret. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties/parameters to unset for the secret, which resets them back to their defaults: COMMENT"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "String (literal) that specifies a comment for the secret. Default: No value"
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-session",
    "title": "ALTER SESSION",
    "description": "Sets parameters that change the behavior for the current session.",
    "syntax": "ALTER SESSION SET sessionParams\n\nALTER SESSION UNSET <param_name> [ , <param_name> , ... ]\n\nsessionParams ::=\n  ABORT_DETACHED_QUERY = TRUE | FALSE\n  ACTIVE_PYTHON_PROFILER = 'LINE' | 'MEMORY'\n  AUTOCOMMIT = TRUE | FALSE\n  BINARY_INPUT_FORMAT = <string>\n  BINARY_OUTPUT_FORMAT = <string>\n  DATE_INPUT_FORMAT = <string>\n  DATE_OUTPUT_FORMAT = <string>\n  ERROR_ON_NONDETERMINISTIC_MERGE = TRUE | FALSE\n  ERROR_ON_NONDETERMINISTIC_UPDATE = TRUE | FALSE\n  GEOGRAPHY_OUTPUT_FORMAT = 'GeoJSON' | 'WKT' | 'WKB' | 'EWKT' | 'EWKB'\n  HYBRID_TABLE_LOCK_TIMEOUT = <num>\n  JSON_INDENT = <num>\n  LOG_LEVEL = <string>\n  LOCK_TIMEOUT = <num>\n  PYTHON_PROFILER_TARGET_STAGE = <string>\n  PYTHON_PROFILER_MODULES = <string>\n  QUERY_TAG = <string>\n  ROWS_PER_RESULTSET = <num>\n  S3_STAGE_VPCE_DNS_NAME = <string>\n  SEARCH_PATH = <string>\n  SIMULATED_DATA_SHARING_CONSUMER = <string>\n  STATEMENT_TIMEOUT_IN_SECONDS = <num>\n  STRICT_JSON_OUTPUT = TRUE | FALSE\n  TIMESTAMP_DAY_IS_ALWAYS_24H = TRUE | FALSE\n  TIMESTAMP_INPUT_FORMAT = <string>\n  TIMESTAMP_LTZ_OUTPUT_FORMAT = <string>\n  TIMESTAMP_NTZ_OUTPUT_FORMAT = <string>\n  TIMESTAMP_OUTPUT_FORMAT = <string>\n  TIMESTAMP_TYPE_MAPPING = <string>\n  TIMESTAMP_TZ_OUTPUT_FORMAT = <string>\n  TIMEZONE = <string>\n  TIME_INPUT_FORMAT = <string>\n  TIME_OUTPUT_FORMAT = <string>\n  TRACE_LEVEL = <string>\n  TRANSACTION_DEFAULT_ISOLATION_LEVEL = <string>\n  TWO_DIGIT_CENTURY_START = <num>\n  UNSUPPORTED_DDL_ACTION = <string>\n  USE_CACHED_RESULT = TRUE | FALSE\n  WEEK_OF_YEAR_POLICY = <num>\n  WEEK_START = <num>",
    "examples": [
        {
            "code": "ALTER SESSION SET LOCK_TIMEOUT = 3600;"
        },
        {
            "code": "ALTER SESSION UNSET LOCK_TIMEOUT;"
        }
    ],
    "parameters": [
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set for the session (separated by blank spaces, commas, or new lines). For descriptions of each of the parameters you can set for a session, see Parameters ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) parameters to unset for the session, which resets them to the defaults. You can reset multiple parameters with a single ALTER statement; however, each property must be separated by a comma. When resetting\na property, specify only the name; specifying a value for the property will return an error."
        }
    ],
    "usage_notes": "Parameters are typed. The supported types are BOOLEAN, NUMBER, and STRING.\nTo see the current parameter values for the session, use SHOW PARAMETERS."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-session-policy",
    "title": "ALTER SESSION POLICY",
    "description": "Modifies the properties for an existing session policy.",
    "syntax": "ALTER SESSION POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER SESSION POLICY [ IF EXISTS ] <name> SET\n  [ SESSION_IDLE_TIMEOUT_MINS = <integer> ]\n  [ SESSION_UI_IDLE_TIMEOUT_MINS = <integer> ]\n  [ ALLOWED_SECONDARY_ROLES = ( [ { 'ALL' | <role_name> [ , <role_name> ... ] } ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER SESSION POLICY [ IF EXISTS ] <name> SET\n  TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER SESSION POLICY [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER SESSION POLICY [ IF EXISTS ] <name> UNSET\n  [ SESSION_IDLE_TIMEOUT_MINS ]\n  [ SESSION_UI_IDLE_TIMEOUT_MINS ]\n  [ ALLOWED_SECONDARY_ROLES ]\n  [ COMMENT ]",
    "examples": [
        {
            "code": "DESC SESSION POLICY session_policy_prod_1;"
        },
        {
            "code": "+---------------------------------+-----------------------+------------------------+--------------------------+--------------------------------------------------+\n| createdOn                       | name                  | sessionIdleTimeoutMins | sessionUIIdleTimeoutMins | comment                                          |\n+---------------------------------+-----------------------+------------------------+--------------------------+--------------------------------------------------+\n| Mon, 11 Jan 2021 00:00:00 -0700 | session_policy_prod_1 | 30                     | 30                       | session policy for use in the prod_1 environment |\n+---------------------------------+-----------------------+------------------------+--------------------------+--------------------------------------------------+"
        },
        {
            "code": "ALTER SESSION POLICY session_policy_prod_1 SET SESSION_UI_IDLE_TIMEOUT_MINS = 15;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the session policy; must be unique for your account. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the session policy; must be unique for your account. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more parameters to set for the session policy separated by blank spaces, commas, or new lines. For Snowflake clients and programmatic clients, the number of minutes in which a session can be idle before users must authenticate to\nSnowflake again. If a value is not specified, Snowflake uses the default value. The number of minutes can be any integer between 5 and 240 , inclusive. Default: 240 (4 hours) For Snowsight, the number of minutes in which a session can be idle before a user must authenticate to Snowflake again. If a\nvalue is not specified, Snowflake uses the default value. The number of minutes can be any integer between 5 and 240 , inclusive. Default: 240 (4 hours) Specifies the secondary roles for a session policy, if any. The possible values for the property are: Disallows secondary roles. Allows all secondary roles. Allows the specified roles as secondary roles. The secondary roles can be user-defined account roles or system roles. Specify the\nrole name as it is stored in Snowflake. For details, see Identifier requirements . Default: ('ALL') . If you do not set the property when you create a new session policy, all secondary roles are allowed.\nAdditionally, if you unset this property, the value of this property in the output of a DESCRIBE SESSION POLICY command is 'ALL' Adds a comment or overwrites an existing comment for the session policy. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more parameters to unset for the session policy, which resets them to the system defaults. You can reset multiple properties with a single ALTER statement. Each property must be separated by a comma. When\nresetting a property, specify only the name. Specifying a value for the property will return an error."
        },
        {
            "name": "SESSION_IDLE_TIMEOUT_MINS   =   integer",
            "description": "For Snowflake clients and programmatic clients, the number of minutes in which a session can be idle before users must authenticate to\nSnowflake again. If a value is not specified, Snowflake uses the default value. The number of minutes can be any integer between 5 and 240 , inclusive. Default: 240 (4 hours)"
        },
        {
            "name": "SESSION_UI_IDLE_TIMEOUT_MINS   =   integer",
            "description": "For Snowsight, the number of minutes in which a session can be idle before a user must authenticate to Snowflake again. If a\nvalue is not specified, Snowflake uses the default value. The number of minutes can be any integer between 5 and 240 , inclusive. Default: 240 (4 hours)"
        },
        {
            "name": "ALLOWED_SECONDARY_ROLES   =   (   [   {   'ALL'   |   role_name   [   ,   role_name   ...   ]   }   ]   )",
            "description": "Specifies the secondary roles for a session policy, if any. The possible values for the property are: Disallows secondary roles. Allows all secondary roles. Allows the specified roles as secondary roles. The secondary roles can be user-defined account roles or system roles. Specify the\nrole name as it is stored in Snowflake. For details, see Identifier requirements . Default: ('ALL') . If you do not set the property when you create a new session policy, all secondary roles are allowed.\nAdditionally, if you unset this property, the value of this property in the output of a DESCRIBE SESSION POLICY command is 'ALL'"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the session policy."
        },
        {
            "name": "()",
            "description": "Disallows secondary roles."
        },
        {
            "name": "('ALL')",
            "description": "Allows all secondary roles."
        },
        {
            "name": "(   role_name   [   ,   role_name   ...   ]   )",
            "description": "Allows the specified roles as secondary roles. The secondary roles can be user-defined account roles or system roles. Specify the\nrole name as it is stored in Snowflake. For details, see Identifier requirements ."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "If you want to update an existing session policy and need to see the current definition of the policy, call the\nGET_DDL function or run the DESCRIBE SESSION POLICY command.\nBefore executing an ALTER statement, you can execute a DESCRIBE SESSION POLICY statement to determine the attribute values of the policy.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-sequence",
    "title": "ALTER SEQUENCE",
    "description": "Modifies the properties for an existing sequence.",
    "syntax": "ALTER SEQUENCE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER SEQUENCE [ IF EXISTS ] <name> [ SET ] [ INCREMENT [ BY ] [ = ] <sequence_interval> ]\n\nALTER SEQUENCE [ IF EXISTS ] <name> SET\n  [ { ORDER | NOORDER } ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER SEQUENCE [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER SEQUENCE myseq RENAME TO newseq;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the sequence to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double quotes are also case-sensitive. For more details about identifiers, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the sequence; must be unique for the schema. For more details about identifiers, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET...",
            "description": "Specifies the properties to set for the sequence: Specifies the step interval of the sequence: For positive sequence interval n , the next n-1 values are reserved by each sequence call. For negative sequence interval -n , the next n-1 lower values are reserved by each sequence call. Supported values are any non-zero value that can be represented by a 64-bit two’s complement integer. Specifies whether or not the values are generated for the sequence in increasing order . ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). Note If a sequence is set to NOORDER, you cannot change the sequence to ORDER. Adds a comment or overwrites an existing comment for the sequence."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the sequence, which resets them to the defaults. Currently, the only property you can unset is COMMENT, which removes the comment, if one exists, for the sequence."
        },
        {
            "name": "[   INCREMENT   [   BY   ]   sequence_interval   ]",
            "description": "Specifies the step interval of the sequence: For positive sequence interval n , the next n-1 values are reserved by each sequence call. For negative sequence interval -n , the next n-1 lower values are reserved by each sequence call. Supported values are any non-zero value that can be represented by a 64-bit two’s complement integer."
        },
        {
            "name": "{   ORDER   |   NOORDER   }",
            "description": "Specifies whether or not the values are generated for the sequence in increasing order . ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 5 , 7 , 9 , etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2 , the generated values might be 1 , 3 , 101 , 5 , 103 , etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). Note If a sequence is set to NOORDER, you cannot change the sequence to ORDER."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the sequence."
        }
    ],
    "usage_notes": "The first/initial value for a sequence cannot be changed after the sequence is created.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-aggregation-policy",
    "title": "ALTER AGGREGATION POLICY",
    "description": "Replaces the existing rules or comment of an aggregation policy. Also allows you to rename an\naggregation policy.",
    "syntax": "ALTER AGGREGATION POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER AGGREGATION POLICY [ IF EXISTS ] <name> SET BODY -> <expression>\n\nALTER AGGREGATION POLICY <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER AGGREGATION POLICY <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER AGGREGATION POLICY [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER AGGREGATION POLICY [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER AGGREGATION POLICY my_policy SET BODY -> AGGREGATION_CONSTRAINT(MIN_GROUP_SIZE=>2);"
        },
        {
            "code": "ALTER AGGREGATION POLICY my_policy RENAME TO agg_policy_table1;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the aggregation policy to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the aggregation policy; must be unique for your schema. The new identifier cannot be used if the\nidentifier is already in place for a different aggregation policy. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the aggregation policy: SQL expression that determines the restrictions of an aggregation policy. To define the constraints of the aggregation policy, use the SQL expression to call one or more of the following functions: When the policy body returns a value from this function, queries can return data from an aggregation-constrained table or view\nwithout restriction. For example, the body of the policy could call this function when an administrator needs to obtain unaggregated\nresults from the aggregation-constrained table or view. Call NO_AGGREGATION_CONSTRAINT without an argument. When the policy body returns a value from this function, queries must aggregate data in order to return results. Use the\nMIN_GROUP_SIZE argument to specify how many records must be included in each aggregation group. The syntax of the AGGREGATION_CONSTRAINT function is: Where: Specifies how many rows or entities must be included in the groups returned by\na query against the aggregation-constrained table or view. There is a difference between passing a 1 and a 0 as the argument to the function. Both require results to be aggregated. Passing a 1 also requires that each aggregation group contain at least one record from the aggregation-constrained table. So for\nouter joins, at least one record from the aggregation-constrained table must match a record from an unprotected table. Passing a 0 allows the query to return groups that consist entirely of records from another table. So for outer joins between an\naggregation-constrained table and an unprotected table, a group could consist of records from the unprotected table that do not match\nany records in the aggregation-constrained table. The body of a policy cannot reference user-defined functions, tables, or views. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites the existing comment for the aggregation policy. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties and/or parameters to unset, by resetting them to their defaults, for the aggregation policy: TAG tag_name [ , tag_name ... ] COMMENT When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "BODY   ->   expression",
            "description": "SQL expression that determines the restrictions of an aggregation policy. To define the constraints of the aggregation policy, use the SQL expression to call one or more of the following functions: When the policy body returns a value from this function, queries can return data from an aggregation-constrained table or view\nwithout restriction. For example, the body of the policy could call this function when an administrator needs to obtain unaggregated\nresults from the aggregation-constrained table or view. Call NO_AGGREGATION_CONSTRAINT without an argument. When the policy body returns a value from this function, queries must aggregate data in order to return results. Use the\nMIN_GROUP_SIZE argument to specify how many records must be included in each aggregation group. The syntax of the AGGREGATION_CONSTRAINT function is: Where: Specifies how many rows or entities must be included in the groups returned by\na query against the aggregation-constrained table or view. There is a difference between passing a 1 and a 0 as the argument to the function. Both require results to be aggregated. Passing a 1 also requires that each aggregation group contain at least one record from the aggregation-constrained table. So for\nouter joins, at least one record from the aggregation-constrained table must match a record from an unprotected table. Passing a 0 allows the query to return groups that consist entirely of records from another table. So for outer joins between an\naggregation-constrained table and an unprotected table, a group could consist of records from the unprotected table that do not match\nany records in the aggregation-constrained table. The body of a policy cannot reference user-defined functions, tables, or views."
        },
        {
            "name": "NO_AGGREGATION_CONSTRAINT",
            "description": "When the policy body returns a value from this function, queries can return data from an aggregation-constrained table or view\nwithout restriction. For example, the body of the policy could call this function when an administrator needs to obtain unaggregated\nresults from the aggregation-constrained table or view. Call NO_AGGREGATION_CONSTRAINT without an argument."
        },
        {
            "name": "AGGREGATION_CONSTRAINT",
            "description": "When the policy body returns a value from this function, queries must aggregate data in order to return results. Use the\nMIN_GROUP_SIZE argument to specify how many records must be included in each aggregation group. The syntax of the AGGREGATION_CONSTRAINT function is: Where: Specifies how many rows or entities must be included in the groups returned by\na query against the aggregation-constrained table or view. There is a difference between passing a 1 and a 0 as the argument to the function. Both require results to be aggregated. Passing a 1 also requires that each aggregation group contain at least one record from the aggregation-constrained table. So for\nouter joins, at least one record from the aggregation-constrained table must match a record from an unprotected table. Passing a 0 allows the query to return groups that consist entirely of records from another table. So for outer joins between an\naggregation-constrained table and an unprotected table, a group could consist of records from the unprotected table that do not match\nany records in the aggregation-constrained table."
        },
        {
            "name": "MIN_GROUP_SIZE   =>   integer_expression",
            "description": "Specifies how many rows or entities must be included in the groups returned by\na query against the aggregation-constrained table or view. There is a difference between passing a 1 and a 0 as the argument to the function. Both require results to be aggregated. Passing a 1 also requires that each aggregation group contain at least one record from the aggregation-constrained table. So for\nouter joins, at least one record from the aggregation-constrained table must match a record from an unprotected table. Passing a 0 allows the query to return groups that consist entirely of records from another table. So for outer joins between an\naggregation-constrained table and an unprotected table, a group could consist of records from the unprotected table that do not match\nany records in the aggregation-constrained table."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the aggregation policy. Default: No value"
        }
    ],
    "usage_notes": "If you want to update an existing aggregation policy and need to see the current body of the policy, run the\nDESCRIBE AGGREGATION POLICY command. You can also use the GET_DDL function to\nobtain the full definition of the aggregation policy, including its body.\nMoving an aggregation policy to a managed access schema\n(using the ALTER AGGREGATION POLICY … RENAME TO syntax) is prohibited unless the aggregation policy owner\n(i.e. the role that has the OWNERSHIP privilege on the aggregation policy) also owns the target schema.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-replication-group",
    "title": "ALTER REPLICATION GROUP",
    "description": "Modifies the properties for an existing replication group.",
    "syntax": "ALTER REPLICATION GROUP [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> SET\n  [ OBJECT_TYPES = <object_type> [ , <object_type> , ... ] ]\n  [ ALLOWED_DATABASES = <db_name> [ , <db_name> , ... ] ]\n  [ ALLOWED_SHARES = <share_name> [ , <share_name> , ... ] ]\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> SET\n  OBJECT_TYPES = INTEGRATIONS [ , <object_type> , ... ]\n  ALLOWED_INTEGRATION_TYPES = <integration_type_name> [ , <integration_type_name> ... ]\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> SET\n  COMMENT = '<string_literal>'\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> SET\n  REPLICATION_SCHEDULE = '{ <num> MINUTE | USING CRON <expr> <time_zone> }'\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> SET\n  ERROR_INTEGRATION = <integration_name>\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> SET\n  TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> UNSET\n  { COMMENT | REPLICATION_SCHEDULE | ERROR_INTEGRATION } [ , ... ]\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> UNSET\n  TAG <tag_name> [ , <tag_name> ... ]\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  ADD <db_name> [ , <db_name> ,  ... ] TO ALLOWED_DATABASES\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  MOVE DATABASES <db_name> [ , <db_name> ,  ... ] TO REPLICATION GROUP <move_to_rg_name>\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  REMOVE <db_name> [ , <db_name> ,  ... ] FROM ALLOWED_DATABASES\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  ADD <share_name> [ , <share_name> ,  ... ] TO ALLOWED_SHARES\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  MOVE SHARES <share_name> [ , <share_name> ,  ... ] TO REPLICATION GROUP <move_to_rg_name>\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  REMOVE <share_name> [ , <share_name> ,  ... ] FROM ALLOWED_SHARES\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  ADD <org_name>.<target_account_name> [ , <org_name>.<target_account_name> ,  ... ] TO ALLOWED_ACCOUNTS\n  [ IGNORE EDITION CHECK ]\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name>\n  REMOVE <org_name>.<target_account_name> [ , <org_name>.<target_account_name> ,  ... ] FROM ALLOWED_ACCOUNTS\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> REFRESH\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> SUSPEND [ IMMEDIATE ]\n\nALTER REPLICATION GROUP [ IF EXISTS ] <name> RESUME",
    "examples": [
        {
            "code": "ALTER REPLICATION GROUP myrg ADD myorg.myaccount3 TO ALLOWED_ACCOUNTS;"
        },
        {
            "code": "ALTER REPLICATION GROUP myrg SET\n  OBJECT_TYPES = DATABASES, SHARES;"
        },
        {
            "code": "ALTER REPLICATION GROUP myrg\n  ADD db1 to ALLOWED_DATABASES;"
        },
        {
            "code": "ALTER REPLICATION GROUP myrg\n  ADD s2 TO ALLOWED_SHARES;"
        },
        {
            "code": "ALTER REPLICATION GROUP myrg\n  MOVE DATABASES db1 TO REPLICATION GROUP myrg2;"
        },
        {
            "code": "ALTER REPLICATION GROUP myrg SET\n  REPLICATION_SCHEDULE = '15 MINUTE';"
        },
        {
            "code": "ALTER REPLICATION GROUP myrg REFRESH;"
        },
        {
            "code": "ALTER REPLICATION GROUP myrg SUSPEND;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the replication group."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the replication group. The new identifier cannot be used if the identifier is already in place for a\ndifferent replication or failover group. For more details, see Identifier requirements ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies properties to set for the replication group (separated by blank spaces, commas, or new lines). Reset the list of object types to replicate from the source account to target account(s). Note For database and share objects: If DATABASES or SHARES are included in the OBJECT_TYPES list, and remain in the OBJECT_TYPES list after\nthe list is reset, the respective allowed objects list (ALLOWED_DATABASES or ALLOWED_SHARES) remains\nunchanged. If the OBJECT_TYPES list is reset to add or remove DATABASES, the ALLOWED_DATABASES list is set to NULL. If the OBJECT_TYPES list is reset to add or remove SHARES, the ALLOWED_SHARES list is set to NULL. Use the ADD, MOVE, and REMOVE clauses to modify the list of allowed database or share objects. The following object types are supported: Requires Business Critical Edition (or higher). All account-level parameters. This includes account parameters and parameters that can be set for your account . Add database objects to the list of object types. If database objects were already included in the list of specified object\ntypes, the ALLOWED_DATABASES list remains unchanged. To modify the list of databases, use the\nADD, MOVE, or REMOVE clauses. Requires Business Critical Edition (or higher). Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set. Requires Business Critical Edition (or higher). All network policies in the source account. Requires Business Critical Edition (or higher). All resource monitors in the source account. Requires Business Critical Edition (or higher). All roles in the source account. Replicating roles implicitly includes all grants for object types included in the replication group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated. Add share objects to the list of object types. If share objects were already included in the list of specified object types, the ALLOWED_SHARES list remains unchanged. To modify the list of shares, use the ADD, MOVE, or REMOVE clauses. Requires Business Critical Edition (or higher). All users in the source account. Requires Business Critical Edition (or higher). All warehouses in the source account. Note If you replicate users and roles, programmatic access tokens for users are replicated automatically. Specifies the database or list of databases for which you are enabling replication from the source account to the target\naccount. In order for you to set this parameter, the OBJECT_TYPES list must include DATABASES . Specifies the identifier for the database. Specifies the share or list of shares for which you are enabling replication from the source account to the target account.\nIn order for you to set this parameter, the OBJECT_TYPES list must include SHARES . Specifies the identifier for the share. Note If the ALLOWED_DATABASES or ALLOWED_SHARES lists are modified, any objects that were previously in the list and removed\nwill be dropped in any target account with a linked secondary replication group when the next refresh operation occurs. Requires Business Critical Edition (or higher). Type(s) of integrations for which you are enabling replication from the source account to the target account. This property requires that the OBJECT_TYPES list include INTEGRATIONS to set this parameter. The following integration types are supported: Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES . Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations . Specifies storage integrations. Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) . Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication . Adds a comment or overwrites an existing comment for the replication group. NULL Specifies the schedule for refreshing secondary replication groups. Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs. NULL Specifies the name of the notification integration to use to email/push notifications when refresh errors occur for the replication\ngroup. For more details, see Error notifications for replication and failover groups . NULL Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "ADD   db_name   [   ,   db_name   ,    ...   ]   TO   ALLOWED_DATABASES",
            "description": "Specifies a comma-separated list of databases to add to the list of databases enabled for replication. To add a database, DATABASES must\nbe included in the list of specified object types. If the list of object types does not already include DATABASES, you must add it. Specifies the identifier for the database."
        },
        {
            "name": "MOVE   DATABASES   db_name   [   ,   db_name   ,    ...   ]   TO   REPLICATION   GROUP   move_to_rg_name",
            "description": "Specifies a comma-separated list of databases to move from one replication group to another replication group. The replication group the\ndatabases are being moved to must include DATABASES in the list of specified object types. Specifies the identifier for the database. Specifies the identifier for the replication group the databases are being moved to."
        },
        {
            "name": "REMOVE   db_name   [   ,   db_name   ,    ...   ]   FROM   ALLOWED_DATABASES",
            "description": "Specifies a comma-separated list of database to remove from the list of databases enabled for replication. Note When you remove a database from a primary replication group, the database is dropped in any target account with a linked secondary\nreplication group when the next refresh operation occurs. To avoid dropping databases in the target account, you can drop the secondary replication group before the next time the modified\nprimary replication group is replicated to the target account. When you drop the secondary replication group, read-only secondary\ndatabases that were included in the group become standalone read-write databases in the target account."
        },
        {
            "name": "ADD   share_name   [   ,   share_name   ,    ...   ]   TO   ALLOWED_SHARES",
            "description": "Specifies a comma-separated list of shares to the list of shares for replication. To add a share, SHARES must be included in the list of\nspecified object types. If the list of object types does not already include SHARES, you must add it. Specifies the identifier for the share."
        },
        {
            "name": "MOVE   SHARES   share_name   [   ,   share_name   ,    ...   ]   TO   REPLICATION   GROUP   move_to_rg_name",
            "description": "Specifies a comma-separated list of shares to move from one replication group to another replication group. The replication group the\nshares are being moved to must include SHARES in the list of specified object types. Specifies the identifier for the share. Specifies the identifier for the replication group the shares are being moved to."
        },
        {
            "name": "REMOVE   share_name   [   ,   share_name   ,    ...   ]   FROM   ALLOWED_SHARES",
            "description": "Specifies a comma-separated list of shares to remove from the list of shares enabled for replication. Note When you remove a share from a primary replication group, the share is dropped in any target account with a linked secondary\nreplication group when the next refresh operation occurs."
        },
        {
            "name": "ADD   org_name . target_account_name   [   ,   org_name . target_account_name   ,    ...   ]   TO   ALLOWED_ACCOUNTS",
            "description": "Specifies a comma-separated list of target accounts to add to the primary replication group to enable replication of specified objects in\nthe source account to the target account. Name of your Snowflake organization. Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "REMOVE   org_name . target_account_name   [   ,   org_name . target_account_name   ,    ...   ]   FROM   ALLOWED_ACCOUNTS",
            "description": "Specifies a comma-separated list of target accounts to remove from the primary replication group to disable replication of specified\nobjects in the source account to the target account. Name of your Snowflake organization. Target account to which you are disabling replication of the specified objects."
        },
        {
            "name": "new_name",
            "description": "Specifies the new identifier for the replication group. The new identifier cannot be used if the identifier is already in place for a\ndifferent replication or failover group. For more details, see Identifier requirements ."
        },
        {
            "name": "OBJECT_TYPES   =   object_type   [   ,   object_type   ,   ...   ]",
            "description": "Reset the list of object types to replicate from the source account to target account(s). Note For database and share objects: If DATABASES or SHARES are included in the OBJECT_TYPES list, and remain in the OBJECT_TYPES list after\nthe list is reset, the respective allowed objects list (ALLOWED_DATABASES or ALLOWED_SHARES) remains\nunchanged. If the OBJECT_TYPES list is reset to add or remove DATABASES, the ALLOWED_DATABASES list is set to NULL. If the OBJECT_TYPES list is reset to add or remove SHARES, the ALLOWED_SHARES list is set to NULL. Use the ADD, MOVE, and REMOVE clauses to modify the list of allowed database or share objects. The following object types are supported: Requires Business Critical Edition (or higher). All account-level parameters. This includes account parameters and parameters that can be set for your account . Add database objects to the list of object types. If database objects were already included in the list of specified object\ntypes, the ALLOWED_DATABASES list remains unchanged. To modify the list of databases, use the\nADD, MOVE, or REMOVE clauses. Requires Business Critical Edition (or higher). Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set. Requires Business Critical Edition (or higher). All network policies in the source account. Requires Business Critical Edition (or higher). All resource monitors in the source account. Requires Business Critical Edition (or higher). All roles in the source account. Replicating roles implicitly includes all grants for object types included in the replication group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated. Add share objects to the list of object types. If share objects were already included in the list of specified object types, the ALLOWED_SHARES list remains unchanged. To modify the list of shares, use the ADD, MOVE, or REMOVE clauses. Requires Business Critical Edition (or higher). All users in the source account. Requires Business Critical Edition (or higher). All warehouses in the source account. Note If you replicate users and roles, programmatic access tokens for users are replicated automatically."
        },
        {
            "name": "ALLOWED_DATABASES   =   db_name   [   ,   db_name   ,   ...   ]",
            "description": "Specifies the database or list of databases for which you are enabling replication from the source account to the target\naccount. In order for you to set this parameter, the OBJECT_TYPES list must include DATABASES . Specifies the identifier for the database."
        },
        {
            "name": "ALLOWED_SHARES   =   share_name   [   ,   share_name   ,   ...   ]",
            "description": "Specifies the share or list of shares for which you are enabling replication from the source account to the target account.\nIn order for you to set this parameter, the OBJECT_TYPES list must include SHARES . Specifies the identifier for the share."
        },
        {
            "name": "ACCOUNT PARAMETERS :",
            "description": "Requires Business Critical Edition (or higher). All account-level parameters. This includes account parameters and parameters that can be set for your account ."
        },
        {
            "name": "DATABASES :",
            "description": "Add database objects to the list of object types. If database objects were already included in the list of specified object\ntypes, the ALLOWED_DATABASES list remains unchanged. To modify the list of databases, use the\nADD, MOVE, or REMOVE clauses."
        },
        {
            "name": "INTEGRATIONS :",
            "description": "Requires Business Critical Edition (or higher). Currently, only security, API, storage, external access, and certain types of notification integrations are supported.\nFor details, see Integration replication . If integration objects are included in the list of specified object types, the ALLOWED_INTEGRATION_TYPES parameter must be set."
        },
        {
            "name": "NETWORK POLICIES :",
            "description": "Requires Business Critical Edition (or higher). All network policies in the source account."
        },
        {
            "name": "RESOURCE MONITORS :",
            "description": "Requires Business Critical Edition (or higher). All resource monitors in the source account."
        },
        {
            "name": "ROLES :",
            "description": "Requires Business Critical Edition (or higher). All roles in the source account. Replicating roles implicitly includes all grants for object types included in the replication group.\nFor example, if ROLES is the only object type that is replicated, then only hierarchies of roles (that is, roles granted to\nother roles) are replicated to target accounts. If the USERS object type is also included, then role grants to users are\nalso replicated."
        },
        {
            "name": "SHARES :",
            "description": "Add share objects to the list of object types. If share objects were already included in the list of specified object types, the ALLOWED_SHARES list remains unchanged. To modify the list of shares, use the ADD, MOVE, or REMOVE clauses."
        },
        {
            "name": "USERS :",
            "description": "Requires Business Critical Edition (or higher). All users in the source account."
        },
        {
            "name": "WAREHOUSES :",
            "description": "Requires Business Critical Edition (or higher). All warehouses in the source account."
        },
        {
            "name": "db_name",
            "description": "Specifies the identifier for the database."
        },
        {
            "name": "share_name",
            "description": "Specifies the identifier for the share."
        },
        {
            "name": "ALLOWED_INTEGRATION_TYPES   =   integration_type_name   [   ,   integration_type_name   ,   ...   ]",
            "description": "Requires Business Critical Edition (or higher). Type(s) of integrations for which you are enabling replication from the source account to the target account. This property requires that the OBJECT_TYPES list include INTEGRATIONS to set this parameter. The following integration types are supported: Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES . Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations . Specifies storage integrations. Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) . Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "SECURITY INTEGRATIONS :",
            "description": "Specifies security integrations. This property requires that the OBJECT_TYPES list include ROLES ."
        },
        {
            "name": "API INTEGRATIONS :",
            "description": "Specifies API integrations. API integration replication requires additional set up after the API integration is replicated to the target account.\nFor more information, see Updating the remote service for API integrations ."
        },
        {
            "name": "STORAGE INTEGRATIONS :",
            "description": "Specifies storage integrations."
        },
        {
            "name": "EXTERNAL ACCESS INTEGRATIONS :",
            "description": "Specifies external access integrations . For more information, see Replication of stored procedures and user-defined functions (UDFs) ."
        },
        {
            "name": "NOTIFICATION INTEGRATIONS :",
            "description": "Specifies notification integrations. Only some types of notification integrations are replicated. For details, see Integration replication ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the replication group. NULL"
        },
        {
            "name": "REPLICATION_SCHEDULE   ...",
            "description": "Specifies the schedule for refreshing secondary replication groups. Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates. Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs. NULL"
        },
        {
            "name": "ERROR_INTEGRATION   =   integration_name",
            "description": "Specifies the name of the notification integration to use to email/push notifications when refresh errors occur for the replication\ngroup. For more details, see Error notifications for replication and failover groups . NULL"
        },
        {
            "name": "Default :",
            "description": "NULL"
        },
        {
            "name": "USING   CRON   expr   time_zone",
            "description": "Specifies a cron expression and time zone for the secondary group refresh. Supports a subset of standard cron utility syntax. For a list of time zones, see the list of tz database time zones (in Wikipedia). The cron expression consists of the following fields: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the refresh. The cron expression defines all valid run times for the refresh. Snowflake attempts to refresh secondary groups based on\nthis schedule; however, any valid run time is skipped if a previous run has not completed before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the refresh is scheduled on days\nsatisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a refresh at 0AM on any 10th to 20th day of the month and also on any Tuesday or Thursday outside of those dates."
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of a\ngiven month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/ n",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the refresh is scheduled for April, July and October (i.e. every 3 months, starting with the 4th\nmonth of the year). The same schedule is maintained in subsequent years. That is, the refresh is not scheduled to run in\nJanuary (3 months after the October run)."
        },
        {
            "name": "num   MINUTE",
            "description": "Specifies an interval (in minutes) of wait time between refreshes. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set: When the object is created (using CREATE <object>) or When a different interval is set (using ALTER <object> … SET REPLICATION_SCHEDULE) The base interval time starts the interval counter from the current clock time. For example, if an INTERVAL value of 10 is set and\nthe scheduled refresh is enabled at 9:03 AM, then the refresh runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best effort to\nensure absolute precision, but only guarantee that refreshes do not execute before their set interval occurs (e.g. in the\ncurrent example, the refresh could first run at 9:14 AM, but will definitely not run at 9:12 AM). Note The maximum supported value is 11520 (8 days). If the replication schedule has a greater num MINUTE value, the\nrefresh operation never runs."
        },
        {
            "name": "Default :",
            "description": "NULL"
        },
        {
            "name": "Default :",
            "description": "NULL"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "db_name",
            "description": "Specifies the identifier for the database."
        },
        {
            "name": "db_name",
            "description": "Specifies the identifier for the database."
        },
        {
            "name": "move_to_rg_name",
            "description": "Specifies the identifier for the replication group the databases are being moved to."
        },
        {
            "name": "share_name",
            "description": "Specifies the identifier for the share."
        },
        {
            "name": "share_name",
            "description": "Specifies the identifier for the share."
        },
        {
            "name": "move_to_rg_name",
            "description": "Specifies the identifier for the replication group the shares are being moved to."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "target_account_name",
            "description": "Target account to which you are enabling replication of the specified objects."
        },
        {
            "name": "org_name",
            "description": "Name of your Snowflake organization."
        },
        {
            "name": "target_account_name",
            "description": "Target account to which you are disabling replication of the specified objects."
        },
        {
            "name": "IGNORE   EDITION   CHECK",
            "description": "Allows replicating objects to accounts on lower editions in either of the following scenarios: A primary replication group with only database and/or share objects is in a Business Critical (or higher) account but\none or more accounts approved for replication are on lower editions. Business Critical Edition is intended for Snowflake accounts\nwith extremely sensitive data. A primary replication group with any object type is in a Business\nCritical (or higher) account and a signed business associate agreement is in place to store PHI data in the account per HIPAA and HITRUST regulations. However, no such agreement is in place for one or more of the accounts approved\nfor replication, regardless if they are Business Critical (or higher) accounts. Both scenarios are prohibited by default in an effort to help prevent account administrators for Business Critical (or higher) accounts\nfrom inadvertently replicating sensitive data to accounts on lower editions."
        },
        {
            "name": "name",
            "description": "Specifies the identifier for the replication group."
        },
        {
            "name": "REFRESH",
            "description": "Refreshes the objects in the target (current) account from the source account."
        },
        {
            "name": "SUSPEND   [   IMMEDIATE   ]",
            "description": "Suspend the scheduled refresh of the secondary replication group (if the primary replication group has automatically scheduled refresh set\nusing the REPLICATION_SCHEDULE property). The optional IMMEDIATE clause cancels a scheduled refresh operation that is currently in progress for the secondary replication group\n(if there is one). Note that there might be a slight delay between the time that the statement returns and the time that the cancellation\nof the refresh operation is finished."
        },
        {
            "name": "RESUME",
            "description": "Resume scheduled refresh of the secondary replication group (if the primary replication group has automatically scheduled refresh set\nusing the REPLICATION_SCHEDULE property)."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties to unset for the replication group, which resets them to the defaults: COMMENT REPLICATION_SCHEDULE ERROR_INTEGRATION TAG tag_name [ , tag_name ... ] You can reset multiple properties with a single ALTER statement; however, each property must be separated by\na comma. Also, when resetting a property, you only specify the name; no value is required."
        }
    ],
    "usage_notes": "The following minimal privileges are required:\nTo refresh a secondary replication group using ALTER REPLICATION GROUP … REFRESH, the active, primary role must have either the\nOWNERSHIP or REPLICATE privilege on the replication group.\nTo make any other changes to the replication group, only a user with a role with the OWNERSHIP privilege on the group can execute\nthis SQL command.\nTo add a database to a replication group, the active role must have the MONITOR privilege on the database.\nTo add a share to a replication group, the active role must have the OWNERSHIP privilege on the share.\nIdentifiers for failover groups and replication groups in an account must be unique.\nObjects other than databases and shares must be in the same replication group.\nA database can only be added to one replication or failover group.\nTo move databases or shares from one replication group (the move-from group) to another replication group (the move-to group):\nBoth groups must be of the same type: REPLICATION GROUP.\nIf the last database in the move-from group is moved to another group, the allowed_databases property for the move-from group\nis set to NULL. The same behavior applies to shares.\nIf the move-to group does not have the object type that is being moved (databases or shares) in the object_types\nlist, it must be explicitly added to the move-to group before you move the objects.\nIf database or share objects are removed from a primary replication group (by using the REMOVE parameter or SET parameter to\nmodify the ALLOWED_DATABASES or ALLOWED_SHARES lists), those objects are dropped in any target account with a linked secondary replication\ngroup when the next refresh operation occurs.\nTo avoid dropping these objects in the target account, you can drop the secondary replication group before the next time the modified\nprimary replication group is replicated to the target account.\nInbound shares (shares from providers) cannot be added to a replication or failover group.\nTo retrieve the list of accounts in your organization that are enabled for replication, use the\nSHOW REPLICATION ACCOUNTS command.\nTo retrieve the list of replication groups in your organization, use the SHOW REPLICATION GROUPS command. The\nallowed_accounts column lists all target accounts enabled for replication from a source account.\nAutomatically scheduled refresh operations are executed using the role with the OWNERSHIP\nprivilege on the group. If a scheduled refresh operation fails due to insufficient privileges, grant the required privileges\nto the role with the OWNERSHIP privilege on the group.\nThe ALTER REPLICATION GROUP … SUSPEND IMMEDIATE command doesn’t cancel an in-progress refresh operation if it was manually triggered.\nFor more information, see Cancel an in-progress refresh operation that wasn’t automatically scheduled.\nCanceling an in-progress refresh operation that is in the SECONDARY_DOWNLOADING_METADATA or SECONDARY_DOWNLOADING_DATA phase might\nresult in an inconsistent state on the target account. For more information see View the current phase of an in-progress refresh operation.\nIf you create a replication or failover group with a tag or modify a replication or failover group by setting a tag on it,\ntag inheritance does not apply to any objects that you specify in the replication or failover group.\nTag inheritance is only applicable to objects with a parent-child relationship, such\ndatabase, schema, and table. There are no child objects of replication or failover groups.\nYou cannot set a tag or modify a tag on a secondary replication or failover group because these objects are read\nonly.\nWhen you refresh a secondary replication or failover group, any tags that are set on the primary group are then set on\nthe secondary group.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-pipe",
    "title": "ALTER PIPE",
    "description": "Modifies a limited set of properties for an existing pipe object. Also supports the following operations:",
    "syntax": "ALTER PIPE [ IF EXISTS ] <name> SET { [ objectProperties ]\n                                      [ COMMENT = '<string_literal>' ] }\n\nALTER PIPE <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER PIPE <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER PIPE [ IF EXISTS ] <name> UNSET { <property_name> | COMMENT } [ , ... ]\n\nALTER PIPE [ IF EXISTS ] <name> REFRESH { [ PREFIX = '<path>' ] [ MODIFIED_AFTER = <start_time> ] }\n\nobjectProperties ::=\n  PIPE_EXECUTION_PAUSED = TRUE | FALSE",
    "examples": [
        {
            "code": "alter pipe mypipe SET PIPE_EXECUTION_PAUSED = true;"
        },
        {
            "code": "alter pipe mypipe SET COMMENT = \"Pipe for North American sales data\";"
        },
        {
            "code": "CREATE PIPE mypipe AS COPY INTO mytable FROM @mystage/path1/;"
        },
        {
            "code": "ALTER PIPE mypipe REFRESH;"
        },
        {
            "code": "ALTER PIPE mypipe REFRESH PREFIX='d1/';"
        },
        {
            "code": "ALTER PIPE mypipe REFRESH PREFIX='d1/' MODIFIED_AFTER='2018-07-30T13:56:46-07:00';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the pipe to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the pipe (separated by blank spaces, commas, or new lines): Required only when configuring Snowpipe to send error notifications to a cloud messaging service. Specifies the name of the notification\nintegration used to communicate with the messaging service. For more information, see Snowpipe error notifications . Specifies whether to pause a running pipe, typically in preparation for transferring ownership of the pipe: TRUE pauses the pipe. The executionState reported by SYSTEM$PIPE_STATUS is PAUSED .\nNote that the pipe owner can continue to submit files to a paused pipe; however, they won’t be processed until the pipe is resumed. FALSE resumes the pipe. The executionState reported by SYSTEM$PIPE_STATUS is RUNNING . Note Either of the following scenarios requires forcing a pipe to resume by calling the SYSTEM$PIPE_FORCE_RESUME function: Transferring ownership of the pipe to another role. This requirement allows the new owner to evaluate the pipe status and\ndetermine how many files are waiting to be loaded by calling the SYSTEM$PIPE_STATUS function. Allowing a pipe object that leverages cloud messaging to trigger data loads (i.e. where AUTO_INGEST = TRUE in the pipe\ndefinition) to become stale. A pipe is considered stale when it is paused for longer than the limited retention period for event\nmessages received for the pipe (14 days by default). Default: FALSE (the pipe is running by default) Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites an existing comment for the pipe."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties to unset for the pipe, which resets them to the defaults: ERROR_INTEGRATION PIPE_EXECUTION_PAUSED TAG tag_name [ , tag_name ... ] COMMENT You can reset multiple properties with a single ALTER statement; however, each property must be separated by a comma. When resetting\na property, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "REFRESH",
            "description": "Copies a set of staged data files to the Snowpipe ingest queue for loading into the target table. This clause accepts an optional path and can\nfurther filter the list of files to load based on a specified start time. Note This SQL command can only load data files that were staged within the last 7 days. This SQL command checks the load history for both the pipe and the target table. As a result, the command queues only those files\nthat were not loaded already using either: The same pipe, provided the pipe owner did not recreate the pipe after the files were loaded. A COPY INTO <table> statement. Important The REFRESH functionality is intended for short term use to resolve specific issues when Snowpipe fails to load a subset of files and is not\nintended for regular use. Path (or prefix ) appended to the stage reference in the pipe definition. The path limits the set of files to load. Only files that start\nwith the specified path are included in the data load. For example, suppose the pipe definition references @mystage/path1/ . If the path value is d1/ , the ALTER\nPIPE statement limits loads to files in the @mystage stage with the /path1/d1/ path. See the examples for more\ninformation. Note that the path must be enclosed in single quotes. Timestamp (in ISO-8601 format) of the oldest data files to copy into the Snowpipe ingest queue based on the LAST_MODIFIED date (i.e. date\nwhen a file was staged). The default and maximum allowed value is 7 days."
        },
        {
            "name": "ERROR_INTEGRATION   =   ' integration_name '",
            "description": "Required only when configuring Snowpipe to send error notifications to a cloud messaging service. Specifies the name of the notification\nintegration used to communicate with the messaging service. For more information, see Snowpipe error notifications ."
        },
        {
            "name": "PIPE_EXECUTION_PAUSED   =   TRUE   |   FALSE",
            "description": "Specifies whether to pause a running pipe, typically in preparation for transferring ownership of the pipe: TRUE pauses the pipe. The executionState reported by SYSTEM$PIPE_STATUS is PAUSED .\nNote that the pipe owner can continue to submit files to a paused pipe; however, they won’t be processed until the pipe is resumed. FALSE resumes the pipe. The executionState reported by SYSTEM$PIPE_STATUS is RUNNING . Note Either of the following scenarios requires forcing a pipe to resume by calling the SYSTEM$PIPE_FORCE_RESUME function: Transferring ownership of the pipe to another role. This requirement allows the new owner to evaluate the pipe status and\ndetermine how many files are waiting to be loaded by calling the SYSTEM$PIPE_STATUS function. Allowing a pipe object that leverages cloud messaging to trigger data loads (i.e. where AUTO_INGEST = TRUE in the pipe\ndefinition) to become stale. A pipe is considered stale when it is paused for longer than the limited retention period for event\nmessages received for the pipe (14 days by default). Default: FALSE (the pipe is running by default)"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string '",
            "description": "Adds a comment or overwrites an existing comment for the pipe."
        },
        {
            "name": "PREFIX   =   ' path '",
            "description": "Path (or prefix ) appended to the stage reference in the pipe definition. The path limits the set of files to load. Only files that start\nwith the specified path are included in the data load. For example, suppose the pipe definition references @mystage/path1/ . If the path value is d1/ , the ALTER\nPIPE statement limits loads to files in the @mystage stage with the /path1/d1/ path. See the examples for more\ninformation. Note that the path must be enclosed in single quotes."
        },
        {
            "name": "MODIFIED_AFTER   =   ' start_time '",
            "description": "Timestamp (in ISO-8601 format) of the oldest data files to copy into the Snowpipe ingest queue based on the LAST_MODIFIED date (i.e. date\nwhen a file was staged). The default and maximum allowed value is 7 days."
        }
    ],
    "usage_notes": "Only the pipe owner (i.e. the role with the OWNERSHIP privilege on the pipe) can set or unset properties on a pipe.\nA non-owner role with the following minimum privileges can refresh a pipe (using ALTER PIPE … REFRESH …):\nPrivilege\nObject\nNotes\nOPERATE\nPipe\nUSAGE\nStage in the pipe definition\nExternal stages only\nREAD\nStage in the pipe definition\nInternal stages only\nSELECT, INSERT\nTable in the pipe defintion\nA non-owner role with the OPERATE privilege on the pipe can pause or resume a pipe (using ALTER PIPE … SET PIPE_EXECUTION_PAUSED = TRUE\n| FALSE).\nSQL operations on schema objects also require the USAGE privilege on the database and schema that contain the object.\nCurrently, it is not possible to modify the following pipe properties using an ALTER PIPE statement:\nCOPY INTO <table> statement\nAWS_SNS_TOPIC parameter\nINTEGRATION parameter\nInstead, recreate the pipe using a CREATE OR REPLACE PIPE statement.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-materialized-view",
    "title": "ALTER MATERIALIZED VIEW",
    "description": "Alters a materialized view in the current/specified schema. Supported actions include:",
    "syntax": "ALTER MATERIALIZED VIEW <name>\n  {\n  RENAME TO <new_name>                     |\n  CLUSTER BY ( <expr1> [, <expr2> ... ] )  |\n  DROP CLUSTERING KEY                      |\n  SUSPEND RECLUSTER                        |\n  RESUME RECLUSTER                         |\n  SUSPEND                                  |\n  RESUME                                   |\n  SET {\n    [ SECURE ]\n    [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n    [ COMMENT = '<comment>' ]\n    }                                      |\n  UNSET {\n    SECURE\n    CONTACT <purpose>                                 |\n    COMMENT\n    }\n  }\n\nALTER MATERIALIZED VIEW\n  SET DATA_METRIC_SCHEDULE = {\n      '<num> MINUTE'\n    | 'USING CRON <expr> <time_zone>'\n  }\n\nALTER MATERIALIZED VIEW UNSET DATA_METRIC_SCHEDULE",
    "examples": [
        {
            "code": "ALTER MATERIALIZED VIEW table1_MV RENAME TO my_mv;"
        },
        {
            "code": "ALTER MATERIALIZED VIEW my_mv CLUSTER BY(i);"
        },
        {
            "code": "ALTER MATERIALIZED VIEW my_mv SUSPEND RECLUSTER;"
        },
        {
            "code": "ALTER MATERIALIZED VIEW my_mv RESUME RECLUSTER;"
        },
        {
            "code": "ALTER MATERIALIZED VIEW my_mv SUSPEND;"
        },
        {
            "code": "ALTER MATERIALIZED VIEW my_mv RESUME;"
        },
        {
            "code": "ALTER MATERIALIZED VIEW my_mv DROP CLUSTERING KEY;"
        },
        {
            "code": "ALTER MATERIALIZED VIEW mv1 SET SECURE;"
        },
        {
            "code": "ALTER MATERIALIZED VIEW mv1 SET COMMENT = 'Sample view';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier of the materialized view to alter."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "This option allows you to rename a materialized view. The new identifier must be unique for the schema in which the view is created.\nThe new identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier string\nis enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive.\nFor more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. Note that renaming a materialized view does not update references to that view. For example, if\nyou create a view named V1 on top of a materialized view, and then you rename\nthe materialized view, the definition of view V1 becomes out of date."
        },
        {
            "name": "CLUSTER   BY   expr#",
            "description": "This command clusters the materialized view. Clustering\nre-orders the rows in the materialized view to increase performance for queries\nthat filter based on the clustering key expressions. The expr# specifies an expression on which to cluster the materialized view.\nTypically, each expression is the name of a column in the materialized view. For more information about clustering materialized views, see: Materialized Views and Clustering .\nFor more information about clustering in general, see: What is Data Clustering? ."
        },
        {
            "name": "DROP   CLUSTERING   KEY",
            "description": "This command drops the clustering of the materialized view."
        },
        {
            "name": "SUSPEND   RECLUSTER",
            "description": "The SUSPEND RECLUSTER option suspends re-clustering of the materialized\nview. For more information about clustering materialized views,\nsee Materialized Views and Clustering ."
        },
        {
            "name": "RESUME   RECLUSTER",
            "description": "The RESUME RECLUSTER option resumes reclustering of the materialized\nview."
        },
        {
            "name": "SUSPEND",
            "description": "The SUSPEND option suspends the maintenance (updates) and use of the\nmaterialized view. While the view is suspended, updates to the base table are\nnot propagated to the materialized view. The materialized view itself is\nalso inaccessible; if you attempt to use it, you get an error message\nsimilar to: If you suspend a clustered materialized view, suspending the view implicitly\nsuspends reclustering of that view."
        },
        {
            "name": "RESUME",
            "description": "The RESUME option allows you to resume using the materialized view.\nIt also resumes maintenance of the materialized view.\nIf the view is clustered, it also implicitly resumes reclustering of that view."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the property to set for the materialized view: This option turns the view into a secure view. For more information about secure views, see Working with Secure Views . Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts . This option sets a comment for the materialized view. The comment has no effect on the behavior of the view,\nbut can provide useful information to people who use or maintain the view. Specifies the schedule to run the data metric function periodically. Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5 , 15 , 30 , 60 , 720 , or 1440 . Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones . The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is not scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example, DATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the property to unset for the materialized view: SECURE TAG tag_name [ , tag_name ... ] CONTACT purpose COMMENT DATA_METRIC_SCHEDULE"
        },
        {
            "name": "SECURE",
            "description": "This option turns the view into a secure view. For more information about secure views, see Working with Secure Views ."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "This option sets a comment for the materialized view. The comment has no effect on the behavior of the view,\nbut can provide useful information to people who use or maintain the view."
        },
        {
            "name": "DATA_METRIC_SCHEDULE   ...",
            "description": "Specifies the schedule to run the data metric function periodically. Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5 , 15 , 30 , 60 , 720 , or 1440 . Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones . The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is not scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example, DATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
        },
        {
            "name": "' num   MINUTE'",
            "description": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5 , 15 , 30 , 60 , 720 , or 1440 ."
        },
        {
            "name": "'USING   CRON   expr   time_zone '",
            "description": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones . The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is not scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example, DATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
        },
        {
            "name": "*",
            "description": "Wildcard. Specifies any occurrence of the field."
        },
        {
            "name": "L",
            "description": "Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday” (“5L”) of\na given month. In the day-of-month field, it specifies the last day of the month."
        },
        {
            "name": "/{n}",
            "description": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is not scheduled to run in January (3 months after the October run)."
        }
    ],
    "usage_notes": "Use the ALTER VIEW command to set/unset a masking policy, row access policy, or tag on/from a materialized view.\nYou can use data metric functions (DMFs) with materialized views as follows:\nTo set the DATA_METRIC_SCHEDULE parameter on the materialized view, use the ALTER MATERIALIZED VIEW command. For more\ninformation, see Schedule the DMF to run.\nTo add a DMF to a column or drop a DMF from a column in a materialized view, use the ALTER VIEW command.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-external-table",
    "title": "ALTER EXTERNAL TABLE",
    "description": "Modifies the properties, columns, or constraints for an existing external table.",
    "syntax": "ALTER EXTERNAL TABLE [ IF EXISTS ] <name> REFRESH [ '<relative-path>' ]\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> ADD FILES ( '<path>/[<filename>]' [ , '<path>/[<filename>'] ] )\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> REMOVE FILES ( '<path>/[<filename>]' [ , '<path>/[<filename>]' ] )\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> SET\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> UNSET CONTACT <purpose>\n\nALTER EXTERNAL TABLE <name> [ IF EXISTS ] ADD PARTITION ( <part_col_name> = '<string>' [ , <part_col_name> = '<string>' ] ) LOCATION '<path>'\n\nALTER EXTERNAL TABLE <name> [ IF EXISTS ] DROP PARTITION LOCATION '<path>'",
    "examples": [
        {
            "code": "ALTER EXTERNAL TABLE exttable_json REFRESH;"
        },
        {
            "code": "CREATE OR REPLACE STAGE mystage\n  URL='<cloud_platform>://twitter_feed/logs/'\n  .. ;\n\n-- Create the external table\n-- 'daily' path includes paths in </YYYY/MM/DD/> format\nCREATE OR REPLACE EXTERNAL TABLE daily_tweets\n  WITH LOCATION = @twitter_feed/daily/;\n\n-- Refresh the metadata for a single day of data files by date\nALTER EXTERNAL TABLE exttable_part REFRESH '2018/08/05/';"
        },
        {
            "code": "ALTER EXTERNAL TABLE exttable1 ADD FILES ('path1/sales4.json.gz', 'path1/sales5.json.gz');"
        },
        {
            "code": "ALTER EXTERNAL TABLE exttable1 REMOVE FILES ('path1/sales4.json.gz', 'path1/sales5.json.gz');"
        },
        {
            "code": "BEGIN;\n\nALTER EXTERNAL TABLE extable1 REMOVE FILES ('2019/12/log1.json.gz');\n\nALTER EXTERNAL TABLE extable1 ADD FILES ('2019/12/log1.json.gz');\n\nCOMMIT;"
        },
        {
            "code": "ALTER EXTERNAL TABLE et2 ADD PARTITION(col1='2022-01-24', col2='a', col3='12') LOCATION '2022/01';"
        },
        {
            "code": "ALTER EXTERNAL TABLE et2 DROP PARTITION LOCATION '2022/01';"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the external table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case sensitive."
        },
        {
            "name": "REFRESH   [   ' relative-path '   ]",
            "description": "Accesses the staged data files referenced in the external table definition and updates the table metadata: New files in the path are added to the table metadata. Changes to files in the path are updated in the table metadata. Files no longer in the path are removed from the table metadata. Optionally specify a relative path to refresh the metadata for a specific subset of the data files. Using this parameter only needs to be done once, when the external table is created. This step synchronizes the metadata with the latest set\nof associated files in the stage and path in the external table definition. Also, this step ensures the external table can read the data files\nin the specified stage and path, and that no files were missed in the external table definition. Note This parameter is not supported by partitioned external tables when partitions are added manually by the object owner (that is,\nwhen PARTITION_TYPE = USER_SPECIFIED ). If TABLE_FORMAT = DELTA is set on the external table, REFRESH does not support a relative path to refresh the\nmetadata for a specific subset of the data files."
        },
        {
            "name": "ADD   FILES",
            "description": "Registers the specified comma-separated list of files with the external table metadata, and refreshes the table.\nFor each file, list the path and filename relative to [ WITH ] LOCATION in the external table definition.\nFor information, see CREATE EXTERNAL TABLE . This parameter is not supported by partitioned external tables when partitions are added manually by the object owner (that is,\nwhen PARTITION_TYPE = USER_SPECIFIED )."
        },
        {
            "name": "REMOVE   FILES",
            "description": "Deregisters the specified comma-separated list of files from the external table metadata, and refreshes the table.\nFor each file, list the path and filename relative to [ WITH ] LOCATION in the external table definition.\nFor information, see CREATE EXTERNAL TABLE . This parameter is not supported by partitioned external tables when partitions are added manually by the object owner (that is,\nwhen PARTITION_TYPE = USER_SPECIFIED )."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the external table (separated by blank spaces, commas, or new lines): Specifies whether Snowflake should enable triggering automatic refreshes of the external table metadata when new or updated data files\nare available in the named external stage specified in the [ WITH ] LOCATION = setting. Note You must configure an event notification for your storage location to notify Snowflake when new or updated data is available\nto read into the external table metadata. For more information, see the instructions for your cloud storage service: Refreshing external tables automatically for Amazon S3 Refreshing external tables automatically for Google Cloud Storage Refreshing external tables automatically for Azure Blob Storage This parameter is not supported by partitioned external tables when partitions are added manually by the object owner\n(that is, when PARTITION_TYPE = USER_SPECIFIED ). Setting this parameter to TRUE is not supported for external tables that reference data files stored on an S3-compatible external stage . Snowflake enables triggering automatic refreshes of the external table metadata. Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE … REFRESH to synchronize the metadata with the current list of files in the stage path. Default: TRUE Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "UNSET   CONTACT   purpose",
            "description": "Detaches a contact from the external table."
        },
        {
            "name": "AUTO_REFRESH   =     TRUE   |   FALSE",
            "description": "Specifies whether Snowflake should enable triggering automatic refreshes of the external table metadata when new or updated data files\nare available in the named external stage specified in the [ WITH ] LOCATION = setting. Note You must configure an event notification for your storage location to notify Snowflake when new or updated data is available\nto read into the external table metadata. For more information, see the instructions for your cloud storage service: Refreshing external tables automatically for Amazon S3 Refreshing external tables automatically for Google Cloud Storage Refreshing external tables automatically for Azure Blob Storage This parameter is not supported by partitioned external tables when partitions are added manually by the object owner\n(that is, when PARTITION_TYPE = USER_SPECIFIED ). Setting this parameter to TRUE is not supported for external tables that reference data files stored on an S3-compatible external stage . Snowflake enables triggering automatic refreshes of the external table metadata. Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE … REFRESH to synchronize the metadata with the current list of files in the stage path. Default: TRUE"
        },
        {
            "name": "Amazon S3 :",
            "description": "Refreshing external tables automatically for Amazon S3"
        },
        {
            "name": "Google Cloud Storage :",
            "description": "Refreshing external tables automatically for Google Cloud Storage"
        },
        {
            "name": "Microsoft Azure :",
            "description": "Refreshing external tables automatically for Azure Blob Storage"
        },
        {
            "name": "TRUE",
            "description": "Snowflake enables triggering automatic refreshes of the external table metadata."
        },
        {
            "name": "FALSE",
            "description": "Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE … REFRESH to synchronize the metadata with the current list of files in the stage path."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "ADD   PARTITION   (   <part_col_name>   =   '<string>'   [   ,   <part_col_name>   =   '<string>'   ,   ...   ]   )   LOCATION   '<path>'",
            "description": "Manually add a partition for one or more partition columns defined for the external table in a specified location (that is, path). Note The maximum length of user-specified partition column names is 32 characters. Adding a partition also adds any new or updated files in the location to the external table metadata."
        },
        {
            "name": "DROP   PARTITION   LOCATION   '<path>'",
            "description": "Manually drop all partitions in a specified location (that is, path). Dropping a partition also removes any files in the location from the external table metadata."
        }
    ],
    "usage_notes": "Only the external table owner (the role with the OWNERSHIP privilege on the external table) or higher can execute this command.\nThe following commands can be used in explicit transactions (using BEGIN … COMMIT):\nALTER EXTERNAL TABLE ... REFRESH\nALTER EXTERNAL TABLE ... ADD FILES\nALTER EXTERNAL TABLE ... REMOVE FILES\nExplicit transactions could be used to ensure a consistent state when manually replacing updated files in external table metadata.\nAdd or remove columns in an external table using the following syntax:\nNote\nThe default VALUE and METADATA$FILENAME columns cannot be dropped.\nSee the ALTER TABLE topic for examples.\nTo add and drop a row access policy on an external table, or to set or unset a tag, use the ALTER TABLE command.\nHowever, you can create an external table with a row access policy and a tag on the table. See CREATE EXTERNAL TABLE.\nYou can use data metric functions with external tables by executing an ALTER TABLE command. For more information, see\nUse data metric functions to perform data quality checks.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-masking-policy",
    "title": "ALTER MASKING POLICY",
    "description": "Replaces the existing masking policy rules with new rules or a new comment and allows the renaming of a masking policy.",
    "syntax": "ALTER MASKING POLICY [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER MASKING POLICY [ IF EXISTS ] <name> SET BODY -> <expression_on_arg_name_to_mask>\n\nALTER MASKING POLICY [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER MASKING POLICY [ IF EXISTS ] <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER MASKING POLICY [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER MASKING POLICY [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "DESCRIBE MASKING POLICY email_mask;"
        },
        {
            "code": "+-----+------------+---------------+-------------------+-----------------------------------------------------------------------+\n| Row | name       | signature     | return_type       | body                                                                  |\n+-----+------------+---------------+-------------------+-----------------------------------------------------------------------+\n| 1   | EMAIL_MASK | (VAL VARCHAR) | VARCHAR(16777216) | case when current_role() in ('ANALYST') then val else '*********' end |\n+-----+------------+---------------+-------------------+-----------------------------------------------------------------------+"
        },
        {
            "code": "ALTER MASKING POLICY email_mask SET BODY ->\n  CASE\n    WHEN current_role() IN ('ANALYST') THEN VAL\n    ELSE sha2(val, 512)\n  END;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the masking policy; must be unique in the parent schema of the policy. The identifier value must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier\nstring is enclosed in double quotes (e.g. \"My object\" ). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the masking policy; must be unique for your schema. The new identifier cannot be used if the identifier\nis already in place for a different masking policy. For more details, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the masking policy: SQL expression that transforms the data in the column designated by arg_name_mask . The expression can include Conditional expression functions to represent conditional logic, built-in functions, or UDFs to\ntransform the data. If a UDF or external function is used inside the masking policy body, the policy owner must have the USAGE privilege on the UDF or\nexternal function. Users querying a column that has a masking policy applied to it do not need to have USAGE on the UDF or external\nfunction. If a UDF or external function is used inside the conditional masking policy body, the policy owner must have the OWNERSHIP privilege on\nthe UDF or external function. Users querying a column that has a conditional masking policy applied to it do not need to have USAGE on\nthe UDF or external function. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Adds a comment or overwrites the existing comment for the masking policy. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties and/or parameters to unset for the masking policy, which resets them to the defaults: TAG tag_name [ , tag_name ... ] COMMENT When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "BODY   ->   expression_on_arg_name_to_mask",
            "description": "SQL expression that transforms the data in the column designated by arg_name_mask . The expression can include Conditional expression functions to represent conditional logic, built-in functions, or UDFs to\ntransform the data. If a UDF or external function is used inside the masking policy body, the policy owner must have the USAGE privilege on the UDF or\nexternal function. Users querying a column that has a masking policy applied to it do not need to have USAGE on the UDF or external\nfunction. If a UDF or external function is used inside the conditional masking policy body, the policy owner must have the OWNERSHIP privilege on\nthe UDF or external function. Users querying a column that has a conditional masking policy applied to it do not need to have USAGE on\nthe UDF or external function."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the masking policy. Default: No value"
        }
    ],
    "usage_notes": "If you want to update an existing masking policy and need to see the current definition of the policy, call the\nGET_DDL function or run the DESCRIBE MASKING POLICY command.\nYou cannot change the policy signature (i.e. argument name or input/output data type). If you need to change the signature, execute a\nDROP MASKING POLICY statement on the policy and create a new one.\nBefore executing an ALTER statement, you can execute a DESCRIBE MASKING POLICY statement to determine the argument name to use for\nupdating the policy.\nFor masking policies that include a subquery in the masking policy body, use EXISTS in the\nWHEN clause. For a representative example, see the custom entitlement table example in the Examples section in\nCREATE MASKING POLICY.\nIf the policy body contains a mapping table lookup, create a centralized mapping table and store the mapping table\nin the same database as the protected table. This is particularly important if the body calls the\nIS_DATABASE_ROLE_IN_SESSION function. For details, see the function usage notes.\nAdding a masking policy to a column fails if the column is referenced by a row access policy. For more information, see\nALTER ROW ACCESS POLICY.\nIf using a UDF in a masking policy, ensure the data type of the column, UDF, and masking\npolicy match. For more information, see User-defined functions in a masking policy.\nOnce you create a dynamic table, you can’t make changes to the masking policy.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-application-role",
    "title": "ALTER APPLICATION ROLE",
    "description": "Modifies the properties for an existing application role.",
    "syntax": "ALTER APPLICATION ROLE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER APPLICATION ROLE [ IF EXISTS ] <name> SET COMMENT = '<string_literal>'\n\nALTER APPLICATION ROLE [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER APPLICATION ROLE app_role RENAME TO new_app_role;"
        },
        {
            "code": "ALTER APPLICATION ROLE app_role SET\n  COMMENT = 'Application role for the Hello Snowflake application.';"
        },
        {
            "code": "ALTER APPLICATION ROLE app_role UNSET COMMENT;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the application role. If the identifier contains spaces or\nspecial characters, the entire string must be enclosed in double quotes. Identifiers enclosed\nin double quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the application role. The identifier must be unique\nfor within the application. For more details, see Identifier requirements . Note that when specifying the fully-qualified name of the application role, you cannot specify a\ndifferent application. The name of the application, application_name , must remain the same.\nOnly the application_role_name can change during a rename operation."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the application role: Adds a comment or overwrites an existing comment for the application role."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the application role, which resets them to the defaults. COMMENT"
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the application role."
        }
    ],
    "usage_notes": "This command can only be run in the context of an application created using the Native\nApps Framework.\nOnly the application role owner (i.e. the role with the OWNERSHIP privilege on the application\nrole), or a higher role, can run this command.\nRenaming an application role is only allowed within the same application.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-security-integration",
    "title": "ALTER SECURITY INTEGRATION",
    "description": "Modifies the properties for an existing security integration.",
    "syntax": "ALTER [ SECURITY ] INTEGRATION [ IF EXISTS ] <name> SET <parameters>\n\nALTER [ SECURITY ] INTEGRATION [ IF EXISTS ] <name>  UNSET <parameter>\n\nALTER [ SECURITY ] INTEGRATION <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER [ SECURITY ] INTEGRATION <name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
    "examples": [
        {
            "code": "ALTER [ SECURITY ] INTEGRATION [ IF EXISTS ] <name> SET <parameters>\n\nALTER [ SECURITY ] INTEGRATION [ IF EXISTS ] <name>  UNSET <parameter>\n\nALTER [ SECURITY ] INTEGRATION <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER [ SECURITY ] INTEGRATION <name> UNSET TAG <tag_name> [ , <tag_name> ... ]"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-alert",
    "title": "ALTER ALERT",
    "description": "Modifies the properties of an existing alert and suspends or resumes an existing alert.",
    "syntax": "ALTER ALERT [ IF EXISTS ] <name> { RESUME | SUSPEND };\n\nALTER ALERT [ IF EXISTS ] <name> SET\n  [ WAREHOUSE = <string> ]\n  [ SCHEDULE = '{ <number> MINUTE | USING CRON <expr> <time_zone> }' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER ALERT [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER ALERT [ IF EXISTS ] <name> UNSET\n  [ WAREHOUSE ]\n  [ COMMENT ]\n\nALTER ALERT <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER ALERT [ IF EXISTS ] <name> MODIFY CONDITION EXISTS (<condition>)\n\nALTER ALERT [ IF EXISTS ] <name> MODIFY ACTION <action>",
    "examples": [
        {
            "code": "ALTER ALERT [ IF EXISTS ] <name> { RESUME | SUSPEND };\n\nALTER ALERT [ IF EXISTS ] <name> SET\n  [ WAREHOUSE = <string> ]\n  [ SCHEDULE = '{ <number> MINUTE | USING CRON <expr> <time_zone> }' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER ALERT [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER ALERT [ IF EXISTS ] <name> UNSET\n  [ WAREHOUSE ]\n  [ COMMENT ]\n\nALTER ALERT <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER ALERT [ IF EXISTS ] <name> MODIFY CONDITION EXISTS (<condition>)\n\nALTER ALERT [ IF EXISTS ] <name> MODIFY ACTION <action>"
        },
        {
            "code": "# __________ minute (0-59)\n# | ________ hour (0-23)\n# | | ______ day of month (1-31, or L)\n# | | | ____ month (1-12, JAN-DEC)\n# | | | | _ day of week (0-6, SUN-SAT, or L)\n# | | | | |\n# | | | | |\n  * * * * *"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the alert to alter. If the identifier contains spaces or special characters, the entire string must be enclosed\nin double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "{   RESUME   |   SUSPEND   }",
            "description": "Specifies the action to perform on the alert: RESUME makes a suspended alert active. SUSPEND puts the alert into a “Suspended” state. If the alert schedule is set to an interval (i.e. num MINUTE ), then to avoid ambiguity, the base interval time for\nthe schedule is reset to the current time when the alert is resumed. The base interval time starts the interval counter from the current clock time. For example, if an alert is created with 10 MINUTE and the alert is resumed at 9:03 AM, then the alert runs at 9:13 AM, 9:23 AM, and so on. Note that we make a best\neffort to ensure absolute precision, but only guarantee that alerts do not execute before their set interval occurs\n(e.g., in the current example, the alert could first run at 9:14 AM, but will definitely not run at 9:12 AM)."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the alert (separated by blank spaces, commas, or new lines). Specifies the virtual warehouse that provides compute resources for executing this alert. Note For serverless alerts , do not set this property. Specifies the schedule for periodically evaluating the condition for the alert on a schedule. When you create an alert, omitting this parameter or setting it to NULL creates an alert on new data . For alerts on a schedule, you can specify the schedule in one of the following ways: USING CRON expr time_zone Specifies a cron expression and time zone for periodically evaluating the condition for the alert. Supports a subset of\nstandard cron utility syntax. The cron expression consists of the following fields: The following special characters are supported: Special Character Description * Wildcard. When specified for a given field, the alert runs at every unit of time for that field. For example, * in the month field specifies that the alert runs every month. L Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday”\n(“5L”) of a given month. In the day-of-month field, it specifies the last day of the month. / n Indicates the n th instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is specified in the month field, then the evaluation of the condition is scheduled for April,\nJuly and October (i.e. every 3 months, starting with the 4th month of the year). The same schedule is maintained in subsequent years. That is, the condition is not scheduled to be evaluated in January\n(3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value for the account (or setting the value at the user or session level) does not change the time zone for the alert. The cron expression defines all valid times for the evaluation of the condition for the alert. Snowflake attempts\nto evaluate the condition based on this schedule; however, any valid run time is skipped if a previous run has not\ncompleted before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the evaluation of the\ncondition is scheduled on days satisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules an evaluation at 0AM on any 10th to 20th day of the month\nand also on any Tuesday or Thursday outside of those dates. num MINUTE Specifies an interval (in minutes) of wait time inserted between evaluations of the alert. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set when the alert is resumed (using ALTER ALERT … RESUME ). The base interval time starts the interval counter from the current clock time. For example, if an alert is created with 10 MINUTE and the alert is resumed at 9:03 AM, then the condition for the alert is evaluated at 9:13 AM, 9:23 AM, and so\non. Note that we make a best effort to ensure absolute precision, but only guarantee that conditions are not evaluated before their set interval occurs (e.g. in the current example, the condition could be evaluated first at 9:14 AM but\ndefinitely not at 9:12 AM). Note The maximum supported value is 11520 (8 days). Alerts that have a greater num MINUTE value never have their\nconditions evaluated. Specifies a comment for the alert. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties/parameters to unset for the alert, which resets them back to their defaults: WAREHOUSE COMMENT TAG tag_key [ , tag_key ... ]"
        },
        {
            "name": "MODIFY   CONDITION   EXISTS   ( condition )",
            "description": "Specifies the SQL statement that should represent the condition for the alert. You can use the following commands: SELECT SHOW <objects> CALL If the statement returns one or more rows, the action for the alert is executed."
        },
        {
            "name": "MODIFY   ACTION   action",
            "description": "Specifies the SQL statement that should be executed if the condition returns one or more rows. To send a notification, you can call the SYSTEM$SEND_EMAIL or SYSTEM$SEND_SNOWFLAKE_NOTIFICATION stored procedure ."
        },
        {
            "name": "WAREHOUSE   =   warehouse_name",
            "description": "Specifies the virtual warehouse that provides compute resources for executing this alert. Note For serverless alerts , do not set this property."
        },
        {
            "name": "SCHEDULE   ...",
            "description": "Specifies the schedule for periodically evaluating the condition for the alert on a schedule. When you create an alert, omitting this parameter or setting it to NULL creates an alert on new data . For alerts on a schedule, you can specify the schedule in one of the following ways: USING CRON expr time_zone Specifies a cron expression and time zone for periodically evaluating the condition for the alert. Supports a subset of\nstandard cron utility syntax. The cron expression consists of the following fields: The following special characters are supported: Special Character Description * Wildcard. When specified for a given field, the alert runs at every unit of time for that field. For example, * in the month field specifies that the alert runs every month. L Stands for “last”. When used in the day-of-week field, it allows you to specify constructs such as “the last Friday”\n(“5L”) of a given month. In the day-of-month field, it specifies the last day of the month. / n Indicates the n th instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is specified in the month field, then the evaluation of the condition is scheduled for April,\nJuly and October (i.e. every 3 months, starting with the 4th month of the year). The same schedule is maintained in subsequent years. That is, the condition is not scheduled to be evaluated in January\n(3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value for the account (or setting the value at the user or session level) does not change the time zone for the alert. The cron expression defines all valid times for the evaluation of the condition for the alert. Snowflake attempts\nto evaluate the condition based on this schedule; however, any valid run time is skipped if a previous run has not\ncompleted before the next valid run time starts. When both a specific day of month and day of week are included in the cron expression, then the evaluation of the\ncondition is scheduled on days satisfying either the day of month or day of week. For example, SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules an evaluation at 0AM on any 10th to 20th day of the month\nand also on any Tuesday or Thursday outside of those dates. num MINUTE Specifies an interval (in minutes) of wait time inserted between evaluations of the alert. Accepts positive integers only. Also supports num M syntax. To avoid ambiguity, a base interval time is set when the alert is resumed (using ALTER ALERT … RESUME ). The base interval time starts the interval counter from the current clock time. For example, if an alert is created with 10 MINUTE and the alert is resumed at 9:03 AM, then the condition for the alert is evaluated at 9:13 AM, 9:23 AM, and so\non. Note that we make a best effort to ensure absolute precision, but only guarantee that conditions are not evaluated before their set interval occurs (e.g. in the current example, the condition could be evaluated first at 9:14 AM but\ndefinitely not at 9:12 AM). Note The maximum supported value is 11520 (8 days). Alerts that have a greater num MINUTE value never have their\nconditions evaluated."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Specifies a comment for the alert."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        }
    ],
    "usage_notes": "You cannot change an alert on new data to an\nalert on a schedule. Similarly, you cannot change an alert on a schedule to an alert\non new data.\nWhen an alert is resumed, Snowflake verifies that the role with the OWNERSHIP privilege on the alert also has the USAGE\nprivilege on the warehouse assigned to the alert, as well as the global EXECUTE ALERT privilege; if not, an error is produced.\nOnly account administrators (users with the ACCOUNTADMIN role) can grant the EXECUTE ALERT privilege to a role. For ease of use,\nwe recommend creating a custom role (e.g. alert_admin) and assigning the EXECUTE ALERT privilege to this role. Any role that can\ngrant privileges (e.g. SECURITYADMIN or any role with the MANAGE GRANTS privilege) can then grant this custom role to any alert\nowner role to allow altering their own alerts. For instructions for creating custom roles and role hierarchies, see\nConfiguring access control.\nWhen you execute CREATE ALERT or ALTER ALERT, some validation checks are not performed on the statements in the condition and\naction, including:\nThe resolution of the identifiers for objects.\nThe resolution of the data types of expressions.\nThe verification of the number and types of arguments in a function call.\nThe CREATE ALERT and ALTER ALERT commands do not fail if the SQL statement for a condition or action specifies an invalid\nidentifier, incorrect data type, incorrect number and types of function arguments, etc. Instead, the failure occurs when the\nalert executes.\nTo check for failures in an existing alert, use the ALERT_HISTORY table function.\nTo avoid these types of failures, before you specify the conditions and actions for alerts, verify the SQL expressions and\nstatements for those conditions and actions.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/classes/classification/commands/alter-classification",
    "title": "ALTER SNOWFLAKE.ML.CLASSIFICATION",
    "description": "You can change the name, description, and tags of a classification model object using forms of the ALTER command. Models\nthemselves are immutable and cannot be updated in place. To update a model, drop the existing model and train a new one.",
    "syntax": "ALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    RENAME TO '<new_model_name>';\n\nALTER SNOWFLAKE.ML.CLASSIFICATION  [ IF EXISTS ] <name>\n    SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ];\n\nALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    SET COMMENT = '<string_literal>';\n\nALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    UNSET TAG <tag_name> [ , <tag_name> ... ];\n\nALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    UNSET COMMENT;",
    "examples": [
        {
            "code": "ALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    RENAME TO '<new_model_name>';"
        },
        {
            "code": "ALTER SNOWFLAKE.ML.CLASSIFICATION  [ IF EXISTS ] <name>\n    SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ];"
        },
        {
            "code": "ALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    SET COMMENT = '<string_literal>';"
        },
        {
            "code": "ALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    UNSET TAG <tag_name> [ , <tag_name> ... ];"
        },
        {
            "code": "ALTER SNOWFLAKE.ML.CLASSIFICATION [ IF EXISTS ] <name>\n    UNSET COMMENT;"
        }
    ]
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-table-event-table",
    "title": "ALTER TABLE (event tables)",
    "description": "Modifies the properties, columns, or constraints for an existing event table.",
    "syntax": "ALTER TABLE [ IF EXISTS ] <name> RENAME TO <new_table_name>\n\nALTER TABLE [ IF EXISTS ] <name> clusteringAction\n\nALTER TABLE [ IF EXISTS ] <name> dataGovnPolicyTagAction\n\nALTER TABLE [ IF EXISTS ] <name> searchOptimizationAction\n\nALTER TABLE [ IF EXISTS ] <name> SET\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE  } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER TABLE [ IF EXISTS ] <name> UNSET {\n                                       DATA_RETENTION_TIME_IN_DAYS         |\n                                       MAX_DATA_EXTENSION_TIME_IN_DAYS     |\n                                       CHANGE_TRACKING                     |\n                                       CONTACT <purpose>                   |\n                                       COMMENT                             |\n                                       }\n\nclusteringAction ::=\n  {\n     CLUSTER BY ( <expr> [ , <expr> , ... ] )\n   | { SUSPEND | RESUME } RECLUSTER\n   | DROP CLUSTERING KEY\n  }\n\ndataGovnPolicyTagAction ::=\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n\nsearchOptimizationAction ::=\n  {\n     ADD SEARCH OPTIMIZATION [\n       ON <search_method_with_target> [ , <search_method_with_target> ... ]\n     ]\n\n   | DROP SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n\n  }",
    "examples": [
        {
            "code": "CREATE OR REPLACE TABLE t1(a1 number);\n\nSHOW TABLES LIKE 't1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n Tue, 17 Mar 2015 16:52:33 -0700 | T1   | TESTDB        | MY_SCHEMA   | TABLE |         |            | 0    | 0     | PUBLIC | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n\nALTER TABLE t1 RENAME TO tt1;\n\nSHOW TABLES LIKE 'tt1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n Tue, 17 Mar 2015 16:52:33 -0700 | TT1  | TESTDB        | MY_SCHEMA   | TABLE |         |            | 0    | 0     | PUBLIC | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+"
        },
        {
            "code": "CREATE OR REPLACE TABLE T1 (id NUMBER, date TIMESTAMP_NTZ, name STRING) CLUSTER BY (id, date);\n\nSHOW TABLES LIKE 'T1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (ID,DATE)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n\n-- Change the order of the clustering key\nALTER TABLE t1 CLUSTER BY (date, id);\n\nSHOW TABLES LIKE 'T1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (DATE,ID)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+"
        },
        {
            "code": "ALTER TABLE t1\n  ADD ROW ACCESS POLICY rap_t1 ON (empl_id);"
        },
        {
            "code": "ALTER TABLE t1\n  ADD ROW ACCESS POLICY rap_test2 ON (cost, item);"
        },
        {
            "code": "ALTER TABLE t1\n  DROP ROW ACCESS POLICY rap_v1;"
        },
        {
            "code": "alter table t1\n  drop row access policy rap_t1_version_1,\n  add row access policy rap_t1_version_2 on (empl_id);"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Identifier for the event table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_table_name",
            "description": "Renames the specified event table with a new identifier that is not currently used by any other event tables in the schema. Note Not supported on the default event table, SNOWFLAKE.TELEMETRY.EVENTS. For more details about event table identifiers, see Identifier requirements . You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object (table, column, etc.) is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one or more properties/parameters to set for the event table (separated by blank spaces, commas, or new lines): Object-level parameter that modifies the retention period for the event table for Time Travel. For more details, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent event tables 0 or 1 for temporary and transient event tables Note A value of 0 effectively disables Time Travel for the event table. Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the event table to\nprevent streams on the event table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS . Specifies to enable or disable change tracking on the event table. TRUE enables change tracking on the event table. This option adds a pair of hidden columns to the source event table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the event table. FALSE disables change tracking on the event table. The pair of hidden columns is dropped from the event table. Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts . Adds a comment or overwrites the existing comment for the event table."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties/parameters to unset for the event table, which resets them back to their defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS CHANGE_TRACKING CONTACT purpose COMMENT"
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   integer",
            "description": "Object-level parameter that modifies the retention period for the event table for Time Travel. For more details, see Understanding & using Time Travel and Working with Temporary and Transient Tables . For a detailed description of this parameter, as well as more information about object parameters, see Parameters . Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent event tables 0 or 1 for temporary and transient event tables Note A value of 0 effectively disables Time Travel for the event table."
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the event table to\nprevent streams on the event table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "CHANGE_TRACKING   =     TRUE   |   FALSE",
            "description": "Specifies to enable or disable change tracking on the event table. TRUE enables change tracking on the event table. This option adds a pair of hidden columns to the source event table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT statements, or by creating and querying one or more streams on the event table. FALSE disables change tracking on the event table. The pair of hidden columns is dropped from the event table."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the event table."
        }
    ],
    "usage_notes": "Changes to an event table are not automatically propagated to views created on that event table.\nTo alter an event table, you must be using a role that has ownership privilege on the event table.\nTo add clustering to an event table, you must also have USAGE or OWNERSHIP privileges on the schema and database that\ncontain the event table.\nFor row access policies:\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nIf you create a foreign key, the columns in the REFERENCES clause must be listed in the same order as they were\nlisted for the primary key. For example:\nIn both cases, the order of the columns is c_1, c_2. If the order of the columns in the foreign key had been different\n(for example, c_2, c_1), the attempt to create the foreign key would have failed.\nYou can use data metric functions with event tables by executing an ALTER TABLE command. For more information, see\nUse data metric functions to perform data quality checks.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nALTER TABLE … CHANGE_TRACKING = TRUE\nWhen an event table is altered to enable change tracking, the event table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-share",
    "title": "ALTER SHARE",
    "description": "Modifies the properties for an existing share:",
    "syntax": "ALTER SHARE [ IF EXISTS ] <name> { ADD | REMOVE } ACCOUNTS = <consumer_account> [ , <consumer_account> , ... ]\n                                        [ SHARE_RESTRICTIONS = { TRUE | FALSE } ]\n\nALTER SHARE [ IF EXISTS ] <name> SET { [ ACCOUNTS = <consumer_account> [ , <consumer_account> ... ] ]\n                                       [ COMMENT = '<string_literal>' ] }\n\nALTER SHARE [ IF EXISTS ] <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER SHARE <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER SHARE [ IF EXISTS ] <name> UNSET COMMENT",
    "examples": [
        {
            "code": "ALTER SHARE sales_s ADD ACCOUNTS=<orgname.accountname1>,<orgname.accountname2>;\n\n+----------------------------------+\n| status                           |\n|----------------------------------|\n| Statement executed successfully. |\n+----------------------------------+"
        },
        {
            "code": "ALTER SHARE sales_s REMOVE ACCOUNT=<orgname.accountname>;\n\n+----------------------------------+\n| status                           |\n|----------------------------------|\n| Statement executed successfully. |\n+----------------------------------+"
        },
        {
            "code": "GRANT MANAGE SHARE TARGET ON ACCOUNT TO ROLE <role_name>;\n\nGRANT ROLE <role_name> TO USER <user_name>;\n\nUSE ROLE <role_name>;\n\nALTER SHARE <data_share_name> ADD ACCOUNTS = <orgname.accountname1>,<orgname.accountname2>;"
        },
        {
            "code": "ALTER SHARE sales_s SET COMMENT='This share contains sales data for 2017';\n\n+----------------------------------+\n| status                           |\n|----------------------------------|\n| Statement executed successfully. |\n+----------------------------------+"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the share to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "ADD   |   REMOVE   ACCOUNTS   =   consumer_account   [   ,   consumer_account   ,   ...   ]",
            "description": "Specifies the name of the account(s) to add or remove from the list of accounts for the share: Adding an account to a share that was already in the list has no effect. Removing an account that has already imported the shared database immediately revokes that account’s access to the database. If the account\nis later added back to the share, the account must re-create the database before they can use it again. Removing an account from a share that was not already in the list of shared accounts has no effect. This parameter adds to (or removes from) the existing list of accounts for the share. If you want to replace the entire list of accounts, use SET instead. SHARE_RESTRICTIONS = { TRUE | FALSE } FALSE : A Standard or Enterprise consumer account can be added to a share belonging to a Business Critical provider account. TRUE : A Standard or Enterprise consumer account cannot be added to a share belonging to a Business Critical provider account. TRUE Important You must set this parameter each time you add a new non-Business Critical consumer account to the share belonging to a Business Critical\nprovider. For more information see, Enable sharing from a Business critical account to a non-business critical account ."
        },
        {
            "name": "Default :",
            "description": "TRUE"
        },
        {
            "name": "ACCOUNTS   =   consumer_account   [   ,   consumer_account   ...   ]",
            "description": "Specifies the account(s) to replace all previous accounts with which the share was shared. To add/remove individual accounts from the\nlist, use ADD | REMOVE instead."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "COMMENT   =   ' string '",
            "description": "Adds a comment or overwrites an existing comment for the share."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one or more properties/parameters to unset for the share, which resets them back to their defaults: TAG tag_name [ , tag_name ... ] COMMENT"
        }
    ],
    "usage_notes": "One of the following privileges is required to alter a share:\nThe OWNERSHIP privilege which is granted to the role that creates the share.\nThe MANAGE SHARE TARGET privilege determines which roles can add or remove accounts from a share.\nOnly roles granted MANAGE SHARE TARGET can add or remove share account access.\nKeywords ACCOUNT and ACCOUNTS are both supported and can be used interchangeably.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create",
    "title": "CREATE",
    "description": "Creates a new object of the specified type."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-function",
    "title": "ALTER FUNCTION",
    "description": "Modifies the properties of an existing user-defined or external function.",
    "syntax": "ALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , <integration_name> ... ] ) ]\n  [ SECRETS = ( '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , <integration_name> ... ] ) ]\n  [ SECRETS = ( '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ EXTERNAL_ACCESS_INTEGRATIONS = ( <integration_name> [ , <integration_name> ... ] ) ]\n  [ SECRETS = ( '<secret_variable_name>' = <secret_name> [ , '<secret_variable_name>' = <secret_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) RENAME TO <new_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET SECURE\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET { SECURE | LOG_LEVEL | TRACE_LEVEL | COMMENT }\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET\n  [ LOG_LEVEL = '<log_level>' ]\n  [ TRACE_LEVEL = '<trace_level>' ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET API_INTEGRATION = <api_integration_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET HEADERS = ( [ '<header_1>' = '<value>' [ , '<header_2>' = '<value>' ... ] ] )\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET CONTEXT_HEADERS = ( [ <context_function_1> [ , <context_function_2> ...] ] )\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET MAX_BATCH_ROWS = <integer>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET COMPRESSION = <compression_type>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) SET { REQUEST_TRANSLATOR | RESPONSE_TRANSLATOR } = <udf_name>\n\nALTER FUNCTION [ IF EXISTS ] <name> ( [ <arg_data_type> , ... ] ) UNSET\n              { COMMENT | HEADERS | CONTEXT_HEADERS | MAX_BATCH_ROWS | COMPRESSION | SECURE | REQUEST_TRANSLATOR | RESPONSE_TRANSLATOR }",
    "examples": [
        {
            "code": "ALTER FUNCTION IF EXISTS function1(number) RENAME TO function2;"
        },
        {
            "code": "ALTER FUNCTION function2(number) SET SECURE;"
        },
        {
            "code": "ALTER FUNCTION function4(number) SET API_INTEGRATION = api_integration_2;"
        },
        {
            "code": "ALTER FUNCTION function5(number) SET MAX_BATCH_ROWS = 100;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the UDF to alter. The identifier can contain the schema name and database name, as well as the function name.\nIf the identifier contains spaces or special characters, the entire string must be enclosed in double quotes. Identifiers enclosed in\ndouble quotes are also case-sensitive."
        },
        {
            "name": "arg_data_type   [   ,   ...   ]",
            "description": "Specifies the arguments/input data types for the external function. If the function accepts arguments, then the ALTER command must specify the argument types because functions support\nname overloading (i.e. two functions in the same schema can have the same name), and the argument types are used to\nidentify the function."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the properties to set for the function: Specifies whether a function is secure. For more details, see Protecting Sensitive Information with Secure UDFs and Stored Procedures . Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing . Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing . The names of external access integrations needed in order for this\nfunction’s handler code to access external networks. An external access integration contains network rules and secrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview . Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameter’s value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an external access integration’s ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integration’s name must, in turn, be specified as a value of this CREATE FUNCTION call’s\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret. Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output. Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the function, which resets them to the defaults."
        },
        {
            "name": "SECURE",
            "description": "Specifies whether a function is secure. For more details, see Protecting Sensitive Information with Secure UDFs and Stored Procedures ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "EXTERNAL_ACCESS_INTEGRATIONS   =   (   integration_name   [   ,   ...   ]   )",
            "description": "The names of external access integrations needed in order for this\nfunction’s handler code to access external networks. An external access integration contains network rules and secrets that specify the external locations and credentials (if any) needed for handler code\nto make requests of an external network, such as an external REST API. For more information, refer to External network access overview ."
        },
        {
            "name": "SECRETS   =   (   ' secret_variable_name '   =   secret_name   [   ,   ...    ]   )",
            "description": "Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from\nsecrets in handler code. This parameter’s value is a list of assignment expressions with the following parts: secret_name as the name of a secret specified in an external access integration’s ALLOWED_AUTHENTICATION_SECRETS parameter\nvalue. That external access integration’s name must, in turn, be specified as a value of this CREATE FUNCTION call’s\nEXTERNAL_ACCESS_INTEGRATIONS parameter. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the\nEXTERNAL_ACCESS_INTEGRATIONS parameter. ' secret_variable_name ' as the variable that will be used in handler code when retrieving information from the secret."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites the existing comment for the function. The value you specify is displayed in the DESCRIPTION column in the SHOW FUNCTIONS and SHOW USER FUNCTIONS output."
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the UDF; the combination of the identifier and existing argument data types must be unique for the schema. For more details, see Identifier requirements . Note When specifying the new name for the UDF, do not specify argument data types or parentheses; specify only the new name. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form db_name . schema_name . object_name or schema_name . object_name , respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the function. The identifier does not need to be unique for the schema in which the function is created because functions are\nidentified and resolved by their name and argument types. However, the signature (name and parameter data types)\nmust be unique within the schema. The name must follow the rules for Snowflake identifiers .\nFor more details, see Identifier requirements . Note When specifying the new name for the external function, do not specify argument data types or parentheses;\nthe function will continue using the same arguments as before."
        },
        {
            "name": "api_integration_name",
            "description": "This is the name of the API integration object that should be used to authenticate the call to the proxy service. More details about this parameter are in CREATE EXTERNAL FUNCTION ."
        },
        {
            "name": "HEADERS   =   (   ' header_1 '   =   ' value '   [   ,   ' header_2 '   =   ' value '   ...   ]   )",
            "description": "This clause allows users to attach key-value metadata that is sent with every request. The value must be a constant string, not an expression. More details about this parameter are in CREATE EXTERNAL FUNCTION ."
        },
        {
            "name": "CONTEXT_HEADERS   =   (   [   context_function_1   [   ,   context_function_2   ...   ]   ]   )",
            "description": "This is similar to HEADERS, but instead of allowing only constant strings, it allows binding Snowflake\ncontext function results to HTTP headers. Each value must be the name of a context function. The names should not be quoted. More details about this parameter are in CREATE EXTERNAL FUNCTION ."
        },
        {
            "name": "COMPRESSION   =   compression_type",
            "description": "If this clause is specified, the JSON payload is compressed using the specified format when sent from Snowflake to\nthe proxy service, and when sent back from the proxy service to Snowflake. For more details about valid values of compression_type , see CREATE EXTERNAL FUNCTION ."
        },
        {
            "name": "{   REQUEST_TRANSLATOR   |   RESPONSE_TRANSLATOR   }   =   udf_name",
            "description": "Add a request translator or a response translator if the external function does not already have one or replace an existing request translator\nor response translator by specifying the name of a previously-created JavaScript UDF.\nFor more information, see Using request and response translators with data for a remote service ."
        }
    ],
    "usage_notes": "Regarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nIf using a UDF in a masking policy, ensure the data type of the column, UDF, and masking policy match. For\nmore information, see User-defined functions in a masking policy.\nThere is no UNSET command for API_INTEGRATION. You can change the API_INTEGRATION, but you cannot unset it. For more, see\nALTER API INTEGRATION."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-dataset",
    "title": "ALTER DATASET",
    "description": "Modifies a dataset by adding or dropping dataset versions. When you add a version, you can specify properties such as partitioning, comments, or custom metadata."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-user",
    "title": "ALTER USER",
    "description": "Modifies the properties and object/session parameters for an existing user in the system:",
    "syntax": "ALTER USER [ IF EXISTS ] [ <name> ] RENAME TO <new_name>\n\nALTER USER [ IF EXISTS ] [ <name> ] RESET PASSWORD\n\nALTER USER [ IF EXISTS ] [ <name> ] ABORT ALL QUERIES\n\nALTER USER [ IF EXISTS ] [ <name> ] ADD DELEGATED AUTHORIZATION OF ROLE <role_name> TO SECURITY INTEGRATION <integration_name>\n\nALTER USER [ IF EXISTS ] [ <name> ] REMOVE DELEGATED { AUTHORIZATION OF ROLE <role_name> | AUTHORIZATIONS } FROM SECURITY INTEGRATION <integration_name>\n\nALTER USER [ IF EXISTS ] [ <name> ] mfaActions\n\nALTER USER [ IF EXISTS ] [ <name> ] SET { AUTHENTICATION | PASSWORD | SESSION } POLICY <policy_name>\n\nALTER USER [ IF EXISTS ] [ <name> ] UNSET { AUTHENTICATION | PASSWORD | SESSION } POLICY\n\nALTER USER [ IF EXISTS ] [ <name> ] SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER USER [ IF EXISTS ] [ <name> ] UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER USER [ IF EXISTS ] [ <name> ] SET { [ objectProperties ] [ objectParams ] [ sessionParams ] }\n\nALTER USER [ IF EXISTS ] [ <name> ] UNSET { <object_property_name> | <object_param_name> | <session_param_name> } [ , ... ]\n\nmfaActions ::=\n  {\n    ENROLL MFA\n    SET DEFAULT_MFA_METHOD = { PASSKEY | TOTP | DUO }\n    REMOVE MFA METHOD <mfa_method>\n    MODIFY MFA METHOD <mfa_method> SET COMMENT = '<string>'\n  }\n\nobjectProperties ::=\n    PASSWORD = '<string>'\n    LOGIN_NAME = <string>\n    DISPLAY_NAME = <string>\n    FIRST_NAME = <string>\n    MIDDLE_NAME = <string>\n    LAST_NAME = <string>\n    EMAIL = <string>\n    MUST_CHANGE_PASSWORD = TRUE | FALSE\n    DISABLED = TRUE | FALSE\n    DAYS_TO_EXPIRY = <integer>\n    MINS_TO_UNLOCK = <integer>\n    DEFAULT_WAREHOUSE = <string>\n    DEFAULT_NAMESPACE = <string>\n    DEFAULT_ROLE = <string>\n    DEFAULT_SECONDARY_ROLES = ( 'ALL' )\n    MINS_TO_BYPASS_MFA = <integer>\n    DISABLE_MFA = TRUE | FALSE\n    RSA_PUBLIC_KEY = <string>\n    RSA_PUBLIC_KEY_FP = <string>\n    RSA_PUBLIC_KEY_2 = <string>\n    RSA_PUBLIC_KEY_2_FP = <string>\n    TYPE = PERSON | SERVICE | LEGACY_SERVICE | NULL\n    COMMENT = '<string>'\n\nobjectParams ::=\n    ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR = TRUE | FALSE\n    ENABLE_UNREDACTED_SECURE_OBJECT_ERROR = TRUE | FALSE\n    NETWORK_POLICY = <string>\n    PREVENT_UNLOAD_TO_INLINE_URL = TRUE | FALSE\n    PREVENT_UNLOAD_TO_INTERNAL_STAGES = TRUE | FALSE\n\nsessionParams ::=\n    ABORT_DETACHED_QUERY = TRUE | FALSE\n    AUTOCOMMIT = TRUE | FALSE\n    BINARY_INPUT_FORMAT = <string>\n    BINARY_OUTPUT_FORMAT = <string>\n    DATE_INPUT_FORMAT = <string>\n    DATE_OUTPUT_FORMAT = <string>\n    DEFAULT_NULL_ORDERING = <string>\n    ENABLE_NOTEBOOK_CREATION_IN_PERSONAL_DB = TRUE | FALSE\n    ERROR_ON_NONDETERMINISTIC_MERGE = TRUE | FALSE\n    ERROR_ON_NONDETERMINISTIC_UPDATE = TRUE | FALSE\n    JSON_INDENT = <num>\n    LOCK_TIMEOUT = <num>\n    QUERY_TAG = <string>\n    ROWS_PER_RESULTSET = <num>\n    S3_STAGE_VPCE_DNS_NAME = <string>\n    SEARCH_PATH = <string>\n    SIMULATED_DATA_SHARING_CONSUMER = <string>\n    STATEMENT_TIMEOUT_IN_SECONDS = <num>\n    STRICT_JSON_OUTPUT = TRUE | FALSE\n    TIMESTAMP_DAY_IS_ALWAYS_24H = TRUE | FALSE\n    TIMESTAMP_INPUT_FORMAT = <string>\n    TIMESTAMP_LTZ_OUTPUT_FORMAT = <string>\n    TIMESTAMP_NTZ_OUTPUT_FORMAT = <string>\n    TIMESTAMP_OUTPUT_FORMAT = <string>\n    TIMESTAMP_TYPE_MAPPING = <string>\n    TIMESTAMP_TZ_OUTPUT_FORMAT = <string>\n    TIMEZONE = <string>\n    TIME_INPUT_FORMAT = <string>\n    TIME_OUTPUT_FORMAT = <string>\n    TRANSACTION_DEFAULT_ISOLATION_LEVEL = <string>\n    TWO_DIGIT_CENTURY_START = <num>\n    UNSUPPORTED_DDL_ACTION = <string>\n    USE_CACHED_RESULT = TRUE | FALSE\n    WEEK_OF_YEAR_POLICY = <num>\n    WEEK_START = <num>",
    "examples": [
        {
            "code": "ALTER USER user1 RENAME TO user2;"
        },
        {
            "code": "ALTER USER user1 SET PASSWORD = 'H8MZRqa8gEe/kvHzvJ+Giq94DuCYoQXmfbb$Xnt' MUST_CHANGE_PASSWORD = TRUE;"
        },
        {
            "code": "ALTER USER user1 SET TYPE = SERVICE;"
        },
        {
            "code": "ALTER USER user1 UNSET COMMENT;"
        },
        {
            "code": "ALTER USER user1 SET DEFAULT_SECONDARY_ROLES = ();"
        },
        {
            "code": "ALTER USER user1 UNSET DEFAULT_SECONDARY_ROLES;"
        },
        {
            "code": "ALTER USER user1 SET DEFAULT_SECONDARY_ROLES = ('ALL');"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the user to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double quotes are also case-sensitive. If the identifier is omitted, the statement modifies the active (i.e. logged in) user. The restrictions described in Usage Notes (in\nthis topic) apply."
        },
        {
            "name": "RENAME   TO   new_name",
            "description": "Specifies the new identifier for the user; must be unique for your account. For more information, see Identifier requirements ."
        },
        {
            "name": "RESET   PASSWORD",
            "description": "Generates a URL, which you can share with the user, that opens a web page from which the user can enter a new password. The generated URL is\nvalid for a single use and expires after 4 hours. Note that specifying this parameter does not invalidate the user’s current password. The user can continue to use their current password\nuntil they reset it through the URL. If you wish to invalidate their current password, use SET PASSWORD = ' string ' instead, which changes their password to a new value."
        },
        {
            "name": "ABORT   ALL   QUERIES",
            "description": "Aborts all the queries and other SQL statements currently running or scheduled by the user, regardless of the warehouse on which the queries\nare running/scheduled. Note that the user can still log into Snowflake and initiate new queries. If you want to abort all running/scheduled queries and prevent the user from logging into Snowflake or initiating new queries, specify SET DISABLED = TRUE instead."
        },
        {
            "name": "ADD   DELEGATED   AUTHORIZATION   OF   ROLE   role_name   TO   SECURITY   INTEGRATION   integration_name ;",
            "description": "Adds user consent to initiate a session using a specified role for a particular integration. For more details, see Adding Delegated Authorizations for OAuth User Consent ."
        },
        {
            "name": "REMOVE   DELEGATED   AUTHORIZATION   OF   ROLE   role_name   FROM   SECURITY   INTEGRATION   integration_name  ,  .   REMOVE   DELEGATED   AUTHORIZATIONS   FROM   SECURITY   INTEGRATION   integration_name",
            "description": "Revokes consent for the user: The first syntax revokes consent for a specified security integration for a specified role. This has the effect of revoking any OAuth\naccess token associated with the integration and specific role. The second syntax revokes all consent from a specified security integration. This has the effect of revoking any OAuth access token\nassociated with the integration. For more details, see: Configure Snowflake OAuth for partner applications Configure Snowflake OAuth for custom clients"
        },
        {
            "name": "{   AUTHENTICATION   |   PASSWORD   |   SESSION   }   POLICY   policy_name",
            "description": "Specifies one of the following policies for the user: Authentication policy Password policy Session policy"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) parameters to set for the user (separated by blank spaces, commas, or new\nlines): Controls how queries that fail due to syntax or parsing errors show up in a query history. If FALSE, the contents of a\nfailed query is redacted from the views, pages, and functions that provide a query history. This parameter controls behavior for the user viewing the query history, not the user who executed the query. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter. Controls whether error messages related to secure objects are redacted in metadata. For more information about\nerror message redaction for secure objects, see Secure objects: Redaction of information in error messages . Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_SECURE_OBJECT_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the\nredacted error messages in metadata, not the user who caused the error. Specifies the network policy that is active for the user."
        },
        {
            "name": "ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR   =   {   TRUE   |   FALSE   }",
            "description": "Controls how queries that fail due to syntax or parsing errors show up in a query history. If FALSE, the contents of a\nfailed query is redacted from the views, pages, and functions that provide a query history. This parameter controls behavior for the user viewing the query history, not the user who executed the query. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter."
        },
        {
            "name": "ENABLE_UNREDACTED_SECURE_OBJECT_ERROR   =   {   TRUE   |   FALSE   }",
            "description": "Controls whether error messages related to secure objects are redacted in metadata. For more information about\nerror message redaction for secure objects, see Secure objects: Redaction of information in error messages . Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_SECURE_OBJECT_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the\nredacted error messages in metadata, not the user who caused the error."
        },
        {
            "name": "NETWORK_POLICY   =   string",
            "description": "Specifies the network policy that is active for the user."
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies the properties to unset for the user, which resets them to the defaults. NETWORK_POLICY SESSION POLICY TAG tag_name [ , tag_name ... ]"
        },
        {
            "name": "SET   session_param_name   =   param_value   [   ...   ]  ,  .   UNSET   session_param_name   [   ,   ...   ]",
            "description": "Specifies one (or more) session parameters to set or unset for the user. Unsetting a session parameter resets it back to the default."
        }
    ],
    "usage_notes": "Only the role with the OWNERSHIP privilege on the user, or a higher role, can execute this command to modify most user properties.\nTip\nWhen changing a user’s password using SET PASSWORD = 'string', we recommend also specifying MUST_CHANGE_PASSWORD = TRUE\nto force the user to log into the web interface and change their password before they can log into Snowflake through any other interface\n(e.g. SnowSQL or another client application).\nAlternatively, use RESET PASSWORD to generate a URL to a web page that the user can access to change their password.\nOnly users with the ACCOUNTADMIN role can set the following parameters:\nPREVENT_UNLOAD_TO_INLINE_URL\nPREVENT_UNLOAD_TO_INTERNAL_STAGES\nIndividual users can execute the ALTER USER command on themselves (i.e. by specifying their user name/identifier in the command) and change\nthe following:\nDEFAULT_WAREHOUSE\nDEFAULT_NAMESPACE\nDEFAULT_ROLE\nAny of their session parameter defaults\nNote that users can not use this command to change their password. For security reasons, Snowflake only allows users to change\ntheir passwords from within the web interface.\nHowever, an administrator with the appropriate privileges can use this command with SET PASSWORD = 'string' to change the password\nfor a user.\nTip\nWhen changing a user’s password, we recommend also specifying MUST_CHANGE_PASSWORD = TRUE to force the user to log into the web\ninterface and change their password before they can log into Snowflake through any other interface (e.g. SnowSQL or another client application).\nAlternatively, use RESET PASSWORD to generate a URL to a web page that the user can access to change their password.\nAn ALTER USER statement does not verify that default objects (DEFAULT_WAREHOUSE, DEFAULT_NAMESPACE,\nand DEFAULT_ROLE) exist. Note that DEFAULT_SECONDARY_ROLES does not accept an object name as the value, but an ALTER\nUSER statement does verify that a supported value is specified.\nYou can set and unset multiple object properties and object/session parameters with a single ALTER statement:\nWhen setting multiple properties/parameters, separate them with blank spaces, commas, or new lines.\nWhen unsetting multiple properties/parameters, they must be separated by a comma. Also, when unsetting a property/parameter,\nspecify only the name; specifying a value for the property/parameter will return an error.\nIf you specify SET DISABLED = TRUE for a user:\nAll queries and other SQL statements currently running or scheduled by the user are aborted and the user cannot initiate additional queries.\nThe user is locked out of Snowflake and cannot log in again.\nIf you only want to abort all running and scheduled queries/statements for a user, use ABORT ALL QUERIES instead.\nIf the user’s TYPE property is SERVICE, the following commands cannot be used:\nALTER USER RESET PASSWORD\nALTER USER SET DISABLE_MFA = TRUE\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-resource-monitor",
    "title": "ALTER RESOURCE MONITOR",
    "description": "Modifies the properties and triggers for an existing resource monitor. Use this command to\nincrease or decrease the credit quota, change the\nscheduling information, or change/replace the triggers for a resource monitor.",
    "syntax": "ALTER RESOURCE MONITOR [ IF EXISTS ] <name> [ SET { [ CREDIT_QUOTA = <num> ]\n                                                    [ FREQUENCY = { MONTHLY | DAILY | WEEKLY | YEARLY | NEVER } ]\n                                                    [ START_TIMESTAMP = { <timestamp> | IMMEDIATELY } ]\n                                                    [ END_TIMESTAMP = <timestamp> ]\n                                                    [ NOTIFY_USERS = ( <user_name> [ , <user_name> , ... ] ) ] } ]\n                                            [ TRIGGERS triggerDefinition [ triggerDefinition ... ] ]\n\ntriggerDefinition ::=\n   ON <threshold> PERCENT DO { SUSPEND | SUSPEND_IMMEDIATE | NOTIFY }",
    "examples": [
        {
            "code": "ALTER RESOURCE MONITOR limiter\n  SET CREDIT_QUOTA=2000\n  TRIGGERS ON 80 PERCENT DO NOTIFY\n           ON 100 PERCENT DO SUSPEND_IMMEDIATE;"
        },
        {
            "code": "ALTER RESOURCE MONITOR limiter\n  SET CREDIT_QUOTA = 2000\n      NOTIFY_USERS = (JDOE, \"Jane Smith\", \"John Doe\")\n  TRIGGERS ON 80 PERCENT DO NOTIFY\n           ON 100 PERCENT DO SUSPEND_IMMEDIATE"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the resource monitor to alter. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "SET   ...",
            "description": "Specifies the number of credits allocated to the resource monitor per frequency interval. When total usage for all warehouses assigned to\nthe monitor reaches this number for the current frequency interval, the resource monitor is considered to be at 100% of quota. If a value is not specified for a resource monitor, the monitor has no quota and will never reach 100% usage within the specified interval. The frequency interval at which the credit usage resets to 0 . If you specify NEVER for the frequency, the credit usage for the warehouse does not reset. The date and time when the resource monitor starts monitoring credit usage for the assigned warehouses. If you specify IMMEDIATELY for the start timestamp, the current timestamp is used. If you specify a date without a time, the current time is used. If you set a time without specifying a time zone, UTC is used as the default time zone. The date and time when the resource monitor suspends the assigned warehouses. Specifies the list of users to receive email notifications on resource monitors. If a user identifier includes spaces or special\ncharacters or is case-sensitive, then the identifier must be enclosed in double quotes (e.g. “Mary Smith”). See Identifier requirements for details. The user identifier, user_name , is the value of the name column from the output of SHOW USERS . Each user listed must have a verified email address. For instructions on verifying email addresses in the web interface, see: For Classic Console: Verifying Your Email Address in the Classic Console . For Snowsight: Verify your email address . Email notifications for non-administrator users do not supersede email notifications for administrators. Any account administrators that\nhave enabled email notifications will continue to receive email notifications. Note The following limitations apply for non-administrator users: Non-administrator users can only receive notifications for warehouse monitors . Non-administrator users are notified by email but can’t see notifications in the Classic Console. Non-administrator users can’t create resource monitors. Non-administrator users can’t assign other users to be notified."
        },
        {
            "name": "TRIGGERS   ...  (aka  actions )",
            "description": "Specifies one or more triggers for the resource monitor. Each trigger definition consists of: ON threshold PERCENT (usage percentage; values larger than 100 are supported) DO SUSPEND | SUSPEND_IMMEDIATE | NOTIFY (action to perform when the threshold is reached). For more details, see CREATE RESOURCE MONITOR ."
        },
        {
            "name": "CREDIT_QUOTA   =   num",
            "description": "Specifies the number of credits allocated to the resource monitor per frequency interval. When total usage for all warehouses assigned to\nthe monitor reaches this number for the current frequency interval, the resource monitor is considered to be at 100% of quota. If a value is not specified for a resource monitor, the monitor has no quota and will never reach 100% usage within the specified interval."
        },
        {
            "name": "FREQUENCY   =   MONTHLY   |   DAILY   |   WEEKLY   |   YEARLY   |   NEVER",
            "description": "The frequency interval at which the credit usage resets to 0 . If you specify NEVER for the frequency, the credit usage for the warehouse does not reset."
        },
        {
            "name": "START_TIMESTAMP   =   timestamp   |   IMMEDIATELY",
            "description": "The date and time when the resource monitor starts monitoring credit usage for the assigned warehouses. If you specify IMMEDIATELY for the start timestamp, the current timestamp is used. If you specify a date without a time, the current time is used. If you set a time without specifying a time zone, UTC is used as the default time zone."
        },
        {
            "name": "END_TIMESTAMP   =   timestamp",
            "description": "The date and time when the resource monitor suspends the assigned warehouses."
        },
        {
            "name": "NOTIFY_USERS   =   (   user_name   [   ,   user_name   ,   ...   ]   )",
            "description": "Specifies the list of users to receive email notifications on resource monitors. If a user identifier includes spaces or special\ncharacters or is case-sensitive, then the identifier must be enclosed in double quotes (e.g. “Mary Smith”). See Identifier requirements for details. The user identifier, user_name , is the value of the name column from the output of SHOW USERS . Each user listed must have a verified email address. For instructions on verifying email addresses in the web interface, see: For Classic Console: Verifying Your Email Address in the Classic Console . For Snowsight: Verify your email address . Email notifications for non-administrator users do not supersede email notifications for administrators. Any account administrators that\nhave enabled email notifications will continue to receive email notifications. Note The following limitations apply for non-administrator users: Non-administrator users can only receive notifications for warehouse monitors . Non-administrator users are notified by email but can’t see notifications in the Classic Console. Non-administrator users can’t create resource monitors. Non-administrator users can’t assign other users to be notified."
        }
    ],
    "usage_notes": "If a SUSPEND or SUSPEND_IMMEDIATE trigger is active for a resource monitor and the trigger threshold has been reached for\nthe specified frequency interval, thereby preventing all assigned warehouses from being started/resumed, you can use this command to\neither increase the credit quota above the trigger threshold or replace the trigger with a new trigger with a higher threshold.\nOnce the credit quota or trigger threshold for the resource monitor has been increased, assigned warehouses can be started or resumed.\nThe TRIGGERS parameter is not additive (i.e. it removes all existing triggers for the resource monitor and replaces them\nwith the specified triggers).\nAs a result, to make additions to the existing triggers, you must specify the new triggers and replicate the existing triggers.\nIf frequency and start_timestamp parameters are set on a resource monitor, the day for the credit usage reset is\ncalculated based on those parameters. The time the credit usage resets to 0 is 12:00 AM UTC regardless of the time specified in\nstart_timestamp.\nIf you specify an end_timestamp, monitoring ends at that specified date and time and all assigned warehouses are suspended\nat that date and time even if the credit quota has not been reached.\nWhen this occurs, a notification is sent that states the resource monitor has reached a percentage of its quota and has triggered a\nsuspend immediate action. The percentage of the quota reflects the number of credits used in the current interval up to the end date\nand might not be a threshold you specified.\nIf there are non-administrator users in the notification list, the following notes apply:\nIf any user in the notification list does not have a verified email,\nthe SQL statement fails.\nIf any user in the notification list changes their email address and does not verify the new email address, the\nnotification silently fails.\nThe notification list is limited to a maximum number of 5 non-administrator users.\nAccount administrators can view the notification list of non-administrator users in the output of\nSHOW RESOURCE MONITORS in the notify_user column.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-application-package-release-directive",
    "title": "ALTER APPLICATION PACKAGE … RELEASE DIRECTIVE",
    "description": "Modifies the properties of an existing application package in the Native Apps Framework. Use this command to add a new version or patch.",
    "syntax": "ALTER APPLICATION PACKAGE <name>\n  MODIFY RELEASE DIRECTIVE <release_directive>\n  VERSION = <version_identifier>\n  PATCH = <patch_num>\n  [ UPGRADE_AFTER = '<timestamp>' ]\n\nALTER APPLICATION PACKAGE <name>\n  SET DEFAULT RELEASE DIRECTIVE\n  VERSION = <version_identifier>\n  PATCH = <patch_num>\n  [ UPGRADE_AFTER = '<timestamp>' ]\n\nALTER APPLICATION PACKAGE <name>\n  SET RELEASE DIRECTIVE <release_directive>\n  ACCOUNTS = ( <organization_name>.<account_name> [ , <organization_name>.<account_name> , ... ] )\n  VERSION = <version_identifier>\n  PATCH = <patch_num>\n  [ UPGRADE_AFTER = '<timestamp>' ]\n\nALTER APPLICATION PACKAGE <name> UNSET RELEASE DIRECTIVE <release_directive>",
    "examples": [
        {
            "code": "ALTER APPLICATION PACKAGE <name>\n  MODIFY RELEASE DIRECTIVE <release_directive>\n  VERSION = <version_identifier>\n  PATCH = <patch_num>\n  [ UPGRADE_AFTER = '<timestamp>' ]\n\nALTER APPLICATION PACKAGE <name>\n  SET DEFAULT RELEASE DIRECTIVE\n  VERSION = <version_identifier>\n  PATCH = <patch_num>\n  [ UPGRADE_AFTER = '<timestamp>' ]\n\nALTER APPLICATION PACKAGE <name>\n  SET RELEASE DIRECTIVE <release_directive>\n  ACCOUNTS = ( <organization_name>.<account_name> [ , <organization_name>.<account_name> , ... ] )\n  VERSION = <version_identifier>\n  PATCH = <patch_num>\n  [ UPGRADE_AFTER = '<timestamp>' ]\n\nALTER APPLICATION PACKAGE <name> UNSET RELEASE DIRECTIVE <release_directive>"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the application package. If the identifier contains spaces, special characters, or mixed-case characters, the entire string must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "MODIFY   RELEASE   DIRECTIVE   release_directive   .   VERSION   =   version_identifier   .   PATCH   =   patch_num",
            "description": "Modifies the version and patch level of the specified custom release directive."
        },
        {
            "name": "SET",
            "description": "Specifies one (or more) properties to set for the application package (separated by blank spaces, commas, or new lines). For more details\nabout the properties you can set, see CREATE APPLICATION . Sets the version and patch level of the application package that should be installed for consumers by default. Creates a custom release directive for the specified accounts. Use the ACCOUNTS clause to specify the list of accounts to which this release directive applies. Use the VERSION and PATCH clauses to specify the version identifier and patch number to be installed for these accounts."
        },
        {
            "name": "UPGRADE_AFTER   =   ' timestamp '",
            "description": "Specifies the date and time when the automated upgrade process begins. Consumers can manually\nupgrade an app to a new version or patch before this date. This value can be any valid date and time format."
        },
        {
            "name": "UNSET",
            "description": "Specifies one (or more) properties and/or session parameters to unset for the application package, which resets them to the defaults. Removes the specified custom release directive from the application package."
        },
        {
            "name": "DEFAULT   RELEASE   DIRECTIVE   VERSION   =   version_identifier   PATCH   =   patch_num",
            "description": "Sets the version and patch level of the application package that should be installed for consumers by default."
        },
        {
            "name": "RELEASE   DIRECTIVE   release_directive   .   ACCOUNTS   =   (   organization_name . account_name   [   ,   organization_name . account_name   ,   ...   ]   )   .   VERSION   =   version_identifier   .   PATCH   =   patch_num",
            "description": "Creates a custom release directive for the specified accounts. Use the ACCOUNTS clause to specify the list of accounts to which this release directive applies. Use the VERSION and PATCH clauses to specify the version identifier and patch number to be installed for these accounts."
        },
        {
            "name": "UNSET   RELEASE   DIRECTIVE   release_directive",
            "description": "Removes the specified custom release directive from the application package."
        }
    ],
    "usage_notes": "Modifying the release directive requires the OWNERSHIP privilege on the application or the global MANAGE VERSIONS privilege.\nIf you do not specify the values for the optional properties, the command uses the values specified in the application manifest file.\nIf you specify values for the properties in the command and in the application manifest file, the values specified in the command take\nprecedence."
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show",
    "title": "SHOW",
    "description": "Lists the existing objects for the specified object type. The output includes metadata for the objects, including:"
},
{
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-database",
    "title": "ALTER DATABASE",
    "description": "Modifies the properties for an existing database.",
    "syntax": "ALTER DATABASE [ IF EXISTS ] <name> RENAME TO <new_db_name>\n\nALTER DATABASE [ IF EXISTS ] <name> SWAP WITH <target_db_name>\n\nALTER DATABASE [ IF EXISTS ] <name> SET [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n                                        [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n                                        [ EXTERNAL_VOLUME = <external_volume_name> ]\n                                        [ CATALOG = <catalog_integration_name> ]\n                                        [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n                                        [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n                                        [ DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU = '<compute_pool_name>' ]\n                                        [ DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU = '<compute_pool_name>' ]\n                                        [ LOG_LEVEL = '<log_level>' ]\n                                        [ TRACE_LEVEL = '<trace_level>' ]\n                                        [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n                                        [ EVENT_TABLE = <event_table_name> ]\n                                        [ COMMENT = '<string_literal>' ]\n                                        [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>' ]\n                                        [ REPLICABLE_WITH_FAILOVER_GROUPS = { 'YES' | 'NO' } ]\n                                        [ BASE_LOCATION_PREFIX = '<string>' ]\n                                        [ DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE = <warehouse_name> ]\n                                        [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nALTER DATABASE <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER DATABASE <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER DATABASE [ IF EXISTS ] <name> UNSET { DATA_RETENTION_TIME_IN_DAYS         |\n                                            MAX_DATA_EXTENSION_TIME_IN_DAYS     |\n                                            EXTERNAL_VOLUME                     |\n                                            CATALOG                             |\n                                            DEFAULT_DDL_COLLATION               |\n                                            DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU   |\n                                            DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU   |\n                                            STORAGE_SERIALIZATION_POLICY        |\n                                            EVENT_TABLE = <event_table_name>    |\n                                            COMMENT                             |\n                                            CATALOG_SYNC                        |\n                                            REPLICABLE_WITH_FAILOVER_GROUPS     |\n                                            BASE_LOCATION_PREFIX                |\n                                            DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE|\n                                            CONTACT <purpose>\n                                          }\n                                          [ , ... ]",
    "examples": [
        {
            "code": "ALTER DATABASE [ IF EXISTS ] <name> RENAME TO <new_db_name>\n\nALTER DATABASE [ IF EXISTS ] <name> SWAP WITH <target_db_name>\n\nALTER DATABASE [ IF EXISTS ] <name> SET [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n                                        [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n                                        [ EXTERNAL_VOLUME = <external_volume_name> ]\n                                        [ CATALOG = <catalog_integration_name> ]\n                                        [ REPLACE_INVALID_CHARACTERS = { TRUE | FALSE } ]\n                                        [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n                                        [ DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU = '<compute_pool_name>' ]\n                                        [ DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU = '<compute_pool_name>' ]\n                                        [ LOG_LEVEL = '<log_level>' ]\n                                        [ TRACE_LEVEL = '<trace_level>' ]\n                                        [ STORAGE_SERIALIZATION_POLICY = { COMPATIBLE | OPTIMIZED } ]\n                                        [ EVENT_TABLE = <event_table_name> ]\n                                        [ COMMENT = '<string_literal>' ]\n                                        [ CATALOG_SYNC = '<snowflake_open_catalog_integration_name>' ]\n                                        [ REPLICABLE_WITH_FAILOVER_GROUPS = { 'YES' | 'NO' } ]\n                                        [ BASE_LOCATION_PREFIX = '<string>' ]\n                                        [ DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE = <warehouse_name> ]\n                                        [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nALTER DATABASE <name> SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n\nALTER DATABASE <name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n\nALTER DATABASE [ IF EXISTS ] <name> UNSET { DATA_RETENTION_TIME_IN_DAYS         |\n                                            MAX_DATA_EXTENSION_TIME_IN_DAYS     |\n                                            EXTERNAL_VOLUME                     |\n                                            CATALOG                             |\n                                            DEFAULT_DDL_COLLATION               |\n                                            DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU   |\n                                            DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU   |\n                                            STORAGE_SERIALIZATION_POLICY        |\n                                            EVENT_TABLE = <event_table_name>    |\n                                            COMMENT                             |\n                                            CATALOG_SYNC                        |\n                                            REPLICABLE_WITH_FAILOVER_GROUPS     |\n                                            BASE_LOCATION_PREFIX                |\n                                            DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE|\n                                            CONTACT <purpose>\n                                          }\n                                          [ , ... ]"
        },
        {
            "code": "ALTER DATABASE <name> ENABLE REPLICATION TO ACCOUNTS <account_identifier> [ , <account_identifier> ... ] [ IGNORE EDITION CHECK ]\n\nALTER DATABASE <name> DISABLE REPLICATION [ TO ACCOUNTS <account_identifier> [ , <account_identifier> ... ] ]\n\nALTER DATABASE <name> REFRESH"
        },
        {
            "code": "ALTER DATABASE <name> ENABLE FAILOVER TO ACCOUNTS <account_identifier> [ , <account_identifier> ... ]\n\nALTER DATABASE <name> DISABLE FAILOVER [ TO ACCOUNTS <account_identifier> [ , <account_identifier> ... ] ]\n\nALTER DATABASE <name> PRIMARY"
        },
        {
            "code": "ALTER DATABASE IF EXISTS db1 RENAME TO db2;"
        }
    ],
    "parameters": [
        {
            "name": "name",
            "description": "Specifies the identifier for the database to alter. If the identifier contains spaces, special characters, or mixed-case characters, the entire\nstring must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
        },
        {
            "name": "RENAME   TO   new_db_name",
            "description": "Specifies the new identifier for the database; must be unique for your account. For more information, see Identifier requirements . When an object is renamed, other objects that reference it must be updated with the new name."
        },
        {
            "name": "SWAP   WITH   target_db_name",
            "description": "Swaps all objects (schemas, tables, views, etc.) and metadata, including identifiers, between the two specified databases. Also swaps all access\ncontrol privileges granted on the databases and objects they contain. SWAP WITH essentially performs a rename of both databases as a\nsingle operation."
        },
        {
            "name": "SET   ...",
            "description": "Specifies one (or more) properties to set for the database (separated by blank spaces, commas, or new lines): Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the\ndefault Time Travel retention time for all schemas created in the database. The value you can specify depends on the Snowflake Edition you are using: Standard Edition: 0 or 1 Enterprise Edition (or higher): 0 to 90 Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the database\nto prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS . Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME . Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG . Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE Specifies a default collation specification for: Any new columns added to existing tables in the database. All columns in new tables added to the database. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION . CPU compute pool name that overrides the default CPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks . GPU compute pool name that overrides the default GPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks . Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing . Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting the trace level, see Setting levels for logging, metrics, and tracing . Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects . Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts . Specifies the fully-qualified name of the event table that should collect telemetry data from objects in the database, such as\nprocedures and UDFs. For more information, see Associate an event table with an object . Associating an event table with a database is available in Enterprise Edition or higher . Adds a comment or overwrites an existing comment for the database. Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the database with an external catalog in your Snowflake Open Catalog account. For more\ninformation about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value Specifies if all the schemas in the database are eligible for replication.\nYou can set this property to NO for a database, and then allow some schemas\nto be replicated by setting the equivalent property to YES for those schemas. For more information about this parameter, see Schema-level replication for failover groups . Default: 'YES' Specifies the default warehouse to be used when creating a notebook using SQL. Specifies a prefix for Snowflake to use in the write path for Snowflake-managed Apache Iceberg™ tables.\nFor more information,\nsee data and metadata directories for Iceberg tables and BASE_LOCATION_PREFIX in the Snowflake Parameters topic. Default: No value"
        },
        {
            "name": "UNSET   ...",
            "description": "Specifies one (or more) properties and/or parameters to unset for the database, which resets them to the defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS EXTERNAL_VOLUME CATALOG DEFAULT_DDL_COLLATION TAG tag_name [ , tag_name ... ] DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU STORAGE_SERIALIZATION_POLICY EVENT_TABLE = event_table_name COMMENT CATALOG_SYNC REPLICABLE_WITH_FAILOVER_GROUPS BASE_LOCATION_PREFIX DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE CONTACT purpose You can reset multiple properties/parameters with a single ALTER statement; however, each property/parameter must be separated by a\ncomma. When resetting a property/parameter, specify only the name; specifying a value for the property will return an error."
        },
        {
            "name": "DATA_RETENTION_TIME_IN_DAYS   =   num",
            "description": "Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the\ndefault Time Travel retention time for all schemas created in the database. The value you can specify depends on the Snowflake Edition you are using: Standard Edition: 0 or 1 Enterprise Edition (or higher): 0 to 90"
        },
        {
            "name": "MAX_DATA_EXTENSION_TIME_IN_DAYS   =   integer",
            "description": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the database\nto prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS ."
        },
        {
            "name": "EXTERNAL_VOLUME   =   external_volume_name",
            "description": "Object parameter that specifies the default external volume to use for Apache Iceberg™ tables . For more information about this parameter, see EXTERNAL_VOLUME ."
        },
        {
            "name": "CATALOG   =   catalog_integration_name",
            "description": "Object parameter that specifies the default catalog integration to use for Apache Iceberg™ tables . For more information about this parameter, see CATALOG ."
        },
        {
            "name": "REPLACE_INVALID_CHARACTERS   =   {   TRUE   |   FALSE   }",
            "description": "Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table .\nYou can only set this parameter for tables that use an external Iceberg catalog. TRUE replaces invalid UTF-8 characters with the Unicode replacement character. FALSE leaves invalid UTF-8 characters unchanged. Snowflake returns a user error message when it encounters invalid UTF-8\ncharacters in a Parquet data file. Default: FALSE"
        },
        {
            "name": "DEFAULT_DDL_COLLATION   =   ' collation_specification '",
            "description": "Specifies a default collation specification for: Any new columns added to existing tables in the database. All columns in new tables added to the database. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION ."
        },
        {
            "name": "DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU   =   compute_pool_name",
            "description": "CPU compute pool name that overrides the default CPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks ."
        },
        {
            "name": "DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU   =   compute_pool_name",
            "description": "GPU compute pool name that overrides the default GPU compute pool Snowflake provisioned in your account for running Notebooks. For more information, see Compute pools for Notebooks ."
        },
        {
            "name": "LOG_LEVEL   =   ' log_level '",
            "description": "Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at\nthe specified level (and at more severe levels) are ingested. For more information about levels, see LOG_LEVEL . For information about setting the log level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "TRACE_LEVEL   =   ' trace_level '",
            "description": "Controls how trace events are ingested into the event table. For information about levels, see TRACE_LEVEL . For information about setting the trace level, see Setting levels for logging, metrics, and tracing ."
        },
        {
            "name": "STORAGE_SERIALIZATION_POLICY   =   {   COMPATIBLE   |   OPTIMIZED   }",
            "description": "Specifies the storage serialization policy for Apache Iceberg™ tables that use Snowflake as the catalog. COMPATIBLE : Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED : Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. Default: OPTIMIZED"
        },
        {
            "name": "TAG   tag_name   =   ' tag_value '   [   ,   tag_name   =   ' tag_value '   ,   ...   ]",
            "description": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects ."
        },
        {
            "name": "CONTACT   (   purpose   =   contact   [   ,   purpose   =   contact   ...   ]   )",
            "description": "Preview Feature — Open Available to all accounts. Associate the existing object with one or more contacts ."
        },
        {
            "name": "EVENT_TABLE   =   event_table_name",
            "description": "Specifies the fully-qualified name of the event table that should collect telemetry data from objects in the database, such as\nprocedures and UDFs. For more information, see Associate an event table with an object . Associating an event table with a database is available in Enterprise Edition or higher ."
        },
        {
            "name": "COMMENT   =   ' string_literal '",
            "description": "Adds a comment or overwrites an existing comment for the database."
        },
        {
            "name": "CATALOG_SYNC   =   ' snowflake_open_catalog_integration_name '",
            "description": "Specifies the name of a catalog integration configured for Snowflake Open Catalog .\nIf specified, Snowflake syncs Snowflake-managed Apache Iceberg™ tables in the database with an external catalog in your Snowflake Open Catalog account. For more\ninformation about syncing Snowflake-managed Iceberg tables with Open Catalog, see Sync a Snowflake-managed table with Snowflake Open Catalog . For more information about this parameter, see CATALOG_SYNC . Default: No value"
        },
        {
            "name": "REPLICABLE_WITH_FAILOVER_GROUPS   =   {   'YES'   |   'NO'   }",
            "description": "Specifies if all the schemas in the database are eligible for replication.\nYou can set this property to NO for a database, and then allow some schemas\nto be replicated by setting the equivalent property to YES for those schemas. For more information about this parameter, see Schema-level replication for failover groups . Default: 'YES'"
        },
        {
            "name": "DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE",
            "description": "Specifies the default warehouse to be used when creating a notebook using SQL."
        },
        {
            "name": "BASE_LOCATION_PREFIX   =   ' string '",
            "description": "Specifies a prefix for Snowflake to use in the write path for Snowflake-managed Apache Iceberg™ tables.\nFor more information,\nsee data and metadata directories for Iceberg tables and BASE_LOCATION_PREFIX in the Snowflake Parameters topic. Default: No value"
        },
        {
            "name": "ENABLE   REPLICATION   TO   ACCOUNTS   account_identifier   [   ,   account_identifier   ...   ]",
            "description": "Promotes a local database to serve as a primary database for replication. A primary database can be replicated in one or more accounts,\nallowing users in those accounts to query objects in each secondary (i.e. replica) database. Alternatively, modify an existing primary database to add to or remove from the list of accounts that can store a replica of the database. Provide a comma-separated list of accounts in your organization that can store a replica of this database. Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes . Allows replicating data to accounts on lower editions in either of the following scenarios: The primary database is in a Business Critical (or higher) account but one or more of the accounts approved for replication are on lower\neditions. Business Critical Edition is intended for Snowflake accounts with extremely sensitive data. The primary database is in a Business Critical (or higher) account and a signed business associate agreement is in place to store PHI data\nin the account per HIPAA and HITRUST regulations, but no such agreement is in place for one or more of the\naccounts approved for replication, regardless if they are Business Critical (or higher) accounts. Both scenarios are prohibited by default in an effort to help prevent account administrators for Business Critical (or higher) accounts from\ninadvertently replicating sensitive data to accounts on lower editions."
        },
        {
            "name": "DISABLE   REPLICATION   [   TO   ACCOUNTS   account_identifier   [   ,   account_identifier   ...   ]   ]",
            "description": "Disables replication for this primary database, meaning no replica of this database (i.e. secondary database) in another account can be refreshed.\nAny secondary databases remain linked to the primary database, but requests to refresh a secondary database are denied. Note that disabling replication for a primary database does not prevent it from being replicated to the same account; therefore, the database\ncontinues to be listed in the SHOW REPLICATION DATABASES output. Optionally provide a comma-separated list of accounts in your organization to disable replication for this database only in the specified\naccounts. Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes ."
        },
        {
            "name": "REFRESH",
            "description": "Refreshes a secondary database from a snapshot of its primary database. A snapshot includes changes to the objects and data."
        },
        {
            "name": "ENABLE   FAILOVER   TO   ACCOUNTS   account_identifier   [   ,   account_identifier   ...   ]",
            "description": "Specifies a comma-separated list of accounts in your organization where a replica of this primary database can be promoted to serve as the\nprimary database. Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes ."
        },
        {
            "name": "DISABLE   FAILOVER   [   TO   ACCOUNTS   account_identifier   [   ,   account_identifier   ...   ]   ]",
            "description": "Disables failover for this primary database, meaning no replica of this database (i.e. secondary database) can be promoted to serve as the\nprimary database. Optionally provide a comma-separated list of accounts in your organization to disable failover for this database only in the specified\naccounts. Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes ."
        },
        {
            "name": "PRIMARY",
            "description": "Promotes the specified secondary (replica) database to serve as the primary database. When promoted, the database becomes writeable. At the same\ntime, the previous primary database becomes a read-only secondary database."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes ."
        },
        {
            "name": "IGNORE   EDITION   CHECK",
            "description": "Allows replicating data to accounts on lower editions in either of the following scenarios: The primary database is in a Business Critical (or higher) account but one or more of the accounts approved for replication are on lower\neditions. Business Critical Edition is intended for Snowflake accounts with extremely sensitive data. The primary database is in a Business Critical (or higher) account and a signed business associate agreement is in place to store PHI data\nin the account per HIPAA and HITRUST regulations, but no such agreement is in place for one or more of the\naccounts approved for replication, regardless if they are Business Critical (or higher) accounts. Both scenarios are prohibited by default in an effort to help prevent account administrators for Business Critical (or higher) accounts from\ninadvertently replicating sensitive data to accounts on lower editions."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes ."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes ."
        },
        {
            "name": "account_identifier",
            "description": "Unique identifier of the account. The preferred identifier is organization_name . account_name . To view the list of accounts\nenabled for replication in your organization, query SHOW REPLICATION ACCOUNTS . Though the legacy account locator can also be used as the account identifier, its use is discouraged as it may not work in the future.\nFor more information about using the account locator as an account identifier, see Database Replication and Failover Usage Notes ."
        }
    ],
    "usage_notes": "To rename a database, the role used to perform the operation must have the CREATE DATABASE global privilege and OWNERSHIP privilege on\nthe database.\nTo swap two databases, the role used to perform the operation must have OWNERSHIP privileges on both databases.\nTo update a comment, the role used to perform the operation must be granted or inherit the MODIFY privilege on the database.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
}
]