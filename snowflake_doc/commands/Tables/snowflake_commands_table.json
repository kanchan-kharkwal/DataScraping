[
  {
    "category": "SHOW OBJECTS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-objects",
    "details": [
      {
        "heading": "SHOW OBJECTS",
        "description": "\nLists the tables and views for which you have access privileges. This command can be used to list the tables and views for a specified\ndatabase or schema (or the current database/schema for the session), or your entire account."
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] OBJECTS [ LIKE '<pattern>' ]\n                       [ IN\n                             {\n                               ACCOUNT                                         |\n\n                               DATABASE                                        |\n                               DATABASE <database_name>                        |\n\n                               SCHEMA                                          |\n                               SCHEMA <schema_name>                            |\n                               <schema_name>\n\n                               APPLICATION <application_name>                  |\n                               APPLICATION PACKAGE <application_package_name>  |\n                             }\n                       ]\n                       [ STARTS WITH '<name_string>' ]\n                       [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Returns only a subset of the output columns: created_on name kind database_name schema_name Default: No value (all columns are included in the output)."
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the object was created."
              ],
              [
                "name",
                "Name of the object."
              ],
              [
                "database_name",
                "Database in which the object is stored."
              ],
              [
                "schema_name",
                "Schema in which the object is stored."
              ],
              [
                "kind",
                "Object type: TABLE, VIEW."
              ],
              [
                "comment",
                "Comment for the object."
              ],
              [
                "cluster_by",
                "Column(s) defined as clustering key(s) for the object."
              ],
              [
                "rows",
                "Number of rows in the object."
              ],
              [
                "bytes",
                "Number of bytes that will be scanned if the entire object is scanned in a query. Note that this number may be different than the number of actual physical bytes (i.e. bytes stored on-disk) for the object."
              ],
              [
                "owner",
                "Role that owns the object."
              ],
              [
                "retention_time",
                "Number of days that modified and deleted data is retained for Time Travel."
              ],
              [
                "is_hybrid",
                "Y if the object is a hybrid table; N otherwise."
              ],
              [
                "is_dynamic",
                "Y if the object is a dynamic table; N otherwise."
              ],
              [
                "is_iceberg",
                "Y if the object is an Apache Icebergâ„¢ table; N otherwise."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nFor materialized views, this returns VIEW, not MATERIALIZED VIEW.\nFor materialized views, this returns VIEW, not MATERIALIZED VIEW.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nShow all tables and views whose names start with HT_ that you have privileges to see in the current database:\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW OBJECTS IN DATABASE STARTS WITH 'HT_';",
          "+-------------------------------+------------------------+---------------+----------------+-------+---------+------------+---------+-----------+--------------+----------------+-----------------+--------+-----------+------------+\n| created_on                    | name                   | database_name | schema_name    | kind  | comment | cluster_by |    rows |     bytes | owner        | retention_time | owner_role_type | budget | is_hybrid | is_dynamic |\n|-------------------------------+------------------------+---------------+----------------+-------+---------+------------+---------+-----------+--------------+----------------+-----------------+--------+-----------+------------|\n| 2024-05-13 19:08:41.946 -0700 | HT_PRECIP              | HYBRID1_DB    | HYBRID1_SCHEMA | TABLE |         |            |       0 |         0 | HYBRID1_ROLE | 1              | ROLE            | NULL   | Y         | N          |\n| 2024-08-23 11:44:13.694 -0700 | HT_SENSOR_DATA_DEVICE1 | HYBRID1_DB    | HYBRID1_SCHEMA | TABLE |         |            | 2678400 | 133920000 | HYBRID1_ROLE | 1              | ROLE            | NULL   | Y         | N          |\n| 2024-05-13 16:37:29.217 -0700 | HT_WEATHER             | HYBRID1_DB    | HYBRID1_SCHEMA | TABLE |         |            |      55 |      2985 | HYBRID1_ROLE | 1              | ROLE            | NULL   | Y         | N          |\n| 2024-07-18 12:17:27.381 -0700 | HT_WEATHER             | HYBRID1_DB    | PUBLIC         | TABLE |         |            |      55 |      3040 | ACCOUNTADMIN | 1              | ROLE            | NULL   | Y         | N          |\n+-------------------------------+------------------------+---------------+----------------+-------+---------+------------+---------+-----------+--------------+----------------+-----------------+--------+-----------+------------+"
        ]
      }
    ]
  },
  {
    "category": "CREATE TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-table",
    "details": [
      {
        "heading": "CREATE TABLE",
        "description": "\nCreates a new table in the current/specified schema, replaces an existing table, or alters an existing table. A table can have multiple\ncolumns, with each column definition consisting of a name, data type, and optionally whether the column:\nRequires a value (NOT NULL).\nHas a default value.\nHas any referential integrity constraints (primary key, foreign key, etc.).\nRequires a value (NOT NULL).\nHas a default value.\nHas any referential integrity constraints (primary key, foreign key, etc.).\nIn addition, this command supports the following variants:\nCREATE OR ALTER TABLE (creates a table if it doesnt exist, or alters it according to the table definition)\nCREATE TABLE  AS SELECT (creates a populated table; also referred to as CTAS)\nCREATE TABLE  USING TEMPLATE (creates a table with the column definitions derived from a set of staged files)\nCREATE TABLE  LIKE (creates an empty copy of an existing table)\nCREATE TABLE  CLONE (creates a clone of an existing table)\nCREATE OR ALTER TABLE (creates a table if it doesnt exist, or alters it according to the table definition)\nCREATE TABLE  AS SELECT (creates a populated table; also referred to as CTAS)\nCREATE TABLE  USING TEMPLATE (creates a table with the column definitions derived from a set of staged files)\nCREATE TABLE  LIKE (creates an empty copy of an existing table)\nCREATE TABLE  CLONE (creates a clone of an existing table)",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER TABLE , DROP TABLE , SHOW TABLES , DESCRIBE TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:\ncol_name is an object identifier. It must follow the requirements for Snowflake identifiers.\ncol_type is one of the Snowflake data types, such as\nNUMBER or VARCHAR.\nFor additional inline constraint details, see CREATE | ALTER TABLE  CONSTRAINT.\nFor additional out-of-line constraint details, see CREATE | ALTER TABLE  CONSTRAINT.\nNote\nDo not specify copy options using the CREATE STAGE, ALTER STAGE, CREATE TABLE, or ALTER TABLE commands. We recommend that you use the COPY INTO <table> command to specify copy options.",
        "syntax": [
          "CREATE [ OR REPLACE ]\n    [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE | TRANSIENT } ]\n  TABLE [ IF NOT EXISTS ] <table_name>\n\n  (\n    -- Column definition\n    <col_name> <col_type>\n      [ inlineConstraint ]\n      [ NOT NULL ]\n      [ COLLATE '<collation_specification>' ]\n      [\n        {\n          DEFAULT <expr>\n          | { AUTOINCREMENT | IDENTITY }\n            [\n              {\n                ( <start_num> , <step_num> )\n                | START <num> INCREMENT <num>\n              }\n            ]\n            [ { ORDER | NOORDER } ]\n        }\n      ]\n      [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n      [ [ WITH ] PROJECTION POLICY <policy_name> ]\n      [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n    -- Out-of-line constraints\n    [ , outoflineConstraint [ ... ] ]\n  )\n\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE } ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] JOIN POLICY <policy_name> [ ALLOWED JOIN KEYS ( <col_name> [ , ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]",
          "inlineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE\n    | PRIMARY KEY\n    | [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ]\n  }\n  [ <constraint_properties> ]",
          "outoflineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n      REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n  }\n  [ <constraint_properties> ]\n  [ COMMENT '<string_literal>' ]"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE OR ALTER TABLE",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nCreates a table if it doesnt exist, or alters it according to the table definition. The CREATE OR ALTER TABLE syntax follows the\nrules of a CREATE TABLE statement and has the same limitations as an ALTER TABLE statement. If the table is transformed, existing\ndata in the table is preserved when possible. If a column must be dropped, data loss might occur.\nThe following changes are supported when altering a table:\nChange table properties and parameters. For example, ENABLE_SCHEMA_EVOLUTION, DATA_RETENTION_TIME_IN_DAYS, or CLUSTER BY.\nChange column data type, default value, nullability, comment, or autoincrement.\nAdd new columns to the end of the column list.\nDrop columns.\nAdd, drop, or modify inline or out-of-line constraints.\nAdd, drop, or modify clustering keys.\nChange table properties and parameters. For example, ENABLE_SCHEMA_EVOLUTION, DATA_RETENTION_TIME_IN_DAYS, or CLUSTER BY.\nChange column data type, default value, nullability, comment, or autoincrement.\nAdd new columns to the end of the column list.\nDrop columns.\nAdd, drop, or modify inline or out-of-line constraints.\nAdd, drop, or modify clustering keys.\nFor more information, see CREATE OR ALTER TABLE usage notes.",
        "syntax": [
          "CREATE OR ALTER\n    [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | TRANSIENT } ]\n  TABLE <table_name> (\n    -- Column definition\n    <col_name> <col_type>\n      [ inlineConstraint ]\n      [ NOT NULL ]\n      [ COLLATE '<collation_specification>' ]\n      [\n        {\n          DEFAULT <expr>\n          | { AUTOINCREMENT | IDENTITY }\n            [\n              {\n                ( <start_num> , <step_num> )\n                | START <num> INCREMENT <num>\n              }\n            ]\n            [ { ORDER | NOORDER } ]\n        }\n      ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n    -- Out-of-line constraints\n    [ , outoflineConstraint [ ... ] ]\n  )\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE } ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COMMENT = '<string_literal>' ]"
        ]
      },
      {
        "heading": "CREATE TABLE  AS SELECT (also referred to as CTAS)",
        "description": "\nCreates a new table populated with the data returned by a query:\nA masking policy can be applied to a column in a CTAS statement. Specify the masking policy after the column data type. Similarly, a\nrow access policy can be applied to the table. For example:\nNote\nIn a CTAS statement, the COPY GRANTS clause is valid only when combined with the OR REPLACE clause. COPY GRANTS copies\npermissions from the table being replaced with CREATE OR REPLACE (if it already exists), not from the source\ntable(s) being queried in the SELECT statement. CTAS with COPY GRANTS allows you to overwrite a table with a new\nset of data while keeping existing grants on that table.\nFor more details about COPY GRANTS, see COPY GRANTS in this document.",
        "syntax": [
          "CREATE [ OR REPLACE ] TABLE <table_name> [ ( <col_name> [ <col_type> ] , <col_name> [ <col_type> ] , ... ) ]\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ COPY GRANTS ]\n  [ ... ]\n  AS <query>",
          "CREATE TABLE <table_name> ( <col1> <data_type> [ WITH ] MASKING POLICY <policy_name> [ , ... ] )\n  ...\n  [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col1> [ , ... ] )\n  [ ... ]\n  AS <query>"
        ]
      },
      {
        "heading": "CREATE TABLE  USING TEMPLATE",
        "description": "\nCreates a new table with the column definitions derived from a set of staged files using the INFER_SCHEMA function.\nThis feature supports Apache Parquet, Apache Avro, ORC, JSON, and CSV files.\nNote\nIf the statement is replacing an existing table of the same name, then the grants are copied from the table\nbeing replaced. If there is no existing table of that name, then the grants are copied from the source table\nbeing cloned.\nFor more details about COPY GRANTS, see COPY GRANTS in this document.",
        "syntax": [
          "CREATE [ OR REPLACE ] TABLE <table_name>\n  [ COPY GRANTS ]\n  USING TEMPLATE <query>\n  [ ... ]"
        ]
      },
      {
        "heading": "CREATE TABLE  LIKE",
        "description": "\nCreates a new table with the same column definitions as an existing table, but without copying data from the existing table. Column\nnames, types, defaults, and constraints are copied to the new table:\nFor more details about COPY GRANTS, see COPY GRANTS in this document.\nNote\nCREATE TABLE  LIKE for a table with an auto-increment sequence accessed through a data share is currently not\nsupported.",
        "syntax": [
          "CREATE [ OR REPLACE ] TABLE <table_name> LIKE <source_table>\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ COPY GRANTS ]\n  [ ... ]"
        ]
      },
      {
        "heading": "CREATE TABLE  CLONE",
        "description": "\nCreates a new table with the same column definitions and containing all the existing data from the source table, without actually\ncopying the data. This variant can also be used to clone a table at a specific time/point in the past (using\nTime Travel):\nNote\nIf the statement is replacing an existing table of the same name,\nthen the grants are copied from the table being replaced.\nIf there is no existing table of that name, then the grants are\ncopied from the source table being cloned.\nIf you directly clone a table, any streams on that table are not cloned.\nIf you clone a schema including tables with streams, then the streams are also cloned.\nIf the statement is replacing an existing table of the same name,\nthen the grants are copied from the table being replaced.\nIf there is no existing table of that name, then the grants are\ncopied from the source table being cloned.\nIf you directly clone a table, any streams on that table are not cloned.\nIf you clone a schema including tables with streams, then the streams are also cloned.\nFor more details about COPY GRANTS, see COPY GRANTS in this document.\nFor more details about cloning, see CREATE <object>  CLONE.\nFor details about cloning dynamic tables to tables, see Clone a dynamic table to a new table.",
        "syntax": [
          "CREATE [ OR REPLACE ]\n    [ {\n          [ { LOCAL | GLOBAL } ] TEMP [ READ ONLY ] |\n          TEMPORARY [ READ ONLY ] |\n          VOLATILE |\n          TRANSIENT\n    } ]\n  TABLE <name> CLONE <source_table>\n    [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n    [ COPY GRANTS ]\n    [ ... ]"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier (i.e. name) for the table; must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements."
          },
          {
            "term": "col_name",
            "definition": "Specifies the column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. For more details, see Identifier requirements and Reserved & limited keywords. Note In addition to the standard reserved keywords, the following keywords cannot be used as column identifiers because they are\nreserved for ANSI-standard context functions: CURRENT_DATE CURRENT_ROLE CURRENT_TIME CURRENT_TIMESTAMP CURRENT_USER For the list of reserved keywords, see Reserved & limited keywords."
          },
          {
            "term": "col_type",
            "definition": "Specifies the data type for the column. For details about the data types that can be specified for table columns, see SQL data types reference."
          },
          {
            "term": "query",
            "definition": "Required for CTAS and USING TEMPLATE. For CTAS, specifies the SELECT statement that populates the table. This query must be\nspecified last in the CTAS statement, regardless of the other parameters that you include. For CREATE TABLE  USING TEMPLATE, specifies the subquery that calls the INFER_SCHEMA function and\nformats the output as an array. Alternatively, USING TEMPLATE accepts the INFER_SCHEMA output as a string\nliteral or variable."
          },
          {
            "term": "source_table",
            "definition": "Required for LIKE and CLONE. For CREATE TABLE  LIKE, specifies the table from which properties and column definitions are copied. For CREATE TABLE  CLONE, specifies the table to use as the source for the clone."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "{ [ { LOCAL | GLOBAL } ] TEMP [ READ ONLY] | . TEMPORARY [ READ ONLY] | . VOLATILE | . TRANSIENT }",
            "definition": "Specifies that the table persists only for the duration of the session that you created it in. A\ntemporary table and all its contents are dropped at the end of the session. The synonyms and abbreviations for TEMPORARY (e.g. GLOBAL TEMPORARY) are provided for compatibility with other databases\n(e.g. to prevent errors when migrating CREATE TABLE statements). Tables created with any of these keywords appear and behave identically\nto a table created with the TEMPORARY keyword. Default: No value. If a table is not declared as TEMPORARY or TRANSIENT, the table is permanent. If you want to avoid unexpected conflicts, avoid naming temporary tables after tables that already exist in the schema. If you created a temporary table with the same name as another table in the schema, all queries and operations used on the table only\naffect the temporary table in the session, until you drop the temporary table. If you drop the table, you drop the temporary table, and\nnot the table that already exists in the schema. For information about temporary or transient tables, and how they can affect storage and cost, refer to the following resources: Working with Temporary and Transient Tables Storage costs for Time Travel and Fail-safe Specifies that the table is read-only. READ ONLY is valid only for a temporary table that is being created with the\nCREATE TABLE  CLONE variant of the CREATE TABLE command. A read-only table does not allow DML operations and only allows the following subset of DDL operations: ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } COMMENT ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } MASKING POLICY ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } TAG ALTER TABLE  RENAME COLUMN  TO ALTER TABLE  RENAME TO ALTER TABLE  { SET | UNSET } COMMENT ALTER TABLE  { SET | UNSET } TAG COMMENT DESCRIBE DROP SHOW UNDROP Read-only tables have a METADATA$ROW_POSITION column. This metadata column assigns a row number to each row in\nthe table that is continuous and starts from 0. The row number assigned to each row remains unchanged until the\nread-only table is dropped."
          },
          {
            "term": "READ ONLY",
            "definition": "Specifies that the table is read-only. READ ONLY is valid only for a temporary table that is being created with the\nCREATE TABLE  CLONE variant of the CREATE TABLE command. A read-only table does not allow DML operations and only allows the following subset of DDL operations: ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } COMMENT ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } MASKING POLICY ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } TAG ALTER TABLE  RENAME COLUMN  TO ALTER TABLE  RENAME TO ALTER TABLE  { SET | UNSET } COMMENT ALTER TABLE  { SET | UNSET } TAG COMMENT DESCRIBE DROP SHOW UNDROP Read-only tables have a METADATA$ROW_POSITION column. This metadata column assigns a row number to each row in\nthe table that is continuous and starts from 0. The row number assigned to each row remains unchanged until the\nread-only table is dropped."
          },
          {
            "term": "TRANSIENT",
            "definition": "Specifies that the table is transient. Like a permanent table, a transient table exists until explicitly dropped and is visible to any\nuser with the appropriate privileges. However, transient tables have a lower level of data protection than permanent tables, meaning\nthat data in a transient table might be lost in the event of a system failure. As such, transient tables should only be used for data\nthat can be recreated externally to Snowflake. Default: No value. If a table is not declared as TRANSIENT or TEMPORARY, the table is permanent. Note Transient tables have some storage considerations. For more information about these and other considerations when deciding whether to create temporary or transient tables, see\nWorking with Temporary and Transient Tables and Storage costs for Time Travel and Fail-safe."
          },
          {
            "term": "CONSTRAINT ...",
            "definition": "Defines an inline or out-of-line constraint for the specified column(s) in the table. For syntax details, see CREATE | ALTER TABLE  CONSTRAINT. For more information about constraints, see Constraints."
          },
          {
            "term": "COLLATE 'collation_specification'",
            "definition": "Specifies the collation to use for column operations such as string comparison. This option applies only to text columns\n(VARCHAR, STRING, TEXT, etc.). For more details, see Collation specifications."
          },
          {
            "term": "DEFAULT ... or . AUTOINCREMENT ...",
            "definition": "Specifies whether a default value is automatically inserted in the column if a value is not explicitly specified via an INSERT\nor CREATE TABLE AS SELECT statement: Column default value is defined by the specified expression which can be any of the following: Constant value. Sequence reference (seq_name.NEXTVAL). Simple expression that returns a scalar value. The simple expression can include a SQL UDF (user-defined function) if the UDF is not a\nsecure UDF. Note If a default expression refers to a SQL UDF, then the function is replaced by its\ndefinition at table creation time. If the user-defined function is redefined in the future, this does not\nupdate the columns default expression. The simple expression cannot contain references to: Subqueries. Aggregates. Window functions. Secure UDFs. UDFs written in languages other than SQL (e.g. Java, JavaScript). External functions. When you specify AUTOINCREMENT or IDENTITY, the default value for the column starts with a specified number and each\nsuccessive value automatically increments by the specified amount. AUTOINCREMENT and IDENTITY are synonymous and can be used only for columns with numeric data types, such as NUMBER, INT,\nFLOAT. Caution Snowflake uses a sequence to generate the values for an auto-incremented column. Sequences have limitations;\nsee Sequence Semantics. The default value for both the start value and the step/increment value is 1. Note Manually inserting values into an AUTOINCREMENT or IDENTITY column can result in duplicate values. If you manually insert the\nvalue 5 into an AUTOINCREMENT or IDENTITY column, a subsequently inserted row might use the same value 5 as the\ndefault value for the column. Use ORDER or NOORDER to specify whether or not the values are generated for the auto-incremented column in\nincreasing or decreasing order. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2, the generated values might be\n1, 3, 5, 7, 9, etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2, the generated values might be 1, 3, 101, 5, 103, etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). If you do not specify ORDER or NOORDER, the NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is\nset. Note DEFAULT and AUTOINCREMENT are mutually exclusive; only one can be specified for a column."
          },
          {
            "term": "DEFAULT expr",
            "definition": "Column default value is defined by the specified expression which can be any of the following: Constant value. Sequence reference (seq_name.NEXTVAL). Simple expression that returns a scalar value. The simple expression can include a SQL UDF (user-defined function) if the UDF is not a\nsecure UDF. Note If a default expression refers to a SQL UDF, then the function is replaced by its\ndefinition at table creation time. If the user-defined function is redefined in the future, this does not\nupdate the columns default expression. The simple expression cannot contain references to: Subqueries. Aggregates. Window functions. Secure UDFs. UDFs written in languages other than SQL (e.g. Java, JavaScript). External functions."
          },
          {
            "term": "{ AUTOINCREMENT | IDENTITY } . [ { ( start_num , step_num ) | START num INCREMENT num } ] . [ { ORDER | NOORDER } ]",
            "definition": "When you specify AUTOINCREMENT or IDENTITY, the default value for the column starts with a specified number and each\nsuccessive value automatically increments by the specified amount. AUTOINCREMENT and IDENTITY are synonymous and can be used only for columns with numeric data types, such as NUMBER, INT,\nFLOAT. Caution Snowflake uses a sequence to generate the values for an auto-incremented column. Sequences have limitations;\nsee Sequence Semantics. The default value for both the start value and the step/increment value is 1. Note Manually inserting values into an AUTOINCREMENT or IDENTITY column can result in duplicate values. If you manually insert the\nvalue 5 into an AUTOINCREMENT or IDENTITY column, a subsequently inserted row might use the same value 5 as the\ndefault value for the column. Use ORDER or NOORDER to specify whether or not the values are generated for the auto-incremented column in\nincreasing or decreasing order. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2, the generated values might be\n1, 3, 5, 7, 9, etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2, the generated values might be 1, 3, 101, 5, 103, etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). If you do not specify ORDER or NOORDER, the NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is\nset."
          },
          {
            "term": "MASKING POLICY = policy_name",
            "definition": "Specifies the masking policy to set on a column. This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "PROJECTION POLICY policy_name",
            "definition": "Specifies the projection policy to set on a column. This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "COMMENT 'string_literal'",
            "definition": "Specifies a comment for the column. (Note that comments can be specified at the column level or the table level. The syntax for each is slightly different.)"
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy."
          },
          {
            "term": "CLUSTER BY ( expr [ , expr , ... ] )",
            "definition": "Specifies one or more columns or column expressions in the table as the clustering key. For more details, see\nClustering Keys & Clustered Tables. Default: No value (no clustering key is defined for the table) Important Clustering keys are not intended or recommended for all tables; they typically benefit very large (i.e. multi-terabyte)\ntables. Before you specify a clustering key for a table, you should understand micro-partitions. For more information, see Understanding Snowflake Table Structures."
          },
          {
            "term": "ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE }",
            "definition": "Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support. The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER, ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false."
          },
          {
            "term": "READ ONLY",
            "definition": "Specifies that the table is read-only. READ ONLY is valid only for a temporary table that is being created with the\nCREATE TABLE  CLONE variant of the CREATE TABLE command. A read-only table does not allow DML operations and only allows the following subset of DDL operations: ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } COMMENT ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } MASKING POLICY ALTER TABLE  { ALTER | MODIFY } COLUMN  { SET | UNSET } TAG ALTER TABLE  RENAME COLUMN  TO ALTER TABLE  RENAME TO ALTER TABLE  { SET | UNSET } COMMENT ALTER TABLE  { SET | UNSET } TAG COMMENT DESCRIBE DROP SHOW UNDROP Read-only tables have a METADATA$ROW_POSITION column. This metadata column assigns a row number to each row in\nthe table that is continuous and starts from 0. The row number assigned to each row remains unchanged until the\nread-only table is dropped."
          },
          {
            "term": "DEFAULT expr",
            "definition": "Column default value is defined by the specified expression which can be any of the following: Constant value. Sequence reference (seq_name.NEXTVAL). Simple expression that returns a scalar value. The simple expression can include a SQL UDF (user-defined function) if the UDF is not a\nsecure UDF. Note If a default expression refers to a SQL UDF, then the function is replaced by its\ndefinition at table creation time. If the user-defined function is redefined in the future, this does not\nupdate the columns default expression. The simple expression cannot contain references to: Subqueries. Aggregates. Window functions. Secure UDFs. UDFs written in languages other than SQL (e.g. Java, JavaScript). External functions."
          },
          {
            "term": "{ AUTOINCREMENT | IDENTITY } . [ { ( start_num , step_num ) | START num INCREMENT num } ] . [ { ORDER | NOORDER } ]",
            "definition": "When you specify AUTOINCREMENT or IDENTITY, the default value for the column starts with a specified number and each\nsuccessive value automatically increments by the specified amount. AUTOINCREMENT and IDENTITY are synonymous and can be used only for columns with numeric data types, such as NUMBER, INT,\nFLOAT. Caution Snowflake uses a sequence to generate the values for an auto-incremented column. Sequences have limitations;\nsee Sequence Semantics. The default value for both the start value and the step/increment value is 1. Note Manually inserting values into an AUTOINCREMENT or IDENTITY column can result in duplicate values. If you manually insert the\nvalue 5 into an AUTOINCREMENT or IDENTITY column, a subsequently inserted row might use the same value 5 as the\ndefault value for the column. Use ORDER or NOORDER to specify whether or not the values are generated for the auto-incremented column in\nincreasing or decreasing order. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2, the generated values might be\n1, 3, 5, 7, 9, etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2, the generated values might be 1, 3, 101, 5, 103, etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). If you do not specify ORDER or NOORDER, the NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is\nset."
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Specifies the retention period for the table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical\ndata in the table. For more details, see Understanding & using Time Travel and Working with Temporary and Transient Tables. For a detailed description of this object-level parameter, as well as more information about object parameters, see\nParameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "CHANGE_TRACKING = { TRUE | FALSE }",
            "definition": "Specifies whether to enable change tracking on the table. TRUE enables change tracking on the table. This setting adds a pair of hidden columns to the source table and begins\nstoring change-tracking metadata in the columns. These columns consume a small amount of storage. The change-tracking metadata can be queried using the CHANGES clause for\nSELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the table. Default: FALSE"
          },
          {
            "term": "DEFAULT_DDL_COLLATION = 'collation_specification'",
            "definition": "Specifies a default collation specification for the columns in the table, including columns\nadded to the table in the future. For more details about the parameter, see DEFAULT_DDL_COLLATION."
          },
          {
            "term": "COPY GRANTS",
            "definition": "Specifies to retain the access privileges from the original table when a new table is created using any of the following\nCREATE TABLE variants: CREATE OR REPLACE TABLE CREATE TABLE  LIKE CREATE TABLE  CLONE The parameter copies all privileges, except OWNERSHIP, from the existing table to the new table. The new table does not\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE TABLE statement\nowns the new table. If the parameter is not included in the CREATE TABLE statement, then the new table does not inherit any explicit access\nprivileges granted on the original table, but does inherit any future grants defined for the object type in the schema. Note: With data sharing: If the existing table was shared to another account, the replacement table is also shared. If the existing table was shared with your account as a data consumer, and access was further granted to other roles in\nthe account (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement\ntable. The SHOW GRANTS output for the replacement table lists the grantee for the copied privileges as the\nrole that executed the CREATE TABLE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE TABLE command (i.e. within the same transaction). This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment for the table. Default: No value (Note that comments can be specified at the column level, constraint level, or table level. The syntax for each is slightly different.)"
          },
          {
            "term": "ROW ACCESS POLICY policy_name ON ( col_name [ , col_name ... ] )",
            "definition": "Specifies the row access policy to set on a table. This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "AGGREGATION POLICY policy_name [ ENTITY KEY ( col_name [ , col_name ... ] ) ]",
            "definition": "Specifies an aggregation policy to set on a table. You can apply one or more aggregation\npolicies on a table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. You can specify one or more entity keys for an aggregation policy. This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "JOIN POLICY policy_name [ ALLOWED JOIN KEYS ( col_name [ , ... ] ) ]",
            "definition": "Specifies the join policy to set on a table. Use the optional ALLOWED JOIN KEYS parameter to define which columns are allowed to be used as joining columns when\nthis policy is in effect. For more information, see Join policies. This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "TAG ( tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ] )",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects. This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "WITH CONTACT ( purpose = contact [ , purpose = contact ...] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the new object with one or more contacts."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE TABLE",
                "Schema",
                "Note that creating a temporary table does not require the CREATE TABLE privilege."
              ],
              [
                "SELECT",
                "Table, external table, view",
                "Required on queried tables and/or views only when cloning a table or executing CTAS statements."
              ],
              [
                "APPLY",
                "Masking policy, row access policy, tag",
                "Required only when applying a masking policy, row access policy, object tags, or any combination of these\ngovernance features when creating tables."
              ],
              [
                "USAGE (external stage) or READ (internal stage)",
                "Stage",
                "Required to derive table column definitions from staged files using CREATE TABLE â€¦ USING TEMPLATE statements."
              ],
              [
                "OWNERSHIP",
                "Table",
                "A role must be granted or inherit the OWNERSHIP privilege on the object to create a temporary object that has the same name as the object\nthat already exists in the schema.\nRequired to execute a CREATE OR ALTER TABLE statement for an existing table.\n\nOWNERSHIP is a special privilege on an object that is automatically granted to the role that created the object, but can also be transferred using the GRANT OWNERSHIP command to a different role by the owning role (or any role with the MANAGE GRANTS privilege).\nNote that in a managed access schema, only the schema owner (i.e. the role with the OWNERSHIP privilege on the schema) or a role with the MANAGE GRANTS privilege can grant or revoke privileges on objects in the schema, including future grants."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nA schema cannot contain tables and/or views with the same name. When creating a table:\n\n\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\n\n\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\n\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then creating a new table\nwith the same name; however, the dropped table is not permanently removed from the system. Instead, it is retained in\nTime Travel. This is important to note because dropped tables in Time Travel can be recovered, but they also contribute to data\nstorage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data. Any stream on the table becomes stale. In\naddition, any stream on a view that has this table as an underlying table, becomes stale. A stale stream is unreadable.\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\n\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then creating a new table\nwith the same name; however, the dropped table is not permanently removed from the system. Instead, it is retained in\nTime Travel. This is important to note because dropped tables in Time Travel can be recovered, but they also contribute to data\nstorage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data. Any stream on the table becomes stale. In\naddition, any stream on a view that has this table as an underlying table, becomes stale. A stale stream is unreadable.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nSimilar to reserved keywords, ANSI-reserved function names\n(CURRENT_DATE, CURRENT_TIMESTAMP, etc.) cannot be used as column\nnames.\nCREATE OR ALTER TABLE:\nFor more information, see CREATE OR ALTER TABLE usage notes.\nCREATE TABLE  CLONE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is suspended\nfor the new table  even if Automatic Clustering was not suspended for the source table.\nCREATE TABLE  CHANGE_TRACKING = TRUE:\nWhen change tracking is enabled, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nCREATE TABLE  LIKE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is not\nsuspended for the new table  even if Automatic Clustering was suspended for the source table.\nCREATE TABLE  AS SELECT (CTAS):\n\nIf the aliases for the column names in the SELECT list are valid columns, then the column definitions\nare not required in the CTAS statement; if omitted, the column names and types are inferred from the underlying query:\n\nCREATE TABLE <table_name> AS SELECT ...\n\nCopy\n\nAlternatively, the names can be explicitly specified using the following syntax:\n\nCREATE TABLE <table_name> ( <col1_name> , <col2_name> , ... ) AS SELECT ...\n\nCopy\n\nThe number of column names specified must match the number of SELECT list items in the query; the\ntypes of the columns are inferred from the types produced by the query.\n\nWhen clustering keys are specified in a CTAS statement:\n\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table  even if Automatic Clustering is suspended for the\nsource table.\n\n\nIf you want the table to be created with rows in a specific order, then use an ORDER BY sub-clause in the SELECT clause of the\nCTAS. Specifying CLUSTER BY does not cluster the data at the time that the table is created; instead, CLUSTER BY relies on\nautomatic clustering to recluster the data over time.\nThe ORDER BY sub-clause in a CREATE TABLE statement does not affect the order of the rows returned by future SELECT statements\non that table. To specify the order of rows in future SELECT statements, use an ORDER BY sub-clause in those statements.\nIf the aliases for the column names in the SELECT list are valid columns, then the column definitions\nare not required in the CTAS statement; if omitted, the column names and types are inferred from the underlying query:\n\nCREATE TABLE <table_name> AS SELECT ...\n\nCopy\n\nAlternatively, the names can be explicitly specified using the following syntax:\n\nCREATE TABLE <table_name> ( <col1_name> , <col2_name> , ... ) AS SELECT ...\n\nCopy\n\nThe number of column names specified must match the number of SELECT list items in the query; the\ntypes of the columns are inferred from the types produced by the query.\nWhen clustering keys are specified in a CTAS statement:\n\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table  even if Automatic Clustering is suspended for the\nsource table.\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table  even if Automatic Clustering is suspended for the\nsource table.\nIf you want the table to be created with rows in a specific order, then use an ORDER BY sub-clause in the SELECT clause of the\nCTAS. Specifying CLUSTER BY does not cluster the data at the time that the table is created; instead, CLUSTER BY relies on\nautomatic clustering to recluster the data over time.\nThe ORDER BY sub-clause in a CREATE TABLE statement does not affect the order of the rows returned by future SELECT statements\non that table. To specify the order of rows in future SELECT statements, use an ORDER BY sub-clause in those statements.\nInside a transaction, any DDL statement (including CREATE TEMPORARY/TRANSIENT TABLE) commits\nthe transaction before executing the DDL statement itself. The DDL statement then runs in its own transaction. The\nnext statement after the DDL statement starts a new transaction. Therefore, you cant create, use, and drop a\ntemporary or transient table within a single transaction. If you want to use a temporary or transient table inside a\ntransaction, then create the table before the transaction, and drop the table after the transaction.\nRecreating a table (using the optional OR REPLACE keyword) drops its history, which makes any stream on the table stale.\nA stale stream is unreadable.\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen creating a table with a masking policy on one or more table columns, or a row access policy added to the table, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the table\nprotected by a row access policy.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nA schema cannot contain tables and/or views with the same name. When creating a table:\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\n\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then creating a new table\nwith the same name; however, the dropped table is not permanently removed from the system. Instead, it is retained in\nTime Travel. This is important to note because dropped tables in Time Travel can be recovered, but they also contribute to data\nstorage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data. Any stream on the table becomes stale. In\naddition, any stream on a view that has this table as an underlying table, becomes stale. A stale stream is unreadable.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then creating a new table\nwith the same name; however, the dropped table is not permanently removed from the system. Instead, it is retained in\nTime Travel. This is important to note because dropped tables in Time Travel can be recovered, but they also contribute to data\nstorage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data. Any stream on the table becomes stale. In\naddition, any stream on a view that has this table as an underlying table, becomes stale. A stale stream is unreadable.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nSimilar to reserved keywords, ANSI-reserved function names\n(CURRENT_DATE, CURRENT_TIMESTAMP, etc.) cannot be used as column\nnames.\nCREATE OR ALTER TABLE:\nFor more information, see CREATE OR ALTER TABLE usage notes.\nCREATE TABLE  CLONE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is suspended\nfor the new table  even if Automatic Clustering was not suspended for the source table.\nCREATE TABLE  CHANGE_TRACKING = TRUE:\nWhen change tracking is enabled, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nCREATE TABLE  LIKE:\nIf the source table has clustering keys, then the new table has clustering keys. By default, Automatic Clustering is not\nsuspended for the new table  even if Automatic Clustering was suspended for the source table.\nCREATE TABLE  AS SELECT (CTAS):\nIf the aliases for the column names in the SELECT list are valid columns, then the column definitions\nare not required in the CTAS statement; if omitted, the column names and types are inferred from the underlying query:\n\nCREATE TABLE <table_name> AS SELECT ...\n\nCopy\n\nAlternatively, the names can be explicitly specified using the following syntax:\n\nCREATE TABLE <table_name> ( <col1_name> , <col2_name> , ... ) AS SELECT ...\n\nCopy\n\nThe number of column names specified must match the number of SELECT list items in the query; the\ntypes of the columns are inferred from the types produced by the query.\nWhen clustering keys are specified in a CTAS statement:\n\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table  even if Automatic Clustering is suspended for the\nsource table.\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table  even if Automatic Clustering is suspended for the\nsource table.\nIf you want the table to be created with rows in a specific order, then use an ORDER BY sub-clause in the SELECT clause of the\nCTAS. Specifying CLUSTER BY does not cluster the data at the time that the table is created; instead, CLUSTER BY relies on\nautomatic clustering to recluster the data over time.\nThe ORDER BY sub-clause in a CREATE TABLE statement does not affect the order of the rows returned by future SELECT statements\non that table. To specify the order of rows in future SELECT statements, use an ORDER BY sub-clause in those statements.\nIf the aliases for the column names in the SELECT list are valid columns, then the column definitions\nare not required in the CTAS statement; if omitted, the column names and types are inferred from the underlying query:\nAlternatively, the names can be explicitly specified using the following syntax:\nThe number of column names specified must match the number of SELECT list items in the query; the\ntypes of the columns are inferred from the types produced by the query.\nWhen clustering keys are specified in a CTAS statement:\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table  even if Automatic Clustering is suspended for the\nsource table.\nColumn definitions are required and must be explicitly specified in the statement.\nBy default, Automatic Clustering is not suspended for the new table  even if Automatic Clustering is suspended for the\nsource table.\nIf you want the table to be created with rows in a specific order, then use an ORDER BY sub-clause in the SELECT clause of the\nCTAS. Specifying CLUSTER BY does not cluster the data at the time that the table is created; instead, CLUSTER BY relies on\nautomatic clustering to recluster the data over time.\nThe ORDER BY sub-clause in a CREATE TABLE statement does not affect the order of the rows returned by future SELECT statements\non that table. To specify the order of rows in future SELECT statements, use an ORDER BY sub-clause in those statements.\nInside a transaction, any DDL statement (including CREATE TEMPORARY/TRANSIENT TABLE) commits\nthe transaction before executing the DDL statement itself. The DDL statement then runs in its own transaction. The\nnext statement after the DDL statement starts a new transaction. Therefore, you cant create, use, and drop a\ntemporary or transient table within a single transaction. If you want to use a temporary or transient table inside a\ntransaction, then create the table before the transaction, and drop the table after the transaction.\nRecreating a table (using the optional OR REPLACE keyword) drops its history, which makes any stream on the table stale.\nA stale stream is unreadable.\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen creating a table with a masking policy on one or more table columns, or a row access policy added to the table, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the table\nprotected by a row access policy.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.",
        "syntax": [
          "CREATE TABLE <table_name> AS SELECT ...",
          "CREATE TABLE <table_name> ( <col1_name> , <col2_name> , ... ) AS SELECT ..."
        ]
      },
      {
        "heading": "CREATE OR ALTER TABLE usage notes",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nLimitations\n\nCurrently only supports permanent, temporary, and transient tables. Read-only, external, dynamic, Apache Iceberg, and hybrid tables\nare not supported.\nAll limitations of the ALTER TABLE command apply.\nCurrently does not support the following:\n\nCREATE TABLE  AS SELECT (CTAS) variant syntax.\nCREATE TABLE  USING TEMPLATE variant syntax.\nCREATE TABLE  LIKE variant syntax.\nCREATE TABLE  CLONE variant syntax.\nCurrently only supports permanent, temporary, and transient tables. Read-only, external, dynamic, Apache Iceberg, and hybrid tables\nare not supported.\nAll limitations of the ALTER TABLE command apply.\nCurrently does not support the following:\n\nCREATE TABLE  AS SELECT (CTAS) variant syntax.\nCREATE TABLE  USING TEMPLATE variant syntax.\nCREATE TABLE  LIKE variant syntax.\nCREATE TABLE  CLONE variant syntax.\nCREATE TABLE  AS SELECT (CTAS) variant syntax.\nCREATE TABLE  USING TEMPLATE variant syntax.\nCREATE TABLE  LIKE variant syntax.\nCREATE TABLE  CLONE variant syntax.\nTable parameters and properties\n\nThe absence of a property or parameter that was previously set in the modified table definition results in unsetting it.\nUnsetting an explicit parameter value results in setting it to the default parameter value.\nIf the parameter is set on the schema or database that contain the table, the table inherits the parameter value set on\nthe schema or database.\nThe absence of a property or parameter that was previously set in the modified table definition results in unsetting it.\nUnsetting an explicit parameter value results in setting it to the default parameter value.\nIf the parameter is set on the schema or database that contain the table, the table inherits the parameter value set on\nthe schema or database.\nData governance\n\nSetting or unsetting a tag or policy on a table or column using a CREATE OR ALTER TABLE statement is not supported.\nExisting policies or tags are not altered by a CREATE OR ALTER statement and remain unchanged.\nSetting or unsetting a tag or policy on a table or column using a CREATE OR ALTER TABLE statement is not supported.\nExisting policies or tags are not altered by a CREATE OR ALTER statement and remain unchanged.\nConstraints\n\nSetting or unsetting an inline primary key changes the nullability of the column accordingly. This aligns with the behavior of\nthe CREATE TABLE command, but is different from the behavior of the ALTER TABLE command.\nColumns\n\nNew columns can only be added to the end of the column list.\nColumns cannot be renamed. If you attempt to rename a column, the column is dropped and a new column is added.\nThe default value for a column can only be modified to use a sequence.\nThe default sequence for a column (for example, SET DEFAULT seq_name.NEXTVAL) can only be changed if the column\nalready has a sequence.\nFor more information about modifying columns, see ALTER TABLE  ALTER COLUMN.\nNew columns can only be added to the end of the column list.\nColumns cannot be renamed. If you attempt to rename a column, the column is dropped and a new column is added.\nThe default value for a column can only be modified to use a sequence.\nThe default sequence for a column (for example, SET DEFAULT seq_name.NEXTVAL) can only be changed if the column\nalready has a sequence.\nFor more information about modifying columns, see ALTER TABLE  ALTER COLUMN.\nCollation\n\nCollation specifications cannot be altered.\nSetting the DEFAULT_DDL_COLLATION parameter in the CREATE OR ALTER TABLE command\nsets the default collation specification for existing columns, which ensures the CREATE OR ALTER TABLE command\nyields the same results as the CREATE TABLE command. Therefore, you cant use the CREATE OR ALTER TABLE command to set the\nDEFAULT_DDL_COLLATION parameter on a table that has existing text columns. You can, however, make collations explicit\nfor existing columns when changing the DEFAULT_DDL_COLLATION parameter for a table.\nFor example, create a new table my_table and  set the default collation specification for the table to fr:\nCREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(20)\n)\nDEFAULT_DDL_COLLATION = 'fr';\n\nCopy\nThe collation specification for column b is fr and cannot be changed. To change the default collation specification for\ntable my_table, you must explicitly set the collation for text column b in the CREATE OR ALTER statement:\nCREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(200) COLLATE 'fr'\n)\nDEFAULT_DDL_COLLATION = 'de';\n\nCopy\nCollation specifications cannot be altered.\nSetting the DEFAULT_DDL_COLLATION parameter in the CREATE OR ALTER TABLE command\nsets the default collation specification for existing columns, which ensures the CREATE OR ALTER TABLE command\nyields the same results as the CREATE TABLE command. Therefore, you cant use the CREATE OR ALTER TABLE command to set the\nDEFAULT_DDL_COLLATION parameter on a table that has existing text columns. You can, however, make collations explicit\nfor existing columns when changing the DEFAULT_DDL_COLLATION parameter for a table.\nFor example, create a new table my_table and  set the default collation specification for the table to fr:\nCREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(20)\n)\nDEFAULT_DDL_COLLATION = 'fr';\n\nCopy\nThe collation specification for column b is fr and cannot be changed. To change the default collation specification for\ntable my_table, you must explicitly set the collation for text column b in the CREATE OR ALTER statement:\nCREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(200) COLLATE 'fr'\n)\nDEFAULT_DDL_COLLATION = 'de';\n\nCopy\nLimitations\nCurrently only supports permanent, temporary, and transient tables. Read-only, external, dynamic, Apache Iceberg, and hybrid tables\nare not supported.\nAll limitations of the ALTER TABLE command apply.\nCurrently does not support the following:\n\nCREATE TABLE  AS SELECT (CTAS) variant syntax.\nCREATE TABLE  USING TEMPLATE variant syntax.\nCREATE TABLE  LIKE variant syntax.\nCREATE TABLE  CLONE variant syntax.\nCREATE TABLE  AS SELECT (CTAS) variant syntax.\nCREATE TABLE  USING TEMPLATE variant syntax.\nCREATE TABLE  LIKE variant syntax.\nCREATE TABLE  CLONE variant syntax.\nCurrently only supports permanent, temporary, and transient tables. Read-only, external, dynamic, Apache Iceberg, and hybrid tables\nare not supported.\nAll limitations of the ALTER TABLE command apply.\nCurrently does not support the following:\nCREATE TABLE  AS SELECT (CTAS) variant syntax.\nCREATE TABLE  USING TEMPLATE variant syntax.\nCREATE TABLE  LIKE variant syntax.\nCREATE TABLE  CLONE variant syntax.\nCREATE TABLE  AS SELECT (CTAS) variant syntax.\nCREATE TABLE  USING TEMPLATE variant syntax.\nCREATE TABLE  LIKE variant syntax.\nCREATE TABLE  CLONE variant syntax.\nTable parameters and properties\nThe absence of a property or parameter that was previously set in the modified table definition results in unsetting it.\nUnsetting an explicit parameter value results in setting it to the default parameter value.\nIf the parameter is set on the schema or database that contain the table, the table inherits the parameter value set on\nthe schema or database.\nThe absence of a property or parameter that was previously set in the modified table definition results in unsetting it.\nUnsetting an explicit parameter value results in setting it to the default parameter value.\nIf the parameter is set on the schema or database that contain the table, the table inherits the parameter value set on\nthe schema or database.\nData governance\nSetting or unsetting a tag or policy on a table or column using a CREATE OR ALTER TABLE statement is not supported.\nExisting policies or tags are not altered by a CREATE OR ALTER statement and remain unchanged.\nSetting or unsetting a tag or policy on a table or column using a CREATE OR ALTER TABLE statement is not supported.\nExisting policies or tags are not altered by a CREATE OR ALTER statement and remain unchanged.\nConstraints\nSetting or unsetting an inline primary key changes the nullability of the column accordingly. This aligns with the behavior of\nthe CREATE TABLE command, but is different from the behavior of the ALTER TABLE command.\nColumns\nNew columns can only be added to the end of the column list.\nColumns cannot be renamed. If you attempt to rename a column, the column is dropped and a new column is added.\nThe default value for a column can only be modified to use a sequence.\nThe default sequence for a column (for example, SET DEFAULT seq_name.NEXTVAL) can only be changed if the column\nalready has a sequence.\nFor more information about modifying columns, see ALTER TABLE  ALTER COLUMN.\nNew columns can only be added to the end of the column list.\nColumns cannot be renamed. If you attempt to rename a column, the column is dropped and a new column is added.\nThe default value for a column can only be modified to use a sequence.\nThe default sequence for a column (for example, SET DEFAULT seq_name.NEXTVAL) can only be changed if the column\nalready has a sequence.\nFor more information about modifying columns, see ALTER TABLE  ALTER COLUMN.\nCollation\nCollation specifications cannot be altered.\nSetting the DEFAULT_DDL_COLLATION parameter in the CREATE OR ALTER TABLE command\nsets the default collation specification for existing columns, which ensures the CREATE OR ALTER TABLE command\nyields the same results as the CREATE TABLE command. Therefore, you cant use the CREATE OR ALTER TABLE command to set the\nDEFAULT_DDL_COLLATION parameter on a table that has existing text columns. You can, however, make collations explicit\nfor existing columns when changing the DEFAULT_DDL_COLLATION parameter for a table.\nFor example, create a new table my_table and  set the default collation specification for the table to fr:\nCREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(20)\n)\nDEFAULT_DDL_COLLATION = 'fr';\n\nCopy\nThe collation specification for column b is fr and cannot be changed. To change the default collation specification for\ntable my_table, you must explicitly set the collation for text column b in the CREATE OR ALTER statement:\nCREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(200) COLLATE 'fr'\n)\nDEFAULT_DDL_COLLATION = 'de';\n\nCopy\nCollation specifications cannot be altered.\nSetting the DEFAULT_DDL_COLLATION parameter in the CREATE OR ALTER TABLE command\nsets the default collation specification for existing columns, which ensures the CREATE OR ALTER TABLE command\nyields the same results as the CREATE TABLE command. Therefore, you cant use the CREATE OR ALTER TABLE command to set the\nDEFAULT_DDL_COLLATION parameter on a table that has existing text columns. You can, however, make collations explicit\nfor existing columns when changing the DEFAULT_DDL_COLLATION parameter for a table.\nFor example, create a new table my_table and  set the default collation specification for the table to fr:\nThe collation specification for column b is fr and cannot be changed. To change the default collation specification for\ntable my_table, you must explicitly set the collation for text column b in the CREATE OR ALTER statement:\nAtomicity\nThe CREATE OR ALTER TABLE command currently does not guarantee atomicity. This means that if a CREATE OR ALTER TABLE statement\nfails during execution, it is possible that a subset of changes might have been applied to the table. If there is a possibility\nof partial changes, the error message, in most cases, includes the following text:\nCREATE OR ALTER execution failed. Partial updates may have been applied.\n\n\nFor example, if the statement is attempting to drop column A and add a new column B to a table, and the\nstatement is aborted, it is possible that column A was dropped but column B was not added.\n\nNote\nIf changes are partially applied, the resulting table is still in a valid state, and you can use additional ALTER TABLE\nstatements to complete the original set of changes.\n\nTo recover from partial updates, Snowflake recommends the following recovery mechanisms:\n\nFix forward\n\nRe-execute the CREATE OR ALTER TABLE statement. If the statements succeeds on the second attempt, the target\nstate is achieved.\nInvestigate the error message. If possible, fix the error and re-execute the CREATE OR ALTER TABLE statement.\n\n\nRoll back\nIf it is not possible to fix forward, Snowflake recommends manually rolling back partial changes:\n\nInvestigate the state of the table using the DESCRIBE TABLE and SHOW TABLES commands. Determine which partial\nchanges were applied, if any.\nIf any partial changes were applied, execute the appropriate ALTER TABLE statements to transform the table back to its\noriginal state.\n\nNote\nIn some cases, you might not be able to undo partial changes. For more information, see the supported and unsupported\nactions for modifying column properties in the ALTER TABLE  ALTER COLUMN topic.\n\n\n\n\nIf you need help recovering from a partial update, contact Snowflake Support.\nFix forward\n\nRe-execute the CREATE OR ALTER TABLE statement. If the statements succeeds on the second attempt, the target\nstate is achieved.\nInvestigate the error message. If possible, fix the error and re-execute the CREATE OR ALTER TABLE statement.\nRe-execute the CREATE OR ALTER TABLE statement. If the statements succeeds on the second attempt, the target\nstate is achieved.\nInvestigate the error message. If possible, fix the error and re-execute the CREATE OR ALTER TABLE statement.\nRoll back\nIf it is not possible to fix forward, Snowflake recommends manually rolling back partial changes:\n\nInvestigate the state of the table using the DESCRIBE TABLE and SHOW TABLES commands. Determine which partial\nchanges were applied, if any.\nIf any partial changes were applied, execute the appropriate ALTER TABLE statements to transform the table back to its\noriginal state.\n\nNote\nIn some cases, you might not be able to undo partial changes. For more information, see the supported and unsupported\nactions for modifying column properties in the ALTER TABLE  ALTER COLUMN topic.\nInvestigate the state of the table using the DESCRIBE TABLE and SHOW TABLES commands. Determine which partial\nchanges were applied, if any.\nIf any partial changes were applied, execute the appropriate ALTER TABLE statements to transform the table back to its\noriginal state.\n\nNote\nIn some cases, you might not be able to undo partial changes. For more information, see the supported and unsupported\nactions for modifying column properties in the ALTER TABLE  ALTER COLUMN topic.\nIf you need help recovering from a partial update, contact Snowflake Support.\nAtomicity\nThe CREATE OR ALTER TABLE command currently does not guarantee atomicity. This means that if a CREATE OR ALTER TABLE statement\nfails during execution, it is possible that a subset of changes might have been applied to the table. If there is a possibility\nof partial changes, the error message, in most cases, includes the following text:\nFor example, if the statement is attempting to drop column A and add a new column B to a table, and the\nstatement is aborted, it is possible that column A was dropped but column B was not added.\nNote\nIf changes are partially applied, the resulting table is still in a valid state, and you can use additional ALTER TABLE\nstatements to complete the original set of changes.\nTo recover from partial updates, Snowflake recommends the following recovery mechanisms:\nFix forward\n\nRe-execute the CREATE OR ALTER TABLE statement. If the statements succeeds on the second attempt, the target\nstate is achieved.\nInvestigate the error message. If possible, fix the error and re-execute the CREATE OR ALTER TABLE statement.\nRe-execute the CREATE OR ALTER TABLE statement. If the statements succeeds on the second attempt, the target\nstate is achieved.\nInvestigate the error message. If possible, fix the error and re-execute the CREATE OR ALTER TABLE statement.\nRoll back\nIf it is not possible to fix forward, Snowflake recommends manually rolling back partial changes:\n\nInvestigate the state of the table using the DESCRIBE TABLE and SHOW TABLES commands. Determine which partial\nchanges were applied, if any.\nIf any partial changes were applied, execute the appropriate ALTER TABLE statements to transform the table back to its\noriginal state.\n\nNote\nIn some cases, you might not be able to undo partial changes. For more information, see the supported and unsupported\nactions for modifying column properties in the ALTER TABLE  ALTER COLUMN topic.\nInvestigate the state of the table using the DESCRIBE TABLE and SHOW TABLES commands. Determine which partial\nchanges were applied, if any.\nIf any partial changes were applied, execute the appropriate ALTER TABLE statements to transform the table back to its\noriginal state.\n\nNote\nIn some cases, you might not be able to undo partial changes. For more information, see the supported and unsupported\nactions for modifying column properties in the ALTER TABLE  ALTER COLUMN topic.\nIf you need help recovering from a partial update, contact Snowflake Support.\nFix forward\nRe-execute the CREATE OR ALTER TABLE statement. If the statements succeeds on the second attempt, the target\nstate is achieved.\nInvestigate the error message. If possible, fix the error and re-execute the CREATE OR ALTER TABLE statement.\nRe-execute the CREATE OR ALTER TABLE statement. If the statements succeeds on the second attempt, the target\nstate is achieved.\nInvestigate the error message. If possible, fix the error and re-execute the CREATE OR ALTER TABLE statement.\nRoll back\nIf it is not possible to fix forward, Snowflake recommends manually rolling back partial changes:\nInvestigate the state of the table using the DESCRIBE TABLE and SHOW TABLES commands. Determine which partial\nchanges were applied, if any.\nIf any partial changes were applied, execute the appropriate ALTER TABLE statements to transform the table back to its\noriginal state.\n\nNote\nIn some cases, you might not be able to undo partial changes. For more information, see the supported and unsupported\nactions for modifying column properties in the ALTER TABLE  ALTER COLUMN topic.\nInvestigate the state of the table using the DESCRIBE TABLE and SHOW TABLES commands. Determine which partial\nchanges were applied, if any.\nIf any partial changes were applied, execute the appropriate ALTER TABLE statements to transform the table back to its\noriginal state.\nNote\nIn some cases, you might not be able to undo partial changes. For more information, see the supported and unsupported\nactions for modifying column properties in the ALTER TABLE  ALTER COLUMN topic.\nIf you need help recovering from a partial update, contact Snowflake Support.",
        "syntax": [
          "CREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(20)\n)\nDEFAULT_DDL_COLLATION = 'fr';",
          "CREATE OR ALTER TABLE my_table (\n  a INT PRIMARY KEY,\n  b VARCHAR(200) COLLATE 'fr'\n)\nDEFAULT_DDL_COLLATION = 'de';",
          "CREATE OR ALTER execution failed. Partial updates may have been applied."
        ]
      },
      {
        "heading": "Examples"
      },
      {
        "heading": "Basic examples",
        "description": "\nCreate a simple table in the current database and insert a row in the table:\nCreate a simple table and specify comments for both the table and the column in the table:",
        "syntax": [
          "CREATE TABLE mytable (amount NUMBER);\n\n+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table MYTABLE successfully created. |\n+-------------------------------------+\n\nINSERT INTO mytable VALUES(1);\n\nSHOW TABLES like 'mytable';\n\n+---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:32:28 -0700 | MYTABLE | TESTDB        | PUBLIC      | TABLE |         |            |    1 |  1024 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n\nDESC TABLE mytable;\n\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name   | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| AMOUNT | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+",
          "CREATE TABLE example (col1 NUMBER COMMENT 'a column comment') COMMENT='a table comment';\n\n+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table EXAMPLE successfully created. |\n+-------------------------------------+\n\nSHOW TABLES LIKE 'example';\n\n+---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment         | cluster_by | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:35:59 -0700 | EXAMPLE | TESTDB        | PUBLIC      | TABLE | a table comment |            |    0 |     0 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+-----------------+------------+------+-------+--------------+----------------+\n\nDESC TABLE example;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment          |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------|\n| COL1 | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | a column comment |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+------------------+"
        ]
      },
      {
        "heading": "CTAS examples",
        "description": "\nCreate a table by selecting from an existing table:\nMore advanced example of creating a table by selecting from an existing table; in this example, the values in the summary_amount\ncolumn in the new table are derived from two columns in the source table:\nCreate a table by selecting columns from a staged Parquet data file:",
        "syntax": [
          "CREATE TABLE mytable_copy (b) AS SELECT * FROM mytable;\n\nDESC TABLE mytable_copy;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| B    | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nCREATE TABLE mytable_copy2 AS SELECT b+1 AS c FROM mytable_copy;\n\nDESC TABLE mytable_copy2;\n\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| C    | NUMBER(39,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nSELECT * FROM mytable_copy2;\n\n+---+\n| C |\n|---|\n| 2 |\n+---+",
          "CREATE TABLE testtable_summary (name, summary_amount) AS SELECT name, amount1 + amount2 FROM testtable;",
          "CREATE OR REPLACE TABLE parquet_col (\n  custKey NUMBER DEFAULT NULL,\n  orderDate DATE DEFAULT NULL,\n  orderStatus VARCHAR(100) DEFAULT NULL,\n  price VARCHAR(255)\n)\nAS SELECT\n  $1:o_custkey::number,\n  $1:o_orderdate::date,\n  $1:o_orderstatus::text,\n  $1:o_totalprice::text\nFROM @my_stage;\n\n+-----------------------------------------+\n| status                                  |\n|-----------------------------------------|\n| Table PARQUET_COL successfully created. |\n+-----------------------------------------+\n\nDESC TABLE parquet_col;\n\n+-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name        | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| CUSTKEY     | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| ORDERDATE   | DATE         | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| ORDERSTATUS | VARCHAR(100) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| PRICE       | VARCHAR(255) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+-------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+"
        ]
      },
      {
        "heading": "CREATE TABLE  LIKE examples",
        "description": "\nCreate a table with the same column definitions as another table, but with no rows:",
        "syntax": [
          "CREATE TABLE mytable (amount NUMBER);\n\nINSERT INTO mytable VALUES(1);\n\nSELECT * FROM mytable;\n\n+--------+\n| AMOUNT |\n|--------|\n|      1 |\n+--------+\n\nCREATE TABLE mytable_2 LIKE mytable;\n\nDESC TABLE mytable_2;\n\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name   | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| AMOUNT | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+--------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n\nSELECT * FROM mytable_2;\n\n+--------+\n| AMOUNT |\n|--------|\n+--------+"
        ]
      },
      {
        "heading": "CREATE TABLE examples that set parameters and properties",
        "description": "\nCreate a table with a multi-column clustering key:\nSpecify collation for columns in a table:",
        "syntax": [
          "CREATE TABLE mytable (date TIMESTAMP_NTZ, id NUMBER, content VARIANT) CLUSTER BY (date, id);\n\nSHOW TABLES LIKE 'mytable';\n\n+---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------+\n| created_on                      | name    | database_name | schema_name | kind  | comment | cluster_by       | rows | bytes | owner        | retention_time |\n|---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------|\n| Mon, 11 Sep 2017 16:20:41 -0700 | MYTABLE | TESTDB        | PUBLIC      | TABLE |         | LINEAR(DATE, ID) |    0 |     0 | ACCOUNTADMIN | 1              |\n+---------------------------------+---------+---------------+-------------+-------+---------+------------------+------+-------+--------------+----------------+",
          "CREATE OR REPLACE TABLE collation_demo (\n  uncollated_phrase VARCHAR, \n  utf8_phrase VARCHAR COLLATE 'utf8',\n  english_phrase VARCHAR COLLATE 'en',\n  spanish_phrase VARCHAR COLLATE 'es');\n\nINSERT INTO collation_demo (\n      uncollated_phrase, \n      utf8_phrase, \n      english_phrase, \n      spanish_phrase) \n   VALUES (\n     'pinata', \n     'pinata', \n     'pinata', \n     'piÃ±ata');"
        ]
      },
      {
        "heading": "CREATE TABLE  USING TEMPLATE examples",
        "description": "\nCreate a table where the column definitions are derived from a set of staged files that contain Avro, Parquet, or ORC data.\nNote that the mystage stage and my_parquet_format file format referenced in the statement must already exist. A set of files\nmust already be staged in the cloud storage location referenced in the stage definition.\nThe following example creates a table using the detected schema from staged files and sorts the columns by order_id.\nIt builds on an example in the INFER_SCHEMA topic.\nNote that sorting the columns by order_id only applies if all staged files share a single schema. If the set of staged data\nfiles includes multiple schemas with shared column names, the order represented in the order_id column might not match any\nsingle file.\nNote\nUsing * for ARRAY_AGG(OBJECT_CONSTRUCT()) might result in an error if the returned result is larger than 128 MB.\nWe recommend that you avoid using * for larger result sets, and only use the required columns, COLUMN NAME,\nTYPE, and NULLABLE, for the query. Optional column ORDER_ID can be included when using\nWITHIN GROUP (ORDER BY order_id).",
        "syntax": [
          "CREATE TABLE mytable\n  USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    WITHIN GROUP (ORDER BY order_id)\n      FROM TABLE(\n        INFER_SCHEMA(\n          LOCATION=>'@mystage',\n          FILE_FORMAT=>'my_parquet_format'\n        )\n      ));"
        ]
      },
      {
        "heading": "Temporary table examples",
        "description": "\nCreate a temporary table that is dropped automatically at the end of the session:\nFor compatibility with other vendors, Snowflake also supports using the keywords below as synonyms for TEMPORARY:",
        "syntax": [
          "CREATE TEMPORARY TABLE demo_temporary (i INTEGER);\nCREATE TEMP TABLE demo_temp (i INTEGER);",
          "CREATE LOCAL TEMPORARY TABLE demo_local_temporary (i INTEGER);\nCREATE LOCAL TEMP TABLE demo_local_temp (i INTEGER);\n\nCREATE GLOBAL TEMPORARY TABLE demo_global_temporary (i INTEGER);\nCREATE GLOBAL TEMP TABLE demo_global_temp (i INTEGER);\n\nCREATE VOLATILE TABLE demo_volatile (i INTEGER);"
        ]
      },
      {
        "heading": "CREATE OR ALTER TABLE examples",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nCreate a table my_table using the CREATE OR ALTER TABLE command:\nNote\nCREATE OR ALTER TABLE statements for existing tables can only be executed by a role\nwith the OWNERSHIP privilege on table my_table.\nAlter table my_table to add and modify columns and set the DATA_RETENTION_TIME_IN_DAYS and\nDEFAULT_DDL_COLLATION parameters:\nUnset the DATA_RETENTION_TIME_IN_DAYS parameter. The absence of a parameter in the modified table definition results in unsetting it.\nIn this case, unsetting the DATA_RETENTION_TIME_IN_DAYS parameter for the table resets it to the default value of 1:\nThe CREATE OR ALTER TABLE command supports adding columns at the end of the column list. If you attempt to rename an existing column, the existing\ncolumn is dropped, and a new column with the new column name is added. This might result in data loss if data exists in the original column.\nThe following example illustrates this behavior.\nCreate a table:\nCREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    b INT\n  );\n\nCopy\nInsert data into table my_table:\nINSERT INTO my_table VALUES (1, 2), (2, 3);\n\nSELECT * FROM my_table;\n\nCopy\nReturns:\n+---+---+\n| A | B |\n|---+---|\n| 1 | 2 |\n| 2 | 3 |\n+---+---+\nAttempt to rename column b:\nCREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    c INT\n  );\n\nCopy\nColumn b is dropped and column c is added:\nSELECT * FROM my_table;\n\nCopy\nReturns:\n+---+------+\n| A | C    |\n|---+------|\n| 1 | NULL |\n| 2 | NULL |\n+---+------+\n\n\n\nNote\nYou can recover dropped columns using Time Travel.\nCreate a table:\nInsert data into table my_table:\nReturns:\nAttempt to rename column b:\nColumn b is dropped and column c is added:\nReturns:\nNote\nYou can recover dropped columns using Time Travel.\nSetting or unsetting an inline primary key changes the nullability of the column in a way that aligns with the behavior of the\nCREATE TABLE command, but is different from the behavior of the ALTER TABLE command. For example, adding a primary key constraint\non a column using an ALTER TABLE statement does not change column nullability.\nThe following example illustrates this behavior.\nCreate a table:\nCREATE TABLE t(a INT);\n\nCopy\nAlter the table to add a PRIMARY KEY constraint:\nCREATE OR ALTER TABLE t(a INT PRIMARY KEY);\n\nCopy\nColumn a is now the primary key and is set to NOT NULL:\nDESC TABLE t;\n\nCopy\nReturns:\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name | privacy domain |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------|\n| A    | NUMBER(38,0) | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | NULL    | NULL        | NULL           |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+\nReplace table t:\nCREATE OR REPLACE TABLE t(a INT);\n\nCopy\nInsert a NULL value:\nINSERT INTO t VALUES (null);\n\nCopy\nAdd primary key constraint to column a.\nThe NULL value in column a causes the following statement to fail:\nCREATE OR ALTER TABLE t(a INT PRIMARY KEY);\n\nCopy\nReturns:\n001471 (42601): SQL compilation error:\nColumn 'A' contains null values. Not null constraint cannot be added.\nCreate a table:\nAlter the table to add a PRIMARY KEY constraint:\nColumn a is now the primary key and is set to NOT NULL:\nReturns:\nReplace table t:\nInsert a NULL value:\nAdd primary key constraint to column a.\nThe NULL value in column a causes the following statement to fail:\nReturns:\nOn this page\nSyntax\nVariant syntax\nCREATE OR ALTER TABLE\nCREATE TABLE  AS SELECT (also referred to as CTAS)\nCREATE TABLE  USING TEMPLATE\nCREATE TABLE  LIKE\nCREATE TABLE  CLONE\nRequired parameters\nOptional parameters\nAccess control requirements\nUsage notes\nCREATE OR ALTER TABLE usage notes\nExamples\nBasic examples\nCTAS examples\nCREATE TABLE  LIKE examples\nCREATE TABLE examples that set parameters and properties\nCREATE TABLE  USING TEMPLATE examples\nTemporary table examples\nCREATE OR ALTER TABLE examples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR ALTER TABLE my_table(a INT);",
          "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    b VARCHAR(200)\n  )\n  DATA_RETENTION_TIME_IN_DAYS = 5\n  DEFAULT_DDL_COLLATION = 'de';",
          "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    c VARCHAR(200)\n  )\n  DEFAULT_DDL_COLLATION = 'de';",
          "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    b INT\n  );",
          "INSERT INTO my_table VALUES (1, 2), (2, 3);\n\nSELECT * FROM my_table;",
          "+---+---+\n| A | B |\n|---+---|\n| 1 | 2 |\n| 2 | 3 |\n+---+---+",
          "CREATE OR ALTER TABLE my_table(\n    a INT PRIMARY KEY,\n    c INT\n  );",
          "SELECT * FROM my_table;",
          "+---+------+\n| A | C    |\n|---+------|\n| 1 | NULL |\n| 2 | NULL |\n+---+------+",
          "CREATE TABLE t(a INT);",
          "CREATE OR ALTER TABLE t(a INT PRIMARY KEY);",
          "DESC TABLE t;",
          "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name | privacy domain |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------|\n| A    | NUMBER(38,0) | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | NULL    | NULL        | NULL           |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+----------------+",
          "CREATE OR REPLACE TABLE t(a INT);",
          "INSERT INTO t VALUES (null);",
          "CREATE OR ALTER TABLE t(a INT PRIMARY KEY);",
          "001471 (42601): SQL compilation error:\nColumn 'A' contains null values. Not null constraint cannot be added."
        ]
      }
    ]
  },
  {
    "category": "ALTER TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-table",
    "details": [
      {
        "heading": "ALTER TABLE",
        "description": "\nModifies the properties, columns, or constraints for an existing table.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER TABLE  ALTER COLUMN , CREATE TABLE , DROP TABLE , SHOW TABLES , DESCRIBE TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:\nFor detailed syntax and examples for altering columns, see ALTER TABLE  ALTER COLUMN. .\nFor detailed syntax and examples for creating/altering inline constraints, see CREATE | ALTER TABLE  CONSTRAINT.\nFor detailed syntax and examples for creating/altering out-of-line constraints, see CREATE | ALTER TABLE  CONSTRAINT.\nFor details, see Search optimization actions (searchOptimizationAction).",
        "syntax": [
          "ALTER TABLE [ IF EXISTS ] <name> RENAME TO <new_table_name>\n\nALTER TABLE [ IF EXISTS ] <name> SWAP WITH <target_table_name>\n\nALTER TABLE [ IF EXISTS ] <name> { clusteringAction | tableColumnAction | constraintAction  }\n\nALTER TABLE [ IF EXISTS ] <name> dataMetricFunctionAction\n\nALTER TABLE [ IF EXISTS ] <name> dataGovnPolicyTagAction\n\nALTER TABLE [ IF EXISTS ] <name> extTableColumnAction\n\nALTER TABLE [ IF EXISTS ] <name> searchOptimizationAction\n\nALTER TABLE [ IF EXISTS ] <name> SET\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE  } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER TABLE [ IF EXISTS ] <name> UNSET {\n                                       DATA_RETENTION_TIME_IN_DAYS         |\n                                       MAX_DATA_EXTENSION_TIME_IN_DAYS     |\n                                       CHANGE_TRACKING                     |\n                                       DEFAULT_DDL_COLLATION               |\n                                       ENABLE_SCHEMA_EVOLUTION             |\n                                       CONTACT <purpose>                   |\n                                       COMMENT                             |\n                                       }\n                                       [ , ... ]",
          "clusteringAction ::=\n  {\n     CLUSTER BY ( <expr> [ , <expr> , ... ] )\n     /* RECLUSTER is deprecated */\n   | RECLUSTER [ MAX_SIZE = <budget_in_bytes> ] [ WHERE <condition> ]\n     /* { SUSPEND | RESUME } RECLUSTER is valid action */\n   | { SUSPEND | RESUME } RECLUSTER\n   | DROP CLUSTERING KEY\n  }",
          "tableColumnAction ::=\n  {\n     ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n        [\n           {\n              DEFAULT <default_value>\n              | { AUTOINCREMENT | IDENTITY }\n                 /* AUTOINCREMENT (or IDENTITY) is supported only for           */\n                 /* columns with numeric data types (NUMBER, INT, FLOAT, etc.). */\n                 /* Also, if the table is not empty (that is, if the table contains */\n                 /* any rows), only DEFAULT can be altered.                     */\n                 [\n                    {\n                       ( <start_num> , <step_num> )\n                       | START <num> INCREMENT <num>\n                    }\n                 ]\n                 [  { ORDER | NOORDER } ]\n           }\n        ]\n        [ inlineConstraint ]\n        [ COLLATE '<collation_specification>' ]\n\n   | RENAME COLUMN <col_name> TO <new_col_name>\n\n   | ALTER | MODIFY [ ( ]\n                            [ COLUMN ] <col1_name> DROP DEFAULT\n                          , [ COLUMN ] <col1_name> SET DEFAULT <seq_name>.NEXTVAL\n                          , [ COLUMN ] <col1_name> { [ SET ] NOT NULL | DROP NOT NULL }\n                          , [ COLUMN ] <col1_name> [ [ SET DATA ] TYPE ] <type>\n                          , [ COLUMN ] <col1_name> COMMENT '<string>'\n                          , [ COLUMN ] <col1_name> UNSET COMMENT\n                        [ , [ COLUMN ] <col2_name> ... ]\n                        [ , ... ]\n                    [ ) ]\n\n   | DROP [ COLUMN ] [ IF EXISTS ] <col1_name> [, <col2_name> ... ]\n  }\n\n  inlineConstraint ::=\n    [ NOT NULL ]\n    [ CONSTRAINT <constraint_name> ]\n    { UNIQUE | PRIMARY KEY | { [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ] } }\n    [ <constraint_properties> ]",
          "dataMetricFunctionAction ::=\n\n    SET DATA_METRIC_SCHEDULE = {\n        '<num> MINUTE'\n      | 'USING CRON <expr> <time_zone>'\n      | 'TRIGGER_ON_CHANGES'\n    }\n\n  | UNSET DATA_METRIC_SCHEDULE\n\n  | { ADD | DROP } DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] )\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) ]\n\n  | MODIFY DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME }\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME } ]",
          "dataGovnPolicyTagAction ::=\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n  |\n  {\n      SET AGGREGATION POLICY <policy_name>\n        [ ENTITY KEY ( <col_name> [, ... ] ) ]\n        [ FORCE ]\n    | UNSET AGGREGATION POLICY\n  }\n  |\n  {\n      SET JOIN POLICY <policy_name>\n        [ FORCE ]\n    | UNSET JOIN POLICY\n  }\n  |\n  ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n    [ [ WITH ] MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] ]\n    [ [ WITH ] PROJECTION POLICY <policy_name> ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>'\n          [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] [ FORCE ]\n      | UNSET MASKING POLICY\n  }\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET PROJECTION POLICY <policy_name>\n          [ FORCE ]\n      | UNSET PROJECTION POLICY\n  }\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> SET TAG\n      <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n      , [ COLUMN ] <col2_name> SET TAG\n          <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n                   , [ COLUMN ] <col2_name> UNSET TAG <tag_name> [ , <tag_name> ... ]",
          "extTableColumnAction ::=\n  {\n     ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type> AS ( <expr> )\n\n   | RENAME COLUMN <col_name> TO <new_col_name>\n\n   | DROP [ COLUMN ] [ IF EXISTS ] <col1_name> [, <col2_name> ... ]\n  }",
          "constraintAction ::=\n  {\n     ADD outoflineConstraint\n   | RENAME CONSTRAINT <constraint_name> TO <new_constraint_name>\n   | { ALTER | MODIFY } { CONSTRAINT <constraint_name> | PRIMARY KEY | UNIQUE | FOREIGN KEY } ( <col_name> [ , ... ] )\n                         [ [ NOT ] ENFORCED ] [ VALIDATE | NOVALIDATE ] [ RELY | NORELY ]\n   | DROP { CONSTRAINT <constraint_name> | PRIMARY KEY | UNIQUE | FOREIGN KEY } ( <col_name> [ , ... ] )\n                         [ CASCADE | RESTRICT ]\n  }\n\n  outoflineConstraint ::=\n    [ CONSTRAINT <constraint_name> ]\n    {\n       UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n     | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n     | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n                          REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n    }\n    [ <constraint_properties> ]",
          "searchOptimizationAction ::=\n  {\n     ADD SEARCH OPTIMIZATION [\n       ON <search_method_with_target> [ , <search_method_with_target> ... ]\n     ]\n\n   | DROP SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n  }"
        ]
      },
      {
        "heading": "Parameters",
        "description": "\nTo rename a table or swap two tables, the role used to perform the operation must have OWNERSHIP privileges on the table(s). In addition,\nrenaming a table requires the CREATE TABLE privilege on the schema for the table.\nDo not specify copy options using the CREATE STAGE, ALTER STAGE, CREATE TABLE, or ALTER TABLE commands. We recommend that you use the COPY INTO <table> command to specify copy options.",
        "definitions": [
          {
            "term": "name",
            "definition": "Identifier for the table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case sensitive."
          },
          {
            "term": "RENAME TO new_table_name",
            "definition": "Renames the specified table with a new identifier that is not currently used by any other tables in the schema. For more information about table identifiers, see Identifier requirements. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object (table, column, etc.) is renamed, other objects that reference it must be updated with the new name."
          },
          {
            "term": "SWAP WITH target_table_name",
            "definition": "Swap renames two tables in a single transaction. Note that swapping a permanent or transient table with a temporary table, which persists only for the duration of the user session in which\nit was created, is not allowed. This restriction prevents a naming conflict that could occur when a temporary table is swapped with a permanent\nor transient table, and an existing permanent or transient table has the same name as the temporary table. To swap a permanent or transient\ntable with a temporary table, use three ALTER TABLE ... RENAME TO statements: Rename  table a to c, b\nto a, and then c to b."
          },
          {
            "term": "SET ...",
            "definition": "Specifies one or more properties/parameters to set for the table (separated by blank spaces, commas, or new lines): Object-level parameter that modifies the retention period for the table for Time Travel. For more information, see\nUnderstanding & using Time Travel and Working with Temporary and Transient Tables. For a detailed description of this parameter, as well as more information about object parameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Note A value of 0 effectively disables Time Travel for the table. Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS. Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the table. This option adds several hidden columns to the source table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT\nstatements, or by creating and querying one or more streams on the table. FALSE disables change tracking on the table. Associated hidden columns are dropped from the table. Specifies a default collation specification for any new columns added to the table. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION. Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support. The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER, ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false. Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts. Adds a comment or overwrites the existing comment for the table."
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Object-level parameter that modifies the retention period for the table for Time Travel. For more information, see\nUnderstanding & using Time Travel and Working with Temporary and Transient Tables. For a detailed description of this parameter, as well as more information about object parameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Note A value of 0 effectively disables Time Travel for the table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "CHANGE_TRACKING =  TRUE | FALSE",
            "definition": "Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the table. This option adds several hidden columns to the source table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT\nstatements, or by creating and querying one or more streams on the table. FALSE disables change tracking on the table. Associated hidden columns are dropped from the table."
          },
          {
            "term": "DEFAULT_DDL_COLLATION = 'collation_specification'",
            "definition": "Specifies a default collation specification for any new columns added to the table. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION."
          },
          {
            "term": "ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE }",
            "definition": "Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support. The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER, ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the table."
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Object-level parameter that modifies the retention period for the table for Time Travel. For more information, see\nUnderstanding & using Time Travel and Working with Temporary and Transient Tables. For a detailed description of this parameter, as well as more information about object parameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Note A value of 0 effectively disables Time Travel for the table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "CHANGE_TRACKING =  TRUE | FALSE",
            "definition": "Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the table. This option adds several hidden columns to the source table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT\nstatements, or by creating and querying one or more streams on the table. FALSE disables change tracking on the table. Associated hidden columns are dropped from the table."
          },
          {
            "term": "DEFAULT_DDL_COLLATION = 'collation_specification'",
            "definition": "Specifies a default collation specification for any new columns added to the table. Setting the parameter does not change the collation specification for any existing columns. For more information about the parameter, see DEFAULT_DDL_COLLATION."
          },
          {
            "term": "ENABLE_SCHEMA_EVOLUTION = { TRUE | FALSE }",
            "definition": "Enables or disables automatic changes to the table schema from data loaded into the table from source files, including: Added columns. By default, schema evolution is limited to a maximum of 100 added columns per load operation. To request more than 100 added columns per load operation, contact Snowflake Support. The NOT NULL constraint can be dropped from any number of columns missing in new data files. Setting it to TRUE enables automatic table schema evolution. The default FALSE disables automatic table schema evolution. Note Loading data from files evolves the table columns when all of the following are true: The COPY INTO <table> statement includes the MATCH_BY_COLUMN_NAME option. The role used to load the data has the EVOLVE SCHEMA or OWNERSHIP privilege on the table. Additionally, for schema evolution with CSV, when used with MATCH_BY_COLUMN_NAME and PARSE_HEADER, ERROR_ON_COLUMN_COUNT_MISMATCH must be set to false."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the table."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies one or more properties/parameters to unset for the table, which resets them back to their defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS CHANGE_TRACKING DEFAULT_DDL_COLLATION ENABLE_SCHEMA_EVOLUTION CONTACT purpose COMMENT"
          }
        ]
      },
      {
        "heading": "Clustering actions (clusteringAction)",
        "description": "\nFor more information about clustering keys and reclustering, see Understanding Snowflake Table Structures.",
        "definitions": [
          {
            "term": "CLUSTER BY ( expr [ , expr , ... ] )",
            "definition": "Specifies (or modifies) one or more table columns or column expressions as the clustering key for the table. These are the\ncolumns/expressions for which clustering is maintained by Automatic Clustering. Important Clustering keys are not intended or recommended for all tables; they typically benefit very large (that is, multi-terabyte) tables. Before you specify a clustering key for a table, please see Understanding Snowflake Table Structures."
          },
          {
            "term": "RECLUSTER ...",
            "definition": "Deprecated Performs manual, incremental reclustering of a table that has a clustering key defined: Deprecated  use a larger warehouse to achieve more effective manual reclustering Specifies the upper-limit on the amount of data (in bytes) in the table to recluster. Specifies a condition or range on which to recluster data in the table. Note Only roles with the OWNERSHIP or INSERT privilege on a table can recluster the table."
          },
          {
            "term": "MAX_SIZE = budget_in_bytes",
            "definition": "Deprecated  use a larger warehouse to achieve more effective manual reclustering Specifies the upper-limit on the amount of data (in bytes) in the table to recluster."
          },
          {
            "term": "WHERE condition",
            "definition": "Specifies a condition or range on which to recluster data in the table."
          },
          {
            "term": "MAX_SIZE = budget_in_bytes",
            "definition": "Deprecated  use a larger warehouse to achieve more effective manual reclustering Specifies the upper-limit on the amount of data (in bytes) in the table to recluster."
          },
          {
            "term": "WHERE condition",
            "definition": "Specifies a condition or range on which to recluster data in the table."
          },
          {
            "term": "SUSPEND | RESUME RECLUSTER",
            "definition": "Enables or disables Automatic Clustering for the table."
          },
          {
            "term": "DROP CLUSTERING KEY",
            "definition": "Drops the clustering key for the table."
          }
        ]
      },
      {
        "heading": "Table column actions (tableColumnAction)",
        "syntax": [
          "ALTER TABLE t1 ADD COLUMN c5 VARCHAR DEFAULT 12345::VARCHAR;",
          "002263 (22000): SQL compilation error:\nInvalid column default expression [CAST(12345 AS VARCHAR(134217728))]",
          "ALTER TABLE t1 ADD COLUMN c6 DATE DEFAULT '20230101';",
          "002023 (22000): SQL compilation error:\nExpression type does not match column data type, expecting DATE but got VARCHAR(8) for column C6"
        ],
        "definitions": [
          {
            "term": "ADD [ COLUMN ] [ IF NOT EXISTS ] col_name col_data_type . [ DEFAULT default_value | AUTOINCREMENT ... ] . [ inlineConstraint ] [ COLLATE 'collation_specification' ] . [ [ WITH ] MASKING POLICY policy_name ] . [ [ WITH ] PROJECTION POLICY policy_name ] . [ [ WITH ] TAG ( tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ] ) ] [ , ...]",
            "definition": "Adds a new column. You can specify a default value, an inline constraint, a collation specification,\na masking policy, and/or one or more tags. A default value for a column that you are adding must be a literal value; it cannot be an expression or a value\nreturned by a function. For example, the following command returns an expected error: When you first create a table, you can use expressions as default values, but not when you add columns. The default value for a column must match the data type of the column. An attempt to\nset a default value with a non-matching data type fails with an error. For example: For additional details about table column actions, see: CREATE TABLE CREATE | ALTER TABLE  CONSTRAINT CREATE MASKING POLICY CREATE TAG ADD COLUMN operations can be performed on multiple columns in the same command. If you are not sure if the column already exists, you can specify IF NOT EXISTS when adding the column. If the column already\nexists, ADD COLUMN has no effect on the existing column and does not result in an error. Note You cannot specify IF NOT EXISTS if you are also specifying any of the following for the new column: DEFAULT, AUTOINCREMENT, or IDENTITY UNIQUE, PRIMARY KEY, or FOREIGN KEY"
          },
          {
            "term": "RENAME COLUMN col_name to new_col_name",
            "definition": "Renames the specified column to a new name that is not currently used for any other columns in the table. You cannot rename a column that is part of a clustering key. When an object (table, column, etc.) is renamed, other objects that reference it must be updated with the new name."
          },
          {
            "term": "DROP COLUMN [ IF EXISTS ] col_name [ CASCADE | RESTRICT ]",
            "definition": "Removes the specified column from the table. If you are not sure if the column already exists, you can specify IF EXISTS when dropping the column. If the column does not\nexist, DROP COLUMN has no effect and does not result in an error. Dropping a column is a metadata-only operation. It does not immediately re-write the micro-partition(s) and\ntherefore does not immediately free up the space used by the column. Typically, the space within an individual\nmicro-partition is freed the next time that the micro-partition is re-written, which is typically when a write is\ndone either due to DML (INSERT, UPDATE, DELETE) or re-clustering."
          }
        ]
      },
      {
        "heading": "Data metric function actions (dataMetricFunctionAction)",
        "description": "\nFor details about the access control requirements for these actions, see DMF privileges.",
        "syntax": [
          "# __________ minute (0-59)\n# | ________ hour (0-23)\n# | | ______ day of month (1-31, or L)\n# | | | ____ month (1-12, JAN-DEC)\n# | | | | _ day of week (0-6, SUN-SAT, or L)\n# | | | | |\n# | | | | |\n  * * * * *"
        ],
        "definitions": [
          {
            "term": "DATA_METRIC_SCHEDULE ...",
            "definition": "Specifies the schedule to run the data metric function periodically. Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440. Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron utility\nsyntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th day\nof the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. If a data metric function is resumed during the minute defined in its cron expression, the first scheduled run of the data metric\nfunction is the next occurrence of the instance of the cron expression. For example, if data metric function scheduled to run daily\nat midnight (USING CRON 0 0 * * *) is resumed at midnight plus 5 seconds (00:00:05), the first data metric function run\nis scheduled for the following midnight. Specifies that the DMF runs when a DML operation modifies the table, such as inserting a new row or\ndeleting a row. You can specify 'TRIGGER_ON_CHANGES' for the following objects: Dynamic tables External tables Apache Iceberg tables Regular tables Temporary tables Transient tables You cannot specify 'TRIGGER_ON_CHANGES' for views. Changes to the table as a result of reclustering do not trigger the DMF to run."
          },
          {
            "term": "'num MINUTE'",
            "definition": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440."
          },
          {
            "term": "'USING CRON expr time_zone'",
            "definition": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron utility\nsyntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th day\nof the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. If a data metric function is resumed during the minute defined in its cron expression, the first scheduled run of the data metric\nfunction is the next occurrence of the instance of the cron expression. For example, if data metric function scheduled to run daily\nat midnight (USING CRON 0 0 * * *) is resumed at midnight plus 5 seconds (00:00:05), the first data metric function run\nis scheduled for the following midnight."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "'TRIGGER_ON_CHANGES'",
            "definition": "Specifies that the DMF runs when a DML operation modifies the table, such as inserting a new row or\ndeleting a row. You can specify 'TRIGGER_ON_CHANGES' for the following objects: Dynamic tables External tables Apache Iceberg tables Regular tables Temporary tables Transient tables You cannot specify 'TRIGGER_ON_CHANGES' for views. Changes to the table as a result of reclustering do not trigger the DMF to run."
          },
          {
            "term": "{ ADD | DROP } DATA METRIC FUNCTION metric_name",
            "definition": "Identifier of the data metric function to add to the table or view or drop from the table or view. The table or view columns on which to associate the data metric function. The data types of the columns must match the data types of\nthe columns specified in the data metric function definition. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns. Additional data metric functions to add to the table or view. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] )",
            "definition": "The table or view columns on which to associate the data metric function. The data types of the columns must match the data types of\nthe columns specified in the data metric function definition. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) ]",
            "definition": "Additional data metric functions to add to the table or view. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "MODIFY DATA METRIC FUNCTION metric_name",
            "definition": "Identifier of the data metric function to modify. Suspends or resumes the data metric function on the specified columns. When a data metric function is set for a table or view, the data\nmetric function is automatically included in the schedule. If the data metric function accepts a second table as an argument, specify\nthe fully qualified name of the table and its columns. SUSPEND removes the data metric function from the schedule. RESUME brings a suspended date metric function back into the schedule. Additional data metric functions to suspend or resume. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its\ncolumns."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) { SUSPEND | RESUME }",
            "definition": "Suspends or resumes the data metric function on the specified columns. When a data metric function is set for a table or view, the data\nmetric function is automatically included in the schedule. If the data metric function accepts a second table as an argument, specify\nthe fully qualified name of the table and its columns. SUSPEND removes the data metric function from the schedule. RESUME brings a suspended date metric function back into the schedule."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE(col_name [ , ... ] ) ] ) { SUSPEND | RESUME } ]",
            "definition": "Additional data metric functions to suspend or resume. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its\ncolumns."
          },
          {
            "term": "'num MINUTE'",
            "definition": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440."
          },
          {
            "term": "'USING CRON expr time_zone'",
            "definition": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron utility\nsyntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th day\nof the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. If a data metric function is resumed during the minute defined in its cron expression, the first scheduled run of the data metric\nfunction is the next occurrence of the instance of the cron expression. For example, if data metric function scheduled to run daily\nat midnight (USING CRON 0 0 * * *) is resumed at midnight plus 5 seconds (00:00:05), the first data metric function run\nis scheduled for the following midnight."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "'TRIGGER_ON_CHANGES'",
            "definition": "Specifies that the DMF runs when a DML operation modifies the table, such as inserting a new row or\ndeleting a row. You can specify 'TRIGGER_ON_CHANGES' for the following objects: Dynamic tables External tables Apache Iceberg tables Regular tables Temporary tables Transient tables You cannot specify 'TRIGGER_ON_CHANGES' for views. Changes to the table as a result of reclustering do not trigger the DMF to run."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] )",
            "definition": "The table or view columns on which to associate the data metric function. The data types of the columns must match the data types of\nthe columns specified in the data metric function definition. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) ]",
            "definition": "Additional data metric functions to add to the table or view. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) { SUSPEND | RESUME }",
            "definition": "Suspends or resumes the data metric function on the specified columns. When a data metric function is set for a table or view, the data\nmetric function is automatically included in the schedule. If the data metric function accepts a second table as an argument, specify\nthe fully qualified name of the table and its columns. SUSPEND removes the data metric function from the schedule. RESUME brings a suspended date metric function back into the schedule."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE(col_name [ , ... ] ) ] ) { SUSPEND | RESUME } ]",
            "definition": "Additional data metric functions to suspend or resume. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its\ncolumns."
          }
        ]
      },
      {
        "heading": "External table column actions (extTableColumnAction)",
        "description": "\nFor all other external table modifications, see ALTER EXTERNAL TABLE.",
        "syntax": [
          "mycol varchar as (value:c1::varchar)",
          "{ \"a\":\"1\", \"b\": { \"c\":\"2\", \"d\":\"3\" } }",
          "mycol varchar as (value:\"b\".\"c\"::varchar)"
        ],
        "definitions": [
          {
            "term": "ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type> AS ( <expr> ) [, ...]",
            "definition": "Adds a new column to the external table. If you are not sure if the column already exists, you can specify IF NOT EXISTS when adding the column. If the column already\nexists, ADD COLUMN has no effect on the existing column and does not result in an error. This operation can be performed on multiple columns in the same command. String that specifies the column identifier (that is, name). All the requirements for table identifiers also apply to column identifiers. For more information, see Identifier requirements. String (constant) that specifies the data type for the column. The data type must match the result of expr for the column. For details about the data types that can be specified for table columns, see SQL data types reference. String that specifies the expression for the column. When queried, the column returns results derived from this expression. External table columns are virtual columns, which are defined using an explicit expression. Add virtual columns as expressions using the\nVALUE column and/or the METADATA$FILENAME pseudocolumn: A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file: A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage."
          },
          {
            "term": "col_name",
            "definition": "String that specifies the column identifier (that is, name). All the requirements for table identifiers also apply to column identifiers. For more information, see Identifier requirements."
          },
          {
            "term": "col_type",
            "definition": "String (constant) that specifies the data type for the column. The data type must match the result of expr for the column. For details about the data types that can be specified for table columns, see SQL data types reference."
          },
          {
            "term": "expr",
            "definition": "String that specifies the expression for the column. When queried, the column returns results derived from this expression. External table columns are virtual columns, which are defined using an explicit expression. Add virtual columns as expressions using the\nVALUE column and/or the METADATA$FILENAME pseudocolumn: A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file: A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage."
          },
          {
            "term": "VALUE:",
            "definition": "A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "CSV:",
            "definition": "The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
          },
          {
            "term": "Semi-structured data:",
            "definition": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "METADATA$FILENAME:",
            "definition": "A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage."
          },
          {
            "term": "RENAME COLUMN col_name to new_col_name",
            "definition": "Renames the specified column to a new name that is not currently used for any other columns in the external table."
          },
          {
            "term": "DROP COLUMN [ IF EXISTS ] col_name",
            "definition": "Removes the specified column from the external table. If you are not sure if the column already exists, you can specify IF EXISTS when dropping the column. If the column does not\nexist, DROP COLUMN has no effect and does not result in an error."
          },
          {
            "term": "col_name",
            "definition": "String that specifies the column identifier (that is, name). All the requirements for table identifiers also apply to column identifiers. For more information, see Identifier requirements."
          },
          {
            "term": "col_type",
            "definition": "String (constant) that specifies the data type for the column. The data type must match the result of expr for the column. For details about the data types that can be specified for table columns, see SQL data types reference."
          },
          {
            "term": "expr",
            "definition": "String that specifies the expression for the column. When queried, the column returns results derived from this expression. External table columns are virtual columns, which are defined using an explicit expression. Add virtual columns as expressions using the\nVALUE column and/or the METADATA$FILENAME pseudocolumn: A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file: A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage."
          },
          {
            "term": "VALUE:",
            "definition": "A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "CSV:",
            "definition": "The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
          },
          {
            "term": "Semi-structured data:",
            "definition": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "METADATA$FILENAME:",
            "definition": "A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage."
          },
          {
            "term": "VALUE:",
            "definition": "A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "CSV:",
            "definition": "The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
          },
          {
            "term": "Semi-structured data:",
            "definition": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "METADATA$FILENAME:",
            "definition": "A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage."
          },
          {
            "term": "CSV:",
            "definition": "The VALUE column structures each row as an object with elements identified by column position (that is,\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
          },
          {
            "term": "Semi-structured data:",
            "definition": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          }
        ]
      },
      {
        "heading": "Constraint actions (constraintAction)",
        "description": "\nFor detailed syntax and examples for adding or altering constraints, see CREATE | ALTER TABLE  CONSTRAINT.",
        "definitions": [
          {
            "term": "ADD CONSTRAINT",
            "definition": "Adds an out-of-line integrity constraint to one or more columns in the table. To add an inline constraint (for a column), see\nColumn Actions (in this topic)."
          },
          {
            "term": "RENAME CONSTRAINT constraint_name TO new_constraint_name",
            "definition": "Renames the specified constraint."
          },
          {
            "term": "ALTER | MODIFY CONSTRAINT ...",
            "definition": "Alters the properties for the specified constraint."
          },
          {
            "term": "DROP CONSTRAINT constraint_name | PRIMARY KEY | UNIQUE | FOREIGN KEY ( col_name [ , ... ] ) [ CASCADE | RESTRICT ]",
            "definition": "Drops the specified constraint for the specified column or set of columns."
          }
        ]
      },
      {
        "heading": "Data Governance policy and tag actions (dataGovnPolicyTagAction)",
        "description": "\nThe following clauses apply to all table kinds that support row access policies, such as but not limited to tables, views, and event tables.\nTo simplify, the clauses just refer to table.",
        "definitions": [
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "policy_name",
            "definition": "Identifier for the policy; must be unique for your schema."
          },
          {
            "term": "ADD ROW ACCESS POLICY policy_name ON (col_name [ , ... ])",
            "definition": "Adds a row access policy to the table. At least one column name must be specified. Additional columns can be specified with a comma separating each column name. Use this\nexpression to add a row access policy to both an event table and an external table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name",
            "definition": "Drops a row access policy from the table. Use this clause to drop the policy from the table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name, ADD ROW ACCESS POLICY policy_name ON ( col_name [ , ... ] )",
            "definition": "Drops the row access policy that is set on the table and adds a row access policy to the same table in a single SQL statement."
          },
          {
            "term": "DROP ALL ROW ACCESS POLICIES",
            "definition": "Drops all row access policy associations from the table. This expression is helpful when a row access policy is dropped from a schema before dropping the policy from an event table. Use this expression to drop row access policy associations from the table."
          },
          {
            "term": "SET AGGREGATION POLICY policy_name",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ ENTITY KEY (col_name [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "UNSET AGGREGATION POLICY",
            "definition": "Detaches an aggregation policy from the table."
          },
          {
            "term": "SET JOIN POLICY policy_name",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "[ FORCE ]",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "UNSET JOIN POLICY",
            "definition": "Detaches a join policy from the table."
          },
          {
            "term": "[ ENTITY KEY (col_name [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ FORCE ]",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "{ ALTER | MODIFY } [ COLUMN ] ...",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query\nresult when a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy. Replaces a masking or projection policy that is currently set on a column with a different policy in a single statement. Note that using the FORCE keyword with a masking policy requires the data type of the policy\nin the ALTER TABLE statement (i.e. STRING) to match the data type of the masking policy currently set on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has no effect. For details, see: Replace a masking policy on a column or Replace a projection policy."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query\nresult when a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy."
          },
          {
            "term": "FORCE",
            "definition": "Replaces a masking or projection policy that is currently set on a column with a different policy in a single statement. Note that using the FORCE keyword with a masking policy requires the data type of the policy\nin the ALTER TABLE statement (i.e. STRING) to match the data type of the masking policy currently set on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has no effect. For details, see: Replace a masking policy on a column or Replace a projection policy."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query\nresult when a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy."
          },
          {
            "term": "FORCE",
            "definition": "Replaces a masking or projection policy that is currently set on a column with a different policy in a single statement. Note that using the FORCE keyword with a masking policy requires the data type of the policy\nin the ALTER TABLE statement (i.e. STRING) to match the data type of the masking policy currently set on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has no effect. For details, see: Replace a masking policy on a column or Replace a projection policy."
          }
        ]
      },
      {
        "heading": "Search optimization actions (searchOptimizationAction)",
        "tables": [
          {
            "headers": [
              "Search method",
              "Description"
            ],
            "rows": [
              [
                "FULL_TEXT",
                "Predicates that use VARCHAR (text), VARIANT, ARRAY, and OBJECT types."
              ],
              [
                "EQUALITY",
                "Equality and IN predicates."
              ],
              [
                "SUBSTRING",
                "Predicates that match substrings and regular expressions (for example, [ NOT ] LIKE,\n[ NOT ] ILIKE, [ NOT ] RLIKE, and\nREGEXP_LIKE)."
              ],
              [
                "GEO",
                "Predicates that use GEOGRAPHY types."
              ]
            ]
          },
          {
            "headers": [
              "Search method",
              "Supported targets"
            ],
            "rows": [
              [
                "FULL_TEXT",
                "Columns of VARCHAR (text), VARIANT, ARRAY, and OBJECT data types, including paths to fields in VARIANTs."
              ],
              [
                "EQUALITY",
                "Columns of numerical, string, binary, and VARIANT data types, including paths to fields in VARIANTs."
              ],
              [
                "SUBSTRING",
                "Columns of string or VARIANT data types, including paths to fields in VARIANTs. Specify paths to fields as described\nabove under EQUALITY; searches on nested fields are improved in the same way."
              ],
              [
                "GEO",
                "Columns of the GEOGRAPHY data type."
              ]
            ]
          },
          {
            "headers": [
              "Character",
              "Unicode code",
              "Description"
            ],
            "rows": [
              [
                "",
                "U+0020",
                "Space"
              ],
              [
                "[",
                "U+005B",
                "Left square bracket"
              ],
              [
                "]",
                "U+005D",
                "Right square bracket"
              ],
              [
                ";",
                "U+003B",
                "Semicolon"
              ],
              [
                "<",
                "U+003C",
                "Less-than sign"
              ],
              [
                ">",
                "U+003E",
                "Greater-than sign"
              ],
              [
                "(",
                "U+0028",
                "Left parenthesis"
              ],
              [
                ")",
                "U+0029",
                "Right parenthesis"
              ],
              [
                "{",
                "U+007B",
                "Left curly bracket"
              ],
              [
                "}",
                "U+007D",
                "Right curly bracket"
              ],
              [
                "|",
                "U+007C",
                "Vertical bar"
              ],
              [
                "!",
                "U+0021",
                "Exclamation mark"
              ],
              [
                ",",
                "U+002C",
                "Comma"
              ],
              [
                "'",
                "U+0027",
                "Apostrophe"
              ],
              [
                "\"",
                "U+0022",
                "Quotation mark"
              ],
              [
                "*",
                "U+002A",
                "Asterisk"
              ],
              [
                "&",
                "U+0026",
                "Ampersand"
              ],
              [
                "?",
                "U+003F",
                "Question mark"
              ],
              [
                "+",
                "U+002B",
                "Plus sign"
              ],
              [
                "/",
                "U+002F",
                "Slash"
              ],
              [
                ":",
                "U+003A",
                "Colon"
              ],
              [
                "=",
                "U+003D",
                "Equal sign"
              ],
              [
                "@",
                "U+0040",
                "At sign"
              ],
              [
                ".",
                "U+002E",
                "Period (full stop)"
              ],
              [
                "-",
                "U+002D",
                "Hyphen"
              ],
              [
                "$",
                "U+0024",
                "Dollar sign"
              ],
              [
                "%",
                "U+0025",
                "Percent sign"
              ],
              [
                "\\",
                "U+005C",
                "Backslash"
              ],
              [
                "_",
                "U+005F",
                "Underscore (low line)"
              ],
              [
                "\\n",
                "U+000A",
                "New line (line feed)"
              ],
              [
                "\\r",
                "U+000D",
                "Carriage return"
              ],
              [
                "\\t",
                "U+0009",
                "Horizontal tab"
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes: General",
        "description": "\nChanges to a table are not automatically propagated to views created on that table. For example, if you drop a\ncolumn in a table, and a view is defined to include that column, the view becomes invalid; the view is not\nadjusted to remove the column.\nChanges to a table are not automatically propagated to views created on that table. For example, if you drop a\ncolumn in a table, and a view is defined to include that column, the view becomes invalid; the view is not\nadjusted to remove the column.\nDropping a column does not immediately free up the columns storage space.\n\nThe space in each micro-partition is not reclaimed until that micro-partition is re-written. Write\noperations (insert, update, delete, etc.) on 1 or more rows in that micro-partition cause the micro-partition to\nbe re-written. If you want to force space to be reclaimed, you can follow these steps:\n\nUse a CREATE TABLE AS SELECT (CTAS) statement to create a new table that contains\nonly the columns of the old table you want to keep.\nSet the DATA_RETENTION_TIME_IN_DAYS parameter to 0 for the old table (optional).\nDrop the old table.\n\n\nIf the table is protected by the Time Travel feature, the space used by the Time Travel storage is not reclaimed\nuntil the Time Travel retention period expires.\nThe space in each micro-partition is not reclaimed until that micro-partition is re-written. Write\noperations (insert, update, delete, etc.) on 1 or more rows in that micro-partition cause the micro-partition to\nbe re-written. If you want to force space to be reclaimed, you can follow these steps:\n\nUse a CREATE TABLE AS SELECT (CTAS) statement to create a new table that contains\nonly the columns of the old table you want to keep.\nSet the DATA_RETENTION_TIME_IN_DAYS parameter to 0 for the old table (optional).\nDrop the old table.\nUse a CREATE TABLE AS SELECT (CTAS) statement to create a new table that contains\nonly the columns of the old table you want to keep.\nSet the DATA_RETENTION_TIME_IN_DAYS parameter to 0 for the old table (optional).\nDrop the old table.\nIf the table is protected by the Time Travel feature, the space used by the Time Travel storage is not reclaimed\nuntil the Time Travel retention period expires.\nIf a new column with a default value is added to a table with existing rows, all of the existing rows are populated with the default value.\nAdding a new column with a default value containing a function is not currently supported. The following error is returned:\n\nInvalid column default expression (expr)\nTo alter a table, you must be using a role that has ownership privilege on the table.\nTo add clustering to a table, you must also have USAGE or OWNERSHIP privileges on the schema and database that\ncontain the table.\nDropping a column does not immediately free up the columns storage space.\nThe space in each micro-partition is not reclaimed until that micro-partition is re-written. Write\noperations (insert, update, delete, etc.) on 1 or more rows in that micro-partition cause the micro-partition to\nbe re-written. If you want to force space to be reclaimed, you can follow these steps:\n\nUse a CREATE TABLE AS SELECT (CTAS) statement to create a new table that contains\nonly the columns of the old table you want to keep.\nSet the DATA_RETENTION_TIME_IN_DAYS parameter to 0 for the old table (optional).\nDrop the old table.\nUse a CREATE TABLE AS SELECT (CTAS) statement to create a new table that contains\nonly the columns of the old table you want to keep.\nSet the DATA_RETENTION_TIME_IN_DAYS parameter to 0 for the old table (optional).\nDrop the old table.\nIf the table is protected by the Time Travel feature, the space used by the Time Travel storage is not reclaimed\nuntil the Time Travel retention period expires.\nThe space in each micro-partition is not reclaimed until that micro-partition is re-written. Write\noperations (insert, update, delete, etc.) on 1 or more rows in that micro-partition cause the micro-partition to\nbe re-written. If you want to force space to be reclaimed, you can follow these steps:\nUse a CREATE TABLE AS SELECT (CTAS) statement to create a new table that contains\nonly the columns of the old table you want to keep.\nSet the DATA_RETENTION_TIME_IN_DAYS parameter to 0 for the old table (optional).\nDrop the old table.\nUse a CREATE TABLE AS SELECT (CTAS) statement to create a new table that contains\nonly the columns of the old table you want to keep.\nSet the DATA_RETENTION_TIME_IN_DAYS parameter to 0 for the old table (optional).\nDrop the old table.\nIf the table is protected by the Time Travel feature, the space used by the Time Travel storage is not reclaimed\nuntil the Time Travel retention period expires.\nIf a new column with a default value is added to a table with existing rows, all of the existing rows are populated with the default value.\nAdding a new column with a default value containing a function is not currently supported. The following error is returned:\nInvalid column default expression (expr)\nTo alter a table, you must be using a role that has ownership privilege on the table.\nTo add clustering to a table, you must also have USAGE or OWNERSHIP privileges on the schema and database that\ncontain the table.\nFor masking policies:\n\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\n\nApply a conditional masking policy on a column\nReplace a masking policy on a column\n\n\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\n\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nFor masking policies:\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\n\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nFor row access policies:\n\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\n\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\n\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nFor row access policies:\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nIf you create a foreign key, the columns in the REFERENCES clause must be listed in the same order as they were\nlisted for the primary key. For example:\nCREATE TABLE parent ... CONSTRAINT primary_key_1 PRIMARY KEY (c_1, c_2) ...\nCREATE TABLE child  ... CONSTRAINT foreign_key_1 FOREIGN KEY (...) REFERENCES parent (c_1, c_2) ...\n\nCopy\nIn both cases, the order of the columns is c_1, c_2. If the order of the columns in the foreign key had been different\n(for example, c_2, c_1), the attempt to create the foreign key would have failed.\nIf you create a foreign key, the columns in the REFERENCES clause must be listed in the same order as they were\nlisted for the primary key. For example:\nIn both cases, the order of the columns is c_1, c_2. If the order of the columns in the foreign key had been different\n(for example, c_2, c_1), the attempt to create the foreign key would have failed.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nALTER TABLE  CHANGE_TRACKING = TRUE\n\n\nWhen a table is altered to enable change tracking, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nWhen a table is altered to enable change tracking, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nIndexes in hybrid tables:\n\n\nWhen you use the ALTER TABLE command to add or drop a UNIQUE or\nFOREIGN KEY constraint in a hybrid table, the corresponding index is\nalso created or dropped. For more information about hybrid\ntable indexes, see CREATE INDEX.\nFOREIGN KEY constraints are supported only across hybrid tables that are\nstored in the same database. You cannot move a hybrid table from\none database to another. The PRIMARY KEY, UNIQUE, and\nFOREIGN KEY constraints defined on hybrid tables have their RELY\nproperty marked as TRUE.\nA column that is used by an index cannot be dropped before the\ncorresponding index is dropped.\nWhen you use the ALTER TABLE command to add or drop a UNIQUE or\nFOREIGN KEY constraint in a hybrid table, the corresponding index is\nalso created or dropped. For more information about hybrid\ntable indexes, see CREATE INDEX.\nFOREIGN KEY constraints are supported only across hybrid tables that are\nstored in the same database. You cannot move a hybrid table from\none database to another. The PRIMARY KEY, UNIQUE, and\nFOREIGN KEY constraints defined on hybrid tables have their RELY\nproperty marked as TRUE.\nA column that is used by an index cannot be dropped before the\ncorresponding index is dropped.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nALTER TABLE  CHANGE_TRACKING = TRUE\nWhen a table is altered to enable change tracking, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nWhen a table is altered to enable change tracking, the table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nIndexes in hybrid tables:\nWhen you use the ALTER TABLE command to add or drop a UNIQUE or\nFOREIGN KEY constraint in a hybrid table, the corresponding index is\nalso created or dropped. For more information about hybrid\ntable indexes, see CREATE INDEX.\nFOREIGN KEY constraints are supported only across hybrid tables that are\nstored in the same database. You cannot move a hybrid table from\none database to another. The PRIMARY KEY, UNIQUE, and\nFOREIGN KEY constraints defined on hybrid tables have their RELY\nproperty marked as TRUE.\nA column that is used by an index cannot be dropped before the\ncorresponding index is dropped.\nWhen you use the ALTER TABLE command to add or drop a UNIQUE or\nFOREIGN KEY constraint in a hybrid table, the corresponding index is\nalso created or dropped. For more information about hybrid\ntable indexes, see CREATE INDEX.\nFOREIGN KEY constraints are supported only across hybrid tables that are\nstored in the same database. You cannot move a hybrid table from\none database to another. The PRIMARY KEY, UNIQUE, and\nFOREIGN KEY constraints defined on hybrid tables have their RELY\nproperty marked as TRUE.\nA column that is used by an index cannot be dropped before the\ncorresponding index is dropped.",
        "syntax": [
          "CREATE TABLE parent ... CONSTRAINT primary_key_1 PRIMARY KEY (c_1, c_2) ...\nCREATE TABLE child  ... CONSTRAINT foreign_key_1 FOREIGN KEY (...) REFERENCES parent (c_1, c_2) ..."
        ]
      },
      {
        "heading": "Usage notes: Data metric functions",
        "definitions": [
          {
            "term": "Add a DMF to a table:",
            "definition": "Prior to adding a data metric function to a table, you must: Set the schedule for the data metric function to run. For details, see\nDATA_METRIC_SCHEDULE. Configure the event table to store the results of calling the data metric function. For details, see\nView results of a data metric function. Ensure that the table is view is not granted to a share because you cannot set a data metric function on a shared table or view. Additionally: You can add a data metric function to a table, external table, view, or materialized view. You cannot set a data metric function on any\nother kind of table, such as a dynamic table. When you specify a column, Snowflake uses the ordinal position. If you rename a column after adding a data metric function to the table\nor view, the association of the data metric function to the column remains valid. Only one data metric function of its kind can be added to a column. For example, a NULL_COUNT data metric function cannot be added to a\nsingle column twice. If you drop a column after adding a data metric function that references the column, Snowflake cannot evaluate the data metric function. Referencing a virtual column is not supported."
          },
          {
            "term": "Schedule a DMF",
            "definition": "It takes ten minutes for the schedule to become effective once the schedule is set. Similarly, it takes ten minutes once the DMF is unset for the scheduling changes to take effect. For more information, see\nSchedule the DMF to run."
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nThe following sections provide examples of using the ALTER COLUMN command:\nRenaming a table\nSwapping tables\nAdding columns\nRenaming columns\nDropping columns\nAdding, renaming, and dropping columns in an external table\nChanging the order of clustering keys\nAdding and dropping row access policies\nRenaming a table\nSwapping tables\nAdding columns\nRenaming columns\nDropping columns\nAdding, renaming, and dropping columns in an external table\nChanging the order of clustering keys\nAdding and dropping row access policies"
      },
      {
        "heading": "Renaming a table",
        "description": "\nThe following creates a table named t1:\nThe following statement changes the name of the table to tt1:",
        "syntax": [
          "CREATE OR REPLACE TABLE t1(a1 number);",
          "SHOW TABLES LIKE 't1';",
          "+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+\n| created_on                    | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | change_tracking | is_external | enable_schema_evolution | owner_role_type | is_event | budget |\n|-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------|\n| 2023-10-19 10:37:04.858 -0700 | T1   | TESTDB        | MY_SCHEMA   | TABLE |         |            |    0 |     0 | PUBLIC | 1              | OFF             | N           | N                       | ROLE            | N        | NULL   |\n+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+",
          "ALTER TABLE t1 RENAME TO tt1;",
          "SHOW TABLES LIKE 'tt1';",
          "+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+\n| created_on                    | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | change_tracking | is_external | enable_schema_evolution | owner_role_type | is_event | budget |\n|-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------|\n| 2023-10-19 10:37:04.858 -0700 | TT1  | TESTDB        | MY_SCHEMA   | TABLE |         |            |    0 |     0 | PUBLIC | 1              | OFF             | N           | N                       | ROLE            | N        | NULL   |\n+-------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+-----------------+-------------+-------------------------+-----------------+----------+--------+"
        ]
      },
      {
        "heading": "Swapping tables",
        "description": "\nThe following statements create tables named t1 and t2:\nThe following statement swaps table t1 with table t2:",
        "syntax": [
          "CREATE OR REPLACE TABLE t1(a1 NUMBER, a2 VARCHAR, a3 DATE);\nCREATE OR REPLACE TABLE t2(b1 VARCHAR);",
          "DESC TABLE t1;",
          "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | DATE              | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+",
          "DESC TABLE t2;",
          "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+",
          "ALTER TABLE t1 SWAP WITH t2;",
          "DESC TABLE t1;",
          "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+",
          "DESC TABLE t2;",
          "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | DATE              | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        ]
      },
      {
        "heading": "Adding columns",
        "description": "\nThe following creates a table named t1:\nThe following statement adds a column named a2 to this table:\nThe following statement adds a column named a3 with a NOT NULL constraint:\nThe following statement adds a column named a4 with a default value and a NOT NULL constraint:\nThe following statement adds a VARCHAR column named a5 with a language-specific\ncollation specification:\nThe following statement uses the IF NOT EXISTS clause to add a column named a2 only if the column does not exist. There is\nan existing column named a2. Specifying the IF NOT EXISTS clause prevents the statement from failing with an error.\nAs shown in the output of the DESCRIBE TABLE command, the statement above has no effect on the existing column named a2:",
        "syntax": [
          "CREATE OR REPLACE TABLE t1(a1 NUMBER);",
          "DESC TABLE t1;",
          "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+",
          "ALTER TABLE t1 ADD COLUMN a2 NUMBER;",
          "ALTER TABLE t1 ADD COLUMN a3 NUMBER NOT NULL;",
          "ALTER TABLE t1 ADD COLUMN a4 NUMBER DEFAULT 0 NOT NULL;",
          "ALTER TABLE t1 ADD COLUMN a5 VARCHAR COLLATE 'en_US';",
          "DESC TABLE t1;",
          "+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type                              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0)                      | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0)                      | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A5   | VARCHAR(16777216) COLLATE 'en_us' | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+",
          "ALTER TABLE t1 ADD COLUMN IF NOT EXISTS a2 NUMBER;",
          "DESC TABLE t1;",
          "+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type                              | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| A1   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | NUMBER(38,0)                      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0)                      | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0)                      | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A5   | VARCHAR(16777216) COLLATE 'en_us' | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+-----------------------------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        ]
      },
      {
        "heading": "Renaming columns",
        "description": "\nThe following statement changes the name of the column a1 to b1:",
        "syntax": [
          "ALTER TABLE t1 RENAME COLUMN a1 TO b1;",
          "DESC TABLE t1;",
          "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A2   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0) | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0) | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        ]
      },
      {
        "heading": "Dropping columns",
        "description": "\nThe following statement drops the column a2:\nThe following statement uses the IF EXISTS clause to drop a column named a2 only if the column exists. There is no existing\ncolumn named a2. Specifying the IF EXISTS clause prevents the statement from failing with an error.\nAs shown in the output of the DESCRIBE TABLE command, the statement above has no effect on the existing table:",
        "syntax": [
          "ALTER TABLE t1 DROP COLUMN a2;",
          "DESC TABLE t1;",
          "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0) | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0) | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+",
          "ALTER TABLE t1 DROP COLUMN IF EXISTS a2;",
          "+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| B1   | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A3   | NUMBER(38,0) | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| A4   | NUMBER(38,0) | COLUMN | N     | 0       | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+"
        ]
      },
      {
        "heading": "Adding, renaming, and dropping columns in an external table",
        "description": "\nThe following statement creates an external table named exttable1:\nThe following statement adds a new column named a1 to the external table:\nThe following statement changes the name of the a1 column to b1:\nThe following statement drops the column named b1:",
        "syntax": [
          "CREATE EXTERNAL TABLE exttable1\n  LOCATION=@mystage/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET)\n  ;",
          "DESC EXTERNAL TABLE exttable1;",
          "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+",
          "ALTER TABLE exttable1 ADD COLUMN a1 VARCHAR AS (value:a1::VARCHAR);",
          "DESC EXTERNAL TABLE exttable1;",
          "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n| A1        | VARCHAR(16777216) | VIRTUAL   | Y     | NULL    | N           | N          | NULL  | TO_CHAR(GET(VALUE, 'a1'))                                | NULL                  |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+",
          "ALTER TABLE exttable1 RENAME COLUMN a1 TO b1;",
          "DESC EXTERNAL TABLE exttable1;",
          "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n| B1        | VARCHAR(16777216) | VIRTUAL   | Y     | NULL    | N           | N          | NULL  | TO_CHAR(GET(VALUE, 'a1'))                                | NULL                  |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+",
          "ALTER TABLE exttable1 DROP COLUMN b1;",
          "DESC EXTERNAL TABLE exttable1;",
          "+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+\n| name      | type              | kind      | null? | default | primary key | unique key | check | expression                                               | comment               |\n|-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------|\n| VALUE     | VARIANT           | COLUMN    | Y     | NULL    | N           | N          | NULL  | NULL                                                     | The value of this row |\n+-----------+-------------------+-----------+-------+---------+-------------+------------+-------+----------------------------------------------------------+-----------------------+"
        ]
      },
      {
        "heading": "Changing the order of clustering keys",
        "description": "\nThe following statement creates a table named t1 that clusters by the id and date columns:\nThe following statement changes the order of the clustering key:",
        "syntax": [
          "CREATE OR REPLACE TABLE T1 (id NUMBER, date TIMESTAMP_NTZ, name STRING) CLUSTER BY (id, date);",
          "SHOW TABLES LIKE 'T1';",
          "---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (ID,DATE)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+",
          "ALTER TABLE t1 CLUSTER BY (date, id);",
          "SHOW TABLES LIKE 'T1';",
          "---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (DATE,ID)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+"
        ]
      },
      {
        "heading": "Adding and dropping row access policies",
        "description": "\nThe following example adds a row access policy on a table while specifying a single column. After setting the policy, you can verify by checking\nthe information schema.\nThe following example adds a row access policy while specifying two columns in a single table.\nThe following example drops a row access policy from a table. Verify the policies were dropped by querying the\ninformation schema.\nThe following example shows how to combine adding and dropping row access policies in a single SQL statement for a table. Verify the\nresults by checking the information schema.",
        "syntax": [
          "ALTER TABLE t1 ADD ROW ACCESS POLICY rap_t1 ON (empl_id);",
          "ALTER TABLE t1 ADD ROW ACCESS POLICY rap_test2 ON (cost, item);",
          "ALTER TABLE t1 DROP ROW ACCESS POLICY rap_v1;",
          "alter table t1\n  drop row access policy rap_t1_version_1,\n  add row access policy rap_t1_version_2 on (empl_id);"
        ]
      },
      {
        "heading": "Schedule for a data metric function to run",
        "description": "\nSet the data metric function schedule to run every 5 minutes:\nSet the data metric function schedule to run at 8:00 AM daily:\nSet the data metric function schedule to run at 8:00 AM on weekdays only:\nSet the data metric function schedule to run three times daily at 0600, 1200, and 1800 UTC:\nSet the data metric function to run when a general DML operation, such as inserting a new row, modifies the table:",
        "syntax": [
          "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = '5 MINUTE';",
          "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'USING CRON 0 8 * * * UTC';",
          "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'USING CRON 0 8 * * MON,TUE,WED,THU,FRI UTC';",
          "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'USING CRON 0 6,12,18 * * * UTC';",
          "ALTER TABLE hr.tables.empl_info SET\n  DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES';"
        ]
      },
      {
        "heading": "Apply a join policy on a table",
        "description": "\nAlter a table to apply a join policy with an allowed joining column:\nOn this page\nSyntax\nParameters\nClustering actions (clusteringAction)\nTable column actions (tableColumnAction)\nData metric function actions (dataMetricFunctionAction)\nExternal table column actions (extTableColumnAction)\nConstraint actions (constraintAction)\nData Governance policy and tag actions (dataGovnPolicyTagAction)\nSearch optimization actions (searchOptimizationAction)\nUsage notes: General\nUsage notes: Data metric functions\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER TABLE join_table_2\n  SET JOIN POLICY jp1 ALLOWED JOIN KEYS (col1);"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-table",
    "details": [
      {
        "heading": "DESCRIBE TABLE",
        "description": "\nDescribes either the columns in a table or the set of stage properties for the table (current values and default values).\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DROP TABLE , ALTER TABLE , CREATE TABLE , SHOW TABLES DESCRIBE VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "{ DESCRIBE | DESC } TABLE <name> [ TYPE =  { COLUMNS | STAGE } ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the table to describe. If the identifier contains spaces or special characters, the entire string must\nbe enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          },
          {
            "term": "TYPE = COLUMNS | STAGE",
            "definition": "Specifies whether to display the columns for the table or the set of stage properties for the table (current values and default values). Default: TYPE = COLUMNS"
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThis command does not show the object parameters for a table. Instead, use SHOW PARAMETERS IN TABLE.\nDESCRIBE TABLE and DESCRIBE VIEW are interchangeable. Both commands return details for the specified table or view; however,\nTYPE = STAGE does not apply for views because views do not have stage properties.\nIf schema evolution is enabled on the table, the output contains a SchemaEvolutionRecord column. This column was introduced with the 2023_08 Bundle (Generally Enabled). For more information, see Table schema evolution.\nThe output includes a policy name column to indicate the masking policy set on the column.\nIf a masking policy is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nThe output includes a privacy domain column to indicate the privacy domain\nset on the column.\nIf a privacy domain is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nThis command does not show the object parameters for a table. Instead, use SHOW PARAMETERS IN TABLE.\nDESCRIBE TABLE and DESCRIBE VIEW are interchangeable. Both commands return details for the specified table or view; however,\nTYPE = STAGE does not apply for views because views do not have stage properties.\nIf schema evolution is enabled on the table, the output contains a SchemaEvolutionRecord column. This column was introduced with the 2023_08 Bundle (Generally Enabled). For more information, see Table schema evolution.\nThe output includes a policy name column to indicate the masking policy set on the column.\nIf a masking policy is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nThe output includes a privacy domain column to indicate the privacy domain\nset on the column.\nIf a privacy domain is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "name",
                "Name of the column in the table."
              ],
              [
                "type",
                "Data type of the column in the table. If collation has been specified for the column, the\ncollation specification is included."
              ],
              [
                "kind",
                "This value is always COLUMN for Snowflake tables."
              ],
              [
                "null?",
                "Whether the column accepts NULL values (Y or N)."
              ],
              [
                "default",
                "The default value for the column, if any (otherwise NULL)."
              ],
              [
                "primary key",
                "Whether the column is the primary key (or part of a multi-column primary key; Y or N)."
              ],
              [
                "unique key",
                "Whether the column has a UNIQUE constraint (Y or N)."
              ],
              [
                "check",
                "Reserved for future use."
              ],
              [
                "expression",
                "Reserved for future use."
              ],
              [
                "comment",
                "The comment set for the column, if any (otherwise NULL)."
              ],
              [
                "policy name",
                "The masking policy set for the column, if any (otherwise NULL)."
              ],
              [
                "privacy domain",
                "The privacy domain set for the column, if any (otherwise NULL)."
              ],
              [
                "schema evolution record",
                "Records information about the latest triggered Schema Evolution for a given table column. This column contains the following subfields:\n\nEvolutionType: The type of the triggered schema evolution (ADD_COLUMN or DROP_NOT_NULL).\nEvolutionMode: The triggering ingestion mechanism (COPY or SNOWPIPE).\nFileName: The file name that triggered the evolution.\nTriggeringTime: The approximate time when the column was evolved.\nQueryId or PipeID: A unique identifier of the triggering query or pipe (QUERY ID for COPY or PIPE ID for SNOWPIPE)."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nThe following examples show how to describe tables."
      },
      {
        "heading": "Example: Describe a table that has constraints and other column attributes",
        "description": "\nCreate a table with five columns, two with constraints. Give one column a DEFAULT value and a\ncomment.\nDescribe the columns in the table:",
        "syntax": [
          "CREATE OR REPLACE TABLE desc_example(\n  c1 INT PRIMARY KEY,\n  c2 INT,\n  c3 INT UNIQUE,\n  c4 VARCHAR(30) DEFAULT 'Not applicable' COMMENT 'This column is rarely populated',\n  c5 VARCHAR(100));",
          "DESCRIBE TABLE desc_example;",
          "+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+-------------+----------------+-------------------------+\n| name | type         | kind   | null? | default          | primary key | unique key | check | expression | comment                         | policy name | privacy domain | schema evolution record |\n|------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+-------------+----------------+-------------------------|\n| C1   | NUMBER(38,0) | COLUMN | N     | NULL             | Y           | N          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n| C2   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n| C3   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | Y          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n| C4   | VARCHAR(30)  | COLUMN | Y     | 'Not applicable' | N           | N          | NULL  | NULL       | This column is rarely populated | NULL        | NULL           | NULL                    |\n| C5   | VARCHAR(100) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | NULL        | NULL           | NULL                    |\n+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+-------------+----------------+-------------------------+"
        ]
      },
      {
        "heading": "Example: Describe a table that has a masking policy on a column",
        "description": "\nCreate a normal masking policy, then recreate the desc_example table with the masking policy set on one column. (To run this example, create the email_mask masking policy first.)",
        "syntax": [
          "CREATE OR REPLACE TABLE desc_example(\n  c1 INT PRIMARY KEY,\n  c2 INT,\n  c3 INT UNIQUE,\n  c4 VARCHAR(30) DEFAULT 'Not applicable' COMMENT 'This column is rarely populated',\n  c5 VARCHAR(100) WITH MASKING POLICY email_mask);",
          "+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+---------------------------------+----------------+-------------------------+\n| name | type         | kind   | null? | default          | primary key | unique key | check | expression | comment                         | policy name                     | privacy domain | schema evolution record |\n|------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+---------------------------------+----------------|-------------------------|\n| C1   | NUMBER(38,0) | COLUMN | N     | NULL             | Y           | N          | NULL  | NULL       | NULL                            | NULL                            | NULL           | NULL                    |\n| C2   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | NULL                            | NULL           | NULL                    |\n| C3   | NUMBER(38,0) | COLUMN | Y     | NULL             | N           | Y          | NULL  | NULL       | NULL                            | NULL                            | NULL           | NULL                    |\n| C4   | VARCHAR(30)  | COLUMN | Y     | 'Not applicable' | N           | N          | NULL  | NULL       | This column is rarely populated | NULL                            | NULL           | NULL                    |\n| C5   | VARCHAR(100) | COLUMN | Y     | NULL             | N           | N          | NULL  | NULL       | NULL                            | HT_SENSORS.HT_SCHEMA.EMAIL_MASK | NULL           | NULL                    |\n+------+--------------+--------+-------+------------------+-------------+------------+-------+------------+---------------------------------+---------------------------------+----------------+-------------------------+"
        ]
      },
      {
        "heading": "Example: Describe stage properties",
        "description": "\nDescribe the current stage properties for the same table (only the first five rows are shown here):\nOn this page\nSyntax\nParameters\nUsage notes\nOutput\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESCRIBE TABLE desc_example TYPE = STAGE;",
          "+--------------------+--------------------------------+---------------+-----------------+------------------+\n| parent_property    | property                       | property_type | property_value  | property_default |\n|--------------------+--------------------------------+---------------+-----------------+------------------|\n| STAGE_FILE_FORMAT  | TYPE                           | String        | CSV             | CSV              |\n| STAGE_FILE_FORMAT  | RECORD_DELIMITER               | String        | \\n              | \\n               |\n| STAGE_FILE_FORMAT  | FIELD_DELIMITER                | String        | ,               | ,                |\n| STAGE_FILE_FORMAT  | FILE_EXTENSION                 | String        |                 |                  |\n| STAGE_FILE_FORMAT  | SKIP_HEADER                    | Integer       | 0               | 0                |\n..."
        ]
      }
    ]
  },
  {
    "category": "DROP TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-table",
    "details": [
      {
        "heading": "DROP TABLE",
        "description": "\nRemoves a table from the current or specified schema, but retains a version of the table so that it can be recovered by using\nUNDROP TABLE. For information, see Usage Notes.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE TABLE , ALTER TABLE , SHOW TABLES , TRUNCATE TABLE , DESCRIBE TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP TABLE [ IF EXISTS ] <name> [ CASCADE | RESTRICT ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the table to drop. If the identifier contains spaces, special characters, or mixed-case characters, the\nentire string must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive\n(for example, \"My Object\"). If the table identifier is not fully-qualified (in the form of db_name.schema_name.table_name or\nschema_name.table_name), the command looks for the table in the current schema for the session."
          },
          {
            "term": "CASCADE | RESTRICT",
            "definition": "Specifies whether the table can be dropped if foreign keys exist that reference the table: CASCADE: Drops the table even if the table has primary or unique keys that are referenced by foreign keys in other tables. RESTRICT: Returns a warning about existing foreign key references and doesnt drop the table. Default: CASCADE for standard tables; RESTRICT for hybrid tables. See also Dropping hybrid tables."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "OWNERSHIP",
                "Table",
                "OWNERSHIP is a special privilege on an object that is automatically granted to the role that created the object, but can also be transferred using the GRANT OWNERSHIP command to a different role by the owning role (or any role with the MANAGE GRANTS privilege)."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nDropping a table does not permanently remove it from the system. A version of the dropped table is retained in\nTime Travel for the number of days specified by the\ndata retention period for the table:\n\n\nWithin the Time Travel retention period, you can restore a dropped table by using the UNDROP TABLE command.\nChanging the Time Travel retention period for the account or for a parent object (a database or a schema) after\nyou drop a table doesnt change the Time Travel retention period for the dropped table.\nFor more information, see the note in the Time Travel topic.\nWhen the Time Travel retention period ends, the next state for the dropped table depends on whether it is permanent, transient, or\ntemporary:\n\nA permanent table moves into Fail-safe. In Fail-safe (7 days), a dropped table can be recovered,\nbut only by Snowflake. When the table leaves Fail-safe, it is purged.\nA transient or temporary table has no Fail-safe, so it is purged when it moves out of Time Travel.\n\nNote\nA long-running Time Travel query delays the movement of any data and objects (tables, schemas, and databases) in the account into\nFail-safe, until the query completes. The purging of temporary and transient tables is delayed in the same way.\n\n\nAfter a dropped table is purged, it cant be recovered; it must be recreated.\nWithin the Time Travel retention period, you can restore a dropped table by using the UNDROP TABLE command.\nChanging the Time Travel retention period for the account or for a parent object (a database or a schema) after\nyou drop a table doesnt change the Time Travel retention period for the dropped table.\nFor more information, see the note in the Time Travel topic.\nWhen the Time Travel retention period ends, the next state for the dropped table depends on whether it is permanent, transient, or\ntemporary:\n\nA permanent table moves into Fail-safe. In Fail-safe (7 days), a dropped table can be recovered,\nbut only by Snowflake. When the table leaves Fail-safe, it is purged.\nA transient or temporary table has no Fail-safe, so it is purged when it moves out of Time Travel.\n\nNote\nA long-running Time Travel query delays the movement of any data and objects (tables, schemas, and databases) in the account into\nFail-safe, until the query completes. The purging of temporary and transient tables is delayed in the same way.\n\n\nAfter a dropped table is purged, it cant be recovered; it must be recreated.\nA permanent table moves into Fail-safe. In Fail-safe (7 days), a dropped table can be recovered,\nbut only by Snowflake. When the table leaves Fail-safe, it is purged.\nA transient or temporary table has no Fail-safe, so it is purged when it moves out of Time Travel.\n\nNote\nA long-running Time Travel query delays the movement of any data and objects (tables, schemas, and databases) in the account into\nFail-safe, until the query completes. The purging of temporary and transient tables is delayed in the same way.\nAfter a dropped table is purged, it cant be recovered; it must be recreated.\nAfter you drop a table, creating a table with the same name creates a new version of the table. You can still restore the dropped version of the\nprevious table by following these steps:\n\nRename the current version of the table.\nUse the UNDROP TABLE command to restore the previous version of the table.\nRename the current version of the table.\nUse the UNDROP TABLE command to restore the previous version of the table.\nBefore dropping a table, verify that no views reference the table. Dropping a table referenced by a view invalidates the view\n(that is, querying the view returns an object does not exist error).\nTo drop a table, you must use a role that has OWNERSHIP privilege on the table.\nDropping a table does not permanently remove it from the system. A version of the dropped table is retained in\nTime Travel for the number of days specified by the\ndata retention period for the table:\nWithin the Time Travel retention period, you can restore a dropped table by using the UNDROP TABLE command.\nChanging the Time Travel retention period for the account or for a parent object (a database or a schema) after\nyou drop a table doesnt change the Time Travel retention period for the dropped table.\nFor more information, see the note in the Time Travel topic.\nWhen the Time Travel retention period ends, the next state for the dropped table depends on whether it is permanent, transient, or\ntemporary:\n\nA permanent table moves into Fail-safe. In Fail-safe (7 days), a dropped table can be recovered,\nbut only by Snowflake. When the table leaves Fail-safe, it is purged.\nA transient or temporary table has no Fail-safe, so it is purged when it moves out of Time Travel.\n\nNote\nA long-running Time Travel query delays the movement of any data and objects (tables, schemas, and databases) in the account into\nFail-safe, until the query completes. The purging of temporary and transient tables is delayed in the same way.\n\n\nAfter a dropped table is purged, it cant be recovered; it must be recreated.\nA permanent table moves into Fail-safe. In Fail-safe (7 days), a dropped table can be recovered,\nbut only by Snowflake. When the table leaves Fail-safe, it is purged.\nA transient or temporary table has no Fail-safe, so it is purged when it moves out of Time Travel.\n\nNote\nA long-running Time Travel query delays the movement of any data and objects (tables, schemas, and databases) in the account into\nFail-safe, until the query completes. The purging of temporary and transient tables is delayed in the same way.\nAfter a dropped table is purged, it cant be recovered; it must be recreated.\nWithin the Time Travel retention period, you can restore a dropped table by using the UNDROP TABLE command.\nChanging the Time Travel retention period for the account or for a parent object (a database or a schema) after\nyou drop a table doesnt change the Time Travel retention period for the dropped table.\nFor more information, see the note in the Time Travel topic.\nWhen the Time Travel retention period ends, the next state for the dropped table depends on whether it is permanent, transient, or\ntemporary:\nA permanent table moves into Fail-safe. In Fail-safe (7 days), a dropped table can be recovered,\nbut only by Snowflake. When the table leaves Fail-safe, it is purged.\nA transient or temporary table has no Fail-safe, so it is purged when it moves out of Time Travel.\n\nNote\nA long-running Time Travel query delays the movement of any data and objects (tables, schemas, and databases) in the account into\nFail-safe, until the query completes. The purging of temporary and transient tables is delayed in the same way.\nAfter a dropped table is purged, it cant be recovered; it must be recreated.\nA permanent table moves into Fail-safe. In Fail-safe (7 days), a dropped table can be recovered,\nbut only by Snowflake. When the table leaves Fail-safe, it is purged.\nA transient or temporary table has no Fail-safe, so it is purged when it moves out of Time Travel.\nNote\nA long-running Time Travel query delays the movement of any data and objects (tables, schemas, and databases) in the account into\nFail-safe, until the query completes. The purging of temporary and transient tables is delayed in the same way.\nAfter a dropped table is purged, it cant be recovered; it must be recreated.\nAfter you drop a table, creating a table with the same name creates a new version of the table. You can still restore the dropped version of the\nprevious table by following these steps:\nRename the current version of the table.\nUse the UNDROP TABLE command to restore the previous version of the table.\nRename the current version of the table.\nUse the UNDROP TABLE command to restore the previous version of the table.\nBefore dropping a table, verify that no views reference the table. Dropping a table referenced by a view invalidates the view\n(that is, querying the view returns an object does not exist error).\nTo drop a table, you must use a role that has OWNERSHIP privilege on the table.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Dropping hybrid tables",
        "description": "\nWhen you drop a hybrid table without specifying the RESTRICT or CASCADE option, and the hybrid table\nhas a primary-key/foreign-key or unique-key/foreign-key relationship with another table, the DROP TABLE\ncommand fails with an error. The default behavior is RESTRICT.\nFor example:\nThe DROP TABLE command fails in this case. If necessary, you can override the default behavior by specifying\nCASCADE in the DROP TABLE command.\nAlternatively in this case, you could drop the dependent table ht2 first, then drop table ht1.",
        "syntax": [
          "CREATE OR REPLACE HYBRID TABLE ht1(\n  col1 NUMBER(38,0) NOT NULL,\n  col2 NUMBER(38,0) NOT NULL,\n  CONSTRAINT pkey_ht1 PRIMARY KEY (col1, col2));\n\nCREATE OR REPLACE HYBRID TABLE ht2(\n  cola NUMBER(38,0) NOT NULL,\n  colb NUMBER(38,0) NOT NULL,\n  colc NUMBER(38,0) NOT NULL,\n  CONSTRAINT pkey_ht2 PRIMARY KEY (cola),\n  CONSTRAINT fkey_ht1 FOREIGN KEY (colb, colc) REFERENCES ht1(col1,col2));\n\nDROP TABLE ht1;",
          "SQL compilation error:\nCannot drop the table because of dependencies",
          "DROP TABLE ht1 CASCADE;"
        ]
      },
      {
        "heading": "Examples",
        "description": "\nDrop a table:\nDrop the table again, but dont raise an error if the table does not exist:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nDropping hybrid tables\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW TABLES LIKE 't2%';\n\n+---------------------------------+------+---------------+-------------+-----------+------------+------------+------+-------+--------------+----------------+\n| created_on                      | name | database_name | schema_name | kind      | comment    | cluster_by | rows | bytes | owner        | retention_time |\n|---------------------------------+------+---------------+-------------+-----------+------------+------------+------+-------+--------------+----------------+\n| Tue, 17 Mar 2015 16:48:16 -0700 | T2   | TESTDB        | PUBLIC      | TABLE     |            |            |    5 | 4096  | PUBLIC       |              1 |\n+---------------------------------+------+---------------+-------------+-----------+------------+------------+------+-------+--------------+----------------+\n\nDROP TABLE t2;\n\n+--------------------------+\n| status                   |\n|--------------------------|\n| T2 successfully dropped. |\n+--------------------------+\n\nSHOW TABLES LIKE 't2%';\n\n+------------+------+---------------+-------------+------+---------+------------+------+-------+-------+----------------+\n| created_on | name | database_name | schema_name | kind | comment | cluster_by | rows | bytes | owner | retention_time |\n|------------+------+---------------+-------------+------+---------+------------+------+-------+-------+----------------|\n+------------+------+---------------+-------------+------+---------+------------+------+-------+-------+----------------+",
          "DROP TABLE IF EXISTS t2;\n\n+------------------------------------------------------------+\n| status                                                     |\n|------------------------------------------------------------|\n| Drop statement executed successfully (T2 already dropped). |\n+------------------------------------------------------------+"
        ]
      }
    ]
  },
  {
    "category": "UNDROP TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/undrop-table",
    "details": [
      {
        "heading": "UNDROP TABLE",
        "description": "\nRestores the most recent version of a dropped table.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE TABLE , ALTER TABLE , DROP TABLE , SHOW TABLES , DESCRIBE TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "UNDROP TABLE <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the table to restore. If the identifier contains spaces or special characters, the entire string must\nbe enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTables can only be restored to the database and schema that contained the table at the time of deletion. For example, if you\ncreate and drop table t1 in schema s1, then change the current schema to s2 and attempt to restore table t1\nby ID (or qualified name, s1.t1), table t1 is restored in schema s1 rather than in the current schema, s2.\nIf a table with the same name already exists, an error is returned.\nIf you have multiple dropped tables with the same name, you can use the IDENTIFIER keyword\nwith the system-generated identifier (from the TABLES view) to specify which table to restore.\nThe name of the restored table remains the same. See Examples.\n\nNote\nYou can only use the system-generated identifier with the IDENTIFIER() keyword when executing the UNDROP command for notebooks, tables,\nschemas, and databases.\nTables can only be restored to the database and schema that contained the table at the time of deletion. For example, if you\ncreate and drop table t1 in schema s1, then change the current schema to s2 and attempt to restore table t1\nby ID (or qualified name, s1.t1), table t1 is restored in schema s1 rather than in the current schema, s2.\nIf a table with the same name already exists, an error is returned.\nIf you have multiple dropped tables with the same name, you can use the IDENTIFIER keyword\nwith the system-generated identifier (from the TABLES view) to specify which table to restore.\nThe name of the restored table remains the same. See Examples.\nNote\nYou can only use the system-generated identifier with the IDENTIFIER() keyword when executing the UNDROP command for notebooks, tables,\nschemas, and databases.\nUNDROP relies on the Snowflake Time Travel feature. An object can be restored only if\nthe object was deleted within the Data retention period. The default value is 24 hours.\nUNDROP relies on the Snowflake Time Travel feature. An object can be restored only if\nthe object was deleted within the Data retention period. The default value is 24 hours.\nYou cannot undrop a hybrid table.\nYou cannot undrop a hybrid table."
      },
      {
        "heading": "Examples"
      },
      {
        "heading": "Basic example",
        "description": "\nRestore the most recent version of a dropped table (this example builds on the examples provided for DROP TABLE):",
        "syntax": [
          "UNDROP TABLE t2;",
          "+---------------------------------+\n| status                          |\n|---------------------------------|\n| Table T2 successfully restored. |\n+---------------------------------+"
        ]
      },
      {
        "heading": "UNDROP table using the table ID",
        "description": "\nRestore a dropped table by ID using IDENTIFIER(). You can find the table ID of the specific table to undrop using the table_id\ncolumn in the TABLES view. For example, if you have multiple dropped tables named my_table, and\nyou want to restore the second-to-last dropped table my_table, follow these steps:\nFind the table ID of the dropped table in the Account Usage TABLES view:\nSELECT table_id,\n  table_name,\n  table_schema,\n  table_catalog,\n  created,\n  deleted,\n  comment\nFROM SNOWFLAKE.ACCOUNT_USAGE.TABLES\nWHERE table_catalog = 'DB1'\nAND table_schema = 'S1'\nAND table_name = 'MY_TABLE'\nAND deleted IS NOT NULL\nORDER BY deleted;\n\nCopy\n+----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------+\n| TABLE_ID | TABLE_NAME | TABLE_SCHEMA | TABLE_CATALOG | CREATED                       | DELETED                       | COMMENT |\n|----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------|\n|   408578 | MY_TABLE   | S1           | DB1           | 2024-07-01 15:39:07.565 -0700 | 2024-07-01 15:40:28.161 -0700 | NULL    |\n+----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------+\n|   408607 | MY_TABLE   | S1           | DB1           | 2024-07-01 17:43:07.565 -0700 | 2024-07-01 17:44:28.161 -0700 | NULL    |\n+----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------+\nUndrop my_table by table ID. To restore the second-to-last deleted table, use table ID 408578 from the output of the previous\nstatement. After you execute the following statement, the table is restored with its original name, my_table:\nUNDROP TABLE IDENTIFIER(408578);\n\nCopy\nFind the table ID of the dropped table in the Account Usage TABLES view:\nUndrop my_table by table ID. To restore the second-to-last deleted table, use table ID 408578 from the output of the previous\nstatement. After you execute the following statement, the table is restored with its original name, my_table:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nBasic example\nUNDROP table using the table ID\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SELECT table_id,\n  table_name,\n  table_schema,\n  table_catalog,\n  created,\n  deleted,\n  comment\nFROM SNOWFLAKE.ACCOUNT_USAGE.TABLES\nWHERE table_catalog = 'DB1'\nAND table_schema = 'S1'\nAND table_name = 'MY_TABLE'\nAND deleted IS NOT NULL\nORDER BY deleted;",
          "+----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------+\n| TABLE_ID | TABLE_NAME | TABLE_SCHEMA | TABLE_CATALOG | CREATED                       | DELETED                       | COMMENT |\n|----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------|\n|   408578 | MY_TABLE   | S1           | DB1           | 2024-07-01 15:39:07.565 -0700 | 2024-07-01 15:40:28.161 -0700 | NULL    |\n+----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------+\n|   408607 | MY_TABLE   | S1           | DB1           | 2024-07-01 17:43:07.565 -0700 | 2024-07-01 17:44:28.161 -0700 | NULL    |\n+----------+------------+--------------+---------------+-------------------------------+-------------------------------+---------+",
          "UNDROP TABLE IDENTIFIER(408578);"
        ]
      }
    ]
  },
  {
    "category": "SHOW TABLES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-tables",
    "details": [
      {
        "heading": "SHOW TABLES",
        "description": "\nLists the tables for which you have access privileges, including dropped tables that are still within the Time Travel retention period\nand, therefore, can be undropped. The command can be used to list tables for the current/specified database or schema, or across your\nentire account.\nThe output returns table metadata and properties, ordered lexicographically by database, schema, and table name (see Output in this\ntopic for descriptions of the output columns). This is important to note if you want to filter the results using the provided filters.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE TABLE , DROP TABLE , UNDROP TABLE , ALTER TABLE , DESCRIBE TABLE TABLES view (Information Schema)"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] TABLES [ HISTORY ] [ LIKE '<pattern>' ]\n                                  [ IN\n                                        {\n                                          ACCOUNT                                         |\n\n                                          DATABASE                                        |\n                                          DATABASE <database_name>                        |\n\n                                          SCHEMA                                          |\n                                          SCHEMA <schema_name>                            |\n                                          <schema_name>\n\n                                          APPLICATION <application_name>                  |\n                                          APPLICATION PACKAGE <application_package_name>  |\n                                        }\n                                  ]\n                                  [ STARTS WITH '<name_string>' ]\n                                  [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Optionally returns only a subset of the output columns: created_on name kind The kind column value is always TABLE. database_name schema_name Default: No value (all columns are included in the output)"
          },
          {
            "term": "HISTORY",
            "definition": "Optionally includes dropped tables that have not yet been purged (i.e. they are still within their respective Time Travel retention\nperiods). If multiple versions of a dropped table exist, the output displays a row for each version. The output also includes an\nadditional dropped_on column, which displays: Date and timestamp (for dropped tables). NULL (for active tables). Default: No value (dropped tables are not included in the output)"
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the table was created."
              ],
              [
                "name",
                "Name of the table."
              ],
              [
                "database_name",
                "Database in which the table is stored."
              ],
              [
                "schema_name",
                "Schema in which the table is stored."
              ],
              [
                "kind",
                "Table type: TABLE (for permanent tables), TEMPORARY, or TRANSIENT."
              ],
              [
                "comment",
                "Comment for the table."
              ],
              [
                "cluster_by",
                "Column(s) defined as clustering key(s) for the table."
              ],
              [
                "rows",
                "Number of rows in the table. Returns NULL for external tables."
              ],
              [
                "bytes",
                "Number of bytes that will be scanned if the entire table is scanned in a query. Note that this number may be different than the number of actual physical bytes (i.e. bytes stored on-disk) for the table."
              ],
              [
                "owner",
                "Role that owns the table."
              ],
              [
                "retention_time",
                "Number of days that modified and deleted data is retained for Time Travel."
              ],
              [
                "dropped_on",
                "Date and time when the table was dropped; NULL if the table is active. This column is only displayed when the HISTORY keyword is specified for the command."
              ],
              [
                "automatic_clustering",
                "If Automatic Clustering is enabled for your account, specifies whether it is explicitly enabled (ON) or disabled (OFF) for the table. This column is not displayed if Automatic Clustering is not enabled for your account."
              ],
              [
                "change_tracking",
                "If ON, change tracking is enabled. You can query this change tracking data using streams or the CHANGES clause for SELECT statements. If OFF, change tracking is currently disabled but could be enabled."
              ],
              [
                "search_optimization",
                "If ON, the table has the search optimization service enabled. Otherwise, the value is OFF."
              ],
              [
                "search_optimization_progress",
                "Percentage of the table that has been optimized for search. This value increases when optimization is first added to a table and when maintenance is done on the search optimization service. Before you measure the performance improvement of search optimization on a newly-optimized table, wait until this shows that the table has been fully optimized."
              ],
              [
                "search_optimization_bytes",
                "Number of additional bytes of storage that the search optimization service consumes for this table."
              ],
              [
                "is_external",
                "Y if it is an external table; N otherwise."
              ],
              [
                "enable_schema_evolution",
                "Y if the table has schema evolution enabled; N otherwise. You can enable automatic table schema evolution by using the CREATE TABLE or ALTER TABLE commands."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ],
              [
                "is_event",
                "Y if it is an event table; N otherwise."
              ],
              [
                "is_hybrid",
                "Y if it is a hybrid table; N otherwise."
              ],
              [
                "is_iceberg",
                "Y if the table is an Apache Icebergâ„¢ table; N otherwise."
              ],
              [
                "is_immutable",
                "Y if the table was created with the READ ONLY property; N otherwise."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nIf an account (or database or schema) has a large number of tables, then searching the entire account (or table or schema)\ncan consume a significant amount of compute resources.\nIn the output, results are sorted by database name, schema name, and then table name. This means results for a database\ncan contain tables from multiple schemas and might break pagination. In order for pagination to work as expected, you\nmust execute the SHOW TABLES command for a single schema. You can use the IN SCHEMA schema_name parameter to\nthe SHOW TABLES command. Alternatively, you can use the schema in the current context by executing a USE SCHEMA command\nbefore executing a SHOW TABLES command.\nIf an account (or database or schema) has a large number of tables, then searching the entire account (or table or schema)\ncan consume a significant amount of compute resources.\nIn the output, results are sorted by database name, schema name, and then table name. This means results for a database\ncan contain tables from multiple schemas and might break pagination. In order for pagination to work as expected, you\nmust execute the SHOW TABLES command for a single schema. You can use the IN SCHEMA schema_name parameter to\nthe SHOW TABLES command. Alternatively, you can use the schema in the current context by executing a USE SCHEMA command\nbefore executing a SHOW TABLES command.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nThese examples show all of the tables that you have privileges to view based on the specified parameters.\nRun SHOW TABLES on tables in the Sample Data Sets. The examples use the TERSE parameter to limit the output.\nShow all the tables with a name that starts with LINE in the tpch_sf1 schema:\nShow all of the tables with a name that includes the substring PART in the tpch_sf1 schema:\nShow the tables in the tpch_sf1 schema, but limit the output to three rows, and start with the table\nnames that begin with J:\nShow a dropped table using the HISTORY parameter.\nCreate a table in your current schema, then drop it:\nUse the HISTORY parameter to include dropped tables in the command output:\nIn the output, the dropped_on column shows the date and time when the table was dropped.\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW TERSE TABLES IN tpch_sf1 STARTS WITH 'LINE';",
          "+-------------------------------+----------+-------+-----------------------+-------------+\n| created_on                    | name     | kind  | database_name         | schema_name |\n|-------------------------------+----------+-------+-----------------------+-------------|\n| 2016-07-08 13:41:59.960 -0700 | LINEITEM | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n+-------------------------------+----------+-------+-----------------------+-------------+",
          "SHOW TERSE TABLES LIKE '%PART%' IN tpch_sf1;",
          "+-------------------------------+-----------+-------+-----------------------+-------------+\n| created_on                    | name      | kind  | database_name         | schema_name |\n|-------------------------------+-----------+-------+-----------------------+-------------|\n| 2016-07-08 13:41:59.960 -0700 | JPART     | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | JPARTSUPP | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | PART      | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | PARTSUPP  | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n+-------------------------------+-----------+-------+-----------------------+-------------+",
          "SHOW TERSE TABLES IN tpch_sf1 LIMIT 3 FROM 'J';",
          "+-------------------------------+-----------+-------+-----------------------+-------------+\n| created_on                    | name      | kind  | database_name         | schema_name |\n|-------------------------------+-----------+-------+-----------------------+-------------|\n| 2016-07-08 13:41:59.960 -0700 | JCUSTOMER | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | JLINEITEM | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n| 2016-07-08 13:41:59.960 -0700 | JNATION   | TABLE | SNOWFLAKE_SAMPLE_DATA | TPCH_SF1    |\n+-------------------------------+-----------+-------+-----------------------+-------------+",
          "CREATE OR REPLACE TABLE test_show_tables_history(c1 NUMBER);\n\nDROP TABLE test_show_tables_history;",
          "SHOW TABLES HISTORY LIKE 'test_show_tables_history';"
        ]
      }
    ]
  },
  {
    "category": "SHOW COLUMNS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-columns",
    "details": [
      {
        "heading": "SHOW COLUMNS",
        "description": "\nLists the columns in the tables or views for which you have access privileges. This command can be used to list the columns for a\nspecified table/view/schema/database (or the current schema/database for the session), or your entire account.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DESCRIBE TABLE COLUMNS view (Information Schema)"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW COLUMNS [ LIKE '<pattern>' ]\n             [ IN { ACCOUNT | DATABASE [ <database_name> ] | SCHEMA [ <schema_name> ] | TABLE | [ TABLE ] <table_name> | VIEW | [ VIEW ] <view_name> } | APPLICATION <application_name> | APPLICATION PACKAGE <application_package_name> ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE '<pattern>'",
            "definition": "Filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL wildcard\ncharacters (% and _). For example, the following patterns return the same results:"
          },
          {
            "term": "IN { ACCOUNT | DATABASE [ <database_name> ] | SCHEMA [ <schema_name> ] | TABLE | [ TABLE ] <table_name> | VIEW | [ VIEW ] <view_name> | APPLICATION <application_name> | APPLICATION PACKAGE <application_package_name> }",
            "definition": "Specifies the scope of the command, which determines whether the command lists records only for the current/specified database,\nschema, table, or view, or across your entire account: If you specify the keyword ACCOUNT, then the command retrieves records for all schemas in all databases\nof the current account. If you specify the keyword DATABASE, then: If you specify a db_name, then the command retrieves records for all schemas of the specified database. If you do not specify a db_name, then: If there is a current database, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and schemas in the account. If you specify the keyword SCHEMA, then: If you specify a qualified schema name (e.g. my_database.my_schema), then the command\nretrieves records for the specified database and schema. If you specify an unqualified schema_name, then: If there is a current database, then the command retrieves records for the specified schema in the current database. If there is no current database, then the command displays the error\nSQL compilation error: Object does not exist, or operation cannot be performed. If you do not specify a schema_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. If you specify the keyword TABLE without a table_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. If you specify a <table_name> (with or without the keyword TABLE), then: If you specify a fully-qualified <table_name> (e.g. my_database_name.my_schema_name.my_table_name),\nthen the command retrieves all records for the specified table. If you specify a schema-qualified <table_name> (e.g. my_schema_name.my_table_name), then: If a current database exists, then the command retrieves all records for the specified table. If no current database exists, then the command displays an error similar to\nCannot perform SHOW <object_type>. This session does not have a current database.... If you specify an unqualified <table_name>, then: If a current database and current schema exist, then the command retrieves records for the specified table in the current\nschema of the current database. If no current database exists or no current schema exists, then the command displays an error similar to:\nSQL compilation error: <object> does not exist or not authorized.. If you specify the keyword VIEW or a view name, the rules for views parallel the rules for tables. If you specify the keywords APPLICATION or APPLICATION PACKAGE, records for the specified Snowflake Native App Framework application or application package are returned. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (i.e. the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (i.e. the command returns the objects you have privileges to view in your account)."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nIf you use the keyword VIEW and specify a view name, the view may be a materialized view or a non-materialized view.\nIf you use the keyword VIEW and specify a view name, the view may be a materialized view or a non-materialized view.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema."
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "table_name",
                "Name of the table the columns belong to."
              ],
              [
                "schema_name",
                "Schema for the table."
              ],
              [
                "column_name",
                "Name of the column."
              ],
              [
                "data_type",
                "Column data type and applicable properties, such as length, precision, scale, nullable, etc.; note that character and numeric columns display their generic data type rather than their defined data type (i.e. TEXT for all character types, FIXED for all fixed-point numeric types, and REAL for all floating-point numeric types)."
              ],
              [
                "null?",
                "Whether the column can contain NULL values."
              ],
              [
                "default",
                "Default value, if any, defined for the column."
              ],
              [
                "kind",
                "Not applicable for columns (always displays COLUMN as the value)."
              ],
              [
                "expression",
                ""
              ],
              [
                "comment",
                "Comment, if any, for the column."
              ],
              [
                "database_name",
                "Database for the table."
              ],
              [
                "autoincrement",
                "Auto-increment start and increment values, if any, for the column. If the column has the NOORDER property, the value includes NOORDER (for example, IDENTITY START 1 INCREMENT 1 NOORDER). Otherwise, the value includes ORDER."
              ],
              [
                "SchemaEvolutionRecord",
                "Records information about the latest triggered Schema Evolution for a given table column. This column contains the following subfields:\n\nEvolutionType: The type of the triggered schema evolution (ADD_COLUMN or DROP_NOT_NULL).\nEvolutionMode: The triggering ingestion mechanism (COPY or SNOWPIPE).\nFileName: The file name that triggered the evolution.\nTriggeringTime: The approximate time when the column was evolved.\nQueryId or PipeID: A unique identifier of the triggering query or pipe (QUERY ID for COPY or PIPE ID for SNOWPIPE)."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nOn this page\nSyntax\nParameters\nUsage notes\nOutput\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "create or replace table dt_test (n1 number default 5, n2_int integer default n1+5, n3_bigint bigint autoincrement, n4_dec decimal identity (1,10),\n                                 f1 float, f2_double double, f3_real real,\n                                 s1 string, s2_var varchar, s3_char char, s4_text text,\n                                 b1 binary, b2_var varbinary,\n                                 bool1 boolean,\n                                 d1 date,\n                                 t1 time,\n                                 ts1 timestamp, ts2_ltz timestamp_ltz, ts3_ntz timestamp_ntz, ts4_tz timestamp_tz);\n\nshow columns in table dt_test;\n\n+------------+-------------+-------------+---------------------------------------------------------------------------------------+-------+----------------+--------+------------+---------+---------------+-------------------------------+\n| table_name | schema_name | column_name | data_type                                                                             | null? | default        | kind   | expression | comment | database_name | autoincrement                 |\n|------------+-------------+-------------+---------------------------------------------------------------------------------------+-------+----------------+--------+------------+---------+---------------+-------------------------------|\n| DT_TEST    | PUBLIC      | N1          | {\"type\":\"FIXED\",\"precision\":38,\"scale\":0,\"nullable\":true}                             | true  | 5              | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | N2_INT      | {\"type\":\"FIXED\",\"precision\":38,\"scale\":0,\"nullable\":true}                             | true  | DT_TEST.N1 + 5 | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | N3_BIGINT   | {\"type\":\"FIXED\",\"precision\":38,\"scale\":0,\"nullable\":true}                             | true  |                | COLUMN |            |         | TEST1         | IDENTITY START 1 INCREMENT 1  |\n| DT_TEST    | PUBLIC      | N4_DEC      | {\"type\":\"FIXED\",\"precision\":38,\"scale\":0,\"nullable\":true}                             | true  |                | COLUMN |            |         | TEST1         | IDENTITY START 1 INCREMENT 10 |\n| DT_TEST    | PUBLIC      | F1          | {\"type\":\"REAL\",\"nullable\":true}                                                       | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | F2_DOUBLE   | {\"type\":\"REAL\",\"nullable\":true}                                                       | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | F3_REAL     | {\"type\":\"REAL\",\"nullable\":true}                                                       | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | S1          | {\"type\":\"TEXT\",\"length\":16777216,\"byteLength\":16777216,\"nullable\":true,\"fixed\":false} | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | S2_VAR      | {\"type\":\"TEXT\",\"length\":16777216,\"byteLength\":16777216,\"nullable\":true,\"fixed\":false} | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | S3_CHAR     | {\"type\":\"TEXT\",\"length\":1,\"byteLength\":4,\"nullable\":true,\"fixed\":false}               | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | S4_TEXT     | {\"type\":\"TEXT\",\"length\":16777216,\"byteLength\":16777216,\"nullable\":true,\"fixed\":false} | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | B1          | {\"type\":\"BINARY\",\"length\":8388608,\"byteLength\":8388608,\"nullable\":true,\"fixed\":true}  | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | B2_VAR      | {\"type\":\"BINARY\",\"length\":8388608,\"byteLength\":8388608,\"nullable\":true,\"fixed\":false} | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | BOOL1       | {\"type\":\"BOOLEAN\",\"nullable\":true}                                                    | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | D1          | {\"type\":\"DATE\",\"nullable\":true}                                                       | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | T1          | {\"type\":\"TIME\",\"precision\":0,\"scale\":9,\"nullable\":true}                               | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | TS1         | {\"type\":\"TIMESTAMP_LTZ\",\"precision\":0,\"scale\":9,\"nullable\":true}                      | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | TS2_LTZ     | {\"type\":\"TIMESTAMP_LTZ\",\"precision\":0,\"scale\":9,\"nullable\":true}                      | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | TS3_NTZ     | {\"type\":\"TIMESTAMP_NTZ\",\"precision\":0,\"scale\":9,\"nullable\":true}                      | true  |                | COLUMN |            |         | TEST1         |                               |\n| DT_TEST    | PUBLIC      | TS4_TZ      | {\"type\":\"TIMESTAMP_TZ\",\"precision\":0,\"scale\":9,\"nullable\":true}                       | true  |                | COLUMN |            |         | TEST1         |                               |\n+------------+-------------+-------------+---------------------------------------------------------------------------------------+-------+----------------+--------+------------+---------+---------------+-------------------------------+"
        ]
      }
    ]
  },
  {
    "category": "SHOW PRIMARY KEYS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-primary-keys",
    "details": [
      {
        "heading": "SHOW PRIMARY KEYS",
        "description": "\nLists primary keys for one or more tables. You can specify the following options:\nA single table\nAll tables in the current or specified schema\nAll tables in the current or specified database\nAll tables in the current account\nA single table\nAll tables in the current or specified schema\nAll tables in the current or specified database\nAll tables in the current account"
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] PRIMARY KEYS\n    [ IN { ACCOUNT | DATABASE [ <database_name> ] | SCHEMA [ <schema_name> ] | TABLE | [ TABLE ] <table_name> } ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "This clause is accepted in the syntax but has no effect on the output."
          },
          {
            "term": "IN { ACCOUNT | DATABASE [ <database_name> ] | SCHEMA [ <schema_name> ] | TABLE | [ TABLE ] <table_name> }",
            "definition": "Specifies the scope of the command, which determines whether the command lists records only for the current or specified database,\nschema, table, or account. If you specify the keyword ACCOUNT, then the command retrieves records for all schemas in all databases\nof the current account. If you specify the keyword DATABASE, then: If you specify a db_name, then the command retrieves records for all schemas of the specified database. If you do not specify a db_name, then: If there is a current database, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and schemas in the account. If you specify the keyword SCHEMA, then: If you specify a qualified schema name (e.g. my_database.my_schema), then the command\nretrieves records for the specified database and schema. If you specify an unqualified schema_name, then: If there is a current database, then the command retrieves records for the specified schema in the current database. If there is no current database, then the command displays the error\nSQL compilation error: Object does not exist, or operation cannot be performed. If you do not specify a schema_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. If you specify the keyword TABLE without a table_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. If you specify a <table_name> (with or without the keyword TABLE), then: If you specify a fully-qualified <table_name> (e.g. my_database_name.my_schema_name.my_table_name),\nthen the command retrieves all records for the specified table. If you specify a schema-qualified <table_name> (e.g. my_schema_name.my_table_name), then: If a current database exists, then the command retrieves all records for the specified table. If no current database exists, then the command displays an error similar to\nCannot perform SHOW <object_type>. This session does not have a current database.... If you specify an unqualified <table_name>, then: If a current database and current schema exist, then the command retrieves records for the specified table in the current\nschema of the current database. If no current database exists or no current schema exists, then the command displays an error similar to:\nSQL compilation error: <object> does not exist or not authorized.. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nFor each single-column primary key, the output contains one row.\nFor each multi-column primary key, the output contains one row for each column in the primary key.\nIf an account (or database or schema) has a large number of tables, searching the entire account (or table or schema)\ncan consume a significant amount of compute resources.\nFor each single-column primary key, the output contains one row.\nFor each multi-column primary key, the output contains one row for each column in the primary key.\nIf an account (or database or schema) has a large number of tables, searching the entire account (or table or schema)\ncan consume a significant amount of compute resources.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nImportant\nFor standard tables, Snowflake does not enforce PRIMARY KEY constraints; however, they are enforced on\nhybrid tables."
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the table was created."
              ],
              [
                "database_name",
                "Database in which the table is stored."
              ],
              [
                "schema_name",
                "Schema in which the table is stored."
              ],
              [
                "table_name",
                "Name of the table."
              ],
              [
                "column_name",
                "Name of the column in the primary key."
              ],
              [
                "key_sequence",
                "If the primary key is composed of multiple columns, the number in the key_sequence column indicates the order of those columns in the primary key. For example, if the primary key is defined as CONSTRAINT pkey1 PRIMARY KEY (column_x, column_y), the key_sequence number for column_x is 1 and the key_sequence number for column_y is 2."
              ],
              [
                "comment",
                "The comment (if any) specified for the constraint when the constraint was created."
              ],
              [
                "constraint_name",
                "The name of the constraint."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nOn this page\nSyntax\nParameters\nUsage notes\nOutput\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW PRIMARY KEYS;\n\nSHOW PRIMARY KEYS IN ACCOUNT;\n\nSHOW PRIMARY KEYS IN DATABASE;\n\nSHOW PRIMARY KEYS IN DATABASE my_database;\n\nSHOW PRIMARY KEYS IN SCHEMA;\n\nSHOW PRIMARY KEYS IN SCHEMA my_schema;\n\nSHOW PRIMARY KEYS IN SCHEMA my_database.my_schema;\n\nSHOW PRIMARY KEYS IN my_table;\n\nSHOW PRIMARY KEYS IN my_database.my_schema.my_table;"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE SEARCH OPTIMIZATION",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-search-optimization",
    "details": [
      {
        "heading": "DESCRIBE SEARCH OPTIMIZATION",
        "description": "\nDescribes the search optimization configuration for a specified table and\nits columns.\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "Search Optimization Service"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] SEARCH OPTIMIZATION ON <table_name>;"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "table_name",
            "definition": "Specifies the identifier for the table to describe. If the identifier contains spaces or special characters, the entire string\nmust be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column Name",
              "Description"
            ],
            "rows": [
              [
                "expression_id",
                "Unique identifier for a search method and target."
              ],
              [
                "method",
                "Search method for optimizing queries for a particular type of predicate:\n\nEQUALITY (for equality and IN predicates).\nSUBSTRING (for predicates that match substrings â€“ e.g. LIKE, ILIKE, etc.).\nGEO (for predicates that use GEOGRAPHY types)."
              ],
              [
                "target",
                "Column or VARIANT field that the method applies to."
              ],
              [
                "target_data_type",
                "Data type of the column or VARIANT field."
              ],
              [
                "active",
                "Specifies whether or not the expression has finished the initial build of the search access paths for the expression."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nSee Displaying the search optimization configuration for a table.\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "TRUNCATE TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/truncate-table",
    "details": [
      {
        "heading": "TRUNCATE TABLE",
        "description": "\nRemoves all rows from a table but leaves the table intact (including all privileges and constraints on the table). Also deletes the load\nmetadata for the table, which allows the same files to be loaded into the table again after the command completes.\nNote that this is different from DROP TABLE, which removes the table from the system but retains a version of the table\n(along with its load history) so that they can be recovered.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "TRUNCATE [ TABLE ] [ IF EXISTS ] <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the table to truncate. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive (e.g. \"My Object\"). If the table identifier is not fully-qualified (in the form of db_name.schema_name.table_name or\nschema_name.table_name), the command looks for the table in the current schema for the session."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nBoth DELETE and TRUNCATE TABLE maintain deleted data for recovery purposes (i.e. using Time Travel) for the data retention period.\nHowever, when a table is truncated, the load metadata cannot be recovered.\nThe TABLE keyword is optional if the table name is fully qualified or a database and schema are currently in use for the session.\nBoth DELETE and TRUNCATE TABLE maintain deleted data for recovery purposes (i.e. using Time Travel) for the data retention period.\nHowever, when a table is truncated, the load metadata cannot be recovered.\nThe TABLE keyword is optional if the table name is fully qualified or a database and schema are currently in use for the session."
      },
      {
        "heading": "Examples",
        "description": "\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "-- create a basic table\nCREATE OR REPLACE TABLE temp (i number);\n\n-- populate it with some rows\nINSERT INTO temp SELECT seq8() FROM table(generator(rowcount=>20)) v;\n\n-- verify that the rows exist\nSELECT COUNT (*) FROM temp;\n\n----------+\n count(*) |\n----------+\n 20       |\n----------+\n\n-- truncate the table\nTRUNCATE TABLE IF EXISTS temp;\n\n-- verify that the table is now empty\nSELECT COUNT (*) FROM temp;\n\n----------+\n count(*) |\n----------+\n 0        |\n----------+"
        ]
      }
    ]
  },
  {
    "category": "CREATE DYNAMIC TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-dynamic-table",
    "details": [
      {
        "heading": "CREATE DYNAMIC TABLE",
        "description": "\nCreates a dynamic table, based on a specified query.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER DYNAMIC TABLE, DESCRIBE DYNAMIC TABLE, DROP DYNAMIC TABLE , SHOW DYNAMIC TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CREATE [ OR REPLACE ] [ TRANSIENT ] DYNAMIC TABLE [ IF NOT EXISTS ] <name> (\n    -- Column definition\n    <col_name> <col_type>\n      [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n      [ [ WITH ] PROJECTION POLICY <policy_name> ]\n      [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n      [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n      [ COMMENT '<string_literal>' ]\n\n    -- Additional column definitions\n    [ , <col_name> <col_type> [ ... ] ]\n\n  )\n  TARGET_LAG = { '<num> { seconds | minutes | hours | days }' | DOWNSTREAM }\n  WAREHOUSE = <warehouse_name>\n  [ REFRESH_MODE = { AUTO | FULL | INCREMENTAL } ]\n  [ INITIALIZE = { ON_CREATE | ON_SCHEDULE } ]\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ REQUIRE USER ]\n  AS <query>"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE DYNAMIC TABLE  CLONE",
        "description": "\nCreates a new dynamic table with the same column definitions and containing all the\nexisting data from the source dynamic table, without actually copying the data. The\ncloned dynamic table inherits the sources scheduling state.\nYou can also clone a dynamic table as it existed at a specific point in the past. For\nmore information, see Cloning considerations.\nIf the source dynamic table has clustering keys, then the cloned dynamic table has\nclustering keys. By default, Automatic Clustering is suspended for the new table, even\nif Automatic Clustering was not suspended for the source table.\nFor more details about cloning, see CREATE <object>  CLONE.",
        "syntax": [
          "CREATE [ OR REPLACE ] [ TRANSIENT ] DYNAMIC TABLE <name>\n  CLONE <source_dynamic_table>\n        [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n  [\n    COPY GRANTS\n    TARGET_LAG = { '<num> { seconds | minutes | hours | days }' | DOWNSTREAM }\n    WAREHOUSE = <warehouse_name>\n  ]"
        ]
      },
      {
        "heading": "CREATE DYNAMIC ICEBERG TABLE",
        "description": "\nCreates a new dynamic Apache Iceberg table. For information about Iceberg tables, see\nApache Iceberg tables and CREATE ICEBERG TABLE (Snowflake as the Iceberg catalog).\nFor more information about usage and limitations, see\nCreate dynamic Apache Iceberg tables.",
        "syntax": [
          "CREATE [ OR REPLACE ] DYNAMIC ICEBERG TABLE <name> (\n  -- Column definition\n  <col_name> <col_type>\n    [ [ WITH ] MASKING POLICY <policy_name> [ USING ( <col_name> , <cond_col1> , ... ) ] ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n    [ COMMENT '<string_literal>' ]\n\n  -- Additional column definitions\n  [ , <col_name> <col_type> [ ... ] ]\n\n)\nTARGET_LAG = { '<num> { seconds | minutes | hours | days }' | DOWNSTREAM }\nWAREHOUSE = <warehouse_name>\n[ EXTERNAL_VOLUME = '<external_volume_name>' ]\n[ CATALOG = 'SNOWFLAKE' ]\n[ BASE_LOCATION = '<optional_directory_for_table_files>' ]\n[ REFRESH_MODE = { AUTO | FULL | INCREMENTAL } ]\n[ INITIALIZE = { ON_CREATE | ON_SCHEDULE } ]\n[ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n[ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n[ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n[ COMMENT = '<string_literal>' ]\n[ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n[ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n[ REQUIRE USER ]\nAS <query>"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier (i.e. name) for the dynamic table; must be unique for the schema in which the dynamic table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements."
          },
          {
            "term": "TARGET_LAG = { num { seconds | minutes | hours | days } | DOWNSTREAM }",
            "definition": "Specifies the lag for the dynamic table: Specifies the maximum amount of time that the dynamic tables content should lag behind updates to the source tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. Must be a minimum of 60 seconds. If the dynamic table depends on another dynamic table, the minimum target lag must\nbe greater than or equal to the target lag of the dynamic table it depends on. Specifies that the dynamic table should be refreshed only when dynamic tables that depend on it are refreshed."
          },
          {
            "term": "'num seconds | minutes | hours | days'",
            "definition": "Specifies the maximum amount of time that the dynamic tables content should lag behind updates to the source tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. Must be a minimum of 60 seconds. If the dynamic table depends on another dynamic table, the minimum target lag must\nbe greater than or equal to the target lag of the dynamic table it depends on."
          },
          {
            "term": "DOWNSTREAM",
            "definition": "Specifies that the dynamic table should be refreshed only when dynamic tables that depend on it are refreshed."
          },
          {
            "term": "WAREHOUSE = warehouse_name",
            "definition": "Specifies the name of the warehouse that provides the compute resources for refreshing the dynamic table. You must use a role that has the USAGE privilege on this warehouse in order to create the dynamic table. For limitations and more\ninformation, see Privileges to create a dynamic table."
          },
          {
            "term": "AS query",
            "definition": "Specifies the query whose results the dynamic table should contain."
          },
          {
            "term": "'num seconds | minutes | hours | days'",
            "definition": "Specifies the maximum amount of time that the dynamic tables content should lag behind updates to the source tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. Must be a minimum of 60 seconds. If the dynamic table depends on another dynamic table, the minimum target lag must\nbe greater than or equal to the target lag of the dynamic table it depends on."
          },
          {
            "term": "DOWNSTREAM",
            "definition": "Specifies that the dynamic table should be refreshed only when dynamic tables that depend on it are refreshed."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "syntax": [
          "Dynamic Table is not initialized. Please run a manual refresh or wait for a scheduled refresh before querying.",
          "CREATE DYNAMIC TABLE my_dynamic_table (pre_tax_profit, taxes, after_tax_profit)\n  TARGET_LAG = '20 minutes'\n    WAREHOUSE = mywh\n    AS\n      SELECT revenue - cost, (revenue - cost) * tax_rate, (revenue - cost) * (1.0 - tax_rate)\n      FROM staging_table;",
          "CREATE DYNAMIC TABLE my_dynamic_table (pre_tax_profit COMMENT 'revenue minus cost',\n                taxes COMMENT 'assumes taxes are a fixed percentage of profit',\n                after_tax_profit)\n  TARGET_LAG = '20 minutes'\n    WAREHOUSE = mywh\n    AS\n      SELECT revenue - cost, (revenue - cost) * tax_rate, (revenue - cost) * (1.0 - tax_rate)\n      FROM staging_table;",
          "CREATE OR REPLACE DYNAMIC TABLE my_dynamic_table\n  TARGET_LAG = DOWNSTREAM\n  WAREHOUSE = mywh\n  AS\n    SELECT * FROM staging_table\n    COPY GRANTS;"
        ],
        "definitions": [
          {
            "term": "TRANSIENT",
            "definition": "Specifies that the table is transient. Like permanent dynamic tables, transient dynamic tables exist until\ntheyre explicitly dropped, and are available to any user with the appropriate privileges. Transient dynamic\ntables dont retain data in fail-safe storage, which helps reduce storage costs, especially for tables that\nrefresh frequently. Due to this reduced level of durability, transient dynamic tables are best used for\ntransitory data that doesnt need the same level of data protection and recovery provided by permanent tables. Default: No value. If a dynamic table is not declared as TRANSIENT, it is permanent."
          },
          {
            "term": "REFRESH_MODE = { AUTO | FULL | INCREMENTAL }",
            "definition": "Specifies the refresh mode for the dynamic table. This property cannot be altered after you create the dynamic table. To modify the property, recreate the dynamic table with a CREATE OR\nREPLACE DYNAMIC TABLE command. When refresh mode is AUTO, the system attempts to apply an incremental refresh by default. However, when incremental refresh isnt\nsupported or expected to perform well, the dynamic table automatically selects full refresh instead. For more information, see\nDynamic table refresh modes and Best practices for choosing dynamic table refresh modes. To determine the best mode for your use case, experiment with refresh modes and automatic recommendations. For consistent behavior across\nSnowflake releases, explicitly set the refresh mode on all dynamic tables. To verify the refresh mode for your dynamic tables, see View dynamic table refresh mode. Enforces a full refresh of the dynamic table, even if the dynamic table can be incrementally refreshed. Enforces an incremental refresh of the dynamic table. If the query that underlies the dynamic table cant perform an incremental refresh,\ndynamic table creation fails and displays an error message. Default: AUTO"
          },
          {
            "term": "AUTO",
            "definition": "When refresh mode is AUTO, the system attempts to apply an incremental refresh by default. However, when incremental refresh isnt\nsupported or expected to perform well, the dynamic table automatically selects full refresh instead. For more information, see\nDynamic table refresh modes and Best practices for choosing dynamic table refresh modes. To determine the best mode for your use case, experiment with refresh modes and automatic recommendations. For consistent behavior across\nSnowflake releases, explicitly set the refresh mode on all dynamic tables. To verify the refresh mode for your dynamic tables, see View dynamic table refresh mode."
          },
          {
            "term": "FULL",
            "definition": "Enforces a full refresh of the dynamic table, even if the dynamic table can be incrementally refreshed."
          },
          {
            "term": "INCREMENTAL",
            "definition": "Enforces an incremental refresh of the dynamic table. If the query that underlies the dynamic table cant perform an incremental refresh,\ndynamic table creation fails and displays an error message."
          },
          {
            "term": "AUTO",
            "definition": "When refresh mode is AUTO, the system attempts to apply an incremental refresh by default. However, when incremental refresh isnt\nsupported or expected to perform well, the dynamic table automatically selects full refresh instead. For more information, see\nDynamic table refresh modes and Best practices for choosing dynamic table refresh modes. To determine the best mode for your use case, experiment with refresh modes and automatic recommendations. For consistent behavior across\nSnowflake releases, explicitly set the refresh mode on all dynamic tables. To verify the refresh mode for your dynamic tables, see View dynamic table refresh mode."
          },
          {
            "term": "FULL",
            "definition": "Enforces a full refresh of the dynamic table, even if the dynamic table can be incrementally refreshed."
          },
          {
            "term": "INCREMENTAL",
            "definition": "Enforces an incremental refresh of the dynamic table. If the query that underlies the dynamic table cant perform an incremental refresh,\ndynamic table creation fails and displays an error message."
          },
          {
            "term": "INITIALIZE",
            "definition": "Specifies the behavior of the initial refresh of the dynamic table. This property cannot be\naltered after you create the dynamic table. To modify the property, replace the dynamic table with a CREATE OR REPLACE DYNAMIC TABLE command. Refreshes the dynamic table synchronously at creation. If this refresh fails, dynamic table creation fails and displays an error message. Refreshes the dynamic table at the next scheduled refresh. The dynamic table is populated when the refresh schedule process runs. No data is populated when the dynamic table is created. If you try to\nquery the table using SELECT * FROM DYNAMIC TABLE, you might see the following error because the first scheduled refresh has not yet\noccurred. Default: ON_CREATE"
          },
          {
            "term": "ON_CREATE",
            "definition": "Refreshes the dynamic table synchronously at creation. If this refresh fails, dynamic table creation fails and displays an error message."
          },
          {
            "term": "ON_SCHEDULE",
            "definition": "Refreshes the dynamic table at the next scheduled refresh. The dynamic table is populated when the refresh schedule process runs. No data is populated when the dynamic table is created. If you try to\nquery the table using SELECT * FROM DYNAMIC TABLE, you might see the following error because the first scheduled refresh has not yet\noccurred."
          },
          {
            "term": "COMMENT 'string_literal'",
            "definition": "Specifies a comment for the column. (Note that comments can be specified at the column level or the table level. The syntax for each is slightly different.)"
          },
          {
            "term": "MASKING POLICY = policy_name",
            "definition": "Specifies the masking policy to set on a column."
          },
          {
            "term": "PROJECTION POLICY policy_name",
            "definition": "Specifies the projection policy to set on a column."
          },
          {
            "term": "ON_CREATE",
            "definition": "Refreshes the dynamic table synchronously at creation. If this refresh fails, dynamic table creation fails and displays an error message."
          },
          {
            "term": "ON_SCHEDULE",
            "definition": "Refreshes the dynamic table at the next scheduled refresh. The dynamic table is populated when the refresh schedule process runs. No data is populated when the dynamic table is created. If you try to\nquery the table using SELECT * FROM DYNAMIC TABLE, you might see the following error because the first scheduled refresh has not yet\noccurred."
          },
          {
            "term": "column_list",
            "definition": "If you want to change the name of a column or add a comment to a column in the dynamic table,\ninclude a column list that specifies the column names and, if needed, comments about\nthe columns. You do not need to specify the data types of the columns. If any of the columns in the dynamic table are based on expressions - for example, not simple column names -\nthen you must supply a column name for each column in the dynamic table. For instance, the column names are\nrequired in the following case: You can specify an optional comment for each column. For example:"
          },
          {
            "term": "WITH CONTACT ( purpose = contact [ , purpose = contact ...] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the new object with one or more contacts."
          },
          {
            "term": "CLUSTER BY ( expr [ , expr , ... ] )",
            "definition": "Specifies one or more columns or column expressions in the dynamic table as the clustering key. Before you specify a clustering\nkey for a dynamic table, you should understand micro-partitions. For more information, see Understanding Snowflake Table Structures. Note the following when using clustering keys with dynamic tables: Column definitions are required and must be explicitly specified in the statement. By default, Automatic Clustering is not suspended for the new dynamic table, even if Automatic Clustering is suspended for the\nsource table. Clustering keys are not intended or recommended for all tables; they typically benefit very large (for example\nmulti-terabyte) tables. Specifying CLUSTER BY doesnt cluster the data at creation time; instead, CLUSTER BY relies on\nAutomatic Clustering to recluster the data over time. For more information, see Clustering Keys & Clustered Tables. Default: No value (no clustering key is defined for the table)"
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Specifies the retention period for the dynamic table so that Time Travel actions (SELECT, CLONE) can be performed on historical\ndata in the dynamic table. Time Travel behaves the same way for dynamic tables as it behaves for traditional tables. For more\ninformation, see Understanding & using Time Travel. For a detailed description of this object-level parameter, as well as more information about object parameters, see\nParameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables 0 or 1 for temporary and transient tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "An object parameter that sets the maximum number of days Snowflake can extend the data retention period to prevent streams on the dynamic\ntable from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment for the dynamic table. (Note that comments can be specified at the column level or the table level. The syntax for each is slightly different.) Default: No value."
          },
          {
            "term": "COPY GRANTS",
            "definition": "Specifies to retain the access privileges from the original dynamic table when a new dynamic table is created using the\nCREATE DYNAMIC TABLE  CLONE variant. This parameter copies all privileges, except OWNERSHIP, from the existing dynamic table to the new dynamic table. The new dynamic\ntable does not inherit any future grants defined for the object type in the schema. By default, the role that executes the\nCREATE DYNAMIC TABLE statement owns the new dynamic table. If this parameter is not included in the CREATE DYNAMIC TABLE statement, then the new table does not inherit any explicit access\nprivileges granted on the original dynamic table, but does inherit any future grants defined for the object type in the schema. Note: With data sharing: If the existing dynamic table was shared to another account, the replacement dynamic table is also shared. If the existing dynamic table was shared with your account as a data consumer, and access was further granted to other roles in\nthe account (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement dynamic\ntable. The SHOW GRANTS output for the replacement dynamic table lists the grantee for the copied privileges as the\nrole that executed the CREATE TABLE statement, with the current timestamp when the statement was executed. The SHOW GRANTS output for the replacement dynamic table lists the grantee for the copied privileges as the\nrole that executed the CREATE TABLE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE DYNAMIC TABLE command (i.e. within the same transaction). Important The COPY GRANTS parameter can be placed anywhere in a CREATE [ OR REPLACE ] DYNAMIC TABLE command, except after the query\ndefinition. For example, the following dynamic table will fail to create:"
          },
          {
            "term": "ROW ACCESS POLICY policy_name ON ( col_name [ , col_name ... ] )",
            "definition": "Specifies the row access policy to set on a dynamic table."
          },
          {
            "term": "TAG ( tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ] )",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "AGGREGATION POLICY policy_name [ ENTITY KEY ( col_name [ , col_name ... ] ) ]",
            "definition": "Specifies an aggregation policy to set on a dynamic table. You can apply one or more aggregation\npolicies on a table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For more information,\nsee Implementing entity-level privacy with aggregation policies. You can specify one or more entity keys for an aggregation policy."
          },
          {
            "term": "REQUIRE USER",
            "definition": "When specified, the dynamic table cannot run unless a user is specified. The dynamic table is not able to refresh unless a user is set\nin a manual refresh with the COPY SESSION parameter specified. If this option is enabled, the dynamic table must be created with the ON_SCHEDULE parameter for\nINITIALIZE."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE DYNAMIC TABLE",
                "Schema in which you plan to create the dynamic table.",
                ""
              ],
              [
                "SELECT",
                "Tables, views, and dynamic tables that you plan to query for the new dynamic table.",
                ""
              ],
              [
                "USAGE",
                "Warehouse that you plan to use to refresh the table.",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nWhen you execute the CREATE DYNAMIC TABLE command, the current role in use becomes\nthe owner of the dynamic table. This role is used to perform refreshes of the dynamic\ntable in the background.\nYou cannot make changes to the schema after you create a dynamic table.\nDynamic tables are updated as underlying database objects change. Change tracking must\nbe enabled on all underlying objects used by a dynamic table. See\nEnable change tracking.\nIf you want to replace an existing dynamic table and need to see its current definition,\ncall the GET_DDL function.\nUsing ORDER BY in the definition of a dynamic table\nmight produce results sorted in an unexpected order. You can use ORDER BY when querying\nyour dynamic table to ensure that rows selected return in a specific order.\nSnowflake doesnt support using ORDER BY to create a view that selects from a dynamic\ntable.\nWhen you execute the CREATE DYNAMIC TABLE command, the current role in use becomes\nthe owner of the dynamic table. This role is used to perform refreshes of the dynamic\ntable in the background.\nYou cannot make changes to the schema after you create a dynamic table.\nDynamic tables are updated as underlying database objects change. Change tracking must\nbe enabled on all underlying objects used by a dynamic table. See\nEnable change tracking.\nIf you want to replace an existing dynamic table and need to see its current definition,\ncall the GET_DDL function.\nUsing ORDER BY in the definition of a dynamic table\nmight produce results sorted in an unexpected order. You can use ORDER BY when querying\nyour dynamic table to ensure that rows selected return in a specific order.\nSnowflake doesnt support using ORDER BY to create a view that selects from a dynamic\ntable.\nSome expressions, clauses, and functions are not currently supported in dynamic tables.\nFor a complete list, see Dynamic table limitations.\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nSome expressions, clauses, and functions are not currently supported in dynamic tables.\nFor a complete list, see Dynamic table limitations.\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
      },
      {
        "heading": "Examples",
        "description": "\nCreate a dynamic table named my_dynamic_table:\nIn the example above:\nThe dynamic table materializes the results of a query of the product_id and product_name columns of the\nstaging_table table.\nThe target lag time is 20 minutes, which means that the data in the dynamic table should ideally be no more than 20 minutes\nolder than the data in staging_table.\nThe automated refresh process uses the compute resources in warehouse mywh to refresh the data in the dynamic table.\nThe dynamic table materializes the results of a query of the product_id and product_name columns of the\nstaging_table table.\nThe target lag time is 20 minutes, which means that the data in the dynamic table should ideally be no more than 20 minutes\nolder than the data in staging_table.\nThe automated refresh process uses the compute resources in warehouse mywh to refresh the data in the dynamic table.\nCreate a dynamic Iceberg table named my_dynamic_table that reads from my_iceberg_table:\nCreate a dynamic table with a multi-column clustering key:\nClone a dynamic table as it existed exactly at the date and time of the specified timestamp:\nConfigure a dynamic table to require a user for refreshes and then refresh the dynamic table:\nOn this page\nSyntax\nVariant syntax\nRequired parameters\nOptional parameters\nAccess control requirements\nUsage notes\nExamples\nRelated content\nDynamic tables\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR REPLACE DYNAMIC TABLE my_dynamic_table\n  TARGET_LAG = '20 minutes'\n  WAREHOUSE = mywh\n  AS\n    SELECT product_id, product_name FROM staging_table;",
          "CREATE DYNAMIC ICEBERG TABLE my_dynamic_table (date TIMESTAMP_NTZ, id NUMBER, content STRING)\n  TARGET_LAG = '20 minutes'\n  WAREHOUSE = mywh\n  EXTERNAL_VOLUME = 'my_external_volume'\n  CATALOG = 'SNOWFLAKE'\n  BASE_LOCATION = 'my_iceberg_table'\n  AS\n    SELECT product_id, product_name FROM staging_table;",
          "CREATE DYNAMIC TABLE my_dynamic_table (date TIMESTAMP_NTZ, id NUMBER, content VARIANT)\n  TARGET_LAG = '20 minutes'\n  WAREHOUSE = mywh\n  CLUSTER BY (date, id)\n  AS\n    SELECT product_id, product_name FROM staging_table;",
          "CREATE DYNAMIC TABLE my_cloned_dynamic_table CLONE my_dynamic_table AT (TIMESTAMP => TO_TIMESTAMP_TZ('04/05/2013 01:02:03', 'mm/dd/yyyy hh24:mi:ss'));",
          "CREATE DYNAMIC TABLE my_dynamic_table\n  TARGET_LAG = 'DOWNSTREAM'\n  WAREHOUSE = mywh\n  INITIALIZE = on_schedule\n  REQUIRE USER\n  AS\n    SELECT product_id, product_name FROM staging_table;",
          "ALTER DYNAMIC TABLE my_dynamic_table REFRESH COPY SESSION;"
        ]
      }
    ]
  },
  {
    "category": "ALTER DYNAMIC TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-dynamic-table",
    "details": [
      {
        "heading": "ALTER DYNAMIC TABLE",
        "description": "\nModifies the properties of a dynamic table.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE DYNAMIC TABLE, DESCRIBE DYNAMIC TABLE, DROP DYNAMIC TABLE, SHOW DYNAMIC TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:\nFor more information, see Clustering Keys & Clustered Tables.\nFor details, see Search optimization actions (searchOptimizationAction).",
        "syntax": [
          "ALTER DYNAMIC TABLE [ IF EXISTS ] <name> { SUSPEND | RESUME }\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> SWAP WITH <target_dynamic_table_name>\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> REFRESH [ COPY SESSION ]\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> { clusteringAction }\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> { tableColumnCommentAction }\n\nALTER DYNAMIC TABLE <name> { SET | UNSET } COMMENT = '<string_literal>'\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> dataGovnPolicyTagAction\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> searchOptimizationAction\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> SET\n  [ TARGET_LAG = { '<num> { seconds | minutes | hours | days }'  | DOWNSTREAM } ]\n  [ WAREHOUSE = <warehouse_name> ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ LOG_LEVEL = '<log_level>' ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nALTER DYNAMIC TABLE [ IF EXISTS ] <name> UNSET\n  [ DATA_RETENTION_TIME_IN_DAYS ],\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS ],\n  [ DEFAULT_DDL_COLLATION ]\n  [ LOG_LEVEL ]\n  [ CONTACT <purpose> ]",
          "clusteringAction ::=\n  {\n    CLUSTER BY ( <expr> [ , <expr> , ... ] )\n    | { SUSPEND | RESUME } RECLUSTER\n    | DROP CLUSTERING KEY\n  }",
          "tableCommentAction ::=\n  {\n    ALTER | MODIFY [ ( ]\n                           [ COLUMN ] <col1_name> COMMENT '<string>'\n                         , [ COLUMN ] <col1_name> UNSET COMMENT\n                       [ , ... ]\n                   [ ) ]\n  }",
          "dataGovnPolicyTagAction ::=\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n  |\n  {\n    SET AGGREGATION POLICY <policy_name>\n      [ ENTITY KEY ( <col_name> [, ... ] ) ]\n      [ FORCE ]\n  | UNSET AGGREGATION POLICY\n  }\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] [ FORCE ]\n      | UNSET MASKING POLICY\n  }\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> SET TAG\n      <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n      , [ COLUMN ] <col2_name> SET TAG\n          <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET PROJECTION POLICY <policy_name>\n          [ FORCE ]\n      | UNSET PROJECTION POLICY\n}\n|\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n                  , [ COLUMN ] <col2_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }",
          "searchOptimizationAction ::=\n  {\n    ADD SEARCH OPTIMIZATION [\n      ON <search_method_with_target> [ , <search_method_with_target> ... ]\n        [ EQUALITY ]\n      ]\n\n    | DROP SEARCH OPTIMIZATION [\n      ON { <search_method_with_target> | <column_name> | <expression_id> }\n        [ EQUALITY ]\n        [ , ... ]\n      ]\n\n    | SUSPEND SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n\n    | RESUME SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n  }"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Identifier for the dynamic table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements."
          },
          {
            "term": "SUSPEND | RESUME",
            "definition": "Specifies the action to perform on the dynamic table: SUSPEND suspends refreshes on the dynamic table. If the dynamic table is used\nby other dynamic tables, they are also suspended. RESUME resumes refreshes on the dynamic table. Resume operations cascade\ndownstream to all downstream dynamic tables not manually suspended."
          },
          {
            "term": "RENAME TO new_name",
            "definition": "Renames the specified dynamic table with a new identifier that is not currently used by\nany other dynamic tables in the schema. Renaming a dynamic table requires the CREATE DYNAMIC TABLE privilege on the schema for\nthe dynamic table. You can also move the dynamic table to a different database and/or schema while\noptionally renaming the dynamic table. To do so, specify a qualified new_name\nvalue that includes the new database and/or schema name in the form\ndb_name.schema_name.new_name or schema_name.new_name,\nrespectively. The following restrictions apply: The destination database and/or schema must already exist. In addition, an object\nwith the same name cannot already exist in the new location; otherwise, the\nstatement returns an error. You cant move an object to a managed access schema unless the object owner\n(that is, the role that has the OWNERSHIP privilege on the object) also owns the\ntarget schema. When an object (table, column, etc.) is renamed, other objects that reference it\nmust be updated with the new name."
          },
          {
            "term": "SWAP WITH target_dynamic_table_name",
            "definition": "Swaps two dynamic tables in a single transaction. The role used to perform this\noperation must have OWNERSHIP privileges on both dynamic tables. The following restrictions apply: You can only swap a dynamic table with another dynamic table."
          },
          {
            "term": "REFRESH [ COPY SESSION ]",
            "definition": "Specifies that the dynamic table should be manually refreshed. Both user-suspended and auto-suspended dynamic tables can be manually refreshed.\nManually refreshed dynamic tables return MANUAL as the output for refresh_trigger\nin the DYNAMIC_TABLE_REFRESH_HISTORY function. Note that refreshing a dynamic table also refreshes all upstream dynamic tables as\nof the same data timestamp. For more information, see Alter the warehouse or target lag for dynamic tables. For information on dynamic table refresh status, see DYNAMIC_TABLE_REFRESH_HISTORY. COPY SESSION Runs the refresh operation in a copy of the current session using the current user and\nwarehouse. This only applies to a single manual refresh; it does not permanently update the credentials for the dynamic table.\nUse the GRANT OWNERSHIP command to transfer the ownership for scheduled\nrefreshes. For more information, see Transfer ownership. The primary role is the role that owns the dynamic table and secondary roles will match\nthe DEFAULT_SECONDARY_ROLES property of the user."
          },
          {
            "term": "SET ...",
            "definition": "Specifies one or more properties/parameters to set for the table (separated by blank\nspaces, commas, or new lines): Specifies the target lag for the dynamic table: Specifies the maximum amount of time that the dynamic tables content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B. Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed. Specifies the name of the warehouse that provides the compute resources for\nrefreshing the dynamic table. The owner role of the dynamic table must have the USAGE privilege on this warehouse. Object-level parameter that modifies the retention period for the dynamic table for\nTime Travel. For more details, see Understanding & using Time Travel and\nWorking with Temporary and Transient Tables. For a detailed description of this parameter and more information about object\nparameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent dynamic tables 0 or 1 for transient dynamic tables Note A value of 0 effectively disables Time Travel for the dynamic table. Object parameter that specifies the maximum number of days Snowflake can extend the\ndata retention period to prevent streams on the dynamic table from becoming stale. For a detailed description of this parameter, see\nMAX_DATA_EXTENSION_TIME_IN_DAYS. Specifies a default collation specification\nfor any new columns added to the dynamic table. Setting this parameter does not change the collation specification for any\nexisting columns. For more information, see DEFAULT_DDL_COLLATION. Specifies the severity level of events for this dynamic table that are\ningested and made available in the active event table. Events at the specified level (and at more severe levels) are\ningested. For more information about levels, see LOG_LEVEL. For information about setting the log level, see\nSetting levels for logging, metrics, and tracing. Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "TARGET_LAG = { num { seconds | minutes | hours | days } | DOWNSTREAM }",
            "definition": "Specifies the target lag for the dynamic table: Specifies the maximum amount of time that the dynamic tables content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B. Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed."
          },
          {
            "term": "'num seconds | minutes | hours | days'",
            "definition": "Specifies the maximum amount of time that the dynamic tables content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B."
          },
          {
            "term": "DOWNSTREAM",
            "definition": "Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed."
          },
          {
            "term": "WAREHOUSE = warehouse_name",
            "definition": "Specifies the name of the warehouse that provides the compute resources for\nrefreshing the dynamic table. The owner role of the dynamic table must have the USAGE privilege on this warehouse."
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Object-level parameter that modifies the retention period for the dynamic table for\nTime Travel. For more details, see Understanding & using Time Travel and\nWorking with Temporary and Transient Tables. For a detailed description of this parameter and more information about object\nparameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent dynamic tables 0 or 1 for transient dynamic tables Note A value of 0 effectively disables Time Travel for the dynamic table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days Snowflake can extend the\ndata retention period to prevent streams on the dynamic table from becoming stale. For a detailed description of this parameter, see\nMAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "DEFAULT_DDL_COLLATION = 'collation_specification'",
            "definition": "Specifies a default collation specification\nfor any new columns added to the dynamic table. Setting this parameter does not change the collation specification for any\nexisting columns. For more information, see DEFAULT_DDL_COLLATION."
          },
          {
            "term": "LOG_LEVEL = 'log_level'",
            "definition": "Specifies the severity level of events for this dynamic table that are\ningested and made available in the active event table. Events at the specified level (and at more severe levels) are\ningested. For more information about levels, see LOG_LEVEL. For information about setting the log level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "TARGET_LAG = { num { seconds | minutes | hours | days } | DOWNSTREAM }",
            "definition": "Specifies the target lag for the dynamic table: Specifies the maximum amount of time that the dynamic tables content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B. Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed."
          },
          {
            "term": "'num seconds | minutes | hours | days'",
            "definition": "Specifies the maximum amount of time that the dynamic tables content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B."
          },
          {
            "term": "DOWNSTREAM",
            "definition": "Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed."
          },
          {
            "term": "WAREHOUSE = warehouse_name",
            "definition": "Specifies the name of the warehouse that provides the compute resources for\nrefreshing the dynamic table. The owner role of the dynamic table must have the USAGE privilege on this warehouse."
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Object-level parameter that modifies the retention period for the dynamic table for\nTime Travel. For more details, see Understanding & using Time Travel and\nWorking with Temporary and Transient Tables. For a detailed description of this parameter and more information about object\nparameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent dynamic tables 0 or 1 for transient dynamic tables Note A value of 0 effectively disables Time Travel for the dynamic table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days Snowflake can extend the\ndata retention period to prevent streams on the dynamic table from becoming stale. For a detailed description of this parameter, see\nMAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "DEFAULT_DDL_COLLATION = 'collation_specification'",
            "definition": "Specifies a default collation specification\nfor any new columns added to the dynamic table. Setting this parameter does not change the collation specification for any\nexisting columns. For more information, see DEFAULT_DDL_COLLATION."
          },
          {
            "term": "LOG_LEVEL = 'log_level'",
            "definition": "Specifies the severity level of events for this dynamic table that are\ningested and made available in the active event table. Events at the specified level (and at more severe levels) are\ningested. For more information about levels, see LOG_LEVEL. For information about setting the log level, see\nSetting levels for logging, metrics, and tracing."
          },
          {
            "term": "'num seconds | minutes | hours | days'",
            "definition": "Specifies the maximum amount of time that the dynamic tables content should lag\nbehind updates to the base tables. For example: If the data in the dynamic table should lag by no more than 5 minutes, specify 5 minutes. If the data in the dynamic table should lag by no more than 5 hours, specify 5 hours. The minimum value is 1 minute. If a dynamic table A depends on another dynamic\ntable B, the minimum lag for A must be greater than or equal to the lag for B."
          },
          {
            "term": "DOWNSTREAM",
            "definition": "Specifies that the dynamic table should be refreshed if any dynamic table\ndownstream of it is refreshed."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies one or more properties/parameters to unset for the dynamic table, which\nresets them back to their defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS DEFAULT_DDL_COLLATION LOG_LEVEL CONTACT purposes"
          }
        ]
      },
      {
        "heading": "Clustering actions (clusteringAction)",
        "description": "\nFor more information about clustering keys and reclustering, see Understanding Snowflake Table Structures.",
        "definitions": [
          {
            "term": "CLUSTER BY ( expr [ , expr , ... ] )",
            "definition": "Specifies (or modifies) one or more table columns or column expressions as the\nclustering key for the dynamic table. These are the columns/expressions for which\nclustering is maintained by Automatic Clustering. Before you specify a clustering key\nfor a dynamic table, you should understand micro-partitions. For more information, see\nUnderstanding Snowflake Table Structures. Note the following when using clustering keys with dynamic tables: Column definitions are required and must be explicitly specified in the statement. Clustering keys are not intended or recommended for all tables; they\ntypically benefit very large (for example, multi-terabyte) tables."
          },
          {
            "term": "SUSPEND | RESUME RECLUSTER",
            "definition": "Enables or disables Automatic Clustering for the dynamic table."
          },
          {
            "term": "DROP CLUSTERING KEY",
            "definition": "Drops the clustering key for the dynamic table."
          }
        ]
      },
      {
        "heading": "Table comment actions (tableCommentAction)",
        "definitions": [
          {
            "term": "ALTER | MODIFY [ ( ]` . [ COLUMN ] <col1_name> COMMENT '<string>' . , [ COLUMN ] <col1_name> UNSET COMMENT . [ , ... ] . [ ) ]",
            "definition": "Alters a comment or overwrites the existing comment for a column in the dynamic table."
          },
          {
            "term": "SET | UNSET COMMENT = '<string_literal>'",
            "definition": "Adds a comment or overwrites the existing comment for the dynamic table."
          }
        ]
      },
      {
        "heading": "Data Governance policy and tag actions (dataGovnPolicyTagAction)",
        "definitions": [
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "policy_name",
            "definition": "Identifier for the policy; must be unique for your schema. Adds a row access policy to the dynamic table. At least one column name must be specified. Additional columns can be specified\nwith a comma separating each column name. Drops a row access policy from the dynamic table. Drops the row access policy that is set on the dynamic table and adds a row access\npolicy to the same dynamic table in a single SQL statement. Drops all row access policy associations from the dynamic table. Specifies the arguments to pass into the conditional masking policy. The first column in the list specifies the data to be masked or tokenized based on\npolicy conditions and must match the column to which the masking policy\nis applied. The additional columns specify which data to evaluate for masking or tokenization\nin each row of the query result when selecting from the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a\nnormal masking policy. Assigns an aggregation policy to the dynamic table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For\nmore information, see Implementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy. Detaches an aggregation policy from the dynamic table. Replaces a masking or projection policy that is currently set on a column with a\ndifferent policy in a single statement. Note that using the FORCE keyword with a masking policy requires the\ndata type of the policy in the ALTER DYNAMIC\nTABLE statement (i.e. STRING) to match the data type of the masking policy currently\nset on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has\nno effect. For details, see: Replace a masking policy on a column or\nReplace a projection policy."
          },
          {
            "term": "ADD ROW ACCESS POLICY policy_name ON (col_name [ , ... ])",
            "definition": "Adds a row access policy to the dynamic table. At least one column name must be specified. Additional columns can be specified\nwith a comma separating each column name."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name",
            "definition": "Drops a row access policy from the dynamic table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name, ADD ROW ACCESS POLICY policy_name ON ( col_name [ , ... ] )",
            "definition": "Drops the row access policy that is set on the dynamic table and adds a row access\npolicy to the same dynamic table in a single SQL statement."
          },
          {
            "term": "DROP ALL ROW ACCESS POLICIES",
            "definition": "Drops all row access policy associations from the dynamic table."
          },
          {
            "term": "{ ALTER | MODIFY } [ COLUMN ] ...",
            "definition": "Specifies the arguments to pass into the conditional masking policy. The first column in the list specifies the data to be masked or tokenized based on\npolicy conditions and must match the column to which the masking policy\nis applied. The additional columns specify which data to evaluate for masking or tokenization\nin each row of the query result when selecting from the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a\nnormal masking policy."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy. The first column in the list specifies the data to be masked or tokenized based on\npolicy conditions and must match the column to which the masking policy\nis applied. The additional columns specify which data to evaluate for masking or tokenization\nin each row of the query result when selecting from the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a\nnormal masking policy."
          },
          {
            "term": "SET AGGREGATION POLICY {policy_name}",
            "definition": "Assigns an aggregation policy to the dynamic table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For\nmore information, see Implementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ ENTITY KEY ({col_name} [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the dynamic table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For\nmore information, see Implementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "UNSET AGGREGATION POLICY",
            "definition": "Detaches an aggregation policy from the dynamic table."
          },
          {
            "term": "FORCE",
            "definition": "Replaces a masking or projection policy that is currently set on a column with a\ndifferent policy in a single statement. Note that using the FORCE keyword with a masking policy requires the\ndata type of the policy in the ALTER DYNAMIC\nTABLE statement (i.e. STRING) to match the data type of the masking policy currently\nset on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has\nno effect. For details, see: Replace a masking policy on a column or\nReplace a projection policy."
          },
          {
            "term": "ADD ROW ACCESS POLICY policy_name ON (col_name [ , ... ])",
            "definition": "Adds a row access policy to the dynamic table. At least one column name must be specified. Additional columns can be specified\nwith a comma separating each column name."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name",
            "definition": "Drops a row access policy from the dynamic table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name, ADD ROW ACCESS POLICY policy_name ON ( col_name [ , ... ] )",
            "definition": "Drops the row access policy that is set on the dynamic table and adds a row access\npolicy to the same dynamic table in a single SQL statement."
          },
          {
            "term": "DROP ALL ROW ACCESS POLICIES",
            "definition": "Drops all row access policy associations from the dynamic table."
          },
          {
            "term": "{ ALTER | MODIFY } [ COLUMN ] ...",
            "definition": "Specifies the arguments to pass into the conditional masking policy. The first column in the list specifies the data to be masked or tokenized based on\npolicy conditions and must match the column to which the masking policy\nis applied. The additional columns specify which data to evaluate for masking or tokenization\nin each row of the query result when selecting from the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a\nnormal masking policy."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy. The first column in the list specifies the data to be masked or tokenized based on\npolicy conditions and must match the column to which the masking policy\nis applied. The additional columns specify which data to evaluate for masking or tokenization\nin each row of the query result when selecting from the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a\nnormal masking policy."
          },
          {
            "term": "SET AGGREGATION POLICY {policy_name}",
            "definition": "Assigns an aggregation policy to the dynamic table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For\nmore information, see Implementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ ENTITY KEY ({col_name} [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the dynamic table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For\nmore information, see Implementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "UNSET AGGREGATION POLICY",
            "definition": "Detaches an aggregation policy from the dynamic table."
          },
          {
            "term": "FORCE",
            "definition": "Replaces a masking or projection policy that is currently set on a column with a\ndifferent policy in a single statement. Note that using the FORCE keyword with a masking policy requires the\ndata type of the policy in the ALTER DYNAMIC\nTABLE statement (i.e. STRING) to match the data type of the masking policy currently\nset on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has\nno effect. For details, see: Replace a masking policy on a column or\nReplace a projection policy."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy. The first column in the list specifies the data to be masked or tokenized based on\npolicy conditions and must match the column to which the masking policy\nis applied. The additional columns specify which data to evaluate for masking or tokenization\nin each row of the query result when selecting from the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a\nnormal masking policy."
          },
          {
            "term": "[ ENTITY KEY ({col_name} [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the dynamic table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the dynamic table. For\nmore information, see Implementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          }
        ]
      },
      {
        "heading": "Search optimization actions (searchOptimizationAction)",
        "syntax": [
          "<search_method>(<target> [, ...])",
          "ON SUBSTRING(*)\nON EQUALITY(*), SUBSTRING(*), GEO(*)",
          "ON EQUALITY(*, c1)\nON EQUALITY(c1, *)\nON EQUALITY(v1:path, *)\nON EQUALITY(c1), EQUALITY(*)",
          "ALTER DYNAMIC TABLE my_dynamic_table ADD SEARCH OPTIMIZATION ON EQUALITY(c1), EQUALITY(c2, c3);",
          "ALTER DYNAMIC TABLE my_dynamic_table ADD SEARCH OPTIMIZATION ON EQUALITY(c1, c2);\nALTER DYNAMIC TABLE my_dynamic_table ADD SEARCH OPTIMIZATION ON EQUALITY(c3, c4);",
          "ALTER DYNAMIC TABLE my_dynamic_table ADD SEARCH OPTIMIZATION ON EQUALITY(c1, c2, c3, c4);"
        ],
        "definitions": [
          {
            "term": "ADD SEARCH OPTIMIZATION",
            "definition": "Adds search optimization for the\nentire dynamic table or, if you specify the optional ON clause, for specific\ncolumns. Search optimization can be expensive to maintain, especially if the data in the table\nchanges frequently. For more information, see\nSearch optimization cost estimation and management."
          },
          {
            "term": "ON search_method_with_target [, search_method_with_target ... ]",
            "definition": "Specifies that you want to configure search optimization for specific columns or\nVARIANT fields (rather than the entire dynamic table). For search_method_with_target, use an expression with the following syntax: Where: search_method specifies one of the following methods that optimizes\nqueries for a particular type of predicate: GEO: Predicates that use GEOGRAPHY types. SUBSTRING: Predicates that match substrings and regular expressions (for\nexample, [ NOT ] LIKE, [ NOT ] ILIKE,\n[ NOT ] RLIKE, REGEXP_LIKE,\netc.) EQUALITY: Equality and IN predicates. target specifies the column, VARIANT field, or an asterisk (*). Depending on the value of search_method, you can specify a column or\nVARIANT field of one of the following types: GEO: Columns of the GEOGRAPHY data type. SUBSTRING: Columns of string or VARIANT data types, including paths to\nfields in VARIANTs. Specify paths to fields as described under EQUALITY;\nsearches on nested fields are improved in the same way. EQUALITY: Columns of numeric, string, binary, and VARIANT data types,\nincluding paths to fields in VARIANT columns. To specify a VARIANT field, use\ndot or bracket notation. For\nexample: my_column:my_field_name.my_nested_field_name my_column['my_field_name']['my_nested_field_name'] You may also use a colon-delimited path to the field. For example: my_column:my_field_name:my_nested_field_name When you specify a VARIANT field, the configuration applies to all nested fields\nunder that field. For example, if you specify ON EQUALITY(src:a.b): This configuration can improve queries on src:a.b and on any nested fields\n(for example, src:a.b.c, src:a.b.c.d, etc.). This configuration only affects queries that use the src:a.b prefix (for\nexample, src:a, src:z, etc.). To specify all applicable columns in the table as targets, use an asterisk (*). Note that you cant specify both an asterisk and specific column names for a\ngiven search method. However, you can specify an asterisk in different search methods. For example, you can specify the following expressions: You cant specify the following expressions: To specify more than one search method on a target, use a comma to separate each\nsubsequent method and target: If you run the ALTER DYNAMIC TABLE  ADD SEARCH OPTIMIZATION ON  command multiple\ntimes on the same table, each subsequent command adds to the existing configuration\nfor the table. For instance, suppose that you run the following commands: This adds equality predicates for the columns c1, c2, c3, and c4 to\nthe configuration for the table. This is equivalent to running the command: For examples, see Enabling search optimization for specific columns."
          },
          {
            "term": "DROP SEARCH OPTIMIZATION",
            "definition": "Removes search optimization for the\nentire dynamic table or, if you specify the optional ON clause, from specific\ncolumns. The following restrictions apply: If a dynamic table has the search optimization property, then dropping the dynamic\ntable and undropping it preserves the search optimization property. Removing the search optimization property from a dynamic table and then adding it\nback incurs the same cost as adding it the first time."
          },
          {
            "term": "ON search_method_with_target | column_name | expression_id [, ... ]",
            "definition": "Specifies that you want to drop the search optimization configuration for specific\ncolumns or VARIANT fields (rather than dropping search optimization for the entire\ndynamic table). To identify the column configuration to drop, specify one of the following: For search_method_with_target, specify a method for optimizing queries for\none or more specific targets, which can be columns or VARIANT fields. Use the\nsyntax described earlier. For column_name, specify the name of the column configured for search\noptimization. Specifying the column name drops all expressions for that column,\nincluding expressions that use VARIANT fields in the column. For expression_id, specify the ID for an expression listed in the output\nof the DESCRIBE SEARCH OPTIMIZATION\ncommand. You can specify any combination of search methods with targets, column names, and\nexpression IDs using a comma between items. For examples, see Dropping search optimization for specific columns."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "OWNERSHIP or OPERATE",
                "The dynamic table you want to alter.",
                "Some actions are only supported with the OWNERSHIP privilege. For more information, see Privileges to alter a dynamic table."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo alter a dynamic table, you must be using a role that has OPERATE privilege on that\ndynamic table. For general information, see Privileges to view a dynamic tables metadata.\nMaking changes to masking policies on a base table causes a reinitialization.\nIf you want to update an existing dynamic table and need to see its current definition,\ncall the GET_DDL function.\nYou can use data metric functions with dynamic tables by executing an ALTER TABLE\ncommand. For more information, see Use data metric functions to perform data quality checks.\nYou cannot use IDENTIFIER() to specify the\nname of the dynamic table to alter. For example, the following statement isnt supported:\nALTER DYNAMIC TABLE IDENTIFIER(my_dynamic_table) SUSPEND;\n\nCopy\nAfter a reinitialization or full refresh, search indexes on dynamic tables are rebuilt.\nThis process involves dropping the existing indexes and rebuilding them from scratch,\nwhich might incur higher costs. For more information, see Search optimization cost estimation and management.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nTo alter a dynamic table, you must be using a role that has OPERATE privilege on that\ndynamic table. For general information, see Privileges to view a dynamic tables metadata.\nMaking changes to masking policies on a base table causes a reinitialization.\nIf you want to update an existing dynamic table and need to see its current definition,\ncall the GET_DDL function.\nYou can use data metric functions with dynamic tables by executing an ALTER TABLE\ncommand. For more information, see Use data metric functions to perform data quality checks.\nYou cannot use IDENTIFIER() to specify the\nname of the dynamic table to alter. For example, the following statement isnt supported:\nAfter a reinitialization or full refresh, search indexes on dynamic tables are rebuilt.\nThis process involves dropping the existing indexes and rebuilding them from scratch,\nwhich might incur higher costs. For more information, see Search optimization cost estimation and management.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.",
        "syntax": [
          "ALTER DYNAMIC TABLE IDENTIFIER(my_dynamic_table) SUSPEND;"
        ]
      },
      {
        "heading": "Examples",
        "description": "\nChange the target lag time of a dynamic table named my_dynamic_table to 1 hour:\nSpecify downstream target lag for my_dynamic_table:\nSuspend a dynamic table:\nResume a dynamic table:\nRename my_dynamic_table:\nSwap my_dynamic_table with my_new_dynamic_table:\nChange the clustering key for a dynamic table:\nRemove clustering from a dynamic table:\nPerform a manual refresh of my_dynamic_table using the user, secondary roles, and warehouse settings\nfrom the current session. This ensures that the refresh operation runs with the exact context\nof the user session.\nOn this page\nSyntax\nParameters\nClustering actions (clusteringAction)\nTable comment actions (tableCommentAction)\nData Governance policy and tag actions (dataGovnPolicyTagAction)\nSearch optimization actions (searchOptimizationAction)\nAccess control requirements\nUsage notes\nExamples\nRelated content\nDynamic tables\nAlter the warehouse or target lag for dynamic tables\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER DYNAMIC TABLE my_dynamic_table SET\n  TARGET_LAG = '1 hour';",
          "ALTER DYNAMIC TABLE my_dynamic_table SET TARGET_LAG = DOWNSTREAM;",
          "ALTER DYNAMIC TABLE my_dynamic_table SUSPEND;",
          "ALTER DYNAMIC TABLE my_dynamic_table RESUME;",
          "ALTER DYNAMIC TABLE my_dynamic_table RENAME TO my_updated_dynamic_table;",
          "ALTER DYNAMIC TABLE my_dynamic_table SWAP WITH my_new_dynamic_table;",
          "ALTER DYNAMIC TABLE my_dynamic_table CLUSTER BY (date);",
          "ALTER DYNAMIC TABLE my_dynamic_table DROP CLUSTERING KEY;",
          "ALTER DYNAMIC TABLE my_dynamic_table REFRESH COPY SESSION"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE DYNAMIC TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-dynamic-table",
    "details": [
      {
        "heading": "DESCRIBE DYNAMIC TABLE",
        "description": "\nDescribes the columns in a dynamic table.\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE DYNAMIC TABLE, ALTER DYNAMIC TABLE, DROP DYNAMIC TABLE, SHOW DYNAMIC TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] DYNAMIC TABLE <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the dynamic table to describe. If the identifier contains spaces or special characters, the entire\nstring must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "SELECT",
                "The dynamic table that you want to describe.",
                "Some metadata is hidden if you donâ€™t have the MONITOR privilege. For more information, see Privileges to view a dynamic tableâ€™s metadata."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo DESCRIBE a dynamic table, you must be using a role that has MONITOR privilege on the table.\nTo DESCRIBE a dynamic table, you must be using a role that has MONITOR privilege on the table.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nDescribe the columns in my_dynamic_table:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExamples\nRelated content\nDynamic tables\nMonitor dynamic tables\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESC DYNAMIC TABLE my_dynamic_table;"
        ]
      }
    ]
  },
  {
    "category": "DROP DYNAMIC TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-dynamic-table",
    "details": [
      {
        "heading": "DROP DYNAMIC TABLE",
        "description": "\nRemoves a dynamic table from the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE DYNAMIC TABLE, ALTER DYNAMIC TABLE, DESCRIBE DYNAMIC TABLE,\nSHOW DYNAMIC TABLES, UNDROP DYNAMIC TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP DYNAMIC TABLE [ IF EXISTS ] <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the dynamic table to drop. If the identifier contains spaces, special characters, or mixed-case\ncharacters, the entire string must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive\n(e.g. \"My Object\"). If the table identifier is not fully-qualified (in the form of db_name.schema_name.table_name or\nschema_name.table_name), the command looks for the table in the current schema for the session."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "OWNERSHIP",
                "The dynamic table that you want to drop.",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo drop a dynamic table, you must be using a role that has OWNERSHIP privilege on that dynamic table.\nYou can also drop a dynamic table using the DROP TABLE command.\nTo drop a dynamic table, you must be using a role that has OWNERSHIP privilege on that dynamic table.\nYou can also drop a dynamic table using the DROP TABLE command.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Examples",
        "description": "\nDrop my_dynamic_table:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExamples\nRelated content\nDynamic tables\nDrop or undrop dynamic tables\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP DYNAMIC TABLE my_dynamic_table;",
          "DROP TABLE my_dynamic_table;"
        ]
      }
    ]
  },
  {
    "category": "SHOW DYNAMIC TABLES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-dynamic-tables",
    "details": [
      {
        "heading": "SHOW DYNAMIC TABLES",
        "description": "\nLists the dynamic tables for which you have access privileges. The command can be used to list dynamic\ntables for the current/specified database or schema, or across your entire account.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE DYNAMIC TABLE, ALTER DYNAMIC TABLE, DESCRIBE DYNAMIC TABLE, DROP DYNAMIC TABLE,\nSHOW OBJECTS, TABLES view (Information Schema)"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW DYNAMIC TABLES [ LIKE '<pattern>' ]\n                    [ IN\n                      {\n                           ACCOUNT              |\n\n                           DATABASE             |\n                           DATABASE <db_name>   |\n\n                           SCHEMA               |\n                           SCHEMA <schema_name> |\n                           <schema_name>\n                      }\n                    ]\n                    [ STARTS WITH '<name_string>' ]\n                    [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "SELECT",
                "The dynamic tables that you want to list.",
                "Some metadata is hidden if you donâ€™t have the MONITOR privilege. For more information, see Privileges to view a dynamic tableâ€™s metadata."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo SHOW a dynamic table, you must be using a role that has MONITOR privilege on the table.\nTo SHOW a dynamic table, you must be using a role that has MONITOR privilege on the table.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema."
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the dynamic table was created."
              ],
              [
                "name",
                "Name of the dynamic table."
              ],
              [
                "database_name",
                "Database in which the dynamic table is stored."
              ],
              [
                "schema_name",
                "Schema in which the dynamic table is stored."
              ],
              [
                "cluster_by",
                "The clustering key(s) for the dynamic table."
              ],
              [
                "rows",
                "Number of rows in the table."
              ],
              [
                "bytes",
                "Number of bytes that will be scanned if the entire dynamic table is scanned in a query. . . Note that this number may be different than the number of actual physical bytes (i.e. bytes stored on-disk) for the table."
              ],
              [
                "owner",
                "Role that owns the dynamic table."
              ],
              [
                "target_lag",
                "The maximum duration that the dynamic tableâ€™s content should lag behind real time."
              ],
              [
                "refresh_mode",
                "INCREMENTAL if the dynamic table will use incremental refreshes, or FULL if it will recompute the whole table on every refresh."
              ],
              [
                "refresh_mode_reason",
                "Explanation for why the refresh mode was chosen. If Snowflake chose FULL when INCREMENTAL is supported, the output provides a reason for why it thinks full refresh performs better. NULL if no pertinent information is available."
              ],
              [
                "warehouse",
                "Warehouse that provides the required resources to perform the incremental refreshes."
              ],
              [
                "comment",
                "Comment for the dynamic table."
              ],
              [
                "text",
                "The text of the command that created this dynamic table (e.g. CREATE DYNAMIC TABLE ...)."
              ],
              [
                "automatic_clustering",
                "Whether auto-clustering is enabled on the dynamic table. Not currently supported for dynamic tables."
              ],
              [
                "scheduling_state",
                "Displays RUNNING for dynamic tables that are actively scheduling refreshes and SUSPENDED for suspended dynamic tables."
              ],
              [
                "last_suspended_on",
                "Timestamp of last suspension."
              ],
              [
                "is_clone",
                "TRUE if the dynamic table is a clone; else FALSE."
              ],
              [
                "is_replica",
                "TRUE if the dynamic table is a replica; else FALSE."
              ],
              [
                "is_iceberg",
                "TRUE if the dynamic table is a dynamic Apache Icebergâ„¢ table; else FALSE."
              ],
              [
                "data_timestamp",
                "Timestamp of the data in the base object(s) that is included in the dynamic table."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . . Database-level roles, for example DATABASE_ROLE, canâ€™t be owners. The owner of a dynamic table must have the USAGE privilege on the warehouse. Since the warehouse is an account-level object, a database role, which operates at the database level, canâ€™t be granted access to it. . . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nShow all the dynamic tables with names that start with product_ in the mydb.myschema schema:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nOutput\nExamples\nRelated content\nDynamic tables\nMonitor dynamic tables\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW DYNAMIC TABLES LIKE 'product_%' IN SCHEMA mydb.myschema;"
        ]
      }
    ]
  },
  {
    "category": "CREATE EVENT TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-event-table",
    "details": [
      {
        "heading": "CREATE EVENT TABLE",
        "description": "\nCreates an event table that captures events, including logged messages\nfrom functions and procedures.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER TABLE (event tables) , DESCRIBE EVENT TABLE, DROP TABLE,\nSHOW EVENT TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CREATE [ OR REPLACE ] EVENT TABLE [ IF NOT EXISTS ] <name>\n  [ CLUSTER BY ( <expr> [ , <expr> , ... ] ) ]\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ DEFAULT_DDL_COLLATION = '<collation_specification>' ]\n  [ COPY GRANTS ]\n  [ [ WITH ] COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE EVENT TABLE  CLONE",
        "description": "\nCreates a new event table with the same predefined column definitions and\ncontaining all the existing data from the source table without actually copying the data. You can also use this variant to clone an\nevent table at a specific time/point in the past (using Time Travel):\nNote\nIf the statement replaces an event table of the same name, the grants are copied from the event table\nbeing replaced. Otherwise, the grants are copied from the source event table being cloned.\nFor more details about COPY GRANTS, see COPY GRANTS in this document.\nFor more details about cloning, see CREATE <object>  CLONE.",
        "syntax": [
          "CREATE [ OR REPLACE ] EVENT TABLE [ IF NOT EXISTS ] <name>\n  CLONE <source_table>\n    [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]\n    [ COPY GRANTS ]\n    [ ... ]"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier (the name) for the event table; must be unique for the schema in which the event table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements."
          },
          {
            "term": "source_table",
            "definition": "Required for CLONE. Specifies the event table to use as the source for the clone."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "CLUSTER BY ( expr [ , expr , ... ] )",
            "definition": "Specifies one or more columns or column expressions in the table as the clustering key. For more details, see\nClustering Keys & Clustered Tables. Default: No value (no clustering key is defined for the table) Important Clustering keys are not intended or recommended for all tables; they typically benefit very large (i.e. multi-terabyte)\ntables. Before you specify a clustering key for a table, please read Understanding Snowflake Table Structures."
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Specifies the retention period for the table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical\ndata in the table. For more details, see Understanding & using Time Travel. For a detailed description of this object-level parameter, as well as more information about object parameters, see\nParameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent tables Default: Standard Edition: 1 Enterprise Edition (or higher): 1 (unless a different default value was specified at the schema, database, or account level) Note A value of 0 effectively disables Time Travel for the table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the table to\nprevent streams on the table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "CHANGE_TRACKING =  TRUE | FALSE",
            "definition": "Specifies whether to enable change tracking on the table. TRUE enables change tracking on the table. This setting adds a pair of hidden columns to the source table and begins\nstoring change tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for\nSELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the table. Default: FALSE"
          },
          {
            "term": "DEFAULT_DDL_COLLATION = 'collation_specification'",
            "definition": "Specifies a default collation specification for the columns in the table. For more details about the parameter, see DEFAULT_DDL_COLLATION."
          },
          {
            "term": "COPY GRANTS",
            "definition": "Specifies to retain the access privileges from the original table when a new table is created using any of the following\nCREATE TABLE variants: CREATE OR REPLACE TABLE CREATE TABLE  CLONE The parameter copies all privileges, except OWNERSHIP, from the existing table to the new table. The new table does not\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE EVENT TABLE statement\nowns the new table. If the parameter is not included in the CREATE EVENT TABLE statement, then the new table does not inherit any explicit access\nprivileges granted on the original table, but does inherit any future grants defined for the object type in the schema. Note: With data sharing: If the existing table was shared to another account, the replacement table is also shared. If the existing table was shared with your account as a data consumer, and access was further granted to other roles in\nthe account (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement\ntable. The SHOW GRANTS output for the replacement table lists the grantee for the copied privileges as the\nrole that executed the CREATE EVENT TABLE statement, with the current timestamp when the statement was executed. The operation to copy grants occurs atomically in the CREATE EVENT TABLE command (i.e. within the same transaction)."
          },
          {
            "term": "ROW ACCESS POLICY policy_name ON ( col_name [ , col_name ... ] )",
            "definition": "Specifies the row access policy to set on a table."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment for the table. Default: No value"
          },
          {
            "term": "TAG ( tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ] )",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "WITH CONTACT ( purpose = contact [ , purpose = contact ...] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the new object with one or more contacts."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE EVENT TABLE",
                "Schema in which you plan to create the event table.",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nA schema cannot contain event tables, tables, and/or views with the same name. When creating an event table:\n\nIf a table or view with the same name already exists in the schema, an error is returned and the event table is not created.\nIf an event table with the same name already exists in the schema, an error is returned and the event table is not created,\nunless the optional OR REPLACE keyword is included in the command.\n\n\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing event table and then\ncreating a new event table with the same name; however, the dropped table is not permanently removed from the system.\nInstead, it is retained in Time Travel. This is important to note because dropped tables in Time Travel can be recovered, but\nthey also contribute to data storage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE EVENT TABLE operation use either the old or new table version.\nIf a table or view with the same name already exists in the schema, an error is returned and the event table is not created.\nIf an event table with the same name already exists in the schema, an error is returned and the event table is not created,\nunless the optional OR REPLACE keyword is included in the command.\nRecreating a table (using the optional OR REPLACE keyword) drops its history, which makes any stream on the table stale.\nA stale stream is unreadable.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE EVENT TABLE  CLONE:\nIf the source event table has clustering keys, the new event table has clustering keys. By default, Automatic Clustering is suspended\nfor the new event tableeven if Automatic Clustering was not suspended for the source table.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nA schema cannot contain event tables, tables, and/or views with the same name. When creating an event table:\nIf a table or view with the same name already exists in the schema, an error is returned and the event table is not created.\nIf an event table with the same name already exists in the schema, an error is returned and the event table is not created,\nunless the optional OR REPLACE keyword is included in the command.\nIf a table or view with the same name already exists in the schema, an error is returned and the event table is not created.\nIf an event table with the same name already exists in the schema, an error is returned and the event table is not created,\nunless the optional OR REPLACE keyword is included in the command.\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing event table and then\ncreating a new event table with the same name; however, the dropped table is not permanently removed from the system.\nInstead, it is retained in Time Travel. This is important to note because dropped tables in Time Travel can be recovered, but\nthey also contribute to data storage for your account. For more information, see Storage costs for Time Travel and Fail-safe.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE EVENT TABLE operation use either the old or new table version.\nRecreating a table (using the optional OR REPLACE keyword) drops its history, which makes any stream on the table stale.\nA stale stream is unreadable.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE EVENT TABLE  CLONE:\nIf the source event table has clustering keys, the new event table has clustering keys. By default, Automatic Clustering is suspended\nfor the new event tableeven if Automatic Clustering was not suspended for the source table.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement."
      },
      {
        "heading": "Examples",
        "description": "\nCreate an event table named my_events:\nOn this page\nSyntax\nVariant syntax\nRequired parameters\nOptional parameters\nAccess control requirements\nUsage notes\nExamples\nRelated content\nEvent table overview\nLogging, tracing, and metrics\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE EVENT TABLE my_events;"
        ]
      }
    ]
  },
  {
    "category": "ALTER TABLE (event tables)",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-table-event-table",
    "details": [
      {
        "heading": "ALTER TABLE (event tables)",
        "description": "\nModifies the properties, columns, or constraints for an existing event table.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE EVENT TABLE , DROP TABLE , SHOW EVENT TABLES , DESCRIBE EVENT TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:\nFor details, see Search optimization actions (searchOptimizationAction).",
        "syntax": [
          "ALTER TABLE [ IF EXISTS ] <name> RENAME TO <new_table_name>\n\nALTER TABLE [ IF EXISTS ] <name> clusteringAction\n\nALTER TABLE [ IF EXISTS ] <name> dataGovnPolicyTagAction\n\nALTER TABLE [ IF EXISTS ] <name> searchOptimizationAction\n\nALTER TABLE [ IF EXISTS ] <name> SET\n  [ DATA_RETENTION_TIME_IN_DAYS = <integer> ]\n  [ MAX_DATA_EXTENSION_TIME_IN_DAYS = <integer> ]\n  [ CHANGE_TRACKING = { TRUE | FALSE  } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER TABLE [ IF EXISTS ] <name> UNSET {\n                                       DATA_RETENTION_TIME_IN_DAYS         |\n                                       MAX_DATA_EXTENSION_TIME_IN_DAYS     |\n                                       CHANGE_TRACKING                     |\n                                       CONTACT <purpose>                   |\n                                       COMMENT                             |\n                                       }",
          "clusteringAction ::=\n  {\n     CLUSTER BY ( <expr> [ , <expr> , ... ] )\n   | { SUSPEND | RESUME } RECLUSTER\n   | DROP CLUSTERING KEY\n  }",
          "dataGovnPolicyTagAction ::=\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }",
          "searchOptimizationAction ::=\n  {\n     ADD SEARCH OPTIMIZATION [\n       ON <search_method_with_target> [ , <search_method_with_target> ... ]\n     ]\n\n   | DROP SEARCH OPTIMIZATION [\n       ON { <search_method_with_target> | <column_name> | <expression_id> }\n          [ , ... ]\n     ]\n\n  }"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Identifier for the event table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case-sensitive."
          },
          {
            "term": "RENAME TO new_table_name",
            "definition": "Renames the specified event table with a new identifier that is not currently used by any other event tables in the schema. Note Not supported on the default event table, SNOWFLAKE.TELEMETRY.EVENTS. For more details about event table identifiers, see Identifier requirements. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object (table, column, etc.) is renamed, other objects that reference it must be updated with the new name."
          },
          {
            "term": "SET ...",
            "definition": "Specifies one or more properties/parameters to set for the event table (separated by blank spaces, commas, or new lines): Object-level parameter that modifies the retention period for the event table for Time Travel. For more details, see\nUnderstanding & using Time Travel and Working with Temporary and Transient Tables. For a detailed description of this parameter, as well as more information about object parameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent event tables 0 or 1 for temporary and transient event tables Note A value of 0 effectively disables Time Travel for the event table. Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the event table to\nprevent streams on the event table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS. Specifies to enable or disable change tracking on the event table. TRUE enables change tracking on the event table. This option adds a pair of hidden columns to the source event table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT\nstatements, or by creating and querying one or more streams on the event table. FALSE disables change tracking on the event table. The pair of hidden columns is dropped from the event table. Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts. Adds a comment or overwrites the existing comment for the event table."
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Object-level parameter that modifies the retention period for the event table for Time Travel. For more details, see\nUnderstanding & using Time Travel and Working with Temporary and Transient Tables. For a detailed description of this parameter, as well as more information about object parameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent event tables 0 or 1 for temporary and transient event tables Note A value of 0 effectively disables Time Travel for the event table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the event table to\nprevent streams on the event table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "CHANGE_TRACKING =  TRUE | FALSE",
            "definition": "Specifies to enable or disable change tracking on the event table. TRUE enables change tracking on the event table. This option adds a pair of hidden columns to the source event table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT\nstatements, or by creating and querying one or more streams on the event table. FALSE disables change tracking on the event table. The pair of hidden columns is dropped from the event table."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the event table."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies one or more properties/parameters to unset for the event table, which resets them back to their defaults: DATA_RETENTION_TIME_IN_DAYS MAX_DATA_EXTENSION_TIME_IN_DAYS CHANGE_TRACKING CONTACT purpose COMMENT"
          },
          {
            "term": "DATA_RETENTION_TIME_IN_DAYS = integer",
            "definition": "Object-level parameter that modifies the retention period for the event table for Time Travel. For more details, see\nUnderstanding & using Time Travel and Working with Temporary and Transient Tables. For a detailed description of this parameter, as well as more information about object parameters, see Parameters. Values: Standard Edition: 0 or 1 Enterprise Edition: 0 to 90 for permanent event tables 0 or 1 for temporary and transient event tables Note A value of 0 effectively disables Time Travel for the event table."
          },
          {
            "term": "MAX_DATA_EXTENSION_TIME_IN_DAYS = integer",
            "definition": "Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for the event table to\nprevent streams on the event table from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS."
          },
          {
            "term": "CHANGE_TRACKING =  TRUE | FALSE",
            "definition": "Specifies to enable or disable change tracking on the event table. TRUE enables change tracking on the event table. This option adds a pair of hidden columns to the source event table and begins storing\nchange tracking metadata in the columns. These columns consume a small amount of storage. The change tracking metadata can be queried using the CHANGES clause for SELECT\nstatements, or by creating and querying one or more streams on the event table. FALSE disables change tracking on the event table. The pair of hidden columns is dropped from the event table."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites the existing comment for the event table."
          }
        ]
      },
      {
        "heading": "Data Governance policy and tag actions (dataGovnPolicyTagAction)",
        "description": "\nThe following clauses apply to all table kinds that support row access policies, such as but not limited to tables, views, and event tables.\nTo simplify, the clauses just refer to table.",
        "definitions": [
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "policy_name",
            "definition": "Identifier for the policy; must be unique for your schema."
          },
          {
            "term": "ADD ROW ACCESS POLICY policy_name ON (col_name [ , ... ])",
            "definition": "Adds a row access policy to the table. At least one column name must be specified. Additional columns can be specified with a comma separating each column name. Use this\nexpression to add a row access policy to both an event table and an external table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name",
            "definition": "Drops a row access policy from the table. Use this clause to drop the policy from the table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name, ADD ROW ACCESS POLICY policy_name ON ( col_name [ , ... ] )",
            "definition": "Drops the row access policy that is set on the table and adds a row access policy to the same table in a single SQL statement."
          },
          {
            "term": "DROP ALL ROW ACCESS POLICIES",
            "definition": "Drops all row access policy associations from the table. This expression is helpful when a row access policy is dropped from a schema before dropping the policy from an event table. Use this expression to drop row access policy associations from the table."
          },
          {
            "term": "SET AGGREGATION POLICY policy_name",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ ENTITY KEY (col_name [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "UNSET AGGREGATION POLICY",
            "definition": "Detaches an aggregation policy from the table."
          },
          {
            "term": "SET JOIN POLICY policy_name",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "[ FORCE ]",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "UNSET JOIN POLICY",
            "definition": "Detaches a join policy from the table."
          },
          {
            "term": "[ ENTITY KEY (col_name [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ FORCE ]",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          }
        ]
      },
      {
        "heading": "Clustering actions (clusteringAction)",
        "description": "\nFor more information about clustering keys and reclustering, see Understanding Snowflake Table Structures.",
        "definitions": [
          {
            "term": "CLUSTER BY ( expr [ , expr , ... ] )",
            "definition": "Specifies (or modifies) one or more event table columns or column expressions as the clustering key for the event table. These are the\ncolumns/expressions for which clustering is maintained by Automatic Clustering. Important Clustering keys are not intended or recommended for all event tables; they typically benefit very large (i.e. multi-terabyte) event tables. Before you specify a clustering key for an event table, please see Understanding Snowflake Table Structures."
          },
          {
            "term": "SUSPEND | RESUME RECLUSTER",
            "definition": "Enables or disables Automatic Clustering for the event table."
          },
          {
            "term": "DROP CLUSTERING KEY",
            "definition": "Drops the clustering key for the event table."
          }
        ]
      },
      {
        "heading": "Search optimization actions (searchOptimizationAction)",
        "tables": [
          {
            "headers": [
              "Search Method",
              "Description"
            ],
            "rows": [
              [
                "EQUALITY",
                "Equality and IN predicates."
              ],
              [
                "SUBSTRING",
                "Predicates that match substrings and regular expressions (e.g. [ NOT ] LIKE,\n[ NOT ] ILIKE, [ NOT ] RLIKE,\nREGEXP_LIKE, etc.)"
              ],
              [
                "GEO",
                "Predicates that use GEOGRAPHY types."
              ]
            ]
          },
          {
            "headers": [
              "Search Method",
              "Supported Targets"
            ],
            "rows": [
              [
                "EQUALITY",
                "Columns of numerical, string, binary, and VARIANT data types, including paths to fields in VARIANTs.\nTo specify a VARIANT field, use a colon-delimited path to the field (e.g.\nmy_column:my_field_name:my_nested_field_name), or use\ndot or bracket notation (e.g.\nmy_column:my_field_name.my_nested_field_name or my_column['my_field_name']['my_nested_field_name']).\nWhen you specify a VARIANT field, the configuration applies to all nested fields under that field.\nFor example, suppose that you specify ON EQUALITY(src:a.b):\n\nThis configuration can improve queries on src:a.b and on any nested fields (e.g. src:a.b.c, src:a.b.c.d,\netc.).\nThis configuration does not affect queries that do not use the src:a.b prefix (e.g. src:a, src:z, etc.)."
              ],
              [
                "SUBSTRING",
                "Columns of string data types."
              ],
              [
                "GEO",
                "Columns of the GEOGRAPHY data type."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nChanges to an event table are not automatically propagated to views created on that event table.\nTo alter an event table, you must be using a role that has ownership privilege on the event table.\nTo add clustering to an event table, you must also have USAGE or OWNERSHIP privileges on the schema and database that\ncontain the event table.\nChanges to an event table are not automatically propagated to views created on that event table.\nTo alter an event table, you must be using a role that has ownership privilege on the event table.\nTo add clustering to an event table, you must also have USAGE or OWNERSHIP privileges on the schema and database that\ncontain the event table.\nFor row access policies:\n\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\n\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\n\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nFor row access policies:\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nIf you create a foreign key, the columns in the REFERENCES clause must be listed in the same order as they were\nlisted for the primary key. For example:\nCREATE TABLE parent ... CONSTRAINT primary_key_1 PRIMARY KEY (c_1, c_2) ...\nCREATE TABLE child  ... CONSTRAINT foreign_key_1 FOREIGN KEY (...) REFERENCES parent (c_1, c_2) ...\n\nCopy\nIn both cases, the order of the columns is c_1, c_2. If the order of the columns in the foreign key had been different\n(for example, c_2, c_1), the attempt to create the foreign key would have failed.\nIf you create a foreign key, the columns in the REFERENCES clause must be listed in the same order as they were\nlisted for the primary key. For example:\nIn both cases, the order of the columns is c_1, c_2. If the order of the columns in the foreign key had been different\n(for example, c_2, c_1), the attempt to create the foreign key would have failed.\nYou can use data metric functions with event tables by executing an ALTER TABLE command. For more information, see\nUse data metric functions to perform data quality checks.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nALTER TABLE  CHANGE_TRACKING = TRUE\n\n\nWhen an event table is altered to enable change tracking, the event table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nWhen an event table is altered to enable change tracking, the event table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nYou can use data metric functions with event tables by executing an ALTER TABLE command. For more information, see\nUse data metric functions to perform data quality checks.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nALTER TABLE  CHANGE_TRACKING = TRUE\nWhen an event table is altered to enable change tracking, the event table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.\nWhen an event table is altered to enable change tracking, the event table is locked for the duration of the operation.\nLocks can cause latency with some associated DDL/DML operations.\nFor more information, refer to Resource locking.",
        "syntax": [
          "CREATE TABLE parent ... CONSTRAINT primary_key_1 PRIMARY KEY (c_1, c_2) ...\nCREATE TABLE child  ... CONSTRAINT foreign_key_1 FOREIGN KEY (...) REFERENCES parent (c_1, c_2) ..."
        ]
      },
      {
        "heading": "Examples",
        "description": "\nRename event table t1 to a1:\nChange the order of the clustering key for an event table:\nThe following example adds a row access policy on an event table while specifying a single column. After setting the policy, you can verify by checking\nthe information schema.\nThe following example adds a row access policy while specifying two columns in a single event table.\nThe following example drops a row access policy from an event table. Verify the policies were dropped by querying the\ninformation schema.\nThe following example shows how to combine adding and dropping row access policies in a single SQL statement for a table. Verify the\nresults by checking the information schema.\nOn this page\nSyntax\nParameters\nData Governance policy and tag actions (dataGovnPolicyTagAction)\nClustering actions (clusteringAction)\nSearch optimization actions (searchOptimizationAction)\nUsage notes\nExamples\nRelated content\nEvent table overview\nLogging, tracing, and metrics\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR REPLACE TABLE t1(a1 number);\n\nSHOW TABLES LIKE 't1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n Tue, 17 Mar 2015 16:52:33 -0700 | T1   | TESTDB        | MY_SCHEMA   | TABLE |         |            | 0    | 0     | PUBLIC | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n\nALTER TABLE t1 RENAME TO tt1;\n\nSHOW TABLES LIKE 'tt1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+\n Tue, 17 Mar 2015 16:52:33 -0700 | TT1  | TESTDB        | MY_SCHEMA   | TABLE |         |            | 0    | 0     | PUBLIC | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+",
          "CREATE OR REPLACE TABLE T1 (id NUMBER, date TIMESTAMP_NTZ, name STRING) CLUSTER BY (id, date);\n\nSHOW TABLES LIKE 'T1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (ID,DATE)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n\n-- Change the order of the clustering key\nALTER TABLE t1 CLUSTER BY (date, id);\n\nSHOW TABLES LIKE 'T1';\n\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n           created_on            | name | database_name | schema_name | kind  | comment | cluster_by | rows | bytes |    owner     | retention_time |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+\n Tue, 21 Jun 2016 15:42:12 -0700 | T1   | TESTDB        | TESTSCHEMA  | TABLE |         | (DATE,ID)  | 0    | 0     | ACCOUNTADMIN | 1              |\n---------------------------------+------+---------------+-------------+-------+---------+------------+------+-------+--------------+----------------+",
          "ALTER TABLE t1\n  ADD ROW ACCESS POLICY rap_t1 ON (empl_id);",
          "ALTER TABLE t1\n  ADD ROW ACCESS POLICY rap_test2 ON (cost, item);",
          "ALTER TABLE t1\n  DROP ROW ACCESS POLICY rap_v1;",
          "alter table t1\n  drop row access policy rap_t1_version_1,\n  add row access policy rap_t1_version_2 on (empl_id);"
        ]
      }
    ]
  },
  {
    "category": "SHOW EVENT TABLES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-event-tables",
    "details": [
      {
        "heading": "SHOW EVENT TABLES",
        "description": "\nLists the event tables for which you have access privileges, including\ndropped tables that are still within the Time Travel retention period and, therefore, can be undropped. The command can be used to list\nevent tables for the current/specified database or schema, or across your entire account.\nThe output returns table metadata and properties, ordered lexicographically by database, schema, and event table name (see\nOutput in this topic for descriptions of the output columns). This is important to note if you wish to filter the results using\nthe provided filters.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE EVENT TABLE, ALTER TABLE (event tables), DROP TABLE,\nUNDROP TABLE TABLES view (Information Schema)"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] EVENT TABLES [ LIKE '<pattern>' ]\n  [ IN { ACCOUNT | DATABASE [ <db_name> ] | SCHEMA [ <schema_name> ] } ]\n  [ STARTS WITH '<name_string>' ]\n  [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Optionally returns only a subset of the output columns: created_on name database_name schema_name Default: No value (all columns are included in the output)"
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "IN ACCOUNT | DATABASE [ db_name ] | SCHEMA [ schema_name ]",
            "definition": "Optionally specifies the scope of the command, which determines whether the command lists records only for the current/specified database or schema, or across your entire account. If you specify the keyword ACCOUNT, then the command retrieves records for all schemas in all databases\nof the current account. If you specify the keyword DATABASE, then: If you specify a db_name, then the command retrieves records for all schemas of the specified database. If you do not specify a db_name, then: If there is a current database, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and schemas in the account. If you specify the keyword SCHEMA, then: If you specify a qualified schema name (e.g. my_database.my_schema), then the command\nretrieves records for the specified database and schema. If you specify an unqualified schema_name, then: If there is a current database, then the command retrieves records for the specified schema in the current database. If there is no current database, then the command displays the error\nSQL compilation error: Object does not exist, or operation cannot be performed. If you do not specify a schema_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (i.e. the command returns the objects you have privileges to view in the current\ndatabase). No database: ACCOUNT is the default (i.e. the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the event table was created."
              ],
              [
                "name",
                "Name of the event table."
              ],
              [
                "database_name",
                "Database in which the event table is stored."
              ],
              [
                "schema_name",
                "Schema in which the event table is stored."
              ],
              [
                "owner",
                "Role that owns the event table."
              ],
              [
                "comment",
                "Comment for the event table."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nIf an account (or database or schema) has a large number of event tables, then searching the entire account (or table or schema)\ncan consume a significant amount of compute resources.\nIf an account (or database or schema) has a large number of event tables, then searching the entire account (or table or schema)\ncan consume a significant amount of compute resources.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nShow all the event tables whose name starts with mylogs that you have privileges to view in the tpch.public\nschema:\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nRelated content\nEvent table overview\nLogging, tracing, and metrics\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW EVENT TABLES LIKE 'mylogs%' IN tpch.public;"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE EVENT TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-event-table",
    "details": [
      {
        "heading": "DESCRIBE EVENT TABLE",
        "description": "\nDescribes the columns in an event table.\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER TABLE (event tables) , CREATE EVENT TABLE , SHOW EVENT TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] EVENT TABLE <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the event table to describe. If the identifier contains spaces or special characters, the entire\nstring must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThis command does not show the object parameters for a table. Instead, use\nSHOW PARAMETERS IN TABLE .\nThis command does not show the object parameters for a table. Instead, use\nSHOW PARAMETERS IN TABLE .\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nDescribe the columns in the event table named my_logged_events:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nRelated content\nEvent table overview\nLogging, tracing, and metrics\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESC EVENT TABLE my_logged_events;"
        ]
      }
    ]
  },
  {
    "category": "CREATE EXTERNAL TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-external-table",
    "details": [
      {
        "heading": "CREATE EXTERNAL TABLE",
        "description": "\nCreates a new external table in the current/specified schema\nor replaces an existing external table. When queried, an external table reads\ndata from a set of one or more files in a specified external stage and outputs the data in a single VARIANT column.\nAdditional columns can be defined, with each column definition consisting of a name, data type, and optionally whether the column requires\na value (NOT NULL) or has any referential integrity constraints (primary key, foreign key, etc.). For more information, see the usage notes.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER EXTERNAL TABLE , DROP EXTERNAL TABLE , SHOW EXTERNAL TABLES , DESCRIBE EXTERNAL TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:\nFor additional inline constraint details, see CREATE | ALTER TABLE  CONSTRAINT.",
        "syntax": [
          "-- Partitions computed from expressions\nCREATE [ OR REPLACE ] EXTERNAL TABLE [IF NOT EXISTS]\n  <table_name>\n    ( [ <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ]\n      [ inlineConstraint ]\n      [ , <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ... ]\n      [ , ... ] )\n  cloudProviderParams\n  [ PARTITION BY ( <part_col_name> [, <part_col_name> ... ] ) ]\n  [ WITH ] LOCATION = externalStage\n  [ REFRESH_ON_CREATE =  { TRUE | FALSE } ]\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ PATTERN = '<regex_pattern>' ]\n  FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET } [ formatTypeOptions ] } )\n  [ AWS_SNS_TOPIC = '<string>' ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON (VALUE) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\n-- Partitions added and removed manually\nCREATE [ OR REPLACE ] EXTERNAL TABLE [IF NOT EXISTS]\n  <table_name>\n    ( [ <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ]\n      [ inlineConstraint ]\n      [ , <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ... ]\n      [ , ... ] )\n  cloudProviderParams\n  [ PARTITION BY ( <part_col_name> [, <part_col_name> ... ] ) ]\n  [ WITH ] LOCATION = externalStage\n  PARTITION_TYPE = USER_SPECIFIED\n  FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET } [ formatTypeOptions ] } )\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON (VALUE) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\n-- Delta Lake\nCREATE [ OR REPLACE ] EXTERNAL TABLE [IF NOT EXISTS]\n  <table_name>\n    ( [ <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ]\n      [ inlineConstraint ]\n      [ , <col_name> <col_type> AS <expr> | <part_col_name> <col_type> AS <part_expr> ... ]\n      [ , ... ] )\n  cloudProviderParams\n  [ PARTITION BY ( <part_col_name> [, <part_col_name> ... ] ) ]\n  [ WITH ] LOCATION = externalStage\n  PARTITION_TYPE = USER_SPECIFIED\n  FILE_FORMAT = ( { FORMAT_NAME = '<file_format_name>' | TYPE = { CSV | JSON | AVRO | ORC | PARQUET } [ formatTypeOptions ] } )\n  [ TABLE_FORMAT = DELTA ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON (VALUE) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]",
          "inlineConstraint ::=\n  [ NOT NULL ]\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE | PRIMARY KEY | [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> ] ) ] }\n  [ <constraint_properties> ]",
          "cloudProviderParams (for Google Cloud Storage) ::=\n  [ INTEGRATION = '<integration_name>' ]\n\ncloudProviderParams (for Microsoft Azure) ::=\n  [ INTEGRATION = '<integration_name>' ]",
          "externalStage ::=\n  @[<namespace>.]<ext_stage_name>[/<path>]",
          "formatTypeOptions ::=\n-- If FILE_FORMAT = ( TYPE = CSV ... )\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     RECORD_DELIMITER = '<string>' | NONE\n     FIELD_DELIMITER = '<string>' | NONE\n     MULTI_LINE = TRUE | FALSE\n     SKIP_HEADER = <integer>\n     SKIP_BLANK_LINES = TRUE | FALSE\n     ESCAPE_UNENCLOSED_FIELD = '<character>' | NONE\n     TRIM_SPACE = TRUE | FALSE\n     FIELD_OPTIONALLY_ENCLOSED_BY = '<character>' | NONE\n     NULL_IF = ( '<string1>' [ , '<string2>' , ... ] )\n     EMPTY_FIELD_AS_NULL = TRUE | FALSE\n     ENCODING = '<string>'\n-- If FILE_FORMAT = ( TYPE = JSON ... )\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     MULTI_LINE = TRUE | FALSE\n     ALLOW_DUPLICATE = TRUE | FALSE\n     STRIP_OUTER_ARRAY = TRUE | FALSE\n     STRIP_NULL_VALUES = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n-- If FILE_FORMAT = ( TYPE = AVRO ... )\n     COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n-- If FILE_FORMAT = ( TYPE = ORC ... )\n     TRIM_SPACE = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE\n     NULL_IF = ( '<string>' [ , '<string>' ... ]\n-- If FILE_FORMAT = ( TYPE = PARQUET ... )\n     COMPRESSION = AUTO | SNAPPY | NONE\n     BINARY_AS_TEXT = TRUE | FALSE\n     REPLACE_INVALID_CHARACTERS = TRUE | FALSE"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE EXTERNAL TABLE  USING TEMPLATE",
        "description": "\nCreates a new external table with the column definitions derived from a set of staged files containing semi-structured data. This feature supports Apache Parquet, Apache Avro, ORC, JSON, and CSV files. The support for CSV and JSON files is currently in preview.\nNote\nIf the statement is replacing an existing table of the same name, then the grants are copied from the table\nbeing replaced. If there is no existing table of that name, then the grants are copied from the source table\nbeing cloned.\nFor more details about COPY GRANTS, see COPY GRANTS in this document.",
        "syntax": [
          "CREATE [ OR REPLACE ] EXTERNAL TABLE <table_name>\n  USING TEMPLATE <query>\n  [ ... ]\n  [ COPY GRANTS ]"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "table_name",
            "definition": "String that specifies the identifier (i.e. name) for the table; must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also case sensitive. For more details, see Identifier requirements."
          },
          {
            "term": "[ WITH ] LOCATION =",
            "definition": "Specifies the external stage and optional path where the files containing data to be read are staged: @[namespace.]ext_stage_name[/path] Files are in the specified named external stage. Neither string literals nor SQL variables are supported. Where: namespace is the database and/or schema in which the external stage resides, in the form of database_name.schema_name\nor schema_name. It is optional if a database and schema are currently in use within the user session; otherwise, it\nis required. path is an optional case-sensitive directory path for files in the cloud storage location that limits the set of files to load.\nPaths are alternatively called prefixes or folders by different cloud storage services. The external table appends this directory path to any path specified in the stage definition. To view the stage definition,\nexecute DESC STAGE stage_name and check the url property value. For example, if the stage URL includes\npath a and the external table location includes path b, then the external table reads files staged in\nstage/a/b. Note Specify a full directory path, and not a partial path (shared prefix) for files in your storage location (for example, use a path like @my_ext_stage/2025/\ninstead of @my_ext_stage/2025-*). To filter for files that share a common prefix, use partition columns instead. Note that the [ WITH ] LOCATION value cannot reference specific filenames. To point an external table to individual\nstaged files, use the PATTERN parameter."
          },
          {
            "term": "FILE_FORMAT = ( FORMAT_NAME = 'file_format_name' ) or . FILE_FORMAT = ( TYPE = CSV | JSON | AVRO | ORC | PARQUET [ ... ] )",
            "definition": "String (constant) that specifies the file format: Specifies an existing named file format that describes the staged data files to scan. The named file format determines the format\ntype (CSV, JSON, etc.), as well as any other format options, for data files. Specifies the format type of the staged data files to scan when querying the external table. If a file format type is specified, additional format-specific options can be specified. For more details, see\nFormat Type Options (in this topic). Default: TYPE = CSV. Important An external table does not inherit FILE_FORMAT options specified in a stage definition when that stage is used for loading data into the table. To specify FILE_FORMAT options, you must explicitly do so in the external table definition. Snowflake uses defaults for any FILE_FORMAT parameters omitted from the external table definition. Note FORMAT_NAME and TYPE are mutually exclusive; to avoid unintended behavior, you should only specify one or the other\nwhen creating an external table."
          },
          {
            "term": "FORMAT_NAME = file_format_name",
            "definition": "Specifies an existing named file format that describes the staged data files to scan. The named file format determines the format\ntype (CSV, JSON, etc.), as well as any other format options, for data files."
          },
          {
            "term": "TYPE = CSV | JSON | AVRO | ORC | PARQUET [ ... ]",
            "definition": "Specifies the format type of the staged data files to scan when querying the external table. If a file format type is specified, additional format-specific options can be specified. For more details, see\nFormat Type Options (in this topic)."
          },
          {
            "term": "FORMAT_NAME = file_format_name",
            "definition": "Specifies an existing named file format that describes the staged data files to scan. The named file format determines the format\ntype (CSV, JSON, etc.), as well as any other format options, for data files."
          },
          {
            "term": "TYPE = CSV | JSON | AVRO | ORC | PARQUET [ ... ]",
            "definition": "Specifies the format type of the staged data files to scan when querying the external table. If a file format type is specified, additional format-specific options can be specified. For more details, see\nFormat Type Options (in this topic)."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "syntax": [
          "mycol varchar as (value:c1::varchar)",
          "{ \"a\":\"1\", \"b\": { \"c\":\"2\", \"d\":\"3\" } }",
          "mycol varchar as (value:\"b\".\"c\"::varchar)"
        ],
        "definitions": [
          {
            "term": "col_name",
            "definition": "String that specifies the column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. For more details, see Identifier requirements."
          },
          {
            "term": "col_type",
            "definition": "String (constant) that specifies the data type for the column. The data type must match the result of expr for the column. For details about the data types that can be specified for table columns, see SQL data types reference."
          },
          {
            "term": "expr",
            "definition": "String that specifies the expression for the column. When queried, the column returns results derived from this expression. External table columns are virtual columns, which are defined using an explicit expression. Add virtual columns as expressions using the\nVALUE column and/or the METADATA$FILENAME pseudocolumn: A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (i.e.\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file: A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage. For\nan example, see Partitions Added Automatically From Partition Column Expressions (in this topic)."
          },
          {
            "term": "VALUE:",
            "definition": "A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (i.e.\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "CSV:",
            "definition": "The VALUE column structures each row as an object with elements identified by column position (i.e.\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
          },
          {
            "term": "Semi-structured data:",
            "definition": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "METADATA$FILENAME:",
            "definition": "A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage. For\nan example, see Partitions Added Automatically From Partition Column Expressions (in this topic)."
          },
          {
            "term": "CONSTRAINT ...",
            "definition": "String that defines an inline or out-of-line constraint for the specified column(s) in the table. For syntax details, see CREATE | ALTER TABLE  CONSTRAINT. For more information about constraints, see\nConstraints."
          },
          {
            "term": "REFRESH_ON_CREATE =  TRUE | FALSE",
            "definition": "Specifies whether to automatically refresh the external table metadata once, immediately after the external table is created. Refreshing\nthe external table metadata synchronizes the metadata with the current list of data files in the specified stage path. This action is\nrequired for the metadata to register any existing data files in the named external stage specified in the\n[ WITH ] LOCATION = setting. Snowflake automatically refreshes the external table metadata once after creation. Note If the specified location contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE. After creating the external table, refresh the metadata\nincrementally by executing ALTER EXTERNAL TABLE  REFRESH statements that specify subpaths in\nthe location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location. Snowflake does not automatically refresh the external table metadata. To register any existing data files in the stage, you must\nmanually refresh the external table metadata once using ALTER EXTERNAL TABLE  REFRESH. Default: TRUE"
          },
          {
            "term": "TRUE",
            "definition": "Snowflake automatically refreshes the external table metadata once after creation. Note If the specified location contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE. After creating the external table, refresh the metadata\nincrementally by executing ALTER EXTERNAL TABLE  REFRESH statements that specify subpaths in\nthe location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
          },
          {
            "term": "FALSE",
            "definition": "Snowflake does not automatically refresh the external table metadata. To register any existing data files in the stage, you must\nmanually refresh the external table metadata once using ALTER EXTERNAL TABLE  REFRESH."
          },
          {
            "term": "AUTO_REFRESH =  TRUE | FALSE",
            "definition": "Specifies whether Snowflake should enable triggering automatic refreshes of the external table metadata when new or updated data\nfiles are available in the named external stage specified in the [ WITH ] LOCATION = setting. Note Setting this parameter to TRUE is not supported by partitioned external tables when partitions are added manually by the\nobject owner (i.e. when PARTITION_TYPE = USER_SPECIFIED). Setting this parameter to TRUE is not supported for external tables that reference data files\nin S3-compatible storage (a storage application or device\nthat provides an API compliant with the S3 REST API). For more information, see Working with Amazon S3-compatible storage. You must manually refresh the metadata by executing an ALTER EXTERNAL TABLE  REFRESH command. You must configure an event notification for your storage location to notify Snowflake when new or updated data is available\nto read into the external table metadata. For more information, see the instructions for your cloud storage service: Refreshing external tables automatically for Amazon S3 Refreshing external tables automatically for Google Cloud Storage Refreshing external tables automatically for Azure Blob Storage When an external table is created, its metadata is refreshed automatically once unless REFRESH_ON_CREATE = FALSE. Snowflake enables triggering automatic refreshes of the external table metadata. Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table\nmetadata periodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the\nstage path. Default: TRUE"
          },
          {
            "term": "Amazon S3:",
            "definition": "Refreshing external tables automatically for Amazon S3"
          },
          {
            "term": "Google Cloud Storage:",
            "definition": "Refreshing external tables automatically for Google Cloud Storage"
          },
          {
            "term": "Microsoft Azure:",
            "definition": "Refreshing external tables automatically for Azure Blob Storage"
          },
          {
            "term": "TRUE",
            "definition": "Snowflake enables triggering automatic refreshes of the external table metadata."
          },
          {
            "term": "FALSE",
            "definition": "Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table\nmetadata periodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the\nstage path."
          },
          {
            "term": "VALUE:",
            "definition": "A VARIANT type column that represents a single row in the external file. The VALUE column structures each row as an object with elements identified by column position (i.e.\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files: Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "CSV:",
            "definition": "The VALUE column structures each row as an object with elements identified by column position (i.e.\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
          },
          {
            "term": "Semi-structured data:",
            "definition": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "METADATA$FILENAME:",
            "definition": "A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage. For\nan example, see Partitions Added Automatically From Partition Column Expressions (in this topic)."
          },
          {
            "term": "CSV:",
            "definition": "The VALUE column structures each row as an object with elements identified by column position (i.e.\n{c1: <column_1_value>, c2: <column_2_value>, c3: <column_1_value> ...}). For example, add a VARCHAR column named mycol that references the first column in the staged CSV files:"
          },
          {
            "term": "Semi-structured data:",
            "definition": "Enclose element names and values in double-quotes. Traverse the path in the VALUE column using dot notation. For example, suppose the following represents a single row of semi-structured data in a staged file: Add a VARCHAR column named mycol that references the nested repeating c element in the staged file:"
          },
          {
            "term": "TRUE",
            "definition": "Snowflake automatically refreshes the external table metadata once after creation. Note If the specified location contains close to 1 million files or more, we recommend that you\nset REFRESH_ON_CREATE = FALSE. After creating the external table, refresh the metadata\nincrementally by executing ALTER EXTERNAL TABLE  REFRESH statements that specify subpaths in\nthe location (i.e. subsets of files to include in the refresh) until the metadata includes\nall of the files in the location."
          },
          {
            "term": "FALSE",
            "definition": "Snowflake does not automatically refresh the external table metadata. To register any existing data files in the stage, you must\nmanually refresh the external table metadata once using ALTER EXTERNAL TABLE  REFRESH."
          },
          {
            "term": "Amazon S3:",
            "definition": "Refreshing external tables automatically for Amazon S3"
          },
          {
            "term": "Google Cloud Storage:",
            "definition": "Refreshing external tables automatically for Google Cloud Storage"
          },
          {
            "term": "Microsoft Azure:",
            "definition": "Refreshing external tables automatically for Azure Blob Storage"
          },
          {
            "term": "TRUE",
            "definition": "Snowflake enables triggering automatic refreshes of the external table metadata."
          },
          {
            "term": "FALSE",
            "definition": "Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table\nmetadata periodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the\nstage path."
          },
          {
            "term": "PATTERN = 'regex_pattern'",
            "definition": "A regular expression pattern string, enclosed in single quotes, specifying the filenames and/or paths on the external stage to match. Tip For the best performance, try to avoid applying patterns that filter on a large number of files."
          },
          {
            "term": "AWS_SNS_TOPIC = 'string'",
            "definition": "Required only when configuring AUTO_REFRESH for Amazon S3 stages using Amazon Simple Notification Service (SNS). Specifies the\nAmazon Resource Name (ARN) for the SNS topic for your S3 bucket. The CREATE EXTERNAL TABLE statement subscribes the Amazon Simple Queue\nService (SQS) queue to the specified SNS topic. Event notifications via the SNS topic trigger metadata refreshes. For more information,\nsee Refreshing external tables automatically for Amazon S3."
          },
          {
            "term": "TABLE_FORMAT = DELTA",
            "definition": "Note This feature is still supported but will be deprecated in a future release. Consider using an Apache Iceberg table instead. Iceberg tables\nuse an external volume\nto connect to Delta table files in your cloud storage. For more information, see Iceberg tables and CREATE ICEBERG TABLE (Delta files in object storage).\nYou can also Migrate a Delta external table to Apache Iceberg. Identifies the external table as referencing a Delta Lake on the cloud storage location. A Delta Lake on Amazon S3, Google Cloud Storage,\nor Microsoft Azure cloud storage is supported. Note This preview feature is available to all accounts. When this parameter is set, the external table scans for Delta Lake transaction log files in the [ WITH ] LOCATION location.\nDelta log files have names like _delta_log/00000000000000000000.json,\n_delta_log/00000000000000000010.checkpoint.parquet, etc. When the metadata for an external table is refreshed, Snowflake parses the Delta Lake transaction logs and determines which Parquet\nfiles are current. In the background, the refresh performs add and remove file operations to keep the external table metadata in sync. Note The external stage and optional path specified in [ WITH ] LOCATION = must contain the data files and metadata for a\nsingle Delta Lake table only. That is, the specified storage location can only contain one __delta_log\ndirectory. The ordering of event notifications triggered by DDL operations in cloud storage is not guaranteed. Therefore, the ability to\nautomatically refresh is not available for external tables that reference Delta Lake files. Both REFRESH_ON_CREATE and\nAUTO_REFRESH must be set to FALSE. Periodically execute an ALTER EXTERNAL TABLE  REFRESH statement to register any\nadded or removed files. The FILE_FORMAT value must specify Parquet as the file type. For optimal performance, we recommend defining partition columns for the external table. The following parameters are not supported when referencing a Delta Lake: AWS_SNS_TOPIC = 'string' PATTERN = 'regex_pattern'"
          },
          {
            "term": "COPY GRANTS",
            "definition": "Specifies to retain the access permissions from the original table when an external table is recreated using the CREATE OR REPLACE TABLE\nvariant. The parameter copies all permissions, except OWNERSHIP, from the existing table to the new table. By default, the role\nthat executes the CREATE EXTERNAL TABLE command owns the new external table. Note The operation to copy grants occurs atomically in the CREATE EXTERNAL TABLE command (i.e. within the same transaction)."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "String (literal) that specifies a comment for the external table. Default: No value"
          },
          {
            "term": "ROW ACCESS POLICY <policy_name> ON (VALUE)",
            "definition": "Specifies the row access policy to set on the table. Specify the VALUE column when applying a row access policy to an external table."
          },
          {
            "term": "TAG ( tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ] )",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "WITH CONTACT ( purpose = contact [ , purpose = contact ...] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the new object with one or more contacts."
          }
        ]
      },
      {
        "heading": "Partitioning parameters",
        "description": "\nUse these parameters to partition your external table.",
        "syntax": [
          "col1 varchar as (parse_json(metadata$external_table_partition):col1::varchar),\ncol2 number as (parse_json(metadata$external_table_partition):col2::number),\ncol3 timestamp_tz as (parse_json(metadata$external_table_partition):col3::timestamp_tz)"
        ],
        "definitions": [
          {
            "term": "part_col_name col_type AS part_expr",
            "definition": "Defines one or more partition columns in the external table. The format of a partition column definition differs depending on whether partitions are computed and added automatically from an\nexpression in each partition column or the partitions are added manually. A partition column must evaluate as an expression that parses the path and/or filename information in the METADATA$FILENAME\npseudocolumn. Partition columns optimize query performance by pruning out the data files that do not need to be scanned (i.e.\npartitioning the external table). A partition consists of all data files that match the path and/or filename in the expression for\nthe partition column. part_col_name String that specifies the partition column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. col_type String (constant) that specifies the data type for the column. The data type must match the result of part_expr for the column. part_expr String that specifies the expression for the column. The expression must include the METADATA$FILENAME pseudocolumn. External tables currently support the following subset of functions in partition expressions: List of supported functions: =, <>, >, >=, <, <= || +, - - (negate) * AND, OR ARRAY_CONSTRUCT CASE CAST , :: CONCAT , || ENDSWITH IS [ NOT ] NULL IFF IFNULL [ NOT ] IN LOWER NOT NULLIF NVL2 SPLIT_PART STARTSWITH SUBSTR , SUBSTRING UPPER ZEROIFNULL Required: Also set the PARTITION_TYPE parameter value to USER_SPECIFIED. A partition column definition is an expression that parses the column metadata in the internal (hidden)\nMETADATA$EXTERNAL_TABLE_PARTITION column. Essentially, the definition only defines the data type for the column. The format of the\npartition column definition is as follows: part_col_name col_type AS ( PARSE_JSON (METADATA$EXTERNALTABLE_PARTITION):part_col_name::data_type ) For example, suppose columns col1, col2, and col3 contain varchar, number, and timestamp (time zone) data, respectively: After defining any partition columns for the table, identify these columns using the PARTITION BY clause. Note The maximum length of user-specified partition column names is 32 characters."
          },
          {
            "term": "Added from an expression:",
            "definition": "A partition column must evaluate as an expression that parses the path and/or filename information in the METADATA$FILENAME\npseudocolumn. Partition columns optimize query performance by pruning out the data files that do not need to be scanned (i.e.\npartitioning the external table). A partition consists of all data files that match the path and/or filename in the expression for\nthe partition column. part_col_name String that specifies the partition column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. col_type String (constant) that specifies the data type for the column. The data type must match the result of part_expr for the column. part_expr String that specifies the expression for the column. The expression must include the METADATA$FILENAME pseudocolumn. External tables currently support the following subset of functions in partition expressions: List of supported functions: =, <>, >, >=, <, <= || +, - - (negate) * AND, OR ARRAY_CONSTRUCT CASE CAST , :: CONCAT , || ENDSWITH IS [ NOT ] NULL IFF IFNULL [ NOT ] IN LOWER NOT NULLIF NVL2 SPLIT_PART STARTSWITH SUBSTR , SUBSTRING UPPER ZEROIFNULL"
          },
          {
            "term": "Added manually:",
            "definition": "Required: Also set the PARTITION_TYPE parameter value to USER_SPECIFIED. A partition column definition is an expression that parses the column metadata in the internal (hidden)\nMETADATA$EXTERNAL_TABLE_PARTITION column. Essentially, the definition only defines the data type for the column. The format of the\npartition column definition is as follows: part_col_name col_type AS ( PARSE_JSON (METADATA$EXTERNALTABLE_PARTITION):part_col_name::data_type ) For example, suppose columns col1, col2, and col3 contain varchar, number, and timestamp (time zone) data, respectively:"
          },
          {
            "term": "PARTITION_TYPE = USER_SPECIFIED",
            "definition": "Defines the partition type for the external table as user-defined. The owner of the external table (i.e. the role that has the\nOWNERSHIP privilege on the external table) must add partitions to the external metadata manually by executing ALTER EXTERNAL\nTABLE  ADD PARTITION statements. Do not set this parameter if partitions are added to the external table metadata automatically upon evaluation of expressions\nin the partition columns."
          },
          {
            "term": "[ PARTITION BY ( part_col_name [, part_col_name ... ] ) ]",
            "definition": "Specifies any partition columns to evaluate for the external table. When querying an external table, include one or more partition columns in a WHERE clause, e.g.: ... WHERE part_col_name = 'filter_value' Snowflake filters on the partition columns to restrict the set of data files to scan. Note that all rows in these files are scanned.\nIf a WHERE clause includes non-partition columns, those filters are evaluated after the data files have been filtered. A common practice is to partition the data files based on increments of time; or, if the data files are staged from multiple sources,\nto partition by a data source identifier and date or timestamp."
          },
          {
            "term": "Usage:",
            "definition": "When querying an external table, include one or more partition columns in a WHERE clause, e.g.: ... WHERE part_col_name = 'filter_value' Snowflake filters on the partition columns to restrict the set of data files to scan. Note that all rows in these files are scanned.\nIf a WHERE clause includes non-partition columns, those filters are evaluated after the data files have been filtered. A common practice is to partition the data files based on increments of time; or, if the data files are staged from multiple sources,\nto partition by a data source identifier and date or timestamp."
          },
          {
            "term": "Added from an expression:",
            "definition": "A partition column must evaluate as an expression that parses the path and/or filename information in the METADATA$FILENAME\npseudocolumn. Partition columns optimize query performance by pruning out the data files that do not need to be scanned (i.e.\npartitioning the external table). A partition consists of all data files that match the path and/or filename in the expression for\nthe partition column. part_col_name String that specifies the partition column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. col_type String (constant) that specifies the data type for the column. The data type must match the result of part_expr for the column. part_expr String that specifies the expression for the column. The expression must include the METADATA$FILENAME pseudocolumn. External tables currently support the following subset of functions in partition expressions: List of supported functions: =, <>, >, >=, <, <= || +, - - (negate) * AND, OR ARRAY_CONSTRUCT CASE CAST , :: CONCAT , || ENDSWITH IS [ NOT ] NULL IFF IFNULL [ NOT ] IN LOWER NOT NULLIF NVL2 SPLIT_PART STARTSWITH SUBSTR , SUBSTRING UPPER ZEROIFNULL"
          },
          {
            "term": "Added manually:",
            "definition": "Required: Also set the PARTITION_TYPE parameter value to USER_SPECIFIED. A partition column definition is an expression that parses the column metadata in the internal (hidden)\nMETADATA$EXTERNAL_TABLE_PARTITION column. Essentially, the definition only defines the data type for the column. The format of the\npartition column definition is as follows: part_col_name col_type AS ( PARSE_JSON (METADATA$EXTERNALTABLE_PARTITION):part_col_name::data_type ) For example, suppose columns col1, col2, and col3 contain varchar, number, and timestamp (time zone) data, respectively:"
          },
          {
            "term": "Usage:",
            "definition": "When querying an external table, include one or more partition columns in a WHERE clause, e.g.: ... WHERE part_col_name = 'filter_value' Snowflake filters on the partition columns to restrict the set of data files to scan. Note that all rows in these files are scanned.\nIf a WHERE clause includes non-partition columns, those filters are evaluated after the data files have been filtered. A common practice is to partition the data files based on increments of time; or, if the data files are staged from multiple sources,\nto partition by a data source identifier and date or timestamp."
          }
        ]
      },
      {
        "heading": "Cloud provider parameters (cloudProviderParams)",
        "description": "\nGoogle Cloud Storage\nMicrosoft Azure",
        "definitions": [
          {
            "term": "INTEGRATION = integration_name",
            "definition": "Specifies the name of the notification integration used to automatically refresh the external table metadata using Google Pub/Sub\nevent notifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party\ncloud message queuing services. This parameter is required to enable auto-refresh operations for the external table. For instructions on configuring the\nauto-refresh capability, see Refreshing external tables automatically for Google Cloud Storage."
          },
          {
            "term": "INTEGRATION = integration_name",
            "definition": "Specifies the name of the notification integration used to automatically refresh the external table metadata using Azure Event Grid\nnotifications. A notification integration is a Snowflake object that provides an interface between Snowflake and third-party cloud\nmessage queuing services. This parameter is required to enable auto-refresh operations for the external table. For instructions on configuring the auto-refresh\ncapability, see Refreshing external tables automatically for Azure Blob Storage."
          }
        ]
      },
      {
        "heading": "Format type options (formatTypeOptions)",
        "description": "\nFormat type options are used for loading data into and unloading data out of\ntables.\nDepending on the file format type specified (FILE_FORMAT = ( TYPE = ... )), you can include one or more of the following\nformat-specific options (separated by blank spaces, commas, or new lines):"
      },
      {
        "heading": "TYPE = CSV",
        "tables": [
          {
            "headers": [
              "Supported Values",
              "Notes"
            ],
            "rows": [
              [
                "AUTO",
                "Compression algorithm detected automatically, except for Brotli-compressed files, which cannot currently be detected automatically. If querying Brotli-compressed files, explicitly use BROTLI instead of AUTO."
              ],
              [
                "GZIP",
                ""
              ],
              [
                "BZ2",
                ""
              ],
              [
                "BROTLI",
                "Must be specified when querying Brotli-compressed files."
              ],
              [
                "ZSTD",
                "Zstandard v0.8 (and higher) supported."
              ],
              [
                "DEFLATE",
                "Deflate-compressed files (with zlib header, RFC1950)."
              ],
              [
                "RAW_DEFLATE",
                "Raw Deflate-compressed files (without header, RFC1951)."
              ],
              [
                "NONE",
                "Data files have not been compressed."
              ]
            ]
          },
          {
            "headers": [
              "CharacterÂ Set",
              "ENCODINGÂ Value",
              "Supported Languages",
              "Notes"
            ],
            "rows": [
              [
                "Big5",
                "BIG5",
                "Traditional Chinese",
                ""
              ],
              [
                "EUC-JP",
                "EUCJP",
                "Japanese",
                ""
              ],
              [
                "EUC-KR",
                "EUCKR",
                "Korean",
                ""
              ],
              [
                "GB18030",
                "GB18030",
                "Chinese",
                ""
              ],
              [
                "IBM420",
                "IBM420",
                "Arabic",
                ""
              ],
              [
                "IBM424",
                "IBM424",
                "Hebrew",
                ""
              ],
              [
                "IBM949",
                "IBM949",
                "Korean",
                ""
              ],
              [
                "ISO-2022-CN",
                "ISO2022CN",
                "Simplified Chinese",
                ""
              ],
              [
                "ISO-2022-JP",
                "ISO2022JP",
                "Japanese",
                ""
              ],
              [
                "ISO-2022-KR",
                "ISO2022KR",
                "Korean",
                ""
              ],
              [
                "ISO-8859-1",
                "ISO88591",
                "Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese, Swedish",
                ""
              ],
              [
                "ISO-8859-2",
                "ISO88592",
                "Czech, Hungarian, Polish, Romanian",
                ""
              ],
              [
                "ISO-8859-5",
                "ISO88595",
                "Russian",
                ""
              ],
              [
                "ISO-8859-6",
                "ISO88596",
                "Arabic",
                ""
              ],
              [
                "ISO-8859-7",
                "ISO88597",
                "Greek",
                ""
              ],
              [
                "ISO-8859-8",
                "ISO88598",
                "Hebrew",
                ""
              ],
              [
                "ISO-8859-9",
                "ISO88599",
                "Turkish",
                ""
              ],
              [
                "ISO-8859-15",
                "ISO885915",
                "Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese, Swedish",
                "Identical to ISO-8859-1 except for 8 characters, including the Euro currency symbol."
              ],
              [
                "KOI8-R",
                "KOI8R",
                "Russian",
                ""
              ],
              [
                "Shift_JIS",
                "SHIFTJIS",
                "Japanese",
                ""
              ],
              [
                "UTF-8",
                "UTF8",
                "All languages",
                "For loading data from delimited files (CSV, TSV, etc.), UTF-8 is the default. . . For loading data from all other supported file formats (JSON, Avro, etc.), as well as unloading data, UTF-8 is the only supported character set."
              ],
              [
                "UTF-16",
                "UTF16",
                "All languages",
                ""
              ],
              [
                "UTF-16BE",
                "UTF16BE",
                "All languages",
                ""
              ],
              [
                "UTF-16LE",
                "UTF16LE",
                "All languages",
                ""
              ],
              [
                "UTF-32",
                "UTF32",
                "All languages",
                ""
              ],
              [
                "UTF-32BE",
                "UTF32BE",
                "All languages",
                ""
              ],
              [
                "UTF-32LE",
                "UTF32LE",
                "All languages",
                ""
              ],
              [
                "windows-874",
                "WINDOWS874",
                "Thai",
                ""
              ],
              [
                "windows-949",
                "WINDOWS949",
                "Korean",
                ""
              ],
              [
                "windows-1250",
                "WINDOWS1250",
                "Czech, Hungarian, Polish, Romanian",
                ""
              ],
              [
                "windows-1251",
                "WINDOWS1251",
                "Russian",
                ""
              ],
              [
                "windows-1252",
                "WINDOWS1252",
                "Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese, Swedish",
                ""
              ],
              [
                "windows-1253",
                "WINDOWS1253",
                "Greek",
                ""
              ],
              [
                "windows-1254",
                "WINDOWS1254",
                "Turkish",
                ""
              ],
              [
                "windows-1255",
                "WINDOWS1255",
                "Hebrew",
                ""
              ],
              [
                "windows-1256",
                "WINDOWS1256",
                "Arabic",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "TYPE = JSON",
        "tables": [
          {
            "headers": [
              "Supported Values",
              "Notes"
            ],
            "rows": [
              [
                "AUTO",
                "Compression algorithm detected automatically, except for Brotli-compressed files, which cannot currently be detected automatically. If querying Brotli-compressed files, explicitly use BROTLI instead of AUTO."
              ],
              [
                "GZIP",
                ""
              ],
              [
                "BZ2",
                ""
              ],
              [
                "BROTLI",
                ""
              ],
              [
                "ZSTD",
                ""
              ],
              [
                "DEFLATE",
                "Deflate-compressed files (with zlib header, RFC1950)."
              ],
              [
                "RAW_DEFLATE",
                "Raw Deflate-compressed files (without header, RFC1951)."
              ],
              [
                "NONE",
                "Indicates the files have not been compressed."
              ]
            ]
          },
          {
            "headers": [
              "Before",
              "After"
            ],
            "rows": [
              [
                "[null]",
                "[]"
              ],
              [
                "[null,null,3]",
                "[,,3]"
              ],
              [
                "{\"a\":null,\"b\":null,\"c\":123}",
                "{\"c\":123}"
              ],
              [
                "{\"a\":[1,null,2],\"b\":{\"x\":null,\"y\":88}}",
                "{\"a\":[1,,2],\"b\":{\"y\":88}}"
              ]
            ]
          }
        ]
      },
      {
        "heading": "TYPE = AVRO",
        "tables": [
          {
            "headers": [
              "Supported Values",
              "Notes"
            ],
            "rows": [
              [
                "AUTO",
                "Compression algorithm detected automatically, except for Brotli-compressed files, which cannot currently be detected automatically. If querying Brotli-compressed files, explicitly use BROTLI instead of AUTO."
              ],
              [
                "GZIP",
                ""
              ],
              [
                "BZ2",
                ""
              ],
              [
                "BROTLI",
                ""
              ],
              [
                "ZSTD",
                ""
              ],
              [
                "DEFLATE",
                "Deflate-compressed files (with zlib header, RFC1950)."
              ],
              [
                "RAW_DEFLATE",
                "Raw Deflate-compressed files (without header, RFC1951)."
              ],
              [
                "NONE",
                "Data files to query have not been compressed."
              ]
            ]
          }
        ]
      },
      {
        "heading": "TYPE = ORC",
        "definitions": [
          {
            "term": "TRIM_SPACE = TRUE | FALSE",
            "definition": "Boolean that specifies whether to remove leading and trailing white space from strings. For example, if your external database software encloses fields in quotes, but inserts a leading space, Snowflake reads the leading space rather than the opening quotation character as the beginning of the field (that is, the quotation marks are interpreted as part of the string of field data). Set this option to TRUE to remove undesirable spaces. This file format option is applied to the following actions only: Querying object values in staged ORC data files. Querying ORC data in separate columns using the MATCH_BY_COLUMN_NAME copy option. Querying ORC data in separate columns by specifying a query in the COPY statement (that is, COPY transformation). Default: FALSE"
          },
          {
            "term": "REPLACE_INVALID_CHARACTERS = TRUE | FALSE",
            "definition": "Boolean that specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (). This\noption performs a one-to-one character replacement. If set to TRUE, Snowflake replaces invalid UTF-8 characters with the Unicode replacement character. If set to FALSE, the load operation produces an error when invalid UTF-8 character encoding is detected. Default: FALSE"
          },
          {
            "term": "NULL_IF = ( 'string1' [ , 'string2' , ... ] )",
            "definition": "String used to convert to and from SQL NULL. Snowflake replaces these strings in the data source with SQL NULL. To specify more than\none string, enclose the list of strings in parentheses and use commas to separate each value. Note that Snowflake converts all instances of the value to NULL, regardless of the data type. For example, if 2 is specified as a\nvalue, all instances of 2 as either a string or number are converted. For example: NULL_IF = ('\\N', 'NULL', 'NUL', '') Note that this option can include empty strings. This file format option is applied when querying object values in staged ORC data files. Default: \\N (that is, NULL)"
          }
        ]
      },
      {
        "heading": "TYPE = PARQUET",
        "tables": [
          {
            "headers": [
              "Supported Values",
              "Notes"
            ],
            "rows": [
              [
                "AUTO",
                "Compression algorithm detected automatically. Supports the following compression algorithms: Brotli, gzip, Lempel-Ziv-Oberhumer (LZO), LZ4, Snappy, or Zstandard v0.8 (and higher)."
              ],
              [
                "SNAPPY",
                ""
              ],
              [
                "NONE",
                "Data files have not been compressed."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE EXTERNAL TABLE",
                "Schema",
                ""
              ],
              [
                "CREATE STAGE",
                "Schema",
                "Required if creating a new stage."
              ],
              [
                "USAGE",
                "Stage",
                "Required if referencing an existing stage."
              ],
              [
                "USAGE",
                "File format",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nExternal tables support external (S3, Azure, or GCS) stages only; internal (Snowflake) stages are not supported.\nExternal tables dont support storage versioning (S3 versioning, Object Versioning in Google Cloud Storage, or versioning for Azure Storage).\nYou cannot access data held in archival cloud storage classes that requires restoration before it can be retrieved. These archival storage classes include, for example, the Amazon S3 Glacier Flexible Retrieval or Glacier Deep Archive storage class, or Microsoft Azure Archive Storage.\nSnowflake does not enforce integrity constraints on external tables. In particular, unlike normal tables, Snowflake does not enforce\nNOT NULL constraints.\nExternal tables include the following metadata column:\n\nMETADATA$FILENAME: Name of each staged data file included in the external table. Includes the path to the data file in the stage.\nMETADATA$FILE_ROW_NUMBER: Row number for each record in the staged data file.\nMETADATA$FILENAME: Name of each staged data file included in the external table. Includes the path to the data file in the stage.\nMETADATA$FILE_ROW_NUMBER: Row number for each record in the staged data file.\nExternal tables support external (S3, Azure, or GCS) stages only; internal (Snowflake) stages are not supported.\nExternal tables dont support storage versioning (S3 versioning, Object Versioning in Google Cloud Storage, or versioning for Azure Storage).\nYou cannot access data held in archival cloud storage classes that requires restoration before it can be retrieved. These archival storage classes include, for example, the Amazon S3 Glacier Flexible Retrieval or Glacier Deep Archive storage class, or Microsoft Azure Archive Storage.\nSnowflake does not enforce integrity constraints on external tables. In particular, unlike normal tables, Snowflake does not enforce\nNOT NULL constraints.\nExternal tables include the following metadata column:\nMETADATA$FILENAME: Name of each staged data file included in the external table. Includes the path to the data file in the stage.\nMETADATA$FILE_ROW_NUMBER: Row number for each record in the staged data file.\nMETADATA$FILENAME: Name of each staged data file included in the external table. Includes the path to the data file in the stage.\nMETADATA$FILE_ROW_NUMBER: Row number for each record in the staged data file.\nThe following are not supported for external tables:\n\nClustering keys\nCloning\nData in XML format\nClustering keys\nCloning\nData in XML format\nTime Travel is not supported for external tables.\nFor details about using an external table with a policy, see:\n\nMasking policies and external tables.\nRow access policies and external tables.\nMasking policies and external tables.\nRow access policies and external tables.\nUsing OR REPLACE is the equivalent of using DROP EXTERNAL TABLE on the existing external table and then creating a new\nexternal table with the same name.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE EXTERNAL TABLE operation use either the old or new external table version.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nWhen creating an external table with a row access policy added to the external table, use the\nPOLICY_CONTEXT function to simulate a query on the external table protected by a row access policy.\nSELECT * always returns the VALUE column, in which all regular or semi-structured data is cast to variant rows.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nThe following are not supported for external tables:\nClustering keys\nCloning\nData in XML format\nClustering keys\nCloning\nData in XML format\nTime Travel is not supported for external tables.\nFor details about using an external table with a policy, see:\nMasking policies and external tables.\nRow access policies and external tables.\nMasking policies and external tables.\nRow access policies and external tables.\nUsing OR REPLACE is the equivalent of using DROP EXTERNAL TABLE on the existing external table and then creating a new\nexternal table with the same name.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE EXTERNAL TABLE operation use either the old or new external table version.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nWhen creating an external table with a row access policy added to the external table, use the\nPOLICY_CONTEXT function to simulate a query on the external table protected by a row access policy.\nSELECT * always returns the VALUE column, in which all regular or semi-structured data is cast to variant rows.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement."
      },
      {
        "heading": "Examples"
      },
      {
        "heading": "Partitions added automatically from partition column expressions",
        "description": "\nCreate an external table with partitions computed from expressions in the partition column definitions.\nIn the following example, the data files are organized in cloud storage with the following structure: logs/YYYY/MM/DD/HH24.\nFor example:\nlogs/2018/08/05/0524/\nlogs/2018/08/27/1408/\nlogs/2018/08/05/0524/\nlogs/2018/08/27/1408/\nCreate an external stage named s1 for the storage location where the data files are stored. For more information, see\nCREATE STAGE.\nThe stage definition includes the path /files/logs/:\nAmazon S3\n\nCREATE STAGE s1\n  URL='s3://mybucket/files/logs/'\n  ...\n  ;\n\nCopy\n\nGoogle Cloud Storage\n\nCREATE STAGE s1\n  URL='gcs://mybucket/files/logs/'\n  ...\n  ;\n\nCopy\n\nMicrosoft Azure\n\nCREATE STAGE s1\n  URL='azure://mycontainer/files/logs/'\n  ...\n  ;\n\nCopy\nQuery the METADATA$FILENAME pseudocolumn in the staged data. Use the results to develop your partition column(s):\nSELECT metadata$filename FROM @s1/;\n\n+----------------------------------------+\n| METADATA$FILENAME                      |\n|----------------------------------------|\n| files/logs/2018/08/05/0524/log.parquet |\n| files/logs/2018/08/27/1408/log.parquet |\n+----------------------------------------+\n\nCopy\nCreate the partitioned external table.\nThe partition column date_part casts YYYY/MM/DD in the METADATA$FILENAME pseudocolumn as a date using\nTO_DATE , DATE. The SQL command also specifies Parquet as the file format type.\nThe external tables for Amazon S3 and Microsoft Azure cloud storage include the parameter required to refresh the metadata\nautomatically when triggered by event notifications from the respective cloud messaging service:\nAmazon S3\n\nCREATE EXTERNAL TABLE et1(\n date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n   || '/' || SPLIT_PART(metadata$filename, '/', 4)\n   || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n timestamp bigint AS (value:timestamp::bigint),\n col2 varchar AS (value:col2::varchar))\n PARTITION BY (date_part)\n LOCATION=@s1/logs/\n AUTO_REFRESH = true\n FILE_FORMAT = (TYPE = PARQUET)\n AWS_SNS_TOPIC = 'arn:aws:sns:us-west-2:001234567890:s3_mybucket';\n\nCopy\n\nGoogle Cloud Storage\n\nCREATE EXTERNAL TABLE et1(\n  date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n    || '/' || SPLIT_PART(metadata$filename, '/', 4)\n    || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n  timestamp bigint AS (value:timestamp::bigint),\n  col2 varchar AS (value:col2::varchar))\n  PARTITION BY (date_part)\n  LOCATION=@s1/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET);\n\nCopy\n\nMicrosoft Azure\n\nCREATE EXTERNAL TABLE et1(\n  date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n    || '/' || SPLIT_PART(metadata$filename, '/', 4)\n    || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n  timestamp bigint AS (value:timestamp::bigint),\n  col2 varchar AS (value:col2::varchar))\n  PARTITION BY (date_part)\n  INTEGRATION = 'MY_INT'\n  LOCATION=@s1/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET);\n\nCopy\nRefresh the external table metadata:\nALTER EXTERNAL TABLE et1 REFRESH;\n\nCopy\nCreate an external stage named s1 for the storage location where the data files are stored. For more information, see\nCREATE STAGE.\nThe stage definition includes the path /files/logs/:\nAmazon S3\nGoogle Cloud Storage\nMicrosoft Azure\nQuery the METADATA$FILENAME pseudocolumn in the staged data. Use the results to develop your partition column(s):\nCreate the partitioned external table.\nThe partition column date_part casts YYYY/MM/DD in the METADATA$FILENAME pseudocolumn as a date using\nTO_DATE , DATE. The SQL command also specifies Parquet as the file format type.\nThe external tables for Amazon S3 and Microsoft Azure cloud storage include the parameter required to refresh the metadata\nautomatically when triggered by event notifications from the respective cloud messaging service:\nAmazon S3\nGoogle Cloud Storage\nMicrosoft Azure\nRefresh the external table metadata:\nWhen querying the external table, filter the data by the partition column using a WHERE clause. Snowflake only scans the files in the\nspecified partitions that match the filter conditions:",
        "syntax": [
          "CREATE STAGE s1\n  URL='s3://mybucket/files/logs/'\n  ...\n  ;",
          "CREATE STAGE s1\n  URL='gcs://mybucket/files/logs/'\n  ...\n  ;",
          "CREATE STAGE s1\n  URL='azure://mycontainer/files/logs/'\n  ...\n  ;",
          "SELECT metadata$filename FROM @s1/;\n\n+----------------------------------------+\n| METADATA$FILENAME                      |\n|----------------------------------------|\n| files/logs/2018/08/05/0524/log.parquet |\n| files/logs/2018/08/27/1408/log.parquet |\n+----------------------------------------+",
          "CREATE EXTERNAL TABLE et1(\n date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n   || '/' || SPLIT_PART(metadata$filename, '/', 4)\n   || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n timestamp bigint AS (value:timestamp::bigint),\n col2 varchar AS (value:col2::varchar))\n PARTITION BY (date_part)\n LOCATION=@s1/logs/\n AUTO_REFRESH = true\n FILE_FORMAT = (TYPE = PARQUET)\n AWS_SNS_TOPIC = 'arn:aws:sns:us-west-2:001234567890:s3_mybucket';",
          "CREATE EXTERNAL TABLE et1(\n  date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n    || '/' || SPLIT_PART(metadata$filename, '/', 4)\n    || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n  timestamp bigint AS (value:timestamp::bigint),\n  col2 varchar AS (value:col2::varchar))\n  PARTITION BY (date_part)\n  LOCATION=@s1/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET);",
          "CREATE EXTERNAL TABLE et1(\n  date_part date AS TO_DATE(SPLIT_PART(metadata$filename, '/', 3)\n    || '/' || SPLIT_PART(metadata$filename, '/', 4)\n    || '/' || SPLIT_PART(metadata$filename, '/', 5), 'YYYY/MM/DD'),\n  timestamp bigint AS (value:timestamp::bigint),\n  col2 varchar AS (value:col2::varchar))\n  PARTITION BY (date_part)\n  INTEGRATION = 'MY_INT'\n  LOCATION=@s1/logs/\n  AUTO_REFRESH = true\n  FILE_FORMAT = (TYPE = PARQUET);",
          "ALTER EXTERNAL TABLE et1 REFRESH;",
          "SELECT timestamp, col2 FROM et1 WHERE date_part = to_date('08/05/2018');"
        ]
      },
      {
        "heading": "Partitions added manually",
        "description": "\nCreate an external table with user-defined partitions (i.e. the partitions are added manually by the external table owner).\nCreate an external stage named s2 for the storage location where the data files are stored:\nThe stage definition includes the path /files/logs/:\nAmazon S3\n\nCREATE STAGE s2\n  URL='s3://mybucket/files/logs/'\n  ...\n  ;\n\nCopy\n\nGoogle Cloud Storage\n\nCREATE STAGE s2\n  URL='gcs://mybucket/files/logs/'\n  ...\n  ;\n\nCopy\n\nMicrosoft Azure\n\nCREATE STAGE s2\n  URL='azure://mycontainer/files/logs/'\n  ...\n  ;\n\nCopy\nCreate the partitioned external table. The external table includes three partition columns with different data types.\nNote that the column names in the partition expressions are case-sensitive. The following rules apply:\n\nA partition column name must be in uppercase, unless the column name is enclosed in double quotes. Alternatively,\nuse GET_IGNORE_CASE instead of the case-sensitive : character in the SQL\nexpression.\nIf a column name is enclosed in double quotes (e.g. Column1), the partition column name must also be enclosed in\ndouble quotes and match the column name exactly.\n\nThe syntax for each of the three cloud storage services (Amazon S3, Google Cloud Storage, and Microsoft Azure) is identical\nbecause the external table metadata is not refreshed:\ncreate external table et2(\n  col1 date as (parse_json(metadata$external_table_partition):COL1::date),\n  col2 varchar as (parse_json(metadata$external_table_partition):COL2::varchar),\n  col3 number as (parse_json(metadata$external_table_partition):COL3::number))\n  partition by (col1,col2,col3)\n  location=@s2/logs/\n  partition_type = user_specified\n  file_format = (type = parquet);\n\nCopy\nA partition column name must be in uppercase, unless the column name is enclosed in double quotes. Alternatively,\nuse GET_IGNORE_CASE instead of the case-sensitive : character in the SQL\nexpression.\nIf a column name is enclosed in double quotes (e.g. Column1), the partition column name must also be enclosed in\ndouble quotes and match the column name exactly.\nAdd partitions for the partition columns:\nALTER EXTERNAL TABLE et2 ADD PARTITION(col1='2022-01-24', col2='a', col3='12') LOCATION '2022/01';\n\nCopy\nSnowflake adds the partitions to the metadata for the external table. The operation also adds any new data files in the specified\nlocation to the metadata:\n+---------------------------------------+----------------+-------------------------------+\n|                       file            |     status     |          description          |\n+---------------------------------------+----------------+-------------------------------+\n| mycontainer/files/logs/2022/01/24.csv | REGISTERED_NEW | File registered successfully. |\n| mycontainer/files/logs/2022/01/25.csv | REGISTERED_NEW | File registered successfully. |\n+---------------------------------------+----------------+-------------------------------+\n\nCopy\nCreate an external stage named s2 for the storage location where the data files are stored:\nThe stage definition includes the path /files/logs/:\nAmazon S3\nGoogle Cloud Storage\nMicrosoft Azure\nCreate the partitioned external table. The external table includes three partition columns with different data types.\nNote that the column names in the partition expressions are case-sensitive. The following rules apply:\nA partition column name must be in uppercase, unless the column name is enclosed in double quotes. Alternatively,\nuse GET_IGNORE_CASE instead of the case-sensitive : character in the SQL\nexpression.\nIf a column name is enclosed in double quotes (e.g. Column1), the partition column name must also be enclosed in\ndouble quotes and match the column name exactly.\nA partition column name must be in uppercase, unless the column name is enclosed in double quotes. Alternatively,\nuse GET_IGNORE_CASE instead of the case-sensitive : character in the SQL\nexpression.\nIf a column name is enclosed in double quotes (e.g. Column1), the partition column name must also be enclosed in\ndouble quotes and match the column name exactly.\nThe syntax for each of the three cloud storage services (Amazon S3, Google Cloud Storage, and Microsoft Azure) is identical\nbecause the external table metadata is not refreshed:\nAdd partitions for the partition columns:\nSnowflake adds the partitions to the metadata for the external table. The operation also adds any new data files in the specified\nlocation to the metadata:\nWhen querying the external table, filter the data by the partition columns using a WHERE clause. This example returns the records in the\norder they are stored in the staged data files:",
        "syntax": [
          "CREATE STAGE s2\n  URL='s3://mybucket/files/logs/'\n  ...\n  ;",
          "CREATE STAGE s2\n  URL='gcs://mybucket/files/logs/'\n  ...\n  ;",
          "CREATE STAGE s2\n  URL='azure://mycontainer/files/logs/'\n  ...\n  ;",
          "create external table et2(\n  col1 date as (parse_json(metadata$external_table_partition):COL1::date),\n  col2 varchar as (parse_json(metadata$external_table_partition):COL2::varchar),\n  col3 number as (parse_json(metadata$external_table_partition):COL3::number))\n  partition by (col1,col2,col3)\n  location=@s2/logs/\n  partition_type = user_specified\n  file_format = (type = parquet);",
          "ALTER EXTERNAL TABLE et2 ADD PARTITION(col1='2022-01-24', col2='a', col3='12') LOCATION '2022/01';",
          "+---------------------------------------+----------------+-------------------------------+\n|                       file            |     status     |          description          |\n+---------------------------------------+----------------+-------------------------------+\n| mycontainer/files/logs/2022/01/24.csv | REGISTERED_NEW | File registered successfully. |\n| mycontainer/files/logs/2022/01/25.csv | REGISTERED_NEW | File registered successfully. |\n+---------------------------------------+----------------+-------------------------------+",
          "SELECT col1, col2, col3 FROM et1 WHERE col1 = TO_DATE('2022-01-24') AND col2 = 'a' ORDER BY METADATA$FILE_ROW_NUMBER;"
        ]
      },
      {
        "heading": "Materialized view on an external table",
        "description": "\nCreate a materialized view based on a subquery of the columns in the external table created in the\nPartitions Added Automatically From Partition Column Expressions example:\nFor general syntax, usage notes, and further examples for this SQL command, see CREATE MATERIALIZED VIEW.",
        "syntax": [
          "CREATE MATERIALIZED VIEW et1_mv\n  AS\n  SELECT col2 FROM et1;"
        ]
      },
      {
        "heading": "External table created with detected column definitions",
        "description": "\nCreate an external table where the column definitions are derived from a set of staged files that contain Avro, Parquet, or ORC data.\nNote that the mystage stage and my_parquet_format file format referenced in the statement must already exist. A set of files must\nalready be staged in the cloud storage location referenced in the stage definition.\nThis example builds on an example in the INFER_SCHEMA topic:\nUsing * for ARRAY_AGG(OBJECT_CONSTRUCT()) may result in an error if the returned result is larger than 16MB. Avoid using * for larger result sets, and only use the required columns, COLUMN NAME, TYPE, and NULLABLE, for the query, as the following example demonstrates. Optional column ORDER_ID can be included when using WITHIN GROUP (ORDER BY order_id).\nOn this page\nSyntax\nVariant syntax\nRequired parameters\nOptional parameters\nCloud provider parameters (cloudProviderParams)\nFormat type options (formatTypeOptions)\nAccess control requirements\nUsage notes\nExamples\nRelated content\nIntroduction to external tables\nSYSTEM$VALIDATE_STORAGE_INTEGRATION\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE EXTERNAL TABLE mytable\n  USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    FROM TABLE(\n      INFER_SCHEMA(\n        LOCATION=>'@mystage',\n        FILE_FORMAT=>'my_parquet_format'\n      )\n    )\n  )\n  LOCATION=@mystage\n  FILE_FORMAT=my_parquet_format\n  AUTO_REFRESH=false;",
          "CREATE EXTERNAL TABLE mytable\n  USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT('COLUMN_NAME',COLUMN_NAME, 'TYPE',TYPE, 'NULLABLE', NULLABLE, 'EXPRESSION',EXPRESSION))\n    FROM TABLE(\n      INFER_SCHEMA(\n        LOCATION=>'@mystage',\n        FILE_FORMAT=>'my_parquet_format'\n      )\n    )\n  )\n  LOCATION=@mystage\n  FILE_FORMAT=my_parquet_format\n  AUTO_REFRESH=false;"
        ]
      }
    ]
  },
  {
    "category": "ALTER EXTERNAL TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-external-table",
    "details": [
      {
        "heading": "ALTER EXTERNAL TABLE",
        "description": "\nModifies the properties, columns, or constraints for an existing external table.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE EXTERNAL TABLE , DROP EXTERNAL TABLE , SHOW EXTERNAL TABLES , DESCRIBE EXTERNAL TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nPartitions added and removed manually",
        "syntax": [
          "ALTER EXTERNAL TABLE [ IF EXISTS ] <name> REFRESH [ '<relative-path>' ]\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> ADD FILES ( '<path>/[<filename>]' [ , '<path>/[<filename>'] ] )\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> REMOVE FILES ( '<path>/[<filename>]' [ , '<path>/[<filename>]' ] )\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> SET\n  [ AUTO_REFRESH = { TRUE | FALSE } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n\nALTER EXTERNAL TABLE [ IF EXISTS ] <name> UNSET CONTACT <purpose>",
          "ALTER EXTERNAL TABLE <name> [ IF EXISTS ] ADD PARTITION ( <part_col_name> = '<string>' [ , <part_col_name> = '<string>' ] ) LOCATION '<path>'\n\nALTER EXTERNAL TABLE <name> [ IF EXISTS ] DROP PARTITION LOCATION '<path>'"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Identifier for the external table to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in double\nquotes. Identifiers enclosed in double quotes are also case sensitive."
          },
          {
            "term": "REFRESH [ 'relative-path' ]",
            "definition": "Accesses the staged data files referenced in the external table definition and updates the table metadata: New files in the path are added to the table metadata. Changes to files in the path are updated in the table metadata. Files no longer in the path are removed from the table metadata. Optionally specify a relative path to refresh the metadata for a specific subset of the data files. Using this parameter only needs to be done once, when the external table is created. This step synchronizes the metadata with the latest set\nof associated files in the stage and path in the external table definition. Also, this step ensures the external table can read the data files\nin the specified stage and path, and that no files were missed in the external table definition. Note This parameter is not supported by partitioned external tables when partitions are added manually by the object owner (that is,\nwhen PARTITION_TYPE = USER_SPECIFIED). If TABLE_FORMAT = DELTA is set on the external table, REFRESH does not support a relative path to refresh the\nmetadata for a specific subset of the data files."
          },
          {
            "term": "ADD FILES",
            "definition": "Registers the specified comma-separated list of files with the external table metadata, and refreshes the table.\nFor each file, list the path and filename relative to [ WITH ] LOCATION in the external table definition.\nFor information, see CREATE EXTERNAL TABLE. This parameter is not supported by partitioned external tables when partitions are added manually by the object owner (that is,\nwhen PARTITION_TYPE = USER_SPECIFIED)."
          },
          {
            "term": "REMOVE FILES",
            "definition": "Deregisters the specified comma-separated list of files from the external table metadata, and refreshes the table.\nFor each file, list the path and filename relative to [ WITH ] LOCATION in the external table definition.\nFor information, see CREATE EXTERNAL TABLE. This parameter is not supported by partitioned external tables when partitions are added manually by the object owner (that is,\nwhen PARTITION_TYPE = USER_SPECIFIED)."
          },
          {
            "term": "SET ...",
            "definition": "Specifies one or more properties/parameters to set for the external table (separated by blank spaces, commas, or new lines): Specifies whether Snowflake should enable triggering automatic refreshes of the external table metadata when new or updated data files\nare available in the named external stage specified in the [ WITH ] LOCATION = setting. Note You must configure an event notification for your storage location to notify Snowflake when new or updated data is available\nto read into the external table metadata. For more information, see the instructions for your cloud storage service: Refreshing external tables automatically for Amazon S3 Refreshing external tables automatically for Google Cloud Storage Refreshing external tables automatically for Azure Blob Storage This parameter is not supported by partitioned external tables when partitions are added manually by the object owner\n(that is, when PARTITION_TYPE = USER_SPECIFIED). Setting this parameter to TRUE is not supported for external tables that reference data files stored on an S3-compatible external stage. Snowflake enables triggering automatic refreshes of the external table metadata. Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the stage path. Default: TRUE Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "AUTO_REFRESH =  TRUE | FALSE",
            "definition": "Specifies whether Snowflake should enable triggering automatic refreshes of the external table metadata when new or updated data files\nare available in the named external stage specified in the [ WITH ] LOCATION = setting. Note You must configure an event notification for your storage location to notify Snowflake when new or updated data is available\nto read into the external table metadata. For more information, see the instructions for your cloud storage service: Refreshing external tables automatically for Amazon S3 Refreshing external tables automatically for Google Cloud Storage Refreshing external tables automatically for Azure Blob Storage This parameter is not supported by partitioned external tables when partitions are added manually by the object owner\n(that is, when PARTITION_TYPE = USER_SPECIFIED). Setting this parameter to TRUE is not supported for external tables that reference data files stored on an S3-compatible external stage. Snowflake enables triggering automatic refreshes of the external table metadata. Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the stage path. Default: TRUE"
          },
          {
            "term": "Amazon S3:",
            "definition": "Refreshing external tables automatically for Amazon S3"
          },
          {
            "term": "Google Cloud Storage:",
            "definition": "Refreshing external tables automatically for Google Cloud Storage"
          },
          {
            "term": "Microsoft Azure:",
            "definition": "Refreshing external tables automatically for Azure Blob Storage"
          },
          {
            "term": "TRUE",
            "definition": "Snowflake enables triggering automatic refreshes of the external table metadata."
          },
          {
            "term": "FALSE",
            "definition": "Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the stage path."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "UNSET CONTACT purpose",
            "definition": "Detaches a contact from the external table."
          },
          {
            "term": "AUTO_REFRESH =  TRUE | FALSE",
            "definition": "Specifies whether Snowflake should enable triggering automatic refreshes of the external table metadata when new or updated data files\nare available in the named external stage specified in the [ WITH ] LOCATION = setting. Note You must configure an event notification for your storage location to notify Snowflake when new or updated data is available\nto read into the external table metadata. For more information, see the instructions for your cloud storage service: Refreshing external tables automatically for Amazon S3 Refreshing external tables automatically for Google Cloud Storage Refreshing external tables automatically for Azure Blob Storage This parameter is not supported by partitioned external tables when partitions are added manually by the object owner\n(that is, when PARTITION_TYPE = USER_SPECIFIED). Setting this parameter to TRUE is not supported for external tables that reference data files stored on an S3-compatible external stage. Snowflake enables triggering automatic refreshes of the external table metadata. Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the stage path. Default: TRUE"
          },
          {
            "term": "Amazon S3:",
            "definition": "Refreshing external tables automatically for Amazon S3"
          },
          {
            "term": "Google Cloud Storage:",
            "definition": "Refreshing external tables automatically for Google Cloud Storage"
          },
          {
            "term": "Microsoft Azure:",
            "definition": "Refreshing external tables automatically for Azure Blob Storage"
          },
          {
            "term": "TRUE",
            "definition": "Snowflake enables triggering automatic refreshes of the external table metadata."
          },
          {
            "term": "FALSE",
            "definition": "Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the stage path."
          },
          {
            "term": "Amazon S3:",
            "definition": "Refreshing external tables automatically for Amazon S3"
          },
          {
            "term": "Google Cloud Storage:",
            "definition": "Refreshing external tables automatically for Google Cloud Storage"
          },
          {
            "term": "Microsoft Azure:",
            "definition": "Refreshing external tables automatically for Azure Blob Storage"
          },
          {
            "term": "TRUE",
            "definition": "Snowflake enables triggering automatic refreshes of the external table metadata."
          },
          {
            "term": "FALSE",
            "definition": "Snowflake does not enable triggering automatic refreshes of the external table metadata. You must manually refresh the external table metadata\nperiodically using ALTER EXTERNAL TABLE  REFRESH to synchronize the metadata with the current list of files in the stage path."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          }
        ]
      },
      {
        "heading": "Partitions added and removed manually",
        "description": "\nUse the following parameters to add or remove partitions when the partition type for the external table is user-specified (that is,\nPARTITION_TYPE = USER_SPECIFIED):",
        "definitions": [
          {
            "term": "ADD PARTITION ( <part_col_name> = '<string>' [ , <part_col_name> = '<string>' , ... ] ) LOCATION '<path>'",
            "definition": "Manually add a partition for one or more partition columns defined for the external table in a specified location (that is, path). Note The maximum length of user-specified partition column names is 32 characters. Adding a partition also adds any new or updated files in the location to the external table metadata."
          },
          {
            "term": "DROP PARTITION LOCATION '<path>'",
            "definition": "Manually drop all partitions in a specified location (that is, path). Dropping a partition also removes any files in the location from the external table metadata."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "OWNERSHIP",
                "External table",
                "OWNERSHIP is a special privilege on an object that is automatically granted to the role that created the object, but can also be transferred using the GRANT OWNERSHIP command to a different role by the owning role (or any role with the MANAGE GRANTS privilege)."
              ],
              [
                "USAGE",
                "Stage",
                "Required to manually refresh the external table metadata."
              ],
              [
                "USAGE",
                "File format",
                "Required to manually refresh the external table metadata."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nOnly the external table owner (the role with the OWNERSHIP privilege on the external table) or higher can execute this command.\nThe following commands can be used in explicit transactions (using BEGIN  COMMIT):\n\nALTER EXTERNAL TABLE ... REFRESH\nALTER EXTERNAL TABLE ... ADD FILES\nALTER EXTERNAL TABLE ... REMOVE FILES\n\nExplicit transactions could be used to ensure a consistent state when manually replacing updated files in external table metadata.\nALTER EXTERNAL TABLE ... REFRESH\nALTER EXTERNAL TABLE ... ADD FILES\nALTER EXTERNAL TABLE ... REMOVE FILES\nAdd or remove columns in an external table using the following syntax:\n\nAdd column:\nALTER TABLE <name> ADD COLUMN ( <col_name> <col_type> AS <expr> ) [, ...]\n\nCopy\n\nRename column:\nALTER TABLE <name> RENAME COLUMN <col_name> to <new_col_name>\n\nCopy\n\nDrop column:\nALTER TABLE <name> DROP COLUMN <col_name>\n\nCopy\n\nNote\nThe default VALUE and METADATA$FILENAME columns cannot be dropped.\n\n\n\nSee the ALTER TABLE topic for examples.\nTo add and drop a row access policy on an external table, or to set or unset a tag, use the ALTER TABLE command.\nHowever, you can create an external table with a row access policy and a tag on the table. See CREATE EXTERNAL TABLE.\nYou can use data metric functions with external tables by executing an ALTER TABLE command. For more information, see\nUse data metric functions to perform data quality checks.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nOnly the external table owner (the role with the OWNERSHIP privilege on the external table) or higher can execute this command.\nThe following commands can be used in explicit transactions (using BEGIN  COMMIT):\nALTER EXTERNAL TABLE ... REFRESH\nALTER EXTERNAL TABLE ... ADD FILES\nALTER EXTERNAL TABLE ... REMOVE FILES\nALTER EXTERNAL TABLE ... REFRESH\nALTER EXTERNAL TABLE ... ADD FILES\nALTER EXTERNAL TABLE ... REMOVE FILES\nExplicit transactions could be used to ensure a consistent state when manually replacing updated files in external table metadata.\nAdd or remove columns in an external table using the following syntax:\nSee the ALTER TABLE topic for examples.\nTo add and drop a row access policy on an external table, or to set or unset a tag, use the ALTER TABLE command.\nHowever, you can create an external table with a row access policy and a tag on the table. See CREATE EXTERNAL TABLE.\nYou can use data metric functions with external tables by executing an ALTER TABLE command. For more information, see\nUse data metric functions to perform data quality checks.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.",
        "syntax": [
          "ALTER TABLE <name> ADD COLUMN ( <col_name> <col_type> AS <expr> ) [, ...]",
          "ALTER TABLE <name> RENAME COLUMN <col_name> to <new_col_name>",
          "ALTER TABLE <name> DROP COLUMN <col_name>"
        ],
        "definitions": [
          {
            "term": "Add column:",
            "definition": "ALTER TABLE <name> ADD COLUMN ( <col_name> <col_type> AS <expr> ) [, ...]\n\nCopy"
          },
          {
            "term": "Rename column:",
            "definition": "ALTER TABLE <name> RENAME COLUMN <col_name> to <new_col_name>\n\nCopy"
          },
          {
            "term": "Drop column:",
            "definition": "Note The default VALUE and METADATA$FILENAME columns cannot be dropped."
          }
        ]
      },
      {
        "heading": "Examples"
      },
      {
        "heading": "Refresh metadata manually",
        "description": "\nManually refresh the entire set of external table metadata based on changes in the referenced data files:\nSimilar to the first example, but manually refresh only a path of the metadata for an external table:",
        "syntax": [
          "ALTER EXTERNAL TABLE exttable_json REFRESH;",
          "CREATE OR REPLACE STAGE mystage\n  URL='<cloud_platform>://twitter_feed/logs/'\n  .. ;\n\n-- Create the external table\n-- 'daily' path includes paths in </YYYY/MM/DD/> format\nCREATE OR REPLACE EXTERNAL TABLE daily_tweets\n  WITH LOCATION = @twitter_feed/daily/;\n\n-- Refresh the metadata for a single day of data files by date\nALTER EXTERNAL TABLE exttable_part REFRESH '2018/08/05/';"
        ]
      },
      {
        "heading": "Add or remove files manually",
        "description": "\nAdd an explicit list of files to the external table metadata:\nRemove an explicit list of files from the external table metadata:\nReplace an updated log file for December 2019 in the external table metadata in an explicit transaction:",
        "syntax": [
          "ALTER EXTERNAL TABLE exttable1 ADD FILES ('path1/sales4.json.gz', 'path1/sales5.json.gz');",
          "ALTER EXTERNAL TABLE exttable1 REMOVE FILES ('path1/sales4.json.gz', 'path1/sales5.json.gz');",
          "BEGIN;\n\nALTER EXTERNAL TABLE extable1 REMOVE FILES ('2019/12/log1.json.gz');\n\nALTER EXTERNAL TABLE extable1 ADD FILES ('2019/12/log1.json.gz');\n\nCOMMIT;"
        ]
      },
      {
        "heading": "Add or remove partitions manually",
        "description": "\nManually add partitions in a specified location for the partition columns:\nSnowflake adds the partitions to the metadata for the external table. The operation also adds any new data files in the specified\nlocation to the metadata.\nManually remove partitions from a specified location:\nSnowflake removes the partitions from the metadata for the external table. The operation also removes any data files in the specified\nlocation from the metadata.\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER EXTERNAL TABLE et2 ADD PARTITION(col1='2022-01-24', col2='a', col3='12') LOCATION '2022/01';",
          "ALTER EXTERNAL TABLE et2 DROP PARTITION LOCATION '2022/01';"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE EXTERNAL TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-external-table",
    "details": [
      {
        "heading": "DESCRIBE EXTERNAL TABLE",
        "description": "\nDescribes the VALUE column and virtual columns in an external table.\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DROP EXTERNAL TABLE , ALTER EXTERNAL TABLE , CREATE EXTERNAL TABLE , SHOW EXTERNAL TABLES DESCRIBE VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] [ EXTERNAL ] TABLE <name> [ TYPE =  { COLUMNS | STAGE } ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the external table to describe. If the identifier contains spaces or special characters, the entire string\nmust be enclosed in double quotes. Identifiers enclosed in double quotes are also case sensitive."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nCreate an example external table:\nDescribe the columns in the table:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE EXTERNAL TABLE emp ( ... );",
          "DESC EXTERNAL TABLE emp;"
        ]
      }
    ]
  },
  {
    "category": "DROP EXTERNAL TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-external-table",
    "details": [
      {
        "heading": "DROP EXTERNAL TABLE",
        "description": "\nRemoves an external table from the current/specified schema. Note that this is a metadata-only operation. None of the files that the\nexternal table refers to are dropped.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE EXTERNAL TABLE , ALTER EXTERNAL TABLE , SHOW EXTERNAL TABLES , DESCRIBE EXTERNAL TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP EXTERNAL TABLE [ IF EXISTS ] <name> [ CASCADE | RESTRICT ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the external table to drop. If the identifier contains spaces, special characters, or mixed-case characters,\nthe entire string must be enclosed in double quotes. Identifiers enclosed in double quotes are also case sensitive\n(for example, \"My Object\"). If the external table identifier is not fully qualified (in the form of db_name.schema_name.table_name or\nschema_name.table_name), the command looks for the external table in the current schema for the session."
          },
          {
            "term": "CASCADE | RESTRICT",
            "definition": "Specifies whether the external table can be dropped if foreign keys exist that reference the table: CASCADE drops the external table even if it has primary/unique keys that are referenced by foreign keys in other tables. RESTRICT returns a warning about existing foreign key references and does not drop the external table. Default: CASCADE"
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nUnlike a standard table, dropping an external table purges it from the system. An external table cannot be recovered using Time Travel;\nalso, there is no UNDROP EXTERNAL TABLE command. A dropped external table must be recreated.\nAfter dropping an external table, creating an external table with the same name recreates the table. No history from the old version\nof the external table is retained.\nBefore dropping an external table, verify that no views reference the table. Dropping an external table referenced by a view\ninvalidates the view (that is, querying the view returns an object does not exist error).\nUnlike a standard table, dropping an external table purges it from the system. An external table cannot be recovered using Time Travel;\nalso, there is no UNDROP EXTERNAL TABLE command. A dropped external table must be recreated.\nAfter dropping an external table, creating an external table with the same name recreates the table. No history from the old version\nof the external table is retained.\nBefore dropping an external table, verify that no views reference the table. Dropping an external table referenced by a view\ninvalidates the view (that is, querying the view returns an object does not exist error).\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Examples",
        "description": "\nDrop an external table:\nDrop the table again, but dont raise an error if the table does not exist:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW EXTERNAL TABLES LIKE 't2%';\n\n+-------------------------------+------------------+---------------+-------------+-----------------------+---------+-----------------------------------------+------------------+------------------+-------+-----------+----------------------+\n| created_on                    | name             | database_name | schema_name | owner                 | comment | location                                | file_format_name | file_format_type | cloud | region    | notification_channel |\n|-------------------------------+------------------+---------------+-------------+-----------------------+---------+-----------------------------------------+------------------+------------------+-------+-----------+----------------------|\n| 2018-08-06 06:00:42.340 -0700 | T2               | MYDB          | PUBLIC      | MYROLE                |         | @MYDB.PUBLIC.MYSTAGE/                   |                  | JSON             | AWS   | us-east-1 | NULL                 |\n+-------------------------------+------------------+---------------+-------------+-----------------------+---------+-----------------------------------------+------------------+------------------+-------+-----------+----------------------+\n\nDROP EXTERNAL TABLE t2;\n\n+--------------------------+\n| status                   |\n|--------------------------|\n| T2 successfully dropped. |\n+--------------------------+\n\nSHOW EXTERNAL TABLES LIKE 't2%';\n\n+------------+------+---------------+-------------+-------+---------+----------+------------------+------------------+-------+--------+----------------------+\n| created_on | name | database_name | schema_name | owner | comment | location | file_format_name | file_format_type | cloud | region | notification_channel |\n|------------+------+---------------+-------------+-------+---------+----------+------------------+------------------+-------+--------+----------------------|\n+------------+------+---------------+-------------+-------+---------+----------+------------------+------------------+-------+--------+----------------------+",
          "DROP EXTERNAL TABLE IF EXISTS t2;\n\n+------------------------------------------------------------+\n| status                                                     |\n|------------------------------------------------------------|\n| Drop statement executed successfully (T2 already dropped). |\n+------------------------------------------------------------+"
        ]
      }
    ]
  },
  {
    "category": "SHOW EXTERNAL TABLES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-external-tables",
    "details": [
      {
        "heading": "SHOW EXTERNAL TABLES",
        "description": "\nLists the external tables for which you have access privileges. The command can be used to list external tables for the current/specified\ndatabase or schema, or across your entire account.\nThe output returns external table metadata and properties, ordered lexicographically by database, schema, and external table name\n(see Output in this topic for descriptions of the output columns). This is important to note if you want to filter the results using\nthe provided filters.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE EXTERNAL TABLE , DROP EXTERNAL TABLE , ALTER EXTERNAL TABLE , DESCRIBE EXTERNAL TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] EXTERNAL TABLES [ LIKE '<pattern>' ]\n                               [ IN\n                                        {\n                                          ACCOUNT                                         |\n\n                                          DATABASE                                        |\n                                          DATABASE <database_name>                        |\n\n                                          SCHEMA                                          |\n                                          SCHEMA <schema_name>                            |\n                                          <schema_name>\n\n                                          APPLICATION <application_name>                  |\n                                          APPLICATION PACKAGE <application_package_name>  |\n                                        }\n                               ]\n                               [ STARTS WITH '<name_string>' ]\n                               [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Returns only a subset of the output columns: created_on name kind database_name schema_name"
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the external table was created."
              ],
              [
                "name",
                "Name of the external table."
              ],
              [
                "database_name",
                "Database for the schema for the external table."
              ],
              [
                "schema_name",
                "Schema for the external table."
              ],
              [
                "invalid",
                "TRUE if either the stage or file format referenced in the external table description is dropped."
              ],
              [
                "invalid_reason",
                "Reason why the external table is invalid, when the INVALID column shows a TRUE value."
              ],
              [
                "owner",
                "Role that owns the external table."
              ],
              [
                "comment",
                "Comment for the external table."
              ],
              [
                "stage",
                "Fully qualified name of the stage referenced in the external table definition."
              ],
              [
                "location",
                "External stage and folder path in the external table definition. NULL for external tables in an imported share in a data consumer account."
              ],
              [
                "file_format_name",
                "Named file format in the external table definition. Does not display a file format specified in the stage definition."
              ],
              [
                "file_format_type",
                "File format type specified in the external table definition. Does not display a file format type specified in the stage definition."
              ],
              [
                "cloud",
                "Cloud in which the staged data files are located."
              ],
              [
                "region",
                "Region in which the staged data files are located."
              ],
              [
                "notification_channel",
                "Amazon Resource Name of the Amazon SQS queue for the external table."
              ],
              [
                "last_refreshed_on",
                "Timestamp that indicates when the metadata for the external table was last synchronized with the latest set of associated files in the external stage and path, either manually or automatically."
              ],
              [
                "table_format",
                "Table format of the staged files referenced by the external table. Possible values: DELTA, UNSPECIFIED."
              ],
              [
                "last_refresh_details",
                "Supports future functionality; currently NULL only."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThis command doesnt list external tables that have been dropped.\nThis command doesnt list external tables that have been dropped.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nShow all the external tables whose name starts with line that you have privileges to view in the tpch.public schema:\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW EXTERNAL TABLES LIKE 'line%' IN tpch.public;"
        ]
      }
    ]
  },
  {
    "category": "CREATE HYBRID TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-hybrid-table",
    "details": [
      {
        "heading": "CREATE HYBRID TABLE",
        "description": "\nFeature  Generally Available\nAvailable to accounts in AWS commercial regions only. For more information, see Clouds and regions.\nCreates a new hybrid table in the current/specified schema or replaces an existing table. A table can have multiple columns,\nwith each column definition consisting of a name, data type, and optionally whether the column:\nRequires a NOT NULL value.\nHas a default value or is an identity column.\nHas any inline constraints.\nRequires a NOT NULL value.\nHas a default value or is an identity column.\nHas any inline constraints.\nNote\nWhen you create a hybrid table, you must define a PRIMARY KEY constraint on one or more columns.\nYou can also use the following CREATE TABLE variants to create hybrid tables:\nCREATE HYBRID TABLE  AS SELECT (CTAS) (creates a populated table; also referred to as CTAS)\nCREATE HYBRID TABLE  LIKE (creates an empty copy of an existing hybrid table)\nCREATE HYBRID TABLE  AS SELECT (CTAS) (creates a populated table; also referred to as CTAS)\nCREATE HYBRID TABLE  LIKE (creates an empty copy of an existing hybrid table)\nFor the full CREATE TABLE syntax used for standard Snowflake tables, see CREATE TABLE.\nTip\nBefore creating and using hybrid tables, you should become familiar with some\nunsupported features and limitations.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE INDEX DROP INDEX, SHOW INDEXES, ALTER TABLE , DROP TABLE , SHOW TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:\nFor inline and out-of-line constraint details, see CREATE | ALTER TABLE  CONSTRAINT.",
        "syntax": [
          "CREATE [ OR REPLACE ] HYBRID TABLE [ IF NOT EXISTS ] <table_name>\n  ( <col_name> <col_type>\n    [\n      {\n        DEFAULT <expr>\n        | { AUTOINCREMENT | IDENTITY }\n          [\n            {\n              ( <start_num> , <step_num> )\n              | START <num> INCREMENT <num>\n            }\n          ]\n          [ { ORDER | NOORDER } ]\n      }\n    ]\n    [ NOT NULL ]\n    [ inlineConstraint ]\n    [ COMMENT '<string_literal>' ]\n    [ , <col_name> <col_type> [ ... ] ]\n    [ , outoflineConstraint ]\n    [ , outoflineIndex ]\n    [ , ... ]\n  )\n  [ COMMENT = '<string_literal>' ]",
          "inlineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE | PRIMARY KEY | { [ FOREIGN KEY ] REFERENCES <ref_table_name> [ ( <ref_col_name> ) ] } }\n  [ <constraint_properties> ]\n\noutoflineConstraint ::=\n  [ CONSTRAINT <constraint_name> ]\n  { UNIQUE [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | PRIMARY KEY [ ( <col_name> [ , <col_name> , ... ] ) ]\n    | [ FOREIGN KEY ] [ ( <col_name> [ , <col_name> , ... ] ) ]\n      REFERENCES <ref_table_name> [ ( <ref_col_name> [ , <ref_col_name> , ... ] ) ]\n  }\n  [ <constraint_properties> ]\n  [ COMMENT '<string_literal>' ]\n\noutoflineIndex ::=\n  INDEX <index_name> ( <col_name> [ , <col_name> , ... ] )\n    [ INCLUDE ( <col_name> [ , <col_name> , ... ] ) ]"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier (i.e. name) for the table; must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements."
          },
          {
            "term": "col_name",
            "definition": "Specifies the column identifier (i.e. name). All the requirements for table identifiers also apply to column identifiers. For more details, see Identifier requirements and Reserved & limited keywords. Note In addition to the standard reserved keywords, the following keywords cannot be used as column identifiers because they are reserved for ANSI-standard context functions: CURRENT_DATE CURRENT_ROLE CURRENT_TIME CURRENT_TIMESTAMP CURRENT_USER For the list of reserved keywords, see Reserved & limited keywords."
          },
          {
            "term": "col_type",
            "definition": "Specifies the data type for the column. For details about the data types that can be specified for table columns, see SQL data types reference."
          },
          {
            "term": "PRIMARY KEY ( col_name [ , col_name , ... ] )",
            "definition": "Specifies the required primary key constraint for the table, either within a column definition (inline) or separately (out-of-line).\nSee also Constraints for hybrid tables. For complete syntax details, see CREATE | ALTER TABLE  CONSTRAINT. For general information about constraints, see\nConstraints."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "DEFAULT ... or . AUTOINCREMENT ...",
            "definition": "Specifies whether a default value is automatically inserted in the column if a value is not explicitly specified via an INSERT or\nCREATE HYBRID TABLE AS SELECT statement: Column default value is defined by the specified expression which can be any of the following: Constant value. Simple expression. Sequence reference (seq_name.NEXTVAL). A simple expression is an expression that returns a scalar value; however, the expression cannot contain\nreferences to: Subqueries. Aggregates. Window functions. External functions. When AUTOINCREMENT is used, the default value for the column starts with a specified number and each successive\nvalue is automatically generated. Values generated by an AUTOINCREMENT column are guaranteed to be unique. The\ndifference between any pair of the generated values is guaranteed to be a multiple of the increment amount. The optional ORDER and NOORDER parameters specify whether or not the generated values provide ordering\nguarantees as specified in Sequence Semantics. NOORDER is the default option for AUTOINCREMENT\ncolumns on hybrid tables. NOORDER typically provides significantly better performance for point writes. These parameters can only be used for columns with numeric data types (NUMBER, INT, FLOAT, etc.) AUTOINCREMENT and IDENTITY are synonymous. If either is specified for a column, Snowflake utilizes a\nsequence to generate the values for the column. For more information about sequences, see\nUsing Sequences. The default value for both start and step/increment is 1. Default: No value (the column has no default value) Note DEFAULT and AUTOINCREMENT are mutually exclusive; only one can be specified for a column. For performance-sensitive workloads, NOORDER is the recommended option for AUTOINCREMENT columns."
          },
          {
            "term": "DEFAULT expr",
            "definition": "Column default value is defined by the specified expression which can be any of the following: Constant value. Simple expression. Sequence reference (seq_name.NEXTVAL). A simple expression is an expression that returns a scalar value; however, the expression cannot contain\nreferences to: Subqueries. Aggregates. Window functions. External functions."
          },
          {
            "term": "{ AUTOINCREMENT | IDENTITY } . [ { ( start_num , step_num ) | START num INCREMENT num } ] . [ { ORDER | NOORDER } ]",
            "definition": "When AUTOINCREMENT is used, the default value for the column starts with a specified number and each successive\nvalue is automatically generated. Values generated by an AUTOINCREMENT column are guaranteed to be unique. The\ndifference between any pair of the generated values is guaranteed to be a multiple of the increment amount. The optional ORDER and NOORDER parameters specify whether or not the generated values provide ordering\nguarantees as specified in Sequence Semantics. NOORDER is the default option for AUTOINCREMENT\ncolumns on hybrid tables. NOORDER typically provides significantly better performance for point writes. These parameters can only be used for columns with numeric data types (NUMBER, INT, FLOAT, etc.) AUTOINCREMENT and IDENTITY are synonymous. If either is specified for a column, Snowflake utilizes a\nsequence to generate the values for the column. For more information about sequences, see\nUsing Sequences. The default value for both start and step/increment is 1."
          },
          {
            "term": "CONSTRAINT ...",
            "definition": "Defines an inline or out-of-line constraint for the specified column(s) in the table. UNIQUE and FOREIGN KEY constraints\nare optional for hybrid table columns. See also Constraints for hybrid tables. For complete syntax details, see CREATE | ALTER TABLE  CONSTRAINT. For general information about constraints, see\nConstraints."
          },
          {
            "term": "INDEX index_name ( col_name [ , col_name , ... ]",
            "definition": "Specifies a secondary index on one or more columns in the table. (When you define constraints on hybrid table columns,\nindexes are automatically created on those columns.) Indexes cannot be defined on the following columns: Semi-structured columns (VARIANT, OBJECT, ARRAY)\nbecause of space constraints associated with the underlying storage engines for the key of each record. Geospatial columns (GEOGRAPHY, GEOMETRY) or\nVECTOR columns. TIMESTAMP_TZ columns (or TIMESTAMP\ncolumns that resolve to TIMESTAMP_TZ). TIMESTAMP_NTZ columns are supported. Indexes can be defined when the table is created, or with the CREATE INDEX command. For more information about creating indexes for\nhybrid tables, see CREATE INDEX."
          },
          {
            "term": "INCLUDE ( col_name [ , col_name , ... ] )",
            "definition": "Specifies one or more included columns for a secondary index. Using included columns with a secondary index is\nparticularly useful when queries frequently contain a set of columns in the SELECT list but not in\nthe list of WHERE predicates. See Create a secondary index with an INCLUDE column. INCLUDE columns cannot be semi-structured columns (VARIANT, OBJECT, ARRAY) or geospatial columns (GEOGRAPHY, GEOMETRY). INCLUDE columns can be specified only when creating a table with a secondary index. INCLUDE is not supported\nby CREATE INDEX."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment at the column, constraint, or table level. For details, see Comments on constraints. Default: No value"
          },
          {
            "term": "DEFAULT expr",
            "definition": "Column default value is defined by the specified expression which can be any of the following: Constant value. Simple expression. Sequence reference (seq_name.NEXTVAL). A simple expression is an expression that returns a scalar value; however, the expression cannot contain\nreferences to: Subqueries. Aggregates. Window functions. External functions."
          },
          {
            "term": "{ AUTOINCREMENT | IDENTITY } . [ { ( start_num , step_num ) | START num INCREMENT num } ] . [ { ORDER | NOORDER } ]",
            "definition": "When AUTOINCREMENT is used, the default value for the column starts with a specified number and each successive\nvalue is automatically generated. Values generated by an AUTOINCREMENT column are guaranteed to be unique. The\ndifference between any pair of the generated values is guaranteed to be a multiple of the increment amount. The optional ORDER and NOORDER parameters specify whether or not the generated values provide ordering\nguarantees as specified in Sequence Semantics. NOORDER is the default option for AUTOINCREMENT\ncolumns on hybrid tables. NOORDER typically provides significantly better performance for point writes. These parameters can only be used for columns with numeric data types (NUMBER, INT, FLOAT, etc.) AUTOINCREMENT and IDENTITY are synonymous. If either is specified for a column, Snowflake utilizes a\nsequence to generate the values for the column. For more information about sequences, see\nUsing Sequences. The default value for both start and step/increment is 1."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE TABLE",
                "Schema",
                "Note that there is no CREATE HYBRID TABLE privilege."
              ],
              [
                "SELECT",
                "Table, external table, view",
                "Required on queried tables and/or views only when cloning a table or executing CTAS statements."
              ],
              [
                "APPLY",
                "Masking policy, row access policy, tag",
                "Required only when applying a masking policy, row access policy, object tags, or any combination of these\ngovernance features when creating tables."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo recreate or replace a hybrid table, call the GET_DDL function to see the definition of the\nhybrid table before running a CREATE OR REPLACE HYBRID TABLE command.\nYou cannot create hybrid tables that are temporary or transient. In turn, you cannot\ncreate hybrid tables within transient schemas or databases.\nA schema cannot contain tables and/or views with the same name. When creating a table:\n\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\n\n\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then\ncreating a new table with the same name.\nNote that the drop and create actions occur in a single atomic operation. This means that any queries concurrent with the\nCREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data.\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nFor information about cloning hybrid tables, see Clone databases that contain hybrid tables.\nSimilar to reserved keywords, ANSI-reserved function names\n(CURRENT_DATE, CURRENT_TIMESTAMP, etc.) cannot be used as\ncolumn names.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nTo recreate or replace a hybrid table, call the GET_DDL function to see the definition of the\nhybrid table before running a CREATE OR REPLACE HYBRID TABLE command.\nYou cannot create hybrid tables that are temporary or transient. In turn, you cannot\ncreate hybrid tables within transient schemas or databases.\nA schema cannot contain tables and/or views with the same name. When creating a table:\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\nIf a view with the same name already exists in the schema, an error is returned and the table is not created.\nIf a table with the same name already exists in the schema, an error is returned and the table is not created, unless the\noptional OR REPLACE keyword is included in the command.\nImportant\nUsing OR REPLACE is the equivalent of using DROP TABLE on the existing table and then\ncreating a new table with the same name.\nNote that the drop and create actions occur in a single atomic operation. This means that any queries concurrent with the\nCREATE OR REPLACE TABLE operation use either the old or new table version.\nRecreating or swapping a table drops its change data.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nFor information about cloning hybrid tables, see Clone databases that contain hybrid tables.\nSimilar to reserved keywords, ANSI-reserved function names\n(CURRENT_DATE, CURRENT_TIMESTAMP, etc.) cannot be used as\ncolumn names.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "Constraints for hybrid tables",
        "description": "\nThe following rules apply to constraints that are defined on hybrid tables.\nA hybrid table must be created with a PRIMARY KEY constraint.\nMulti-column (or composite) primary keys are supported. To define a multi-column primary key, use the\nsyntax shown in the following example, where the constraint is defined out of line and refers to\nmultiple columns that were previously defined for the table:\nCREATE OR REPLACE HYBRID TABLE ht2pk (\n  col1 INTEGER NOT NULL,\n  col2 INTEGER NOT NULL,\n  col3 VARCHAR,\n  CONSTRAINT pkey_1 PRIMARY KEY (col1, col2)\n  );\n\nCopy\nPRIMARY KEY, UNIQUE, and FOREIGN KEY constraints are all enforced on hybrid tables, and you cannot set the NOT ENFORCED\nproperty on these constraints.\nPRIMARY KEY, UNIQUE, and FOREIGN KEY constraints build their own underlying indexes. The creation of indexes results in\nadditional data being stored. Secondary (or covering) indexes can also be defined explicitly when the table is created,\nusing the outoflineIndex syntax.\nConstraints are enforced at the row level, not at the statement or transaction level (that is, deferred constraints).\nConstraints can only be defined at table creation.\nYou cannot alter a column to be UNIQUE.\nA hybrid table must be created with a PRIMARY KEY constraint.\nMulti-column (or composite) primary keys are supported. To define a multi-column primary key, use the\nsyntax shown in the following example, where the constraint is defined out of line and refers to\nmultiple columns that were previously defined for the table:\nPRIMARY KEY, UNIQUE, and FOREIGN KEY constraints are all enforced on hybrid tables, and you cannot set the NOT ENFORCED\nproperty on these constraints.\nPRIMARY KEY, UNIQUE, and FOREIGN KEY constraints build their own underlying indexes. The creation of indexes results in\nadditional data being stored. Secondary (or covering) indexes can also be defined explicitly when the table is created,\nusing the outoflineIndex syntax.\nConstraints are enforced at the row level, not at the statement or transaction level (that is, deferred constraints).\nConstraints can only be defined at table creation.\nYou cannot alter a column to be UNIQUE.\nThe following rules apply specifically to FOREIGN KEY constraints:\nA foreign key in a hybrid table that references a primary key cannot be NULL. If you attempt to\nload a NULL value into a column that has a FOREIGN KEY constraint, the load operation fails with a constraint error.\nSee Create two hybrid tables with a primary-key/foreign-key relationship.\nFOREIGN KEY constraints are supported only among hybrid tables that belong to the same database.\nThe referenced table from a FOREIGN KEY constraint cannot be truncated as long as the FOREIGN KEY relationship exists.\nFOREIGN KEY constraints do not support partial matching.\nFOREIGN KEY constraints do not support deferrable behavior.\nFOREIGN KEY constraints only support RESTRICT and NO ACTION properties\nfor DELETE and UPDATE operations.\nA foreign key in a hybrid table that references a primary key cannot be NULL. If you attempt to\nload a NULL value into a column that has a FOREIGN KEY constraint, the load operation fails with a constraint error.\nSee Create two hybrid tables with a primary-key/foreign-key relationship.\nFOREIGN KEY constraints are supported only among hybrid tables that belong to the same database.\nThe referenced table from a FOREIGN KEY constraint cannot be truncated as long as the FOREIGN KEY relationship exists.\nFOREIGN KEY constraints do not support partial matching.\nFOREIGN KEY constraints do not support deferrable behavior.\nFOREIGN KEY constraints only support RESTRICT and NO ACTION properties\nfor DELETE and UPDATE operations.",
        "syntax": [
          "CREATE OR REPLACE HYBRID TABLE ht2pk (\n  col1 INTEGER NOT NULL,\n  col2 INTEGER NOT NULL,\n  col3 VARCHAR,\n  CONSTRAINT pkey_1 PRIMARY KEY (col1, col2)\n  );"
        ]
      },
      {
        "heading": "CREATE HYBRID TABLE  AS SELECT (CTAS)",
        "description": "\nCreates a new hybrid table that contains the results of a query:\nNote\nWhen using CTAS to create a hybrid table, define the table schema explicitly, including column definitions, the primary key,\nindexes, and other constraints. Do not rely on inferring the schema from a SELECT statement.\nThe number of column names specified must match the number of SELECT list items in the query.\nTo create the table with rows in a specific order, use an ORDER BY clause at the end of the query.\nFor information about loading hybrid tables, see Loading data.",
        "syntax": [
          "CREATE [ OR REPLACE ] HYBRID TABLE <table_name> [ ( <col_name> [ <col_type> ] , <col_name> [ <col_type> ] , ... ) ]\n  AS <query>\n  [ ... ]"
        ]
      },
      {
        "heading": "CREATE HYBRID TABLE  LIKE",
        "description": "\nCreates a new hybrid table with the same column definitions as an existing hybrid table, but without copying data from the\nexisting table.\nColumn names, types, defaults, and constraints are copied to the new table:\nNote\nCREATE HYBRID TABLE  LIKE only supports another hybrid table as the source table type.\nCREATE HYBRID TABLE  LIKE for a table with an auto-increment sequence accessed through a data share is\nnot supported.",
        "syntax": [
          "CREATE [ OR REPLACE ] HYBRID TABLE <table_name> LIKE <source_hybrid_table>\n  [ ... ]"
        ]
      },
      {
        "heading": "Examples",
        "description": "\nCreate a hybrid table in the current database with customer_id as the primary key, a unique constraint on email,\nand a secondary index on full_name:\nInsert a row into this table:\nThe primary key must be unique. For example, if you try to insert the same primary key from the previous example a second time,\nthe command fails with the following error:\nThe email address must also follow the inline UNIQUE constraint. For example, if you attempt to insert two records with the\nsame email address, the statement fails with the following error:\nView table properties and metadata. Note the value of the is_hybrid column:\nView details for all hybrid tables:\nDisplay information about the columns in the table:\nSelect data from the table:",
        "syntax": [
          "CREATE HYBRID TABLE mytable (\n  customer_id INT AUTOINCREMENT PRIMARY KEY,\n  full_name VARCHAR(255),\n  email VARCHAR(255) UNIQUE,\n  extended_customer_info VARIANT,\n  INDEX index_full_name (full_name)\n);",
          "+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Table MYTABLE successfully created. |\n+-------------------------------------+",
          "INSERT INTO mytable (customer_id, full_name, email, extended_customer_info)\n  SELECT 100, 'Jane Doe', 'jdoe@example.com',\n    parse_json('{\"address\": \"1234 Main St\", \"city\": \"San Francisco\", \"state\": \"CA\", \"zip\":\"94110\"}');",
          "+-------------------------+\n| number of rows inserted |\n|-------------------------|\n|                       1 |\n+-------------------------+",
          "200001 (22000): Primary key already exists",
          "Duplicate key value violates unique constraint \"SYS_INDEX_MYTABLE_UNIQUE_EMAIL\"",
          "SHOW TABLES LIKE 'mytable';",
          "+-------------------------------+---------+---------------+-------------+-------+-----------+---------+------------+------+-------+--------+----------------+----------------------+-----------------+---------------------+------------------------------+---------------------------+-------------+\n| created_on                    | name    | database_name | schema_name | kind  | is_hybrid | comment | cluster_by | rows | bytes | owner  | retention_time | automatic_clustering | change_tracking | search_optimization | search_optimization_progress | search_optimization_bytes | is_external |\n|-------------------------------+---------+---------------+-------------+-------+-----------+---------+------------+------+-------+--------+----------------+----------------------+-----------------+---------------------+------------------------------+---------------------------+-------------|\n| 2022-02-23 23:53:19.707 +0000 | MYTABLE | MYDB          | PUBLIC      | TABLE | Y         |         |            | NULL |  NULL | MYROLE | 10             | OFF                  | OFF             | OFF                 |                         NULL |                      NULL | N           |\n+-------------------------------+---------+---------------+-------------+-------+-----------+---------+------------+------+-------+--------+----------------+----------------------+-----------------+---------------------+------------------------------+---------------------------+-------------+",
          "SHOW HYBRID TABLES;",
          "+-------------------------------+---------------------------+---------------+-------------+--------------+--------------+------+-------+---------+\n| created_on                    | name                      | database_name | schema_name | owner        | datastore_id | rows | bytes | comment |\n|-------------------------------+---------------------------+---------------+-------------+--------------+--------------+------+-------+---------|\n| 2022-02-24 02:07:31.877 +0000 | MYTABLE                   | DEMO_DB       | PUBLIC      | ACCOUNTADMIN |         2002 | NULL |  NULL |         |\n+-------------------------------+---------------------------+---------------+-------------+--------------+--------------+------+-------+---------+",
          "DESCRIBE TABLE mytable;",
          "+-------------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+\n| name              | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |\n|-------------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------|\n| CUSTOMER_ID       | NUMBER(38,0) | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | NULL    | NULL        |\n| FULL_NAME         | VARCHAR(256) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n| APPLICATION_STATE | VARIANT      | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        |\n+-------------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+",
          "SELECT customer_id, full_name, email, extended_customer_info\n  FROM mytable\n  WHERE extended_customer_info['state'] = 'CA';",
          "+-------------+-----------+------------------+------------------------------+\n| CUSTOMER_ID | FULL_NAME | EMAIL            | EXTENDED_CUSTOMER_INFO       |\n|-------------+-----------+------------------+------------------------------|\n|         100 | Jane Doe  | jdoe@example.com | {                            |\n|             |           |                  |   \"address\": \"1234 Main St\", |\n|             |           |                  |   \"city\": \"San Francisco\",   |\n|             |           |                  |   \"state\": \"CA\",             |\n|             |           |                  |   \"zip\": \"94110\"             |\n|             |           |                  | }                            |\n+-------------+-----------+------------------+------------------------------+"
        ]
      },
      {
        "heading": "Create two hybrid tables with a primary-key/foreign-key relationship",
        "description": "\nThis example shows the creation of two hybrid tables that reference each other. The first table, team, has a\nPRIMARY KEY constraint on its team_id column. The second table, player, has a FOREIGN KEY constraint on\nits team_id column, which references the team_id column in the team table.\nYou can verify that referential integrity is enforced by inserting some rows into both tables. You can also\nconfirm that NULL values are not allowed in columns defined as foreign keys.\nThe first insert into the player table succeeds as expected. The second insert fails because 3\ndoes not exist as an ID in the team table. The third insert fails because NULL is not allowed as a foreign key.\nA possible workaround for the rejection of NULL in this case is to insert a dummy row into the\nteam table with a team ID of 0. Then you can insert rows into the player table that use a\nmatching placeholder value of 0 instead of NULL. For example:",
        "syntax": [
          "CREATE OR REPLACE HYBRID TABLE team\n  (team_id INT PRIMARY KEY,\n  team_name VARCHAR(40),\n  stadium VARCHAR(40));\n\nCREATE OR REPLACE HYBRID TABLE player\n  (player_id INT PRIMARY KEY,\n  first_name VARCHAR(40),\n  last_name VARCHAR(40),\n  team_id INT,\n  FOREIGN KEY (team_id) REFERENCES team(team_id));",
          "INSERT INTO team VALUES (1, 'Bayern Munich', 'Allianz Arena');\nINSERT INTO player VALUES (100, 'Harry', 'Kane', 1);\nINSERT INTO player VALUES (301, 'Gareth', 'Bale', 3);",
          "200009 (22000): Foreign key constraint \"SYS_INDEX_PLAYER_FOREIGN_KEY_TEAM_ID_TEAM_TEAM_ID\" was violated.",
          "INSERT INTO player VALUES (200, 'Tommy', 'Atkins', NULL);",
          "200009 (22000): Foreign key constraint \"SYS_INDEX_PLAYER_FOREIGN_KEY_TEAM_ID_TEAM_TEAM_ID\" was violated.",
          "SELECT * FROM team t, player p WHERE t.team_id=p.team_id;",
          "+---------+---------------+---------------+-----------+------------+-----------+---------+\n| TEAM_ID | TEAM_NAME     | STADIUM       | PLAYER_ID | FIRST_NAME | LAST_NAME | TEAM_ID |\n|---------+---------------+---------------+-----------+------------+-----------+---------|\n|       1 | Bayern Munich | Allianz Arena |       100 | Harry      | Kane      |       1 |\n+---------+---------------+---------------+-----------+------------+-----------+---------+",
          "INSERT INTO team VALUES (0, 'Unknown', 'Unknown');\nINSERT INTO player VALUES (200, 'Tommy', 'Atkins', 0);\n\nSELECT * FROM team t, player p WHERE t.team_id=p.team_id;",
          "+---------+---------------+---------------+-----------+------------+-----------+---------+\n| TEAM_ID | TEAM_NAME     | STADIUM       | PLAYER_ID | FIRST_NAME | LAST_NAME | TEAM_ID |\n|---------+---------------+---------------+-----------+------------+-----------+---------|\n|       1 | Bayern Munich | Allianz Arena |       100 | Harry      | Kane      |       1 |\n|       0 | Unknown       | Unknown       |       200 | Tommy      | Atkins    |       0 |\n+---------+---------------+---------------+-----------+------------+-----------+---------+"
        ]
      },
      {
        "heading": "Create a secondary index with an INCLUDE column",
        "description": "\nFor example, create the employee table with a secondary index:\nInsert the following rows:\nThe following queries will use the secondary index:\nBoth queries benefit from the secondary index by avoiding lookups to the base table.\nHowever, note that using included columns in indexes may cause an increase in storage\nconsumption because additional columns will also be stored in the secondary index.",
        "syntax": [
          "CREATE HYBRID TABLE employee (\n    employee_id INT PRIMARY KEY,\n    employee_name VARCHAR(200),\n    employee_department VARCHAR(200),\n    INDEX idx_department (employee_department) INCLUDE (employee_name)\n);",
          "INSERT INTO employee VALUES\n  (1, 'John Doe', 'Marketing'),\n  (2, 'Jane Smith', 'Sales'),\n  (3, 'Bob Johnson', 'Finance'),\n  (4, 'Alice Brown', 'Marketing');",
          "SELECT employee_name FROM employee WHERE employee_department = 'Marketing';\nSELECT employee_name FROM employee WHERE employee_department IN ('Marketing', 'Sales');"
        ]
      },
      {
        "heading": "Create a hybrid table with a comment on the primary key column",
        "description": "\nCreate a hybrid table that includes a comment within the column definition for the primary key.\nNote that if you put this comment in the CONSTRAINT clause, the comment will not be visible in the DESCRIBE TABLE output. You can query\nthe TABLE_CONSTRAINTS view to see complete information about constraints.\nOn this page\nSyntax\nRequired parameters\nOptional parameters\nAccess control requirements\nUsage notes\nConstraints for hybrid tables\nCREATE HYBRID TABLE  AS SELECT (CTAS)\nCREATE HYBRID TABLE  LIKE\nExamples\nRelated content\nHybrid tables\nCREATE INDEX\nDROP INDEX\nSHOW INDEXES\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR REPLACE HYBRID TABLE ht1pk\n  (COL1 NUMBER(38,0) NOT NULL COMMENT 'Primary key',\n  COL2 NUMBER(38,0) NOT NULL,\n  COL3 VARCHAR(16777216),\n  CONSTRAINT PKEY_1 PRIMARY KEY (COL1));\n\nDESCRIBE TABLE ht1pk;",
          "+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+-------------+-------------+----------------+\n| name | type              | kind   | null? | default | primary key | unique key | check | expression | comment     | policy name | privacy domain |\n|------+-------------------+--------+-------+---------+-------------+------------+-------+------------+-------------+-------------+----------------|\n| COL1 | NUMBER(38,0)      | COLUMN | N     | NULL    | Y           | N          | NULL  | NULL       | Primary key | NULL        | NULL           |\n| COL2 | NUMBER(38,0)      | COLUMN | N     | NULL    | N           | N          | NULL  | NULL       | NULL        | NULL        | NULL           |\n| COL3 | VARCHAR(16777216) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL        | NULL        | NULL           |\n+------+-------------------+--------+-------+---------+-------------+------------+-------+------------+-------------+-------------+----------------+"
        ]
      }
    ]
  },
  {
    "category": "CREATE INDEX",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-index",
    "details": [
      {
        "heading": "CREATE INDEX",
        "description": "\nFeature  Generally Available\nAvailable to accounts in AWS commercial regions only. For more information, see Clouds and regions.\nCreates a new secondary index in an existing hybrid table and populates the index with data.\nThe creation of an index is an online (non-blocking) operation. The hybrid table will remain available for SELECT and DML\nstatements while the index is being built. However, if the hybrid table is not in active use and downtime is not an issue, it is recommended that you recreate the hybrid table with indexes defined. See also Create hybrid tables.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DROP INDEX , SHOW INDEXES , CREATE HYBRID TABLE , DROP TABLE , DESCRIBE TABLE , SHOW HYBRID TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CREATE [ OR REPLACE ] INDEX [ IF NOT EXISTS ] <index_name>\n  ON <table_name>\n    ( <col_name> [ , <col_name> , ... ] )"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "index_name",
            "definition": "Specifies the identifier for the new index. You must specify a unique name for each new index on a given hybrid table.\nNo other secondary index with either the same name or the same ordered set of columns can exist on the hybrid table."
          },
          {
            "term": "table_name",
            "definition": "Specifies the name of an existing hybrid table that will hold the new index."
          },
          {
            "term": "col_name",
            "definition": "Specifies the name of an existing column in the hybrid table. All the requirements for index columns defined at table creation\napply to column identifiers. A hybrid table cannot contain two secondary indexes defined on the same ordered set of columns. Columns with geospatial data types\n(GEOGRAPHY and GEOMETRY), semi-structured data types\n(ARRAY, OBJECT, VARIANT), and vector data types (VECTOR) are not supported in secondary indexes."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "description": "\nTo create an index, you must use a role that has OWNERSHIP privilege on the hybrid table."
      },
      {
        "heading": "Usage notes",
        "description": "\nThe CREATE INDEX command cannot be used to add a foreign, primary, or unique key constraint.\nThe creation of a new index does not concurrently block other workloads. The hybrid table is available for concurrent SELECT\nand DML statements.\nOnly one active index build operation per hybrid table can run at any time.\nYou can track the progress of an index build by using SHOW INDEXES. The STATUS column can take the following values:\n\nACTIVE: Index is complete and can be used to retrieve data.\nSUSPENDED: Index is only updated and is not used to retrieve data.\nBUILD FAILURE: An error has occurred with the index build process. You need to drop and recreate the index.\nBUILD IN PROGRESS: Index is being built and is not used to retrieve data.\nACTIVE: Index is complete and can be used to retrieve data.\nSUSPENDED: Index is only updated and is not used to retrieve data.\nBUILD FAILURE: An error has occurred with the index build process. You need to drop and recreate the index.\nBUILD IN PROGRESS: Index is being built and is not used to retrieve data.\nYou can rebuild a non-active index, where the status is SUSPENDED, BUILD FAILURE, or BUILD IN PROGRESS, by using DROP INDEX\nand CREATE INDEX.\nIf you want to drop a column that is part of an index that is being built, first stop the index build by dropping the index, then\ndrop the column. If you try to drop the column before dropping the index, you will receive this error message:\nColumn '<col_name>' cannot be dropped because it is used by index '<index-name>'.\nOnline index builds do not make progress until all the active transactions with DMLs on the same table at the time when the\nCREATE INDEX statement was issued are completed. If any of those transactions remain idle for more than 5 minutes, they will\nabort by default. See Transactions.\nDuring the index build process, any DML performs its writes to the new index, but does not use the index to retrieve data.\nA small number of concurrent DMLs, which began executing after the CREATE INDEX command was complete, may fail and return\nthis error:\nDML was unaware of concurrent DDL. Please retry this query.\n\n\nIf the aborted DML statements belong to a multi-statement transaction, the transaction will roll back only if the\nTRANSACTION_ABORT_ON_ERROR parameter is set to TRUE.\nA newly created index will be used for retrieving data only when the index build process concludes successfully and the\nstatus of the index is ACTIVE.\nThe INCLUDE keyword is only supported for CREATE HYBRID TABLE.\nThe CREATE INDEX command cannot be used to add a foreign, primary, or unique key constraint.\nThe creation of a new index does not concurrently block other workloads. The hybrid table is available for concurrent SELECT\nand DML statements.\nOnly one active index build operation per hybrid table can run at any time.\nYou can track the progress of an index build by using SHOW INDEXES. The STATUS column can take the following values:\nACTIVE: Index is complete and can be used to retrieve data.\nSUSPENDED: Index is only updated and is not used to retrieve data.\nBUILD FAILURE: An error has occurred with the index build process. You need to drop and recreate the index.\nBUILD IN PROGRESS: Index is being built and is not used to retrieve data.\nACTIVE: Index is complete and can be used to retrieve data.\nSUSPENDED: Index is only updated and is not used to retrieve data.\nBUILD FAILURE: An error has occurred with the index build process. You need to drop and recreate the index.\nBUILD IN PROGRESS: Index is being built and is not used to retrieve data.\nYou can rebuild a non-active index, where the status is SUSPENDED, BUILD FAILURE, or BUILD IN PROGRESS, by using DROP INDEX\nand CREATE INDEX.\nIf you want to drop a column that is part of an index that is being built, first stop the index build by dropping the index, then\ndrop the column. If you try to drop the column before dropping the index, you will receive this error message:\nOnline index builds do not make progress until all the active transactions with DMLs on the same table at the time when the\nCREATE INDEX statement was issued are completed. If any of those transactions remain idle for more than 5 minutes, they will\nabort by default. See Transactions.\nDuring the index build process, any DML performs its writes to the new index, but does not use the index to retrieve data.\nA small number of concurrent DMLs, which began executing after the CREATE INDEX command was complete, may fail and return\nthis error:\nIf the aborted DML statements belong to a multi-statement transaction, the transaction will roll back only if the\nTRANSACTION_ABORT_ON_ERROR parameter is set to TRUE.\nA newly created index will be used for retrieving data only when the index build process concludes successfully and the\nstatus of the index is ACTIVE.\nThe INCLUDE keyword is only supported for CREATE HYBRID TABLE.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.",
        "syntax": [
          "Column '<col_name>' cannot be dropped because it is used by index '<index-name>'.",
          "DML was unaware of concurrent DDL. Please retry this query."
        ]
      },
      {
        "heading": "Examples",
        "description": "\nTo run the following CREATE INDEX example, first create and load the hybrid table.\nNow you can create an index on the table.\nIf a failure occurs while the index is being built, the SHOW INDEXES command reports the following status:\nIf you decide to stop the index build, use a DROP INDEX command:\nOn this page\nSyntax\nParameters\nAccess control requirements\nUsage notes\nExamples\nRelated content\nHybrid tables\nCREATE HYBRID TABLE\nDROP INDEX\nALTER TABLE\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR REPLACE HYBRID TABLE mytable (\n  pk INT PRIMARY KEY,\n  val INT,\n  val2 INT\n);\n\nINSERT INTO mytable SELECT seq, seq+100, seq+200\n  FROM (SELECT seq8() seq FROM TABLE(GENERATOR(rowcount => 100)) v);",
          "CREATE OR REPLACE INDEX vidx ON mytable (val);",
          "+----------------------------------+\n| status                           |\n|----------------------------------|\n| Statement executed successfully. |\n+----------------------------------+",
          "BUILD FAILURE Index build failed. Please drop the index and re-create it.",
          "DROP INDEX mytable.vidx;",
          "+-------------------------------------+\n| status                              |\n|-------------------------------------|\n| Statement executed successfully.    |\n+-------------------------------------+"
        ]
      }
    ]
  },
  {
    "category": "DROP INDEX",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-index",
    "details": [
      {
        "heading": "DROP INDEX",
        "description": "\nFeature  Generally Available\nAvailable to accounts in AWS commercial regions only. For more information, see Clouds and regions.\nDrops a secondary index.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE INDEX , SHOW INDEXES , CREATE HYBRID TABLE , DROP TABLE , DESCRIBE TABLE , SHOW HYBRID TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP INDEX [ IF EXISTS ] <table_name>.<index_name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "table_name",
            "definition": "Specifies the identifier for the table."
          },
          {
            "term": "index_name",
            "definition": "Specifies the identifier for the index."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThis command can only be used to drop a secondary index. To drop an index that is used to enforce a UNIQUE\nor FOREIGN KEY constraint, use the ALTER TABLE command to drop the constraint.\nIndexes cannot be undropped.\nThis command can only be used to drop a secondary index. To drop an index that is used to enforce a UNIQUE\nor FOREIGN KEY constraint, use the ALTER TABLE command to drop the constraint.\nIndexes cannot be undropped.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Examples",
        "description": "\nRemoves the secondary index c_idx on table t0:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nRelated content\nHybrid tables\nCREATE HYBRID TABLE\nSHOW INDEXES\nALTER TABLE\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP INDEX t0.c_idx;"
        ]
      }
    ]
  },
  {
    "category": "SHOW HYBRID TABLES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-hybrid-tables",
    "details": [
      {
        "heading": "SHOW HYBRID TABLES",
        "description": "\nFeature  Generally Available\nAvailable to accounts in AWS commercial regions only. For more information, see Clouds and regions.\nLists the hybrid tables for which you have access privileges.\nThe command can be used to list hybrid tables for the current/specified database or schema, or across your entire account.\nThis command returns different output columns than SHOW TABLES.\nThe output returns hybrid table metadata and properties, ordered lexicographically by database, schema, and the name of the\nhybrid table (see Output in this topic for descriptions of the output columns). This is important to note if you wish to\nfilter the results using the provided filters.\nNote that this topic refers to hybrid tables as simply tables except where specifying hybrid tables avoids confusion.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE INDEX, DROP INDEX , SHOW INDEXES , CREATE HYBRID TABLE , DROP TABLE , DESCRIBE TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] [ HYBRID ] TABLES [ LIKE '<pattern>' ]\n                                 [ IN { ACCOUNT | DATABASE [ <db_name> ] | SCHEMA [ <schema_name> ] } ]\n                                 [ STARTS WITH '<name_string>' ]\n                                 [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Optionally returns only a subset of the output columns: created_on name kind The kind column value is always HYBRID TABLE. database_name schema_name Default: No value (all columns are included in the output)"
          },
          {
            "term": "HYBRID",
            "definition": "Returns hybrid tables only."
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "IN ACCOUNT | DATABASE [ db_name ] | SCHEMA [ schema_name ]",
            "definition": "Optionally specifies the scope of the command, which determines whether the command lists records only for the\ncurrent/specified database or schema, or across your entire account. If you specify the keyword ACCOUNT, then the command retrieves records for all schemas in all databases\nof the current account. If you specify the keyword DATABASE, then: If you specify a db_name, then the command retrieves records for all schemas of the specified database. If you do not specify a db_name, then: If there is a current database, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and schemas in the account. If you specify the keyword SCHEMA, then: If you specify a qualified schema name (e.g. my_database.my_schema), then the command\nretrieves records for the specified database and schema. If you specify an unqualified schema_name, then: If there is a current database, then the command retrieves records for the specified schema in the current database. If there is no current database, then the command displays the error\nSQL compilation error: Object does not exist, or operation cannot be performed. If you do not specify a schema_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default. The command returns the objects you have privileges to view in the current\ndatabase. No database: ACCOUNT is the default. The command returns the objects you have privileges to view in your account."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nIf an account (or database or schema) has a large number of hybrid tables, then searching the entire account (or database or\nschema) can consume a significant amount of compute resources.\nIf an account (or database or schema) has a large number of hybrid tables, then searching the entire account (or database or\nschema) can consume a significant amount of compute resources.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema."
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the table was created."
              ],
              [
                "name",
                "Name of the table."
              ],
              [
                "database_name",
                "Database in which the table is stored."
              ],
              [
                "schema_name",
                "Schema in which the table is stored."
              ],
              [
                "owner",
                "Role that owns the table."
              ],
              [
                "rows",
                "Number of rows in the table."
              ],
              [
                "bytes",
                "Number of bytes that will be scanned if the entire table is scanned in a query. Note that this number may be different from the number of actual physical bytes (that is, bytes stored on-disk) for the table."
              ],
              [
                "comment",
                "Comment for the table."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Identifying hybrid tables with SHOW TABLES",
        "tables": [
          {
            "headers": [
              "Column Name",
              "Values"
            ],
            "rows": [
              [
                "is_hybrid",
                "Y if the table is a hybrid table; otherwise, N."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nShow all the hybrid tables whose name starts with product_ that you have privileges to view in the mydb.myschema schema:\nOn this page\nSyntax\nParameters\nUsage notes\nOutput\nIdentifying hybrid tables with SHOW TABLES\nExamples\nRelated content\nHybrid tables\nCREATE HYBRID TABLE\nDROP INDEX\nALTER TABLE\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW HYBRID TABLES LIKE 'product_%' IN mydb.myschema;"
        ]
      }
    ]
  },
  {
    "category": "SHOW INDEXES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-indexes",
    "details": [
      {
        "heading": "SHOW INDEXES",
        "description": "\nFeature  Generally Available\nAvailable to accounts in AWS commercial regions only. For more information, see Clouds and regions.\nLists all the indexes in your account for which you have access privileges.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE HYBRID TABLE , CREATE INDEX , DROP INDEX , DROP TABLE , DESCRIBE TABLE , SHOW HYBRID TABLES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] INDEXES\n  [ LIKE '<pattern>' ]\n  [ IN { ACCOUNT | DATABASE [ <database_name> ] | SCHEMA [ <schema_name> ] | TABLE | TABLE <table_name> } ]\n  [ STARTS WITH '<name_string>' ]\n  [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Returns only a subset of the output columns: created_on name kind database_name schema_name"
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "IN { ACCOUNT | DATABASE [ database_name ] | SCHEMA [ schema_name ] | TABLE | TABLE table_name }",
            "definition": "Filters the output by the specified database, schema, table, or account. If you specify the keyword ACCOUNT, then the command retrieves records for all schemas in all databases\nof the current account. If you specify the keyword DATABASE, then: If you specify a db_name, then the command retrieves records for all schemas of the specified database. If you do not specify a db_name, then: If there is a current database, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and schemas in the account. If you specify the keyword SCHEMA, then: If you specify a qualified schema name (e.g. my_database.my_schema), then the command\nretrieves records for the specified database and schema. If you specify an unqualified schema_name, then: If there is a current database, then the command retrieves records for the specified schema in the current database. If there is no current database, then the command displays the error\nSQL compilation error: Object does not exist, or operation cannot be performed. If you do not specify a schema_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. If you specify the keyword TABLE without a table_name, then: If there is a current database, then: If there is a current schema, then the command retrieves records for the current schema in the current database. If there is no current schema, then the command retrieves records for all schemas in the current database. If there is no current database, then the command retrieves records for all databases and all schemas in the account. If you specify a <table_name> (with or without the keyword TABLE), then: If you specify a fully-qualified <table_name> (e.g. my_database_name.my_schema_name.my_table_name),\nthen the command retrieves all records for the specified table. If you specify a schema-qualified <table_name> (e.g. my_schema_name.my_table_name), then: If a current database exists, then the command retrieves all records for the specified table. If no current database exists, then the command displays an error similar to\nCannot perform SHOW <object_type>. This session does not have a current database.... If you specify an unqualified <table_name>, then: If a current database and current schema exist, then the command retrieves records for the specified table in the current\nschema of the current database. If no current database exists or no current schema exists, then the command displays an error similar to:\nSQL compilation error: <object> does not exist or not authorized.. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (i.e. the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (i.e. the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the index was created."
              ],
              [
                "name",
                "Name of the index."
              ],
              [
                "is_unique",
                "Whether the index is a unique index."
              ],
              [
                "columns",
                "List of indexed columns."
              ],
              [
                "included_columns",
                "List of covered columns."
              ],
              [
                "table",
                "Name of the table."
              ],
              [
                "database_name",
                "Database in which the index is stored."
              ],
              [
                "schema_name",
                "Schema in which the index is stored."
              ],
              [
                "owner",
                "Role that owns the index."
              ],
              [
                "owner_role_type",
                "Role type of the owner."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nThese SHOW INDEX examples use the current database and schema.\nReturn a terse list of indexes that contain the string DEVICE in their names:\nOnly return indexes that have covered columns (included_columns). Use the pipe operator\nto select specific rows and columns from the full output of the SHOW INDEXES command.\nThe following output shows the SELECT query result only. One index qualifies for the WHERE clause condition:\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nRelated content\nHybrid tables\nCREATE HYBRID TABLE\nDROP INDEX\nALTER TABLE\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW TERSE INDEXES LIKE '%DEVICE%';",
          "+-------------------------------+---------------------------------------+-----------------+---------------+-------------+\n| created_on                    | name                                  | kind            | database_name | schema_name |\n|-------------------------------+---------------------------------------+-----------------+---------------+-------------|\n| 2024-08-29 12:24:49.197 -0700 | SYS_INDEX_SENSOR_DATA_DEVICE1_PRIMARY | KEY_VALUE_INDEX | HT_SENSORS    | HT_SCHEMA   |\n| 2024-08-29 12:24:49.197 -0700 | DEVICE_IDX                            | KEY_VALUE_INDEX | HT_SENSORS    | HT_SCHEMA   |\n| 2024-08-29 14:03:36.537 -0700 | SYS_INDEX_SENSOR_DATA_DEVICE2_PRIMARY | KEY_VALUE_INDEX | HT_SENSORS    | HT_SCHEMA   |\n| 2024-08-29 14:03:36.537 -0700 | DEVICE_IDX                            | KEY_VALUE_INDEX | HT_SENSORS    | HT_SCHEMA   |\n+-------------------------------+---------------------------------------+-----------------+---------------+-------------+",
          "SHOW INDEXES\n  ->> SELECT \"name\",\n             \"is_unique\",\n             \"table\",\n             \"columns\",\n             \"included_columns\",\n             \"database_name\",\n             \"schema_name\"\n        FROM $1\n        WHERE \"included_columns\" != '[]';",
          "+------------+-----------+---------------------+-------------+------------------+---------------+-------------+\n| name       | is_unique | table               | columns     | included_columns | database_name | schema_name |\n|------------+-----------+---------------------+-------------+------------------+---------------+-------------|\n| DEVICE_IDX | N         | SENSOR_DATA_DEVICE2 | [DEVICE_ID] | [TEMPERATURE]    | HT_SENSORS    | HT_SCHEMA   |\n+------------+-----------+---------------------+-------------+------------------+---------------+-------------+"
        ]
      }
    ]
  },
  {
    "category": "CREATE ICEBERG TABLE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-iceberg-table",
    "details": [
      {
        "heading": "CREATE ICEBERG TABLE",
        "description": "\nCreates or replaces an Apache Iceberg table in the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER ICEBERG TABLE, DROP ICEBERG TABLE , SHOW ICEBERG TABLES , DESCRIBE ICEBERG TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nThis section provides an overview of the syntax for all types of Iceberg tables.\nThe syntax for creating an Iceberg table varies considerably depending on whether you use Snowflake as the Iceberg catalog\nor an external Iceberg catalog.\nTo view the syntax, parameter descriptions, usage notes, and examples for specific use cases, see the following pages:\nSnowflake as the Iceberg catalog\n\nCREATE ICEBERG TABLE (Snowflake as the Iceberg catalog)\nCREATE ICEBERG TABLE (Snowflake as the Iceberg catalog)\nExternal Iceberg catalog\n\nCREATE ICEBERG TABLE (REST or Snowflake Open Catalog)\nCREATE ICEBERG TABLE (Delta files in object storage)\nCREATE ICEBERG TABLE (Iceberg files in object storage)\nCREATE ICEBERG TABLE (REST or Snowflake Open Catalog)\nCREATE ICEBERG TABLE (Delta files in object storage)\nCREATE ICEBERG TABLE (Iceberg files in object storage)\nSnowflake as the Iceberg catalog\nCREATE ICEBERG TABLE (Snowflake as the Iceberg catalog)\nCREATE ICEBERG TABLE (Snowflake as the Iceberg catalog)\nExternal Iceberg catalog\nCREATE ICEBERG TABLE (REST or Snowflake Open Catalog)\nCREATE ICEBERG TABLE (Delta files in object storage)\nCREATE ICEBERG TABLE (Iceberg files in object storage)\nCREATE ICEBERG TABLE (REST or Snowflake Open Catalog)\nCREATE ICEBERG TABLE (Delta files in object storage)\nCREATE ICEBERG TABLE (Iceberg files in object storage)"
      }
    ]
  },
  {
    "category": "CREATE VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-view",
    "details": [
      {
        "heading": "CREATE VIEW",
        "description": "\nCreates a new view in the current/specified schema, based on a query of one or more existing tables (or any other valid query expression).\nThis command supports the following variants:\nCREATE OR ALTER VIEW: Creates a view if it doesnt exist or alters an existing view.\nCREATE OR ALTER VIEW: Creates a view if it doesnt exist or alters an existing view.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER VIEW , DROP VIEW , SHOW VIEWS , DESCRIBE VIEW CREATE OR ALTER <object>"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CREATE [ OR REPLACE ] [ SECURE ] [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE } ] [ RECURSIVE ] VIEW [ IF NOT EXISTS ] <name>\n  [ ( <column_list> ) ]\n  [ <col1> [ WITH ] MASKING POLICY <policy_name> [ USING ( <col1> , <cond_col1> , ... ) ]\n           [ WITH ] PROJECTION POLICY <policy_name>\n           [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ , <col2> [ ... ] ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ CHANGE_TRACKING = { TRUE | FALSE } ]\n  [ COPY GRANTS ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] JOIN POLICY <policy_name> [ ALLOWED JOIN KEYS ( <col_name> [ , ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  AS <select_statement>"
        ]
      },
      {
        "heading": "Variant syntax"
      },
      {
        "heading": "CREATE OR ALTER VIEW",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nCreates a new view if it doesnt already exist, or updates the properties of an existing view to match those defined in the statement.\nA CREATE OR ALTER VIEW statement follows the syntax rules of a CREATE VIEW statement and has the same limitations as an\nALTER VIEW statement.\nThe CREATE OR ALTER VIEW command doesnt support changing a view definition once a view is created. This limitation is inherited\nfrom the ALTER VIEW command.\nThe following modifications are supported:\nConverting to (or reverting from) a secure view.\nAdding, overwriting, removing a comment for a view or a views columns.\nEnabling or disabling change tracking for a view.\nConverting to (or reverting from) a secure view.\nAdding, overwriting, removing a comment for a view or a views columns.\nEnabling or disabling change tracking for a view.\nFor more information, see CREATE OR ALTER VIEW usage notes and CREATE OR ALTER <object>.",
        "syntax": [
          "CREATE OR ALTER [ SECURE ] [ { [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE } ] [ RECURSIVE ] VIEW <name>\n  [ ( <column_list> ) ]\n  [ CHANGE_TRACKING =  { TRUE | FALSE } ]\n  [ COMMENT = '<string_literal>' ]\n  AS <select_statement>"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the view; must be unique for the schema in which the view is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details, see Identifier requirements."
          },
          {
            "term": "select_statement",
            "definition": "Specifies the query used to create the view. Can be on one or more source tables or any other valid SELECT statement. This\nquery serves as the text/definition for the view and is displayed in the SHOW VIEWS output and the\nVIEWS Information Schema view."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "syntax": [
          "CREATE VIEW v1 (pre_tax_profit, taxes, after_tax_profit) AS\n    SELECT revenue - cost, (revenue - cost) * tax_rate, (revenue - cost) * (1.0 - tax_rate)\n    FROM table1;",
          "CREATE VIEW v1 (pre_tax_profit COMMENT 'revenue minus cost',\n                taxes COMMENT 'assumes taxes are a fixed percentage of profit',\n                after_tax_profit)\n    AS\n    SELECT revenue - cost, (revenue - cost) * tax_rate, (revenue - cost) * (1.0 - tax_rate)\n    FROM table1;"
        ],
        "definitions": [
          {
            "term": "SECURE",
            "definition": "Specifies that the view is secure. For more information about secure views, see Working with Secure Views. Default: No value (view is not secure)"
          },
          {
            "term": "{ [ { LOCAL | GLOBAL } ] TEMP | TEMPORARY | VOLATILE }",
            "definition": "Specifies that the view persists only for the duration of the session that you created it in. A\ntemporary view and all its contents are dropped at the end of the session. The synonyms and abbreviations for TEMPORARY (e.g. GLOBAL TEMPORARY) are provided for compatibility with other databases\n(e.g. to prevent errors when migrating CREATE VIEW statements). Views created with any of these keywords appear and behave identically to\na view created with the TEMPORARY keyword. Default: No value. If a view is not declared as TEMPORARY, the view is permanent. If you want to avoid unexpected conflicts, avoid naming temporary views after views that already exist in the schema. If you created a temporary view with the same name as another view in the schema, all queries and operations used on the view only affect\nthe temporary view in the session, until you drop the temporary view. If you drop the view, you drop the temporary view, and not the view\nthat already exists in the schema."
          },
          {
            "term": "RECURSIVE",
            "definition": "Specifies that the view can refer to itself using recursive syntax without necessarily using a CTE (common table\nexpression). For more information about recursive views in general, and the RECURSIVE keyword in particular,\nsee Recursive Views (Non-materialized Views Only) and the recursive view examples below. Default: No value (view is not recursive, or is recursive only by using a CTE)"
          },
          {
            "term": "column_list",
            "definition": "If you want to change the name of a column or add a comment to a column in the new view,\ninclude a column list that specifies the column names and (if needed) comments about\nthe columns. (You do not need to specify the data types of the columns.) If any of the columns in the view are based on expressions (not just simple column names), then you must supply\na column name for each column in the view. For example, the column names are required in the following case: You can specify an optional comment for each column. For example: Comments are particularly helpful when column names are cryptic. To view comments, use DESCRIBE VIEW."
          },
          {
            "term": "MASKING POLICY = policy_name",
            "definition": "Specifies the masking policy to set on a column."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy."
          },
          {
            "term": "PROJECTION POLICY policy_name",
            "definition": "Specifies the projection policy to set on a column."
          },
          {
            "term": "CHANGE_TRACKING = { TRUE | FALSE }",
            "definition": "Specifies whether to enable change tracking on the view. TRUE enables change tracking on the view. This setting adds a pair of hidden columns to the source table and begins\nstoring change tracking metadata in the columns. These columns consume a small amount of storage. The change-tracking metadata can be queried using the CHANGES clause for\nSELECT statements, or by creating and querying one or more streams on the table. FALSE does not enable change tracking on the view."
          },
          {
            "term": "COPY GRANTS",
            "definition": "Retains the access permissions from the original view when a new view is created using the OR REPLACE clause. The parameter copies all privileges, except OWNERSHIP, from the existing view to the new view. The new view does not\ninherit any future grants defined for the object type in the schema. By default, the role that executes the CREATE VIEW statement owns\nthe new view. If the parameter is not included in the CREATE VIEW statement, then the new view does not inherit any explicit access\nprivileges granted on the original view but does inherit any future grants defined for the object type in the schema. Note that the operation to copy grants occurs atomically with the CREATE VIEW statement (i.e. within the same transaction). Default: No value (grants are not copied)"
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment for the view. Default: No value"
          },
          {
            "term": "ROW ACCESS POLICY policy_name ON ( col_name [ , col_name ... ] )",
            "definition": "Specifies the row access policy to set on a view."
          },
          {
            "term": "AGGREGATION POLICY policy_name [ ENTITY KEY ( col_name [ , col_name ... ] ) ]",
            "definition": "Specifies the aggregation policy to set on a view. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the view. For more information, see\nImplementing entity-level privacy with aggregation policies."
          },
          {
            "term": "JOIN POLICY policy_name [ ALLOWED JOIN KEYS ( col_name [ , ... ] ) ]",
            "definition": "Specifies the join policy to set on a view. Use the optional ALLOWED JOIN KEYS parameter to define which columns are allowed to be used as joining columns when\nthis policy is in effect. For more information, see Join policies. This parameter is not supported by the CREATE OR ALTER variant syntax."
          },
          {
            "term": "TAG ( tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ] )",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "WITH CONTACT ( purpose = contact [ , purpose = contact ...] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the new object with one or more contacts."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE VIEW",
                "Schema",
                "Required to create a new view."
              ],
              [
                "SELECT",
                "Table, external table, view",
                "Required on any tables and/or views queried in the view definition."
              ],
              [
                "APPLY",
                "Masking policy, row access policy, tag",
                "Required only when applying a masking policy, row access policy, object tags, or any combination of these\ngovernance features when creating views."
              ],
              [
                "OWNERSHIP",
                "View",
                "A role must be granted or inherit the OWNERSHIP privilege on the object to create a temporary object that has the same name as the object\nthat already exists in the schema.\nRequired to execute a CREATE OR ALTER VIEW statement for an existing view.\n\nOWNERSHIP is a special privilege on an object that is automatically granted to the role that created the object, but can also be transferred using the GRANT OWNERSHIP command to a different role by the owning role (or any role with the MANAGE GRANTS privilege).\nNote that in a managed access schema, only the schema owner (i.e. the role with the OWNERSHIP privilege on the schema) or a role with the MANAGE GRANTS privilege can grant or revoke privileges on objects in the schema, including future grants."
              ]
            ]
          }
        ]
      },
      {
        "heading": "General usage notes",
        "description": "\nA view definition can include an ORDER BY clause\n(e.g. create view v1 as select * from t1 ORDER BY column1). However, Snowflake recommends excluding\nthe ORDER BY clause from most view definitions. If the view is used in contexts that dont benefit from sorting,\nthen the ORDER BY clause adds unnecessary costs. For example, when the view is used in a join, and the join\ncolumn is not the same as the ORDER BY column, the extra cost to sort the views results is typically wasted.\nIf you need to sort the query results, its usually more efficient to specify ORDER BY in the query that uses\nthe view, rather than in the view itself.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\ndefinition of the view, the function returns the database or schema that contains the view, not the database or schema in\nuse for the session.\nThe definition for a view is limited to 95KB.\nNesting levels are limited to a maximum of 20. An attempt to create a view that is nested more than 20 times will fail.\nView definitions are not dynamic. A view is not automatically updated if the underlying sources are modified such that they no longer\nmatch the view definition, particularly when columns are dropped. For example:\n\nA view is created referencing a specific column in a source table, and the column is subsequently dropped from the table.\nA view is created using SELECT * from a table, and changes are made to the columns in the table, such as:\n\nA column is dropped.\nA column is added.\nThe column order changes.\n\n\n\nIn these scenarios, querying the view returns a column-related error.\nA view is created referencing a specific column in a source table, and the column is subsequently dropped from the table.\nA view is created using SELECT * from a table, and changes are made to the columns in the table, such as:\n\nA column is dropped.\nA column is added.\nThe column order changes.\nA column is dropped.\nA column is added.\nThe column order changes.\nIf a source table for a view is dropped, querying the view returns an object does not exist error.\nA schema cannot contain a table and view with the same name. A CREATE VIEW statement produces an error if a table with the same name\nalready exists in the schema.\nWhen a view is created, unqualified references to tables and other database\nobjects are resolved in the views schema, not in the sessions current schema. Similarly, objects that are\npartially qualified (i.e. schema.object) are resolved in the views database, not in the sessions current database.\nThe SEARCH_PATH session parameter (if present) is ignored.\nUsing OR REPLACE is the equivalent of using DROP VIEW on the existing view and then creating a new view with the same\nname.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE VIEW operation use either the old or new view version.\nRecreating or swapping a view drops its change data, which makes any stream on the view stale. A\nstale stream is unreadable.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nUsing COPY GRANTS:\n\nData sharing:\n\nIf the existing secure view was shared to another account, the replacement view is also shared.\nIf the existing secure view was shared with your account as a data consumer, and access was further granted to other roles in the\naccount (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement view.\n\n\nThe SHOW GRANTS output for the replacement view lists the grantee for the copied privileges as the role\nthat executed the CREATE VIEW statement, with the current timestamp when the statement was executed.\nData sharing:\n\nIf the existing secure view was shared to another account, the replacement view is also shared.\nIf the existing secure view was shared with your account as a data consumer, and access was further granted to other roles in the\naccount (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement view.\nIf the existing secure view was shared to another account, the replacement view is also shared.\nIf the existing secure view was shared with your account as a data consumer, and access was further granted to other roles in the\naccount (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement view.\nThe SHOW GRANTS output for the replacement view lists the grantee for the copied privileges as the role\nthat executed the CREATE VIEW statement, with the current timestamp when the statement was executed.\nWhen you create a view and then grant privileges on that view to a role, the role can use the view even if the role does not have\nprivileges on the underlying table(s) that the view accesses. This means that you can create a view to give a role access to only\na subset of a table. For example, you can create a view that accesses medical billing information but not medical diagnosis\ninformation in the same table. Then you can grant privileges on that view to the accountant role so that the accountants\ncan look at the billing information without seeing the patients diagnosis.\nBy design, the SHOW VIEWS command does not provide information about secure views. To view information about a secure view,\nyou must use the VIEWS view in the Information Schema and you must use the role that owns\nthe view.\nA recursive view must provide a column name list.\nWhen defining recursive views, prevent infinite recursion. The WHERE clause in the recursive view definition should enable the\nrecursion to stop eventually, typically by running out of data after processing the last level of a hierarchy of data.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nWhen creating a view with a masking policy on one or more view columns, or a row access policy added to the view, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\nview protected by a row access policy.\nDo not create views with streams as source objects unless the same role owns both the view and source streams (i.e. the same role,\nor a lower role in a role hierarchy, has the OWNERSHIP privilege on the view and source streams). Instead, create views that have\nthe objects to track as the source objects. Then, create streams on those views. For more information, see\nStreams on Views.\nA view definition can include an ORDER BY clause\n(e.g. create view v1 as select * from t1 ORDER BY column1). However, Snowflake recommends excluding\nthe ORDER BY clause from most view definitions. If the view is used in contexts that dont benefit from sorting,\nthen the ORDER BY clause adds unnecessary costs. For example, when the view is used in a join, and the join\ncolumn is not the same as the ORDER BY column, the extra cost to sort the views results is typically wasted.\nIf you need to sort the query results, its usually more efficient to specify ORDER BY in the query that uses\nthe view, rather than in the view itself.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\ndefinition of the view, the function returns the database or schema that contains the view, not the database or schema in\nuse for the session.\nThe definition for a view is limited to 95KB.\nNesting levels are limited to a maximum of 20. An attempt to create a view that is nested more than 20 times will fail.\nView definitions are not dynamic. A view is not automatically updated if the underlying sources are modified such that they no longer\nmatch the view definition, particularly when columns are dropped. For example:\nA view is created referencing a specific column in a source table, and the column is subsequently dropped from the table.\nA view is created using SELECT * from a table, and changes are made to the columns in the table, such as:\n\nA column is dropped.\nA column is added.\nThe column order changes.\nA column is dropped.\nA column is added.\nThe column order changes.\nA view is created referencing a specific column in a source table, and the column is subsequently dropped from the table.\nA view is created using SELECT * from a table, and changes are made to the columns in the table, such as:\nA column is dropped.\nA column is added.\nThe column order changes.\nA column is dropped.\nA column is added.\nThe column order changes.\nIn these scenarios, querying the view returns a column-related error.\nIf a source table for a view is dropped, querying the view returns an object does not exist error.\nA schema cannot contain a table and view with the same name. A CREATE VIEW statement produces an error if a table with the same name\nalready exists in the schema.\nWhen a view is created, unqualified references to tables and other database\nobjects are resolved in the views schema, not in the sessions current schema. Similarly, objects that are\npartially qualified (i.e. schema.object) are resolved in the views database, not in the sessions current database.\nThe SEARCH_PATH session parameter (if present) is ignored.\nUsing OR REPLACE is the equivalent of using DROP VIEW on the existing view and then creating a new view with the same\nname.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE VIEW operation use either the old or new view version.\nRecreating or swapping a view drops its change data, which makes any stream on the view stale. A\nstale stream is unreadable.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nUsing COPY GRANTS:\nData sharing:\n\nIf the existing secure view was shared to another account, the replacement view is also shared.\nIf the existing secure view was shared with your account as a data consumer, and access was further granted to other roles in the\naccount (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement view.\nIf the existing secure view was shared to another account, the replacement view is also shared.\nIf the existing secure view was shared with your account as a data consumer, and access was further granted to other roles in the\naccount (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement view.\nThe SHOW GRANTS output for the replacement view lists the grantee for the copied privileges as the role\nthat executed the CREATE VIEW statement, with the current timestamp when the statement was executed.\nData sharing:\nIf the existing secure view was shared to another account, the replacement view is also shared.\nIf the existing secure view was shared with your account as a data consumer, and access was further granted to other roles in the\naccount (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement view.\nIf the existing secure view was shared to another account, the replacement view is also shared.\nIf the existing secure view was shared with your account as a data consumer, and access was further granted to other roles in the\naccount (using GRANT IMPORTED PRIVILEGES on the parent database), access is also granted to the replacement view.\nThe SHOW GRANTS output for the replacement view lists the grantee for the copied privileges as the role\nthat executed the CREATE VIEW statement, with the current timestamp when the statement was executed.\nWhen you create a view and then grant privileges on that view to a role, the role can use the view even if the role does not have\nprivileges on the underlying table(s) that the view accesses. This means that you can create a view to give a role access to only\na subset of a table. For example, you can create a view that accesses medical billing information but not medical diagnosis\ninformation in the same table. Then you can grant privileges on that view to the accountant role so that the accountants\ncan look at the billing information without seeing the patients diagnosis.\nBy design, the SHOW VIEWS command does not provide information about secure views. To view information about a secure view,\nyou must use the VIEWS view in the Information Schema and you must use the role that owns\nthe view.\nA recursive view must provide a column name list.\nWhen defining recursive views, prevent infinite recursion. The WHERE clause in the recursive view definition should enable the\nrecursion to stop eventually, typically by running out of data after processing the last level of a hierarchy of data.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nWhen creating a view with a masking policy on one or more view columns, or a row access policy added to the view, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\nview protected by a row access policy.\nDo not create views with streams as source objects unless the same role owns both the view and source streams (i.e. the same role,\nor a lower role in a role hierarchy, has the OWNERSHIP privilege on the view and source streams). Instead, create views that have\nthe objects to track as the source objects. Then, create streams on those views. For more information, see\nStreams on Views."
      },
      {
        "heading": "Porting notes",
        "description": "\nSome vendors support the FORCE keyword:\n\nCREATE OR REPLACE FORCE VIEW ...\n\nCopy\n\nSnowflake accepts the FORCE keyword, but does not support it. In other words, you do not get a syntax error if you use this\nkeyword, but using FORCE does not force the creation of a view if the underlying database objects (table(s) or view(s))\ndo not already exist. Attempting to create a view of a non-existent table or view results in an error message even if the\nFORCE keyword is used.\nWhen looking up the tables in a view, some vendors search for unqualified table names in the active schema; Snowflake searches\nfor unqualified table names\nin the same schema as the view.\nWhen porting to Snowflake, consider updating views to use fully-qualified table names.\nSome vendors support the FORCE keyword:\nSnowflake accepts the FORCE keyword, but does not support it. In other words, you do not get a syntax error if you use this\nkeyword, but using FORCE does not force the creation of a view if the underlying database objects (table(s) or view(s))\ndo not already exist. Attempting to create a view of a non-existent table or view results in an error message even if the\nFORCE keyword is used.\nWhen looking up the tables in a view, some vendors search for unqualified table names in the active schema; Snowflake searches\nfor unqualified table names\nin the same schema as the view.\nWhen porting to Snowflake, consider updating views to use fully-qualified table names.",
        "syntax": [
          "CREATE OR REPLACE FORCE VIEW ..."
        ]
      },
      {
        "heading": "CREATE OR ALTER VIEW usage notes",
        "description": "\nPreview Feature  Open\nAvailable to all accounts.\nAll limitations of the ALTER VIEW command apply.\nThis command doesn not support the following:\n\nChanging the definition of a view.\nRenaming a view using the RENAME TO parameter.\nAdding or changing tags and policies. Any existing tags and policies are preserved.\nConverting a TEMPORARY view into a permanent view, or vice versa.\nChanging the definition of a view.\nRenaming a view using the RENAME TO parameter.\nAdding or changing tags and policies. Any existing tags and policies are preserved.\nConverting a TEMPORARY view into a permanent view, or vice versa.\nAll limitations of the ALTER VIEW command apply.\nThis command doesn not support the following:\nChanging the definition of a view.\nRenaming a view using the RENAME TO parameter.\nAdding or changing tags and policies. Any existing tags and policies are preserved.\nConverting a TEMPORARY view into a permanent view, or vice versa.\nChanging the definition of a view.\nRenaming a view using the RENAME TO parameter.\nAdding or changing tags and policies. Any existing tags and policies are preserved.\nConverting a TEMPORARY view into a permanent view, or vice versa."
      },
      {
        "heading": "Examples"
      },
      {
        "heading": "Basic examples",
        "description": "\nCreate a view in the current schema, with a comment, that selects all the rows from a table:\nThe next example is the same as the previous example, except the view is secure:\nThe following shows two ways of creating recursive views:\nFirst, create and load the table:\nCreate a view using a recursive CTE, and then query the view.\nCreate a view using the keyword RECURSIVE, and then query the view.",
        "syntax": [
          "CREATE VIEW myview COMMENT='Test view' AS SELECT col1, col2 FROM mytable;\n\nSHOW VIEWS;\n\n+---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------+\n| created_on                      | name              | reserved | database_name | schema_name | owner    | comment   | text                                                                     |\n|---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------|\n| Thu, 19 Jan 2017 15:00:37 -0800 | MYVIEW            |          | MYTEST1       | PUBLIC      | SYSADMIN | Test view | CREATE VIEW myview COMMENT='Test view' AS SELECT col1, col2 FROM mytable |\n+---------------------------------+-------------------+----------+---------------+-------------+----------+-----------+--------------------------------------------------------------------------+",
          "CREATE OR REPLACE SECURE VIEW myview COMMENT='Test secure view' AS SELECT col1, col2 FROM mytable;\n\nSELECT is_secure FROM information_schema.views WHERE table_name = 'MYVIEW';",
          "CREATE OR REPLACE TABLE employees (title VARCHAR, employee_ID INTEGER, manager_ID INTEGER);",
          "INSERT INTO employees (title, employee_ID, manager_ID) VALUES\n    ('President', 1, NULL),  -- The President has no manager.\n        ('Vice President Engineering', 10, 1),\n            ('Programmer', 100, 10),\n            ('QA Engineer', 101, 10),\n        ('Vice President HR', 20, 1),\n            ('Health Insurance Analyst', 200, 20);",
          "CREATE VIEW employee_hierarchy (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n   WITH RECURSIVE employee_hierarchy_cte (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n      -- Start at the top of the hierarchy ...\n      SELECT title, employee_ID, manager_ID, NULL AS \"MGR_EMP_ID (SHOULD BE SAME)\", 'President' AS \"MGR TITLE\"\n        FROM employees\n        WHERE title = 'President'\n      UNION ALL\n      -- ... and work our way down one level at a time.\n      SELECT employees.title, \n             employees.employee_ID, \n             employees.manager_ID, \n             employee_hierarchy_cte.employee_id AS \"MGR_EMP_ID (SHOULD BE SAME)\", \n             employee_hierarchy_cte.title AS \"MGR TITLE\"\n        FROM employees INNER JOIN employee_hierarchy_cte\n       WHERE employee_hierarchy_cte.employee_ID = employees.manager_ID\n   )\n   SELECT * \n      FROM employee_hierarchy_cte\n);",
          "SELECT * \n    FROM employee_hierarchy \n    ORDER BY employee_ID;\n+----------------------------+-------------+------------+-----------------------------+----------------------------+\n| TITLE                      | EMPLOYEE_ID | MANAGER_ID | MGR_EMP_ID (SHOULD BE SAME) | MGR TITLE                  |\n|----------------------------+-------------+------------+-----------------------------+----------------------------|\n| President                  |           1 |       NULL |                        NULL | President                  |\n| Vice President Engineering |          10 |          1 |                           1 | President                  |\n| Vice President HR          |          20 |          1 |                           1 | President                  |\n| Programmer                 |         100 |         10 |                          10 | Vice President Engineering |\n| QA Engineer                |         101 |         10 |                          10 | Vice President Engineering |\n| Health Insurance Analyst   |         200 |         20 |                          20 | Vice President HR          |\n+----------------------------+-------------+------------+-----------------------------+----------------------------+",
          "CREATE RECURSIVE VIEW employee_hierarchy_02 (title, employee_ID, manager_ID, \"MGR_EMP_ID (SHOULD BE SAME)\", \"MGR TITLE\") AS (\n      -- Start at the top of the hierarchy ...\n      SELECT title, employee_ID, manager_ID, NULL AS \"MGR_EMP_ID (SHOULD BE SAME)\", 'President' AS \"MGR TITLE\"\n        FROM employees\n        WHERE title = 'President'\n      UNION ALL\n      -- ... and work our way down one level at a time.\n      SELECT employees.title, \n             employees.employee_ID, \n             employees.manager_ID, \n             employee_hierarchy_02.employee_id AS \"MGR_EMP_ID (SHOULD BE SAME)\", \n             employee_hierarchy_02.title AS \"MGR TITLE\"\n        FROM employees INNER JOIN employee_hierarchy_02\n        WHERE employee_hierarchy_02.employee_ID = employees.manager_ID\n);",
          "SELECT * \n    FROM employee_hierarchy_02 \n    ORDER BY employee_ID;\n+----------------------------+-------------+------------+-----------------------------+----------------------------+\n| TITLE                      | EMPLOYEE_ID | MANAGER_ID | MGR_EMP_ID (SHOULD BE SAME) | MGR TITLE                  |\n|----------------------------+-------------+------------+-----------------------------+----------------------------|\n| President                  |           1 |       NULL |                        NULL | President                  |\n| Vice President Engineering |          10 |          1 |                           1 | President                  |\n| Vice President HR          |          20 |          1 |                           1 | President                  |\n| Programmer                 |         100 |         10 |                          10 | Vice President Engineering |\n| QA Engineer                |         101 |         10 |                          10 | Vice President Engineering |\n| Health Insurance Analyst   |         200 |         20 |                          20 | Vice President HR          |\n+----------------------------+-------------+------------+-----------------------------+----------------------------+"
        ]
      },
      {
        "heading": "CREATE OR ALTER VIEW examples",
        "description": "\nCreate a table my_table with one column:\nCreate a view named v2 that selects column a from table my_table:\nCreate or alter view v2. Add or update the COMMENT and CHANGE_TRACKING properties for the view:\nCreate or alter view v2 to add a comment to a column:\nThe absence of a previously set property in the CREATE OR ALTER VIEW statement results\nin unsetting it. In the following example, unset the COMMENT property for the view v2 from the previous example:\nOn this page\nSyntax\nVariant syntax\nCREATE OR ALTER VIEW\nRequired parameters\nOptional parameters\nAccess control requirements\nGeneral usage notes\nPorting notes\nCREATE OR ALTER VIEW usage notes\nExamples\nBasic examples\nCREATE OR ALTER VIEW examples\nBasic example\nUnset a property previously set on view\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR ALTER TABLE my_table(a INT);",
          "CREATE OR ALTER VIEW v2(one)\n  AS SELECT a FROM my_table;",
          "CREATE OR ALTER VIEW v2(one)\n  COMMENT = 'fff'\n  CHANGE_TRACKING = true\n  AS SELECT a FROM my_table;",
          "CREATE OR ALTER VIEW v2(one COMMENT 'bar')\n  COMMENT = 'foo'\n  AS SELECT a FROM my_table;",
          "CREATE OR ALTER VIEW v2(one COMMENT 'bar')\n  CHANGE_TRACKING = true\n  AS SELECT a FROM my_table;"
        ]
      }
    ]
  },
  {
    "category": "ALTER VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-view",
    "details": [
      {
        "heading": "ALTER VIEW",
        "description": "\nModifies the properties for an existing view. Currently the only supported operations are:\nRenaming a view.\nConverting to (or reverting from) a secure view.\nAdding, overwriting, removing a comment for a view.\nRenaming a view.\nConverting to (or reverting from) a secure view.\nAdding, overwriting, removing a comment for a view.\nNote that you cannot use this command to change the definition for a view. To change the view definition, you must drop the view and\nthen recreate it.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE VIEW , DROP VIEW , SHOW VIEWS , DESCRIBE VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nWhere:",
        "syntax": [
          "ALTER VIEW [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER VIEW [ IF EXISTS ] <name> SET\n  [ SECURE ]\n  [ CHANGE_TRACKING =  { TRUE | FALSE } ]\n  [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER VIEW [ IF EXISTS ] <name> UNSET\n  [ SECURE ]\n  [ CONTACT <purpose> ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER VIEW <name> dataMetricFunctionAction\n\nALTER VIEW [ IF EXISTS ] <name> dataGovnPolicyTagAction",
          "dataMetricFunctionAction ::=\n\n    SET DATA_METRIC_SCHEDULE = {\n        '<num> MINUTE'\n      | 'USING CRON <expr> <time_zone>'\n      | 'TRIGGER_ON_CHANGES'\n    }\n\n  | UNSET DATA_METRIC_SCHEDULE\n\n  | { ADD | DROP } DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] )\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) ]\n\n  | MODIFY DATA METRIC FUNCTION <metric_name>\n      ON ( <col_name> [ , ... ]\n      [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME }\n      [ , <metric_name_2> ON ( <col_name> [ , ... ]\n        [ , TABLE <table_name>( <col_name> [ , ... ] ) ] ) { SUSPEND | RESUME } ]",
          "dataGovnPolicyTagAction ::=\n  {\n      SET TAG <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n    | UNSET TAG <tag_name> [ , <tag_name> ... ]\n  }\n  |\n  {\n      ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ROW ACCESS POLICY <policy_name>\n    | DROP ROW ACCESS POLICY <policy_name> ,\n        ADD ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , ... ] )\n    | DROP ALL ROW ACCESS POLICIES\n  }\n  |\n  {\n      SET AGGREGATION POLICY <policy_name>\n        [ ENTITY KEY ( <col_name> [, ... ] ) ]\n        [ FORCE ]\n    | UNSET AGGREGATION POLICY\n  }\n  |\n  {\n      SET JOIN POLICY <policy_name>\n        [ FORCE ]\n    | UNSET JOIN POLICY\n  }\n  |\n  ADD [ COLUMN ] [ IF NOT EXISTS ] <col_name> <col_type>\n    [ [ WITH ] MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] ]\n    [ [ WITH ] PROJECTION POLICY <policy_name> ]\n    [ [ WITH ] TAG ( <tag_name> = '<tag_value>'\n          [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET MASKING POLICY <policy_name>\n          [ USING ( <col1_name> , <cond_col_1> , ... ) ] [ FORCE ]\n      | UNSET MASKING POLICY\n  }\n  |\n  {\n    { ALTER | MODIFY } [ COLUMN ] <col1_name>\n        SET PROJECTION POLICY <policy_name>\n          [ FORCE ]\n      | UNSET PROJECTION POLICY\n  }\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> SET TAG\n      <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n      , [ COLUMN ] <col2_name> SET TAG\n          <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' ... ]\n  |\n  { ALTER | MODIFY } [ COLUMN ] <col1_name> UNSET TAG <tag_name> [ , <tag_name> ... ]\n                   , [ COLUMN ] <col2_name> UNSET TAG <tag_name> [ , <tag_name> ... ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the view to alter. If the identifier contains spaces or special characters, the entire string must be enclosed\nin double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          },
          {
            "term": "RENAME TO new_name",
            "definition": "Specifies the new identifier for the view; must be unique for the schema. For more details, see Identifier requirements. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
          },
          {
            "term": "SET ...",
            "definition": "Specifies the property to set for the view: Specifies a view as secure. Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the view, and cascades the setting to all underlying tables. FALSE disables change tracking on the view, and cascades the setting to all underlying tables. Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts. Adds a comment or overwrites an existing comment for the view. Note You must set each view property individually."
          },
          {
            "term": "SECURE",
            "definition": "Specifies a view as secure."
          },
          {
            "term": "CHANGE_TRACKING =  TRUE | FALSE",
            "definition": "Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the view, and cascades the setting to all underlying tables. FALSE disables change tracking on the view, and cascades the setting to all underlying tables."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites an existing comment for the view."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies the property to unset for the view, which resets it to the default: SECURE CONTACT purpose COMMENT When resetting a property, specify only the name; specifying a value for the property will return an error. Note You must reset each view property individually."
          },
          {
            "term": "SECURE",
            "definition": "Specifies a view as secure."
          },
          {
            "term": "CHANGE_TRACKING =  TRUE | FALSE",
            "definition": "Specifies to enable or disable change tracking on the table. TRUE enables change tracking on the view, and cascades the setting to all underlying tables. FALSE disables change tracking on the view, and cascades the setting to all underlying tables."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites an existing comment for the view."
          }
        ]
      },
      {
        "heading": "Data metric function actions (dataMetricFunctionAction)",
        "description": "\nFor details about the access control requirements for these actions, see DMF privileges.",
        "syntax": [
          "# __________ minute (0-59)\n# | ________ hour (0-23)\n# | | ______ day of month (1-31, or L)\n# | | | ____ month (1-12, JAN-DEC)\n# | | | | _ day of week (0-6, SUN-SAT, or L)\n# | | | | |\n# | | | | |\n  * * * * *"
        ],
        "definitions": [
          {
            "term": "DATA_METRIC_SCHEDULE ...",
            "definition": "Specifies the schedule to run the data metric function periodically. Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440. Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron utility\nsyntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th day\nof the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. If a data metric function is resumed during the minute defined in its cron expression, the first scheduled run of the data metric\nfunction is the next occurrence of the instance of the cron expression. For example, if data metric function scheduled to run daily\nat midnight (USING CRON 0 0 * * *) is resumed at midnight plus 5 seconds (00:00:05), the first data metric function run\nis scheduled for the following midnight. Specifies that the DMF runs when a DML operation modifies the table, such as inserting a new row or\ndeleting a row. You can specify 'TRIGGER_ON_CHANGES' for the following objects: Dynamic tables External tables Apache Iceberg tables Regular tables Temporary tables Transient tables You cannot specify 'TRIGGER_ON_CHANGES' for views. Changes to the table as a result of reclustering do not trigger the DMF to run."
          },
          {
            "term": "'num MINUTE'",
            "definition": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440."
          },
          {
            "term": "'USING CRON expr time_zone'",
            "definition": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron utility\nsyntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th day\nof the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. If a data metric function is resumed during the minute defined in its cron expression, the first scheduled run of the data metric\nfunction is the next occurrence of the instance of the cron expression. For example, if data metric function scheduled to run daily\nat midnight (USING CRON 0 0 * * *) is resumed at midnight plus 5 seconds (00:00:05), the first data metric function run\nis scheduled for the following midnight."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "'TRIGGER_ON_CHANGES'",
            "definition": "Specifies that the DMF runs when a DML operation modifies the table, such as inserting a new row or\ndeleting a row. You can specify 'TRIGGER_ON_CHANGES' for the following objects: Dynamic tables External tables Apache Iceberg tables Regular tables Temporary tables Transient tables You cannot specify 'TRIGGER_ON_CHANGES' for views. Changes to the table as a result of reclustering do not trigger the DMF to run."
          },
          {
            "term": "{ ADD | DROP } DATA METRIC FUNCTION metric_name",
            "definition": "Identifier of the data metric function to add to the table or view or drop from the table or view. The table or view columns on which to associate the data metric function. The data types of the columns must match the data types of\nthe columns specified in the data metric function definition. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns. Additional data metric functions to add to the table or view. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] )",
            "definition": "The table or view columns on which to associate the data metric function. The data types of the columns must match the data types of\nthe columns specified in the data metric function definition. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) ]",
            "definition": "Additional data metric functions to add to the table or view. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "MODIFY DATA METRIC FUNCTION metric_name",
            "definition": "Identifier of the data metric function to modify. Suspends or resumes the data metric function on the specified columns. When a data metric function is set for a table or view, the data\nmetric function is automatically included in the schedule. If the data metric function accepts a second table as an argument, specify\nthe fully qualified name of the table and its columns. SUSPEND removes the data metric function from the schedule. RESUME brings a suspended date metric function back into the schedule. Additional data metric functions to suspend or resume. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its\ncolumns."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) { SUSPEND | RESUME }",
            "definition": "Suspends or resumes the data metric function on the specified columns. When a data metric function is set for a table or view, the data\nmetric function is automatically included in the schedule. If the data metric function accepts a second table as an argument, specify\nthe fully qualified name of the table and its columns. SUSPEND removes the data metric function from the schedule. RESUME brings a suspended date metric function back into the schedule."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE(col_name [ , ... ] ) ] ) { SUSPEND | RESUME } ]",
            "definition": "Additional data metric functions to suspend or resume. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its\ncolumns."
          },
          {
            "term": "'num MINUTE'",
            "definition": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440."
          },
          {
            "term": "'USING CRON expr time_zone'",
            "definition": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron utility\nsyntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th day\nof the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes. If a data metric function is resumed during the minute defined in its cron expression, the first scheduled run of the data metric\nfunction is the next occurrence of the instance of the cron expression. For example, if data metric function scheduled to run daily\nat midnight (USING CRON 0 0 * * *) is resumed at midnight plus 5 seconds (00:00:05), the first data metric function run\nis scheduled for the following midnight."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "'TRIGGER_ON_CHANGES'",
            "definition": "Specifies that the DMF runs when a DML operation modifies the table, such as inserting a new row or\ndeleting a row. You can specify 'TRIGGER_ON_CHANGES' for the following objects: Dynamic tables External tables Apache Iceberg tables Regular tables Temporary tables Transient tables You cannot specify 'TRIGGER_ON_CHANGES' for views. Changes to the table as a result of reclustering do not trigger the DMF to run."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] )",
            "definition": "The table or view columns on which to associate the data metric function. The data types of the columns must match the data types of\nthe columns specified in the data metric function definition. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) ]",
            "definition": "Additional data metric functions to add to the table or view. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its columns."
          },
          {
            "term": "ON ( col_name [ , ... ] [ , TABLE( table_name( col_name [ , ... ] ) ) ] ) { SUSPEND | RESUME }",
            "definition": "Suspends or resumes the data metric function on the specified columns. When a data metric function is set for a table or view, the data\nmetric function is automatically included in the schedule. If the data metric function accepts a second table as an argument, specify\nthe fully qualified name of the table and its columns. SUSPEND removes the data metric function from the schedule. RESUME brings a suspended date metric function back into the schedule."
          },
          {
            "term": "[ , metric_name_2 ON ( col_name [ , ... ] [ , TABLE(col_name [ , ... ] ) ] ) { SUSPEND | RESUME } ]",
            "definition": "Additional data metric functions to suspend or resume. Use a comma to separate each data metric function and its specified\ncolumns. If the data metric function accepts a second table as an argument, specify the fully qualified name of the table and its\ncolumns."
          }
        ]
      },
      {
        "heading": "Data Governance policy and tag actions (dataGovnPolicyTagAction)",
        "description": "\nThe following clauses apply to all table kinds that support row access policies, such as but not limited to tables, views, and event tables.\nTo simplify, the clauses just refer to table.",
        "definitions": [
          {
            "term": "TAG tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ]",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "policy_name",
            "definition": "Identifier for the policy; must be unique for your schema."
          },
          {
            "term": "ADD ROW ACCESS POLICY policy_name ON (col_name [ , ... ])",
            "definition": "Adds a row access policy to the table. At least one column name must be specified. Additional columns can be specified with a comma separating each column name. Use this\nexpression to add a row access policy to both an event table and an external table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name",
            "definition": "Drops a row access policy from the table. Use this clause to drop the policy from the table."
          },
          {
            "term": "DROP ROW ACCESS POLICY policy_name, ADD ROW ACCESS POLICY policy_name ON ( col_name [ , ... ] )",
            "definition": "Drops the row access policy that is set on the table and adds a row access policy to the same table in a single SQL statement."
          },
          {
            "term": "DROP ALL ROW ACCESS POLICIES",
            "definition": "Drops all row access policy associations from the table. This expression is helpful when a row access policy is dropped from a schema before dropping the policy from an event table. Use this expression to drop row access policy associations from the table."
          },
          {
            "term": "SET AGGREGATION POLICY policy_name",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ ENTITY KEY (col_name [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "UNSET AGGREGATION POLICY",
            "definition": "Detaches an aggregation policy from the table."
          },
          {
            "term": "SET JOIN POLICY policy_name",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "[ FORCE ]",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "UNSET JOIN POLICY",
            "definition": "Detaches a join policy from the table."
          },
          {
            "term": "[ ENTITY KEY (col_name [ , ... ]) ] [ FORCE ]",
            "definition": "Assigns an aggregation policy to the table. Use the optional ENTITY KEY parameter to define which columns uniquely identity an entity within the table. For more information, see\nImplementing entity-level privacy with aggregation policies. Use the optional FORCE parameter to atomically replace an existing aggregation policy with the new aggregation policy."
          },
          {
            "term": "[ FORCE ]",
            "definition": "Assigns a join policy to the table. Use the optional FORCE parameter to atomically replace an existing join policy with the new join policy."
          },
          {
            "term": "{ ALTER | MODIFY } [ COLUMN ] ...",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query\nresult when a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy. Replaces a masking or projection policy that is currently set on a column with a different policy in a single statement. Note that using the FORCE keyword with a masking policy requires the data type of the policy\nin the ALTER TABLE statement (i.e. STRING) to match the data type of the masking policy currently set on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has no effect. For details, see: Replace a masking policy on a column or Replace a projection policy."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query\nresult when a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy."
          },
          {
            "term": "FORCE",
            "definition": "Replaces a masking or projection policy that is currently set on a column with a different policy in a single statement. Note that using the FORCE keyword with a masking policy requires the data type of the policy\nin the ALTER TABLE statement (i.e. STRING) to match the data type of the masking policy currently set on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has no effect. For details, see: Replace a masking policy on a column or Replace a projection policy."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query\nresult when a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy."
          },
          {
            "term": "FORCE",
            "definition": "Replaces a masking or projection policy that is currently set on a column with a different policy in a single statement. Note that using the FORCE keyword with a masking policy requires the data type of the policy\nin the ALTER TABLE statement (i.e. STRING) to match the data type of the masking policy currently set on the column (i.e. STRING). If a masking policy is not currently set on the column, specifying this keyword has no effect. For details, see: Replace a masking policy on a column or Replace a projection policy."
          }
        ]
      },
      {
        "heading": "Usage notes: General",
        "description": "\nMoving a view to a managed access schema (using the ALTER VIEW  RENAME TO syntax) is prohibited unless the view owner (i.e.\nthe role that has the OWNERSHIP privilege on the view) also owns the target schema.\nMoving a view to a managed access schema (using the ALTER VIEW  RENAME TO syntax) is prohibited unless the view owner (i.e.\nthe role that has the OWNERSHIP privilege on the view) also owns the target schema.\nFor masking policies:\n\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\n\nApply a conditional masking policy on a column\nReplace a masking policy on a column\n\n\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\n\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nFor masking policies:\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\n\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nThe USING clause and the FORCE keyword are both optional; neither are required to set a masking policy on a column. The\nUSING clause and the FORCE keyword can be used separately or together. For details, see:\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nApply a conditional masking policy on a column\nReplace a masking policy on a column\nA single masking policy that uses conditional columns can be applied to multiple tables provided that the column structure of the table\nmatches the columns specified in the policy.\nWhen modifying one or more table columns with a masking policy or the table itself with a row access policy, use the\nPOLICY_CONTEXT function to simulate a query on the column(s) protected by a masking policy and the\ntable protected by a row access policy.\nA single masking policy that uses conditional columns can be applied to multiple views provided that the column structure of the view\nmatches the columns specified in the policy.\nA single masking policy that uses conditional columns can be applied to multiple views provided that the column structure of the view\nmatches the columns specified in the policy.\nFor row access policies:\n\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\n\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\n\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nFor row access policies:\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nSnowflake supports adding and dropping row access policies in a single SQL statement.\nFor example, to replace a row access policy that is already set on a table with a different policy, drop the row access policy first\nand then add the new row access policy.\nFor a given resource (i.e. table or view), to ADD or DROP a row access policy you must have either the\nAPPLY ROW ACCESS POLICY privilege on the schema, or the\nOWNERSHIP privilege on the resource and the APPLY privilege on the row access policy resource.\nA table or view can only be protected by one row access policy at a time. Adding a policy fails if the policy body refers to a table or\nview column that is protected by a row access policy or the column protected by a masking policy.\nSimilarly, adding a masking policy to a table column fails if the masking policy body refers to a table that is protected by a row\naccess policy or another masking policy.\nRow access policies cannot be applied to system views or table functions.\nSimilar to other DROP <object> operations, Snowflake returns an error if attempting to drop a row access policy from a\nresource that does not have a row access policy added to it.\nIf an object has both a row access policy and one or more masking policies, the row access policy is evaluated first.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "Usage notes: Data metric functions",
        "definitions": [
          {
            "term": "Add a DMF to a table:",
            "definition": "Prior to adding a data metric function to a table, you must: Set the schedule for the data metric function to run. For details, see\nDATA_METRIC_SCHEDULE. Configure the event table to store the results of calling the data metric function. For details, see\nView results of a data metric function. Ensure that the table is view is not granted to a share because you cannot set a data metric function on a shared table or view. Additionally: You can add a data metric function to a table, external table, view, or materialized view. You cannot set a data metric function on any\nother kind of table, such as a dynamic table. When you specify a column, Snowflake uses the ordinal position. If you rename a column after adding a data metric function to the table\nor view, the association of the data metric function to the column remains valid. Only one data metric function of its kind can be added to a column. For example, a NULL_COUNT data metric function cannot be added to a\nsingle column twice. If you drop a column after adding a data metric function that references the column, Snowflake cannot evaluate the data metric function. Referencing a virtual column is not supported."
          },
          {
            "term": "Schedule a DMF",
            "definition": "It takes ten minutes for the schedule to become effective once the schedule is set. Similarly, it takes ten minutes once the DMF is unset for the scheduling changes to take effect. For more information, see\nSchedule the DMF to run."
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nRename view view1 to view2:\nConvert a view to a secure view:\nRevert a secure view to a regular view:\nApply a Column-level Security masking policy to a view column:\nUnset a Column-level Security masking policy from a view column:\nThe following example adds a row access policy on a view. After setting policies, you can verify their\nreferenced objects by checking the information schema.\nThe following example drops a row access policy from a view. Verify that the policies were dropped by querying the\ninformation schema.\nThe following example shows how to combine adding and dropping row access policies in a single SQL statement for a view. Verify the\nresults by checking the information schema.\nThe following example sets a join policy on a view:\nOn this page\nSyntax\nParameters\nData metric function actions (dataMetricFunctionAction)\nData Governance policy and tag actions (dataGovnPolicyTagAction)\nUsage notes: General\nUsage notes: Data metric functions\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER VIEW view1 RENAME TO view2;",
          "ALTER VIEW view1 SET SECURE;",
          "ALTER VIEW view1 UNSET SECURE;",
          "-- single column\n\nALTER VIEW user_info_v MODIFY COLUMN ssn_number SET MASKING POLICY ssn_mask_v;\n\n-- multiple columns\n\nALTER VIEW user_info_v MODIFY\n  COLUMN ssn_number SET MASKING POLICY ssn_mask_v,\n  COLUMN dob SET MASKING POLICY dob_mask_v\n  ;",
          "-- single column\n\nALTER VIEW user_info_v MODIFY COLUMN ssn_number UNSET MASKING POLICY;\n\n-- multiple columns\n\nALTER VIEW user_info_v MODIFY\n  COLUMN ssn_number UNSET MASKING POLICY,\n  COLUMN dob UNSET MASKING POLICY\n  ;",
          "ALTER VIEW v1\n  ADD ROW ACCESS POLICY rap_v1 ON (empl_id);",
          "ALTER VIEW v1\n  DROP ROW ACCESS POLICY rap_v1;",
          "ALTER VIEW v1\n  DROP ROW ACCESS POLICY rap_v1_version_1,\n  ADD ROW ACCESS POLICY rap_v1_version_2 ON (empl_id);",
          "ALTER VIEW join_view\n  SET JOIN POLICY jp1;"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-view",
    "details": [
      {
        "heading": "DESCRIBE VIEW",
        "description": "\nDescribes the columns in a view (or table).\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DROP VIEW , ALTER VIEW , CREATE VIEW , SHOW VIEWS DESCRIBE TABLE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] VIEW <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the view to describe. If the identifier contains spaces or special characters, the entire string must be\nenclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command output does not include the view definition. Instead, use SHOW VIEWS.\nDESC VIEW and DESCRIBE TABLE are interchangeable. Either command retrieves the details for the table or view that matches the criteria\nin the statement.\nThe output returns a POLICY NAME column to indicate the masking policy set on the column.\nIf a masking policy is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nThe command output does not include the view definition. Instead, use SHOW VIEWS.\nDESC VIEW and DESCRIBE TABLE are interchangeable. Either command retrieves the details for the table or view that matches the criteria\nin the statement.\nThe output returns a POLICY NAME column to indicate the masking policy set on the column.\nIf a masking policy is not set on the column or if the Snowflake account is not Enterprise Edition or higher, Snowflake returns\nNULL.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nExample setup:\nDescribe the view:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE VIEW emp_view AS SELECT id \"Employee Number\", lname \"Last Name\", location \"Home Base\" FROM emp;",
          "DESC VIEW emp_view;\n\n+-----------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+-----------------+\n| name            | type         | kind   | null? | default | primary key | unique key | check | expression | comment | policy name |  privacy domain |\n|-----------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+-----------------+\n| Employee Number | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        | NULL            |\n| Last Name       | VARCHAR(50)  | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        | NULL            |\n| Home Base       | VARCHAR(100) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    | NULL        | NULL            |\n+-----------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+-------------+-----------------+"
        ]
      }
    ]
  },
  {
    "category": "DROP VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-view",
    "details": [
      {
        "heading": "DROP VIEW",
        "description": "\nRemoves the specified view from the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE VIEW , ALTER VIEW , SHOW VIEWS , DESCRIBE VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP VIEW [ IF EXISTS ] <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the view to drop. If the identifier contains spaces, special characters, or mixed-case characters, the\nentire string must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive. If the view identifier is not fully-qualified (in the form of db_name.schema_name.table_name or\nschema_name.table_name), the command looks for the view in the current schema for the session."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nDropped views cant be recovered; they must be recreated.\nDropped views cant be recovered; they must be recreated.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Examples",
        "description": "\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP VIEW myview;",
          "------------------------------+\n           status             |\n------------------------------+\n MYVIEW successfully dropped. |\n------------------------------+"
        ]
      }
    ]
  },
  {
    "category": "SHOW VIEWS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-views",
    "details": [
      {
        "heading": "SHOW VIEWS",
        "description": "\nLists the views, including secure views, for which you have access privileges. The command can be used to list views for the\ncurrent/specified database or schema, or across your entire account.\nThe output returns view metadata and properties, ordered lexicographically by database, schema, and view name. This is important to note\nif you wish to filter the results using the provided filters.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER VIEW , CREATE VIEW , DROP VIEW , DESCRIBE VIEW VIEWS view (Information Schema)"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] VIEWS [ LIKE '<pattern>' ]\n                     [ IN { ACCOUNT | DATABASE [ <db_name> ] | [ SCHEMA ] [ <schema_name> ] | APPLICATION <application_name> | APPLICATION PACKAGE <application_package_name> } ]\n                     [ STARTS WITH '<name_string>' ]\n                     [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Optionally returns only a subset of the output columns: created_on name kind database_name schema_name Default: No value (all columns are included in the output)"
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "IN ACCOUNT | [ DATABASE ] db_name | [ SCHEMA ] schema_name | [ APPLICATION ] application_name | [ APPLICATION PACKAGE ] application_package_name",
            "definition": "Optionally specifies the scope of the command, which determines whether the command lists records only for the current/specified\ndatabase or schema, or across your entire account: The APPLICATION and APPLICATION PACKAGE keywords are not required, but they specify the scope for the named Snowflake Native App. The DATABASE or SCHEMA keyword is not required; you can set the scope by specifying only the database or schema name.\nLikewise, the database or schema name is not required if the session currently has a database in use: If DATABASE or SCHEMA is specified without a name and the session does not currently have a database in use, the\nparameter has no effect on the output. If SCHEMA is specified with a name and the session does not currently have a database in use, the schema name must\nbe fully qualified with the database name (e.g. testdb.testschema). Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (i.e. the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (i.e. the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "The timestamp at which the view was created."
              ],
              [
                "name",
                "The name of the view."
              ],
              [
                "reserved",
                "(Reserved for future use.)"
              ],
              [
                "kind",
                "The kind of view, either VIEW or MATERIALIZED_VIEW."
              ],
              [
                "database_name",
                "The name of the database in which the view exists."
              ],
              [
                "schema_name",
                "The name of the schema in which the view exists."
              ],
              [
                "owner",
                "The owner of the view."
              ],
              [
                "comment",
                "Optional comment."
              ],
              [
                "text",
                "The text of the command that created the view (e.g. CREATE VIEW â€¦)."
              ],
              [
                "is_secure",
                "True if the view is a secure view; false otherwise."
              ],
              [
                "is_materialized",
                "True if the view is a materialized view; false otherwise."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ],
              [
                "change_tracking",
                "Either ON or OFF. ON indicates enabled, and you query the change tracking data using streams or the CHANGES clause for SELECT statements. OFF indicates disabled, but you can optionally enable change tracking as needed."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nBy design, the command output includes secure views, but does not provide certain information about these views unless you are using\nthe role that has ownership of the view. To view details for secure views, you must use the role that owns the view or use the\nVIEWS view in the Information Schema.\nBy design, the command output includes secure views, but does not provide certain information about these views unless you are using\nthe role that has ownership of the view. To view details for secure views, you must use the role that owns the view or use the\nVIEWS view in the Information Schema.\nThe output of this command might include objects with names like SN_TEMP_OBJECT_<n> (where <n> is a number). These are\ntemporary objects that are created by the Snowpark library on behalf of the user.\nThe output of this command might include objects with names like SN_TEMP_OBJECT_<n> (where <n> is a number). These are\ntemporary objects that are created by the Snowpark library on behalf of the user.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema.\nThe value for LIMIT rows cant exceed 10000. If LIMIT rows is omitted, the command results in an error\nif the result set is larger than ten thousand rows.\nTo view results for which more than ten thousand records exist, either include LIMIT rows or query the corresponding\nview in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nShow all views whose names start with line that you have privileges to see in the mydb.public schema:\nOn this page\nSyntax\nParameters\nOutput\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW VIEWS LIKE 'line%' IN mydb.public;\n\n+-------------------------------+---------+----------+---------------+-------------+----------+---------+-------------------------------------------------------+-----------+-----------------+-----------------+-----------------+\n| created_on                    | name    | reserved | database_name | schema_name | owner    | comment | text                                                  | is_secure | is_materialized | change_tracking | owner_role_type |\n+-------------------------------+---------+----------+---------------+-------------+----------+---------+-------------------------------------------------------+-----------+-----------------+-----------------+-----------------+\n| 2019-05-24 18:41:14.247 -0700 | liners1 |          | MYDB          | PUBLIC      | SYSADMIN |         | create materialized views liners1 as select * from t; | false     | false           | on              | ROLE            |\n+-------------------------------+---------+----------+---------------+-------------+----------+---------+-------------------------------------------------------+-----------+-----------------+-----------------+-----------------+"
        ]
      }
    ]
  },
  {
    "category": "CREATE MATERIALIZED VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-materialized-view",
    "details": [
      {
        "heading": "CREATE MATERIALIZED VIEW",
        "description": "\nEnterprise Edition Feature\nMaterialized views require Enterprise Edition. To inquire about upgrading, please contact Snowflake Support.\nCreates a new materialized view in the current/specified schema, based on a query of an existing table, and populates the view with data.\nFor more details, see Working with Materialized Views.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER MATERIALIZED VIEW , DROP MATERIALIZED VIEW , SHOW MATERIALIZED VIEWS , DESCRIBE MATERIALIZED VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CREATE [ OR REPLACE ] [ SECURE ] MATERIALIZED VIEW [ IF NOT EXISTS ] <name>\n  [ COPY GRANTS ]\n  ( <column_list> )\n  [ <col1> [ WITH ] MASKING POLICY <policy_name> [ USING ( <col1> , <cond_col1> , ... ) ]\n           [ WITH ] PROJECTION POLICY <policy_name>\n           [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ , <col2> [ ... ] ]\n  [ COMMENT = '<string_literal>' ]\n  [ [ WITH ] ROW ACCESS POLICY <policy_name> ON ( <col_name> [ , <col_name> ... ] ) ]\n  [ [ WITH ] AGGREGATION POLICY <policy_name> [ ENTITY KEY ( <col_name> [ , <col_name> ... ] ) ] ]\n  [ [ WITH ] TAG ( <tag_name> = '<tag_value>' [ , <tag_name> = '<tag_value>' , ... ] ) ]\n  [ WITH CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n  [ CLUSTER BY ( <expr1> [, <expr2> ... ] ) ]\n  AS <select_statement>"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the view; must be unique for the schema in which the view is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire\nidentifier string is enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also case-sensitive. For more details, see Identifier requirements."
          },
          {
            "term": "select_statement",
            "definition": "Specifies the query used to create the view. This query serves as the text/definition for the view. This query is displayed in the output\nof SHOW VIEWS and SHOW MATERIALIZED VIEWS. There are limitations on the select_statement. For details, see: Usage notes. Limitations on Creating Materialized Views."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "column_list:",
            "definition": "If you do not want the column names in the view to be the same as the column names of the underlying table, you may include a column list in\nwhich you specify the column names. (You do not need to specify the data types of the columns.) If you include a CLUSTER BY clause for the materialized view, then you\nmust include the column name list."
          },
          {
            "term": "MASKING POLICY = policy_name",
            "definition": "Specifies the masking policy to set on a column."
          },
          {
            "term": "USING ( col_name , cond_col_1 ... )",
            "definition": "Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the\ncolumn to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result\nwhen a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal\nmasking policy."
          },
          {
            "term": "PROJECTION POLICY policy_name",
            "definition": "Specifies the projection policy to set on a column."
          },
          {
            "term": "string_literal",
            "definition": "Specifies a comment for the view. The string literal should be in single quotes. (The string literal should not contain single\nquotes unless they are escaped.) Default: No value."
          },
          {
            "term": "expr#",
            "definition": "Specifies an expression on which to cluster the materialized view. Typically, each expression is the name of a column in the\nmaterialized view. For more information about clustering materialized views, see Materialized Views and Clustering. For more\ninformation about clustering in general, see What is Data Clustering?."
          },
          {
            "term": "SECURE",
            "definition": "Specifies that the view is secure. For more information about secure views, see Working with Secure Views. Default: No value (view is not secure)"
          },
          {
            "term": "COPY GRANTS",
            "definition": "If you are replacing an existing view by using the OR REPLACE clause, then the replacement view retains the access permissions\nfrom the original view. This parameter copies all privileges, except OWNERSHIP, from the existing view to the new view. The\nnew view does not inherit any future grants defined for the object type in the schema. By default, the role that executes\nthe CREATE MATERIALIZED VIEW statement owns the new view. If the parameter is not included in the CREATE VIEW statement, then the new view does not inherit any explicit access privileges\ngranted on the original view but does inherit any future grants defined for the object type in the schema. Note that the operation to copy grants occurs atomically with the CREATE VIEW statement (i.e. within the same transaction). Default: No value (grants are not copied)."
          },
          {
            "term": "ROW ACCESS POLICY policy_name ON ( col_name [ , col_name ... ] )",
            "definition": "Specifies the row access policy to set on the materialized view."
          },
          {
            "term": "AGGREGATION POLICY policy_name",
            "definition": "Specifies the aggregation policy to set on the materialized view."
          },
          {
            "term": "TAG ( tag_name = 'tag_value' [ , tag_name = 'tag_value' , ... ] )",
            "definition": "Specifies the tag name and the tag string value. The tag value is always a string, and the maximum number of characters for the tag value is 256. For information about specifying tags in a statement, see Tag quota for objects."
          },
          {
            "term": "WITH CONTACT ( purpose = contact [ , purpose = contact ...] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the new object with one or more contacts."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nCreating a materialized view requires CREATE MATERIALIZED VIEW privilege on the schema, and SELECT privilege on\nthe base table. For more information about privileges and materialized views, see Privileges on a Materialized Views Schema.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\ndefinition of the view, the function returns the database or schema that contains the view, not the database or schema in\nuse for the session.\nWhen you choose a name for the materialized view, note that a schema cannot contain a table and view with the same name. CREATE\n[ MATERIALIZED ] VIEW produces an error if a table with the same name already exists in the schema.\nWhen specifying the select_statement, note the following:\n\nYou cannot specify a HAVING clause or an ORDER BY clause.\nIf you include a CLUSTER BY clause for the materialized view, you must include the column_list clause.\nIf you refer to the base table more than once in the select_statement, use the same\nqualifier for all references for the base table.\nFor example, dont use a mix of base_table, schema.base_table, and database.schema.base_table in the\nsame select_statement. Instead, choose one of these forms (e.g. database.schema.base_table), and use this\nconsistently throughout the select_statement.\n\nDo not query stream objects in the SELECT statement. Streams are not designed to serve as source objects for views or materialized\nviews.\nYou cannot specify a HAVING clause or an ORDER BY clause.\nIf you include a CLUSTER BY clause for the materialized view, you must include the column_list clause.\nIf you refer to the base table more than once in the select_statement, use the same\nqualifier for all references for the base table.\nFor example, dont use a mix of base_table, schema.base_table, and database.schema.base_table in the\nsame select_statement. Instead, choose one of these forms (e.g. database.schema.base_table), and use this\nconsistently throughout the select_statement.\nDo not query stream objects in the SELECT statement. Streams are not designed to serve as source objects for views or materialized\nviews.\nSome column names are not allowed in materialized views. If a column name is not allowed, you can define an alias for the\ncolumn. For details, see Handling Column Names That Are Not Allowed in Materialized Views.\nIf the materialized view queries external tables, you must refresh the file-level metadata\nfor the external tables to reflect changes in the referenced cloud storage location, including\nnew, updated, and removed files.\nYou can refresh the metadata for an external table\nautomatically using the event notification service\nfor your cloud storage service or manually using\nALTER EXTERNAL TABLE  REFRESH statements.\nMaterialized views have a number of other restrictions. For details, see\nLimitations on Creating Materialized Views and Limitations on Working With Materialized Views.\nView definitions are not updated if the schema of the underlying source table is changed so that the view definition becomes\ninvalid. For example:\n\nA view is created from a base table, and a column is subsequently dropped from that base table.\nThe base table for the materialized view is dropped.\n\nIn these scenarios, querying the view returns an error that includes the reason why the view was invalidated. For example:\nFailure during expansion of view 'MV1':\n  SQL compilation error: Materialized View MV1 is invalid.\n  Invalidation reason: DDL Statement was executed on the base table 'MY_INVENTORY'.\n  Marked Materialized View as invalid.\n\n\nWhen this occurs, you can do the following:\n\nIf the base table has been dropped and this is within the\ndata retention period for Time Travel, you can\nundrop the base table to make the materialized view valid again.\nUse the CREATE OR REPLACE MATERIALIZED VIEW command to recreate the view.\nA view is created from a base table, and a column is subsequently dropped from that base table.\nThe base table for the materialized view is dropped.\nIf the base table has been dropped and this is within the\ndata retention period for Time Travel, you can\nundrop the base table to make the materialized view valid again.\nUse the CREATE OR REPLACE MATERIALIZED VIEW command to recreate the view.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nUsing OR REPLACE is the equivalent of using DROP MATERIALIZED VIEW on the existing materialized view and then creating a\nnew view with the same name.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE MATERIALIZED VIEW operation use either the old or new materialized\nview version.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nWhen creating a materialized view with a masking policy on one or more materialized view columns, or a row access policy added to the\nmaterialized view, use the POLICY_CONTEXT function to simulate a query on the column(s) protected by a\nmasking policy and the materialized view protected by a row access policy.\nCreating a materialized view requires CREATE MATERIALIZED VIEW privilege on the schema, and SELECT privilege on\nthe base table. For more information about privileges and materialized views, see Privileges on a Materialized Views Schema.\nIf you specify the CURRENT_DATABASE or CURRENT_SCHEMA function in the\ndefinition of the view, the function returns the database or schema that contains the view, not the database or schema in\nuse for the session.\nWhen you choose a name for the materialized view, note that a schema cannot contain a table and view with the same name. CREATE\n[ MATERIALIZED ] VIEW produces an error if a table with the same name already exists in the schema.\nWhen specifying the select_statement, note the following:\nYou cannot specify a HAVING clause or an ORDER BY clause.\nIf you include a CLUSTER BY clause for the materialized view, you must include the column_list clause.\nIf you refer to the base table more than once in the select_statement, use the same\nqualifier for all references for the base table.\nFor example, dont use a mix of base_table, schema.base_table, and database.schema.base_table in the\nsame select_statement. Instead, choose one of these forms (e.g. database.schema.base_table), and use this\nconsistently throughout the select_statement.\nDo not query stream objects in the SELECT statement. Streams are not designed to serve as source objects for views or materialized\nviews.\nYou cannot specify a HAVING clause or an ORDER BY clause.\nIf you include a CLUSTER BY clause for the materialized view, you must include the column_list clause.\nIf you refer to the base table more than once in the select_statement, use the same\nqualifier for all references for the base table.\nFor example, dont use a mix of base_table, schema.base_table, and database.schema.base_table in the\nsame select_statement. Instead, choose one of these forms (e.g. database.schema.base_table), and use this\nconsistently throughout the select_statement.\nDo not query stream objects in the SELECT statement. Streams are not designed to serve as source objects for views or materialized\nviews.\nSome column names are not allowed in materialized views. If a column name is not allowed, you can define an alias for the\ncolumn. For details, see Handling Column Names That Are Not Allowed in Materialized Views.\nIf the materialized view queries external tables, you must refresh the file-level metadata\nfor the external tables to reflect changes in the referenced cloud storage location, including\nnew, updated, and removed files.\nYou can refresh the metadata for an external table\nautomatically using the event notification service\nfor your cloud storage service or manually using\nALTER EXTERNAL TABLE  REFRESH statements.\nMaterialized views have a number of other restrictions. For details, see\nLimitations on Creating Materialized Views and Limitations on Working With Materialized Views.\nView definitions are not updated if the schema of the underlying source table is changed so that the view definition becomes\ninvalid. For example:\nA view is created from a base table, and a column is subsequently dropped from that base table.\nThe base table for the materialized view is dropped.\nA view is created from a base table, and a column is subsequently dropped from that base table.\nThe base table for the materialized view is dropped.\nIn these scenarios, querying the view returns an error that includes the reason why the view was invalidated. For example:\nWhen this occurs, you can do the following:\nIf the base table has been dropped and this is within the\ndata retention period for Time Travel, you can\nundrop the base table to make the materialized view valid again.\nUse the CREATE OR REPLACE MATERIALIZED VIEW command to recreate the view.\nIf the base table has been dropped and this is within the\ndata retention period for Time Travel, you can\nundrop the base table to make the materialized view valid again.\nUse the CREATE OR REPLACE MATERIALIZED VIEW command to recreate the view.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nUsing OR REPLACE is the equivalent of using DROP MATERIALIZED VIEW on the existing materialized view and then creating a\nnew view with the same name.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThis means that any queries concurrent with the CREATE OR REPLACE MATERIALIZED VIEW operation use either the old or new materialized\nview version.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nWhen creating a materialized view with a masking policy on one or more materialized view columns, or a row access policy added to the\nmaterialized view, use the POLICY_CONTEXT function to simulate a query on the column(s) protected by a\nmasking policy and the materialized view protected by a row access policy.",
        "syntax": [
          "Failure during expansion of view 'MV1':\n  SQL compilation error: Materialized View MV1 is invalid.\n  Invalidation reason: DDL Statement was executed on the base table 'MY_INVENTORY'.\n  Marked Materialized View as invalid."
        ]
      },
      {
        "heading": "Examples",
        "description": "\nCreate a materialized view in the current schema, with a comment, that selects all the rows from a table:\nFor more examples, see the examples in Working with Materialized Views.\nOn this page\nSyntax\nRequired parameters\nOptional parameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE MATERIALIZED VIEW mymv\n    COMMENT='Test view'\n    AS\n    SELECT col1, col2 FROM mytable;"
        ]
      }
    ]
  },
  {
    "category": "ALTER MATERIALIZED VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-materialized-view",
    "details": [
      {
        "heading": "ALTER MATERIALIZED VIEW",
        "description": "\nEnterprise Edition Feature\nMaterialized views require Enterprise Edition. To inquire about upgrading, please contact Snowflake Support.\nAlters a materialized view in the current/specified schema. Supported actions include:\nRenaming the materialized view.\nSuspending and resuming use and maintenance of the materialized view.\nClustering the materialized view.\nSuspending and resuming reclustering of the materialized view.\nDropping clustering of the materialized view.\nRenaming the materialized view.\nSuspending and resuming use and maintenance of the materialized view.\nClustering the materialized view.\nSuspending and resuming reclustering of the materialized view.\nDropping clustering of the materialized view.\nFor more details, see Working with Materialized Views.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE MATERIALIZED VIEW , DROP MATERIALIZED VIEW , SHOW MATERIALIZED VIEWS , DESCRIBE MATERIALIZED VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "ALTER MATERIALIZED VIEW <name>\n  {\n  RENAME TO <new_name>                     |\n  CLUSTER BY ( <expr1> [, <expr2> ... ] )  |\n  DROP CLUSTERING KEY                      |\n  SUSPEND RECLUSTER                        |\n  RESUME RECLUSTER                         |\n  SUSPEND                                  |\n  RESUME                                   |\n  SET {\n    [ SECURE ]\n    [ CONTACT ( <purpose> = <contact_name> [ , <purpose> = <contact_name> ... ] ) ]\n    [ COMMENT = '<comment>' ]\n    }                                      |\n  UNSET {\n    SECURE\n    CONTACT <purpose>                                 |\n    COMMENT\n    }\n  }\n\nALTER MATERIALIZED VIEW\n  SET DATA_METRIC_SCHEDULE = {\n      '<num> MINUTE'\n    | 'USING CRON <expr> <time_zone>'\n  }\n\nALTER MATERIALIZED VIEW UNSET DATA_METRIC_SCHEDULE"
        ]
      },
      {
        "heading": "Parameters",
        "syntax": [
          "Failure during expansion of view 'MV1':\n  SQL compilation error: Materialized View MV1 is invalid.\n  Invalidation reason: Marked Materialized View as invalid manually.",
          "# __________ minute (0-59)\n# | ________ hour (0-23)\n# | | ______ day of month (1-31, or L)\n# | | | ____ month (1-12, JAN-DEC)\n# | | | | _ day of week (0-6, SUN-SAT, or L)\n# | | | | |\n# | | | | |\n  * * * * *"
        ],
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier of the materialized view to alter."
          },
          {
            "term": "RENAME TO new_name",
            "definition": "This option allows you to rename a materialized view. The new identifier must be unique for the schema in which the view is created.\nThe new identifier must start with an alphabetic character and cannot contain spaces or special characters unless the entire identifier string\nis enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also case-sensitive.\nFor more details, see Identifier requirements. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. Note that renaming a materialized view does not update references to that view. For example, if\nyou create a view named V1 on top of a materialized view, and then you rename\nthe materialized view, the definition of view V1 becomes out of date."
          },
          {
            "term": "CLUSTER BY expr#",
            "definition": "This command clusters the materialized view. Clustering\nre-orders the rows in the materialized view to increase performance for queries\nthat filter based on the clustering key expressions. The expr# specifies an expression on which to cluster the materialized view.\nTypically, each expression is the name of a column in the materialized view. For more information about clustering materialized views, see:\nMaterialized Views and Clustering.\nFor more information about clustering in general, see:\nWhat is Data Clustering?."
          },
          {
            "term": "DROP CLUSTERING KEY",
            "definition": "This command drops the clustering of the materialized view."
          },
          {
            "term": "SUSPEND RECLUSTER",
            "definition": "The SUSPEND RECLUSTER option suspends re-clustering of the materialized\nview. For more information about clustering materialized views,\nsee Materialized Views and Clustering."
          },
          {
            "term": "RESUME RECLUSTER",
            "definition": "The RESUME RECLUSTER option resumes reclustering of the materialized\nview."
          },
          {
            "term": "SUSPEND",
            "definition": "The SUSPEND option suspends the maintenance (updates) and use of the\nmaterialized view. While the view is suspended, updates to the base table are\nnot propagated to the materialized view. The materialized view itself is\nalso inaccessible; if you attempt to use it, you get an error message\nsimilar to: If you suspend a clustered materialized view, suspending the view implicitly\nsuspends reclustering of that view."
          },
          {
            "term": "RESUME",
            "definition": "The RESUME option allows you to resume using the materialized view.\nIt also resumes maintenance of the materialized view.\nIf the view is clustered, it also implicitly resumes reclustering of that view."
          },
          {
            "term": "SET ...",
            "definition": "Specifies the property to set for the materialized view: This option turns the view into a secure view. For more information about secure views, see\nWorking with Secure Views. Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts. This option sets a comment for the materialized view. The comment has no effect on the behavior of the view,\nbut can provide useful information to people who use or maintain the view. Specifies the schedule to run the data metric function periodically. Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440. Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
          },
          {
            "term": "SECURE",
            "definition": "This option turns the view into a secure view. For more information about secure views, see\nWorking with Secure Views."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "This option sets a comment for the materialized view. The comment has no effect on the behavior of the view,\nbut can provide useful information to people who use or maintain the view."
          },
          {
            "term": "DATA_METRIC_SCHEDULE ...",
            "definition": "Specifies the schedule to run the data metric function periodically. Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440. Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
          },
          {
            "term": "'num MINUTE'",
            "definition": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440."
          },
          {
            "term": "'USING CRON expr time_zone'",
            "definition": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies the property to unset for the materialized view: SECURE TAG tag_name [ , tag_name ... ] CONTACT purpose COMMENT DATA_METRIC_SCHEDULE"
          },
          {
            "term": "SECURE",
            "definition": "This option turns the view into a secure view. For more information about secure views, see\nWorking with Secure Views."
          },
          {
            "term": "CONTACT ( purpose = contact [ , purpose = contact ... ] )",
            "definition": "Preview Feature  Open Available to all accounts. Associate the existing object with one or more contacts."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "This option sets a comment for the materialized view. The comment has no effect on the behavior of the view,\nbut can provide useful information to people who use or maintain the view."
          },
          {
            "term": "DATA_METRIC_SCHEDULE ...",
            "definition": "Specifies the schedule to run the data metric function periodically. Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440. Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
          },
          {
            "term": "'num MINUTE'",
            "definition": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440."
          },
          {
            "term": "'USING CRON expr time_zone'",
            "definition": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "'num MINUTE'",
            "definition": "Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Accepts positive integers only. Also supports num M syntax. For data metric functions, use one of the following values: 5, 15, 30, 60, 720, or 1440."
          },
          {
            "term": "'USING CRON expr time_zone'",
            "definition": "Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron\nutility syntax. For a list of time zones, see the list of tz database time zones. The cron expression consists of the following fields, and the periodic interval must be at least 5 minutes: The following special characters are supported: Wildcard. Specifies any occurrence of the field. Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month. Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run). Note The cron expression currently evaluates against the specified time zone only. Altering the TIMEZONE parameter value\nfor the account (or setting the value at the user or session level) does not change the time zone for the data metric\nfunction. The cron expression defines all valid run times for the data metric function. Snowflake attempts to run a data metric\nfunction based on this schedule; however, any valid run time is skipped if a previous run has not completed before the next valid\nrun time starts. When both a specific day of month and day of week are included in the cron expression, then the data metric function is scheduled\non days satisfying either the day of month or day of week. For example,\nDATA_METRIC_SCHEDULE = 'USING CRON 0 0 10-20 * TUE,THU UTC' schedules a data metric function at 0AM on any 10th to 20th\nday of the month and also on any Tuesday or Thursday outside of those dates. The shortest granularity of time in cron is minutes."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          },
          {
            "term": "*",
            "definition": "Wildcard. Specifies any occurrence of the field."
          },
          {
            "term": "L",
            "definition": "Stands for last. When used in the day-of-week field, it allows you to specify constructs such as the last Friday (5L) of\na given month. In the day-of-month field, it specifies the last day of the month."
          },
          {
            "term": "/{n}",
            "definition": "Indicates the nth instance of a given unit of time. Each quanta of time is computed independently. For example, if 4/3 is\nspecified in the month field, then the data metric function is scheduled for April, July and October (i.e. every 3 months, starting\nwith the 4th month of the year). The same schedule is maintained in subsequent years. That is, the data metric function is\nnot scheduled to run in January (3 months after the October run)."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nUse the ALTER VIEW command to set/unset a masking policy, row access policy, or tag on/from a materialized view.\nYou can use data metric functions (DMFs) with materialized views as follows:\n\nTo set the DATA_METRIC_SCHEDULE parameter on the materialized view, use the ALTER MATERIALIZED VIEW command. For more\ninformation, see Schedule the DMF to run.\nTo add a DMF to a column or drop a DMF from a column in a materialized view, use the ALTER VIEW command.\nTo set the DATA_METRIC_SCHEDULE parameter on the materialized view, use the ALTER MATERIALIZED VIEW command. For more\ninformation, see Schedule the DMF to run.\nTo add a DMF to a column or drop a DMF from a column in a materialized view, use the ALTER VIEW command.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nUse the ALTER VIEW command to set/unset a masking policy, row access policy, or tag on/from a materialized view.\nYou can use data metric functions (DMFs) with materialized views as follows:\nTo set the DATA_METRIC_SCHEDULE parameter on the materialized view, use the ALTER MATERIALIZED VIEW command. For more\ninformation, see Schedule the DMF to run.\nTo add a DMF to a column or drop a DMF from a column in a materialized view, use the ALTER VIEW command.\nTo set the DATA_METRIC_SCHEDULE parameter on the materialized view, use the ALTER MATERIALIZED VIEW command. For more\ninformation, see Schedule the DMF to run.\nTo add a DMF to a column or drop a DMF from a column in a materialized view, use the ALTER VIEW command.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "Examples",
        "description": "\nRename a materialized view:\nCluster a materialized view:\nSuspend clustering of a materialized view, but not use of the view:\nResume clustering of a materialized view:\nSuspend all use and automatic maintenance of the specified materialized view:\nResume all use and automatic maintenance of the specified materialized view:\nStop clustering a materialized view:\nModify the view to be a secure view:\nAdd or replace the comment for a materialized view:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER MATERIALIZED VIEW table1_MV RENAME TO my_mv;",
          "ALTER MATERIALIZED VIEW my_mv CLUSTER BY(i);",
          "ALTER MATERIALIZED VIEW my_mv SUSPEND RECLUSTER;",
          "ALTER MATERIALIZED VIEW my_mv RESUME RECLUSTER;",
          "ALTER MATERIALIZED VIEW my_mv SUSPEND;",
          "ALTER MATERIALIZED VIEW my_mv RESUME;",
          "ALTER MATERIALIZED VIEW my_mv DROP CLUSTERING KEY;",
          "ALTER MATERIALIZED VIEW mv1 SET SECURE;",
          "ALTER MATERIALIZED VIEW mv1 SET COMMENT = 'Sample view';"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE MATERIALIZED VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-materialized-view",
    "details": [
      {
        "heading": "DESCRIBE MATERIALIZED VIEW",
        "description": "\nEnterprise Edition Feature\nMaterialized views require Enterprise Edition. To inquire about upgrading, please contact Snowflake Support.\nDescribes the columns in a materialized view.\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE MATERIALIZED VIEW , DROP MATERIALIZED VIEW , ALTER MATERIALIZED VIEW , SHOW MATERIALIZED VIEWS"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] MATERIALIZED VIEW <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the materialized view to describe. If the identifier contains spaces or special characters, the\nentire string must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command output does not include the view definition. To see the materialized views definition, use SHOW MATERIALIZED VIEWS\nor GET_DDL.\nDESC MATERIALIZED VIEW and DESCRIBE TABLE are interchangeable. Either command retrieves the details for the table\nor view that matches the criteria in the statement.\nThe command output does not include the view definition. To see the materialized views definition, use SHOW MATERIALIZED VIEWS\nor GET_DDL.\nDESC MATERIALIZED VIEW and DESCRIBE TABLE are interchangeable. Either command retrieves the details for the table\nor view that matches the criteria in the statement.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nExample setup:\nDescribe the materialized view:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE MATERIALIZED VIEW emp_view\n    AS\n    SELECT id \"Employee Number\", lname \"Last Name\", location \"Home Base\" FROM emp;",
          "DESC MATERIALIZED VIEW emp_view;",
          "+-----------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+\n| name            | type         | kind   | null? | default | primary key | unique key | check | expression | comment |\n|-----------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------|\n| Employee Number | NUMBER(38,0) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| Last Name       | VARCHAR(50)  | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n| Home Base       | VARCHAR(100) | COLUMN | Y     | NULL    | N           | N          | NULL  | NULL       | NULL    |\n+-----------------+--------------+--------+-------+---------+-------------+------------+-------+------------+---------+"
        ]
      }
    ]
  },
  {
    "category": "DROP MATERIALIZED VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-materialized-view",
    "details": [
      {
        "heading": "DROP MATERIALIZED VIEW",
        "description": "\nEnterprise Edition Feature\nMaterialized views require Enterprise Edition. To inquire about upgrading, please contact Snowflake Support.\nRemoves the specified materialized view from the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER MATERIALIZED VIEW , CREATE MATERIALIZED VIEW , SHOW MATERIALIZED VIEWS , DESCRIBE MATERIALIZED VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP MATERIALIZED VIEW [ IF EXISTS ] <view_name>"
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nDropping a materialized view does not update references to that view. For example, if you create a view named V1 on top of a\nmaterialized view, and then you drop the materialized view, the definition of view V1 will become out of date.\nDropped materialized views cant be recovered; they must be recreated.\nDropping a materialized view does not update references to that view. For example, if you create a view named V1 on top of a\nmaterialized view, and then you drop the materialized view, the definition of view V1 will become out of date.\nDropped materialized views cant be recovered; they must be recreated.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Examples",
        "description": "\nOn this page\nSyntax\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP MATERIALIZED VIEW mv1;\n\n---------------------------+\n           status          |\n---------------------------+\n MV1 successfully dropped. |\n---------------------------+"
        ]
      }
    ]
  },
  {
    "category": "SHOW MATERIALIZED VIEWS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-materialized-views",
    "details": [
      {
        "heading": "SHOW MATERIALIZED VIEWS",
        "description": "\nEnterprise Edition Feature\nMaterialized views require Enterprise Edition. To inquire about upgrading, please contact Snowflake Support.\nLists the materialized views that you have privileges to access.\nFor more information about materialized views, see Working with Materialized Views.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE MATERIALIZED VIEW ,  ALTER MATERIALIZED VIEW ,  DROP MATERIALIZED VIEW , DESCRIBE MATERIALIZED VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW MATERIALIZED VIEWS [ LIKE '<pattern>' ]\n                        [ IN\n                             {\n                               ACCOUNT                                         |\n\n                               DATABASE                                        |\n                               DATABASE <database_name>                        |\n\n                               SCHEMA                                          |\n                               SCHEMA <schema_name>                            |\n                               <schema_name>\n\n                               APPLICATION <application_name>                  |\n                               APPLICATION PACKAGE <application_package_name>  |\n                             }\n                        ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe output columns are similar to the output columns for SHOW TABLES, but includes the following additional columns:\n\nrefreshed_on: time of the last DML operation on the base table that was processed by a\nrefresh operation.\ncompacted_on: time of the last DML operation on the base table that was processed by a\ncompaction operation.\nbehind_by: If the background process that updates the materialized view\nwith changes from the base table has not yet brought the materialized view\nup to date, then this column shows approximately how many seconds the\nmaterialized view is behind the base table. Note that even if this shows\nthat the materialized view is not up to date, any queries on the\nmaterialized view will still return up-to-date results (they just might\ntake a little longer as extra information is retrieved from the base table).\nrefreshed_on: time of the last DML operation on the base table that was processed by a\nrefresh operation.\ncompacted_on: time of the last DML operation on the base table that was processed by a\ncompaction operation.\nbehind_by: If the background process that updates the materialized view\nwith changes from the base table has not yet brought the materialized view\nup to date, then this column shows approximately how many seconds the\nmaterialized view is behind the base table. Note that even if this shows\nthat the materialized view is not up to date, any queries on the\nmaterialized view will still return up-to-date results (they just might\ntake a little longer as extra information is retrieved from the base table).\nThe command SHOW VIEWS also shows information about materialized views.\nThe output columns are similar to the output columns for SHOW TABLES, but includes the following additional columns:\nrefreshed_on: time of the last DML operation on the base table that was processed by a\nrefresh operation.\ncompacted_on: time of the last DML operation on the base table that was processed by a\ncompaction operation.\nbehind_by: If the background process that updates the materialized view\nwith changes from the base table has not yet brought the materialized view\nup to date, then this column shows approximately how many seconds the\nmaterialized view is behind the base table. Note that even if this shows\nthat the materialized view is not up to date, any queries on the\nmaterialized view will still return up-to-date results (they just might\ntake a little longer as extra information is retrieved from the base table).\nrefreshed_on: time of the last DML operation on the base table that was processed by a\nrefresh operation.\ncompacted_on: time of the last DML operation on the base table that was processed by a\ncompaction operation.\nbehind_by: If the background process that updates the materialized view\nwith changes from the base table has not yet brought the materialized view\nup to date, then this column shows approximately how many seconds the\nmaterialized view is behind the base table. Note that even if this shows\nthat the materialized view is not up to date, any queries on the\nmaterialized view will still return up-to-date results (they just might\ntake a little longer as extra information is retrieved from the base table).\nThe command SHOW VIEWS also shows information about materialized views.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema."
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "The timestamp at which the materialized view was created."
              ],
              [
                "name",
                "The name of the materialized view."
              ],
              [
                "reserved",
                "Reserved for future use."
              ],
              [
                "database_name",
                "The name of the database in which the materialized view exists."
              ],
              [
                "schema_name",
                "The name of the schema in which the materialized view exists."
              ],
              [
                "cluster_by",
                "Information about the clustering columns (if the materialized view is clustered)."
              ],
              [
                "rows",
                "The number of rows in the materialized view."
              ],
              [
                "bytes",
                "The number of bytes of data in the materialized view."
              ],
              [
                "source_database_name",
                "The name of the database in which the materialized viewâ€™s base table exists."
              ],
              [
                "source_schema_name",
                "The name of the schema in which the materialized viewâ€™s base table exists."
              ],
              [
                "source_table_name",
                "The name of the materialized viewâ€™s base table."
              ],
              [
                "refreshed_on",
                "The timestamp of the last DML operation on the base table that was processed by a â€œrefreshâ€ operation."
              ],
              [
                "compacted_on",
                "The timestamp of the last DML operation on the base table that was processed by a â€œcompactionâ€ operation."
              ],
              [
                "owner",
                "The owner of the materialized view."
              ],
              [
                "invalid",
                "True if the materialized view is currently invalid (for example, if the base table dropped a column that the view used); false otherwise."
              ],
              [
                "invalid_reason",
                "The reason (if any) that the materialized view is currently invalid."
              ],
              [
                "behind_by",
                "How far the updates of the materialized view are behind the updates of the base table."
              ],
              [
                "comment",
                "Optional comment."
              ],
              [
                "text",
                "The text of the command that created this materialized view (e.g. CREATE MATERIALIZED VIEW â€¦)."
              ],
              [
                "is_secure",
                "True if the materialized view is a secure view; false otherwise."
              ],
              [
                "automatic_clustering",
                "True if the view is clustered and the clustering is automatic."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nShow all materialized views:\nShow only materialized views with names matching the specified regular expression:\nOn this page\nSyntax\nParameters\nUsage notes\nOutput\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW MATERIALIZED VIEWS;",
          "SHOW MATERIALIZED VIEWS LIKE 'mv1%';\n\n+-------------------------------+------+----------+---------------+-------------+------------+------+-------+----------------------+--------------------+-------------------+-------------------------------+--------------+----------+---------+----------------+-----------+---------+--------------------------------------------+-----------+----------------------+-----------------+\n| created_on                    | name | reserved | database_name | schema_name | cluster_by | rows | bytes | source_database_name | source_schema_name | source_table_name | refreshed_on                  | compacted_on | owner    | invalid | invalid_reason | behind_by | comment | text                                       | is_secure | automatic_clustering | owner_role_type |\n|-------------------------------+------+----------+---------------+-------------+------------+------+-------+----------------------+--------------------+-------------------+-------------------------------+--------------+----------+---------+----------------+-----------+---------+--------------------------------------------+-----------|----------------------+-----------------|\n| 2018-10-05 17:13:17.579 -0700 | MV1  |          | TEST_DB1      | PUBLIC      |            |    0 |     0 | TEST_DB1             | PUBLIC             | INVENTORY         | 2018-10-05 17:13:50.373 -0700 | NULL         | SYSADMIN | false   | NULL           | 0s        |         | CREATE OR REPLACE MATERIALIZED VIEW mv1 AS | false     | OFF                  | ROLE            |\n|                               |      |          |               |             |            |      |       |                      |                    |                   |                               |              |          |         |                |           |         |       SELECT ID, price FROM inventory;     |           |                      |                 |          |\n+-------------------------------+------+----------+---------------+-------------+------------+------+-------+----------------------+--------------------+-------------------+-------------------------------+--------------+----------+---------+----------------+-----------+---------+--------------------------------------------+-----------+----------------------+-----------------+"
        ]
      }
    ]
  },
  {
    "category": "TRUNCATE MATERIALIZED VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/truncate-materialized-view",
    "details": [
      {
        "heading": "TRUNCATE MATERIALIZED VIEW",
        "description": "\nObsolete Feature\nSnowflake no longer supports truncation of materialized views.\nIf you have an existing materialized view that you want to truncate, but not drop, the closest alternative is:\nCREATE OR REPLACE\nMATERIALIZED VIEW <name>\nCOPY GRANTS ...\nThe CREATE OR REPLACE MATERIALIZED VIEW command executes as a foreground statement and requires a warehouse.\nEnterprise Edition Feature\nMaterialized views require Enterprise Edition. To inquire about upgrading, please contact Snowflake Support.\nRemoves all rows from a materialized view, but leaves the view intact (including all privileges and constraints on the materialized view).\nNote that this is different from DROP MATERIALIZED VIEW, which removes the materialized view from the system.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER MATERIALIZED VIEW , CREATE MATERIALIZED VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "TRUNCATE MATERIALIZED VIEW <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the materialized view to truncate. If the identifier contains spaces or special characters, the entire\nstring must be enclosed in double quotes. Identifiers enclosed in double quotes are also case-sensitive (e.g. \"My Object\"). If the materialized view identifier is not fully-qualified (in the form of db_name.schema_name.materialized_view_name\nor schema_name.materialized_view_name), then the command looks for the materialized view in the current schema for the\nsession."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nSnowflake no longer supports truncation of materialized views.\nIf you truncate a materialized view, the background maintenance service automatically updates the materialized view. If\nany queries are executed on the view while it is in the process of being updated, Snowflake ensures consistent results\nby retrieving any rows, as needed, from the base table.\nHowever, the maintenance service uses computing resources to update the materialized view and it is usually more efficient\n(i.e. less costly) to let an out-of-date materialized view catch up naturally over time than to truncate the view. As such,\nwe do not generally recommend truncating a materialized view.\nAlthough each query on the view will still show up-to-date results, the query might run more slowly as Snowflake\nupdates the materialized view or looks up data in the base table.\nSnowflake no longer supports truncation of materialized views.\nIf you truncate a materialized view, the background maintenance service automatically updates the materialized view. If\nany queries are executed on the view while it is in the process of being updated, Snowflake ensures consistent results\nby retrieving any rows, as needed, from the base table.\nHowever, the maintenance service uses computing resources to update the materialized view and it is usually more efficient\n(i.e. less costly) to let an out-of-date materialized view catch up naturally over time than to truncate the view. As such,\nwe do not generally recommend truncating a materialized view.\nAlthough each query on the view will still show up-to-date results, the query might run more slowly as Snowflake\nupdates the materialized view or looks up data in the base table."
      },
      {
        "heading": "Examples",
        "description": "\nThis feature has been obsoleted.\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "CREATE SEMANTIC VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-semantic-view",
    "details": [
      {
        "heading": "CREATE SEMANTIC VIEW",
        "description": "\nCreates a new semantic view in the current/specified schema.\nThe semantic view must comply with these validation rules.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DESCRIBE SEMANTIC VIEW , DROP SEMANTIC VIEW , SHOW SEMANTIC VIEWS"
          }
        ]
      },
      {
        "heading": "Syntax",
        "description": "\nwhere:\nThe parameters for logical tables are:\nlogicalTable ::=\n  [ <table_alias> AS ] <table_name>\n  [ PRIMARY KEY ( <primary_key_column_name> [ , ... ] ) ]\n  [\n    UNIQUE ( <unique_column_name> [ , ... ] )\n    [ ... ]\n  ]\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_table>' ]\n\nCopy\nThe parameters for relationships are:\nrelationshipDef ::=\n  [ <relationship_identifier> AS ]\n  <table_alias> ( <column_name> [ , ... ] )\n  REFERENCES\n  <ref_table_alias> [ ( <ref_column_name> [ , ... ] ) ]\n\nCopy\nThe\nparameters for expressions in the definitions of facts, dimensions, and metrics\nare:\nsemanticExpression ::=\n  <table_alias>.<dim_fact_or_metric> AS <sql_expr>\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_dim_fact_or_metric>' ]\n\nCopy\nThe parameters for logical tables are:\nThe parameters for relationships are:\nThe\nparameters for expressions in the definitions of facts, dimensions, and metrics\nare:\nYou can define a metric that uses a window function (a window function metric) by using the following syntax:\nwindowFunctionMetricDefinition ::=\n  <window_function>( <metric> ) OVER (\n    [ PARTITION BY { <exprs_using_dimensions_or_metrics> | EXCLUDING <dimensions> } ]\n    [ ORDER BY <exprs_using_dimensions_or_metrics> [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ]\n    [ <windowFrameClause> ]\n  )\n\nCopy\nFor information about this syntax, see Parameters for window function metrics.\nYou can define a metric that uses a window function (a window function metric) by using the following syntax:\nFor information about this syntax, see Parameters for window function metrics.\nNote\nThe order of the clauses is important. For example, you must specify the FACTS clause before the DIMENSIONS clause.\nYou can refer to semantic expressions that are defined in later clauses. For example, even if fact_2 is defined after\nfact_1, you can still use fact_2 in the definition of fact_1.",
        "syntax": [
          "CREATE [ OR REPLACE ] SEMANTIC VIEW [ IF NOT EXISTS ] <name>\n  TABLES ( logicalTable [ , ... ] )\n  [ RELATIONSHIPS ( relationshipDef [ , ... ] ) ]\n  [ FACTS ( semanticExpression [ , ... ] ) ]\n  [ DIMENSIONS ( semanticExpression [ , ... ] ) ]\n  [ METRICS ( semanticExpression [ , ... ] ) ]\n  [ COMMENT = '<comment_about_semantic_view>' ]\n  [ COPY GRANTS ]",
          "logicalTable ::=\n  [ <table_alias> AS ] <table_name>\n  [ PRIMARY KEY ( <primary_key_column_name> [ , ... ] ) ]\n  [\n    UNIQUE ( <unique_column_name> [ , ... ] )\n    [ ... ]\n  ]\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_table>' ]",
          "relationshipDef ::=\n  [ <relationship_identifier> AS ]\n  <table_alias> ( <column_name> [ , ... ] )\n  REFERENCES\n  <ref_table_alias> [ ( <ref_column_name> [ , ... ] ) ]",
          "semanticExpression ::=\n  <table_alias>.<dim_fact_or_metric> AS <sql_expr>\n  [ WITH SYNONYMS [ = ] ( '<synonym>' [ , ... ] ) ]\n  [ COMMENT = '<comment_about_dim_fact_or_metric>' ]",
          "windowFunctionMetricDefinition ::=\n  <window_function>( <metric> ) OVER (\n    [ PARTITION BY { <exprs_using_dimensions_or_metrics> | EXCLUDING <dimensions> } ]\n    [ ORDER BY <exprs_using_dimensions_or_metrics> [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ]\n    [ <windowFrameClause> ]\n  )"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the name of the semantic view; the name must be unique for the schema in which the table is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (for example, \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more information, see Identifier requirements."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "COMMENT = 'comment_about_semantic_view'",
            "definition": "Specifies a comment about the semantic view."
          },
          {
            "term": "COPY GRANTS",
            "definition": "When you specify OR REPLACE to replace an existing semantic view with a new semantic view, you can set this parameter to copy\nany privileges granted on the existing semantic view to the new semantic view. The command copies all privilege grants except OWNERSHIP from the existing semantic view to the new semantic view. The\nrole that executes the CREATE SEMANTIC VIEW statement owns the new view. The new semantic view does not inherit any future grants defined for the object type in the schema. The operation to copy grants occurs atomically with the CREATE SEMANTIC VIEW statement (in other words, within the same\ntransaction). If you omit COPY GRANTS, the new semantic view does not inherit any explicit access privileges granted on the existing\nsemantic view but does inherit any future grants defined for the object type in the schema."
          }
        ]
      },
      {
        "heading": "Parameters for logical tables",
        "description": "\nThese parameters are part of the syntax for logical tables:",
        "syntax": [
          "TABLES(\n  ...\n  product_table UNIQUE (service_id)",
          "TABLES(\n  ...\n  product_table UNIQUE (product_area_id, product_id)\n  ...",
          "TABLES(\n  ...\n  product_table UNIQUE (product_area_id, product_id) UNIQUE (service_id)\n  ..."
        ],
        "definitions": [
          {
            "term": "table_alias AS",
            "definition": "Specifies an optional alias for the logical table. If you specify an alias, you must use this alias when referring to the logical table in relationships, facts, dimensions,\nand metrics. If you do not specify an alias, you use the unqualified logical table name to refer to the table."
          },
          {
            "term": "table_name",
            "definition": "Specifies the name of the logical table."
          },
          {
            "term": "PRIMARY KEY ( primary_key_column_name [ , ... ] )",
            "definition": "Specifies the names of one or more columns in the logical table that serve as the primary key of the table."
          },
          {
            "term": "UNIQUE ( unique_column_name [ , ... ] )",
            "definition": "Specifies the name of a column containing a unique value or the names of columns that contain unique combinations of values. For example, if the column service_id contains unique values, specify: If the combination of values in the product_area_id and product_id columns is unique, specify: You can identify multiple columns and multiple combinations of columns as unique in a given logical table: Note If you already identified a column as a primary key column (by using PRIMARY KEY), do not add the UNIQUE clause for that\ncolumn."
          },
          {
            "term": "WITH SYNONYMS [ = ] ( 'synonym' [ , ... ] )",
            "definition": "Specifies one or more synonyms for the logical table. Unlike aliases, synonyms are used for informational purposes only. You do\nnot use synonyms to refer to the logical table in relationships, dimensions, metrics, and facts."
          },
          {
            "term": "COMMENT = 'comment_about_table'",
            "definition": "Specifies a comment about the logical table."
          }
        ]
      },
      {
        "heading": "Parameters for relationships",
        "description": "\nThese parameters are part of the syntax for relationships:",
        "definitions": [
          {
            "term": "relationship_identifier AS",
            "definition": "Specifies an optional identifier for the relationship."
          },
          {
            "term": "table_alias ( column_name [ , ... ] )",
            "definition": "Specifies one of the logical tables and one or more of its columns that refers to columns in another logical table."
          },
          {
            "term": "ref_table_alias [ ( ref_column_name [ , ... ] ) ]",
            "definition": "Specifies the other logical table and one or more of its columns that are referred to by the first logical table. The columns must be identified as a PRIMARY KEY or UNIQUE in the\nlogical table definition."
          }
        ]
      },
      {
        "heading": "Parameters for facts, dimensions, and metrics",
        "description": "\nIn a semantic view, you must define at least one dimension or metric, which means that you must specify at least one DIMENSIONS\nor METRICS clause.\nThese parameters are part of the\nsyntax for defining a fact, dimension, or metric:",
        "definitions": [
          {
            "term": "table_alias.semantic_expression_name AS sql_expr",
            "definition": "Specifies a name for a dimension, fact, or metric and the SQL expression for computing that dimension, fact, or metric. See How Snowflake validates semantic views for the rules for defining a valid semantic view."
          },
          {
            "term": "WITH SYNONYMS [ = ] ( 'synonym' [ , ... ] )",
            "definition": "Specifies one or more optional synonyms for the dimension, fact, or metric. Note that synonyms are used for informational\npurposes only. You cannot use a synonym to refer to a dimension, fact, or metric in another dimension, fact, or metric."
          },
          {
            "term": "COMMENT = 'comment_about_dim_fact_or_metric'",
            "definition": "Specifies an optional comment about the dimension, fact, or metric."
          }
        ]
      },
      {
        "heading": "Parameters for window function metrics",
        "description": "\nThese parameters are part of the\nsyntax for defining window function metrics:\nFor additional information about the parameters for window functions and examples, see\nDefining and querying window function metrics.",
        "syntax": [
          "CREATE SEMANTIC VIEW sv\n  ...\n  METRICS (\n    table_1.metric_2 AS SUM(table_1.metric_1) OVER\n      (PARTITION BY EXCLUDING table_l.dimension_1 ORDER BY table_1.dimension_2)\n  )\n  ...",
          "SELECT * FROM SEMANTIC VIEW(\n  sv\n  METRICS (\n    table_1.metric_2\n  )\n  DIMENSIONS (\n    table_1.dimension_1,\n    table_1.dimension_2,\n    table_1.dimension_3\n  );",
          "SUM(table_1.metric_1) OVER (\n  PARTITION BY table_1.dimension_2, table_1.dimension_3\n  ORDER BY table_1.dimension_2\n)"
        ],
        "definitions": [
          {
            "term": "metric",
            "definition": "Specifies a metric expression for this window function. You can specify a metric or any valid metric expression that you can use\nto define a metric in this entity."
          },
          {
            "term": "PARTITION BY ...",
            "definition": "Groups rows into partitions. You can either partition by a specified set of expressions or by all dimensions (except selected\ndimensions) specified in the query: Groups rows into partitions by SQL expressions. In the SQL expression: Any dimensions in the expression must be accessible from the same entity that defines the window function metric. Any metrics must belong to the same table where this metric is being defined. You cannot specify aggregates, window functions, or subqueries. Groups rows into partitions by all of the dimensions specified in the SEMANTIC_VIEW clause of\nthe query, except for the dimensions specified by dimensions. dimensions must only refer to dimensions that are accessible from the entity that defines the window function\nmetric. For example, suppose that you exclude the dimension table_1.dimension_1 from partitioning: Suppose that you run a query that specifies the dimension table_1.dimension_1: In the query, the metric table_1.metric_2 is evaluated as: Note how table_1.dimension_1 is excluded from the PARTITION BY clause. Note You cannot use EXCLUDING outside of metric definitions in semantic views. EXCLUDING is not supported in window function\ncalls in any other context."
          },
          {
            "term": "PARTITION BY exprs_using_dimensions_or_metrics",
            "definition": "Groups rows into partitions by SQL expressions. In the SQL expression: Any dimensions in the expression must be accessible from the same entity that defines the window function metric. Any metrics must belong to the same table where this metric is being defined. You cannot specify aggregates, window functions, or subqueries."
          },
          {
            "term": "PARTITION BY EXCLUDING dimensions",
            "definition": "Groups rows into partitions by all of the dimensions specified in the SEMANTIC_VIEW clause of\nthe query, except for the dimensions specified by dimensions. dimensions must only refer to dimensions that are accessible from the entity that defines the window function\nmetric. For example, suppose that you exclude the dimension table_1.dimension_1 from partitioning: Suppose that you run a query that specifies the dimension table_1.dimension_1: In the query, the metric table_1.metric_2 is evaluated as: Note how table_1.dimension_1 is excluded from the PARTITION BY clause. Note You cannot use EXCLUDING outside of metric definitions in semantic views. EXCLUDING is not supported in window function\ncalls in any other context."
          },
          {
            "term": "ORDER BY exprs_using_dimensions_or_metrics [ ASC | DESC ] [ NULLS  FIRST | LAST  ] [, ... ]",
            "definition": "Orders rows within each partition. In the SQL expression: Any dimensions in the expression must be accessible from the same entity that defines the window function metric. Any metrics must belong to the same table where this metric is being defined. You cannot specify aggregates, window functions, or subqueries."
          },
          {
            "term": "windowFrameClause",
            "definition": "See Window function syntax and usage."
          },
          {
            "term": "PARTITION BY exprs_using_dimensions_or_metrics",
            "definition": "Groups rows into partitions by SQL expressions. In the SQL expression: Any dimensions in the expression must be accessible from the same entity that defines the window function metric. Any metrics must belong to the same table where this metric is being defined. You cannot specify aggregates, window functions, or subqueries."
          },
          {
            "term": "PARTITION BY EXCLUDING dimensions",
            "definition": "Groups rows into partitions by all of the dimensions specified in the SEMANTIC_VIEW clause of\nthe query, except for the dimensions specified by dimensions. dimensions must only refer to dimensions that are accessible from the entity that defines the window function\nmetric. For example, suppose that you exclude the dimension table_1.dimension_1 from partitioning: Suppose that you run a query that specifies the dimension table_1.dimension_1: In the query, the metric table_1.metric_2 is evaluated as: Note how table_1.dimension_1 is excluded from the PARTITION BY clause. Note You cannot use EXCLUDING outside of metric definitions in semantic views. EXCLUDING is not supported in window function\ncalls in any other context."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE SEMANTIC VIEW",
                "Schema",
                "Required to create a new semantic view."
              ],
              [
                "SELECT",
                "Table, view",
                "Required on any tables and/or views used in the semantic view definition."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe semantic view must be valid and must follow the rules described in\nHow Snowflake validates semantic views.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThe semantic view must be valid and must follow the rules described in\nHow Snowflake validates semantic views.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
      },
      {
        "heading": "Examples",
        "description": "\nSee Creating a semantic view.\nOn this page\nSyntax\nRequired parameters\nOptional parameters\nParameters for logical tables\nParameters for relationships\nParameters for facts, dimensions, and metrics\nParameters for window function metrics\nAccess control requirements\nUsage notes\nExamples\nRelated content\nOverview of semantic views\nUsing SQL commands to create and manage semantic views\nHow Snowflake validates semantic views\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "DESCRIBE SEMANTIC VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-semantic-view",
    "details": [
      {
        "heading": "DESCRIBE SEMANTIC VIEW",
        "description": "\nDescribes the properties of the logical tables, dimensions, facts, and metrics that make up a\nsemantic view.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE SEMANTIC VIEW , DROP SEMANTIC VIEW , SHOW SEMANTIC VIEWS"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "{ DESCRIBE | DESC } SEMANTIC VIEW <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the semantic view to describe. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "object_kind",
                "Type of the object that has the property for this row. The value can be one of the following:\n\nTABLE (the logical tables for the view)\nRELATIONSHIP\nDIMENSION\nFACT\nMETRIC\nNULL (properties that apply to the semantic view itself, such as a comment)"
              ],
              [
                "object_name",
                "Name of the dimension, fact, metric, logical table, or relationship that has the property for this row.\nFor rows that represent properties of the semantic view itself, the value in this column is NULL."
              ],
              [
                "parent_entity",
                "Name of the parent entity of the dimension, fact, metric, or relationship.\nFor rows that represent properties of logical tables or the semantic view itself, the value in this column is NULL."
              ],
              [
                "property",
                "Name of the property of the logical table, relationship, dimension, fact, metric, or semantic view.\nThe value in this column depends on the type of the object (object_kind).\nSee the following sections for details about the properties and their possible values, based on the value in the\nobject_kind column:\n\nFor TABLE, see Properties for logical tables.\nFor RELATIONSHIP, see Properties for relationships.\nFor FACT, DIMENSION, and METRIC, see Properties for facts, dimensions, and metrics.\nFor NULL, see Properties for semantic views."
              ],
              [
                "property_value",
                "Value of the property of the logical table, relationship, dimension, fact, metric, or semantic view."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Properties for logical tables",
        "tables": [
          {
            "headers": [
              "Property name",
              "Description"
            ],
            "rows": [
              [
                "BASE_TABLE_DATABASE_NAME",
                "Name of the database containing the logical table."
              ],
              [
                "BASE_TABLE_SCHEMA_NAME",
                "Name of the schema containing the logical table."
              ],
              [
                "BASE_TABLE_NAME",
                "Name of the logical table."
              ],
              [
                "SYNONYMS",
                "Array of VARCHAR values, representing the synonyms for the logical table."
              ],
              [
                "PRIMARY_KEY",
                "Array of VARCHAR values, specifying the names of the columns that make up the primary key\nfor the logical table."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Properties for relationships",
        "tables": [
          {
            "headers": [
              "Property name",
              "Description"
            ],
            "rows": [
              [
                "TABLE",
                "Name of one of the logical tables in the relationship."
              ],
              [
                "FOREIGN_KEY",
                "Name of the column in that logical table used in the relationship."
              ],
              [
                "REF_TABLE",
                "Name of the other logical table in the relationship."
              ],
              [
                "REF_KEY",
                "Name of the column in the other logical table in the relationship."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Properties for facts, dimensions, and metrics",
        "tables": [
          {
            "headers": [
              "Property name",
              "Description"
            ],
            "rows": [
              [
                "TABLE",
                "Name of the logical table used to define the dimension, fact, or metric."
              ],
              [
                "EXPRESSION",
                "The SQL expression for the dimension, fact, or metric."
              ],
              [
                "DATA_TYPE",
                "The SQL data type of the evaluated SQL expression."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Properties for semantic views",
        "tables": [
          {
            "headers": [
              "Property name",
              "Description"
            ],
            "rows": [
              [
                "COMMENT",
                "Comment about the semantic view."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "Any",
                "Semantic view",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nThe following example describes the semantic view named tpch_rev_analysis:\nOn this page\nSyntax\nParameters\nOutput\nAccess control requirements\nUsage notes\nExamples\nRelated content\nOverview of semantic views\nUsing SQL commands to create and manage semantic views\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESC SEMANTIC VIEW tpch_rev_analysis;",
          "+--------------+------------------------------+---------------+--------------------------+----------------------------------------+\n| object_kind  | object_name                  | parent_entity | property                 | property_value                         |\n|--------------+------------------------------+---------------+--------------------------+----------------------------------------|\n| NULL         | NULL                         | NULL          | COMMENT                  | Comment about the semantic view        |\n| TABLE        | CUSTOMERS                    | NULL          | BASE_TABLE_DATABASE_NAME | SNOWFLAKE_SAMPLE_DATA                  |\n| TABLE        | CUSTOMERS                    | NULL          | BASE_TABLE_SCHEMA_NAME   | TPCH_SF1                               |\n| TABLE        | CUSTOMERS                    | NULL          | BASE_TABLE_NAME          | CUSTOMER                               |\n| TABLE        | CUSTOMERS                    | NULL          | PRIMARY_KEY              | [\"C_CUSTKEY\"]                          |\n| TABLE        | CUSTOMERS                    | NULL          | COMMENT                  | Main table for customer data           |\n| DIMENSION    | CUSTOMER_NAME                | CUSTOMERS     | TABLE                    | CUSTOMERS                              |\n| DIMENSION    | CUSTOMER_NAME                | CUSTOMERS     | EXPRESSION               | customers.c_name                       |\n| DIMENSION    | CUSTOMER_NAME                | CUSTOMERS     | DATA_TYPE                | VARCHAR(25)                            |\n| DIMENSION    | CUSTOMER_NAME                | CUSTOMERS     | SYNONYMS                 | [\"customer name\"]                      |\n| DIMENSION    | CUSTOMER_NAME                | CUSTOMERS     | COMMENT                  | Name of the customer                   |\n| TABLE        | LINE_ITEMS                   | NULL          | BASE_TABLE_DATABASE_NAME | SNOWFLAKE_SAMPLE_DATA                  |\n| TABLE        | LINE_ITEMS                   | NULL          | BASE_TABLE_SCHEMA_NAME   | TPCH_SF1                               |\n| TABLE        | LINE_ITEMS                   | NULL          | BASE_TABLE_NAME          | LINEITEM                               |\n| TABLE        | LINE_ITEMS                   | NULL          | PRIMARY_KEY              | [\"L_ORDERKEY\",\"L_LINENUMBER\"]          |\n| TABLE        | LINE_ITEMS                   | NULL          | COMMENT                  | Line items in orders                   |\n| RELATIONSHIP | LINE_ITEM_TO_ORDERS          | LINE_ITEMS    | TABLE                    | LINE_ITEMS                             |\n| RELATIONSHIP | LINE_ITEM_TO_ORDERS          | LINE_ITEMS    | REF_TABLE                | ORDERS                                 |\n| RELATIONSHIP | LINE_ITEM_TO_ORDERS          | LINE_ITEMS    | FOREIGN_KEY              | [\"L_ORDERKEY\"]                         |\n| RELATIONSHIP | LINE_ITEM_TO_ORDERS          | LINE_ITEMS    | REF_KEY                  | [\"O_ORDERKEY\"]                         |\n| FACT         | DISCOUNTED_PRICE             | LINE_ITEMS    | TABLE                    | LINE_ITEMS                             |\n| FACT         | DISCOUNTED_PRICE             | LINE_ITEMS    | EXPRESSION               | l_extendedprice * (1 - l_discount)     |\n| FACT         | DISCOUNTED_PRICE             | LINE_ITEMS    | DATA_TYPE                | NUMBER(25,4)                           |\n| FACT         | DISCOUNTED_PRICE             | LINE_ITEMS    | COMMENT                  | Extended price after discount          |\n| FACT         | LINE_ITEM_ID                 | LINE_ITEMS    | TABLE                    | LINE_ITEMS                             |\n| FACT         | LINE_ITEM_ID                 | LINE_ITEMS    | EXPRESSION               | CONCAT(l_orderkey, '-', l_linenumber)  |\n| FACT         | LINE_ITEM_ID                 | LINE_ITEMS    | DATA_TYPE                | VARCHAR(134217728)                     |\n| TABLE        | ORDERS                       | NULL          | BASE_TABLE_DATABASE_NAME | SNOWFLAKE_SAMPLE_DATA                  |\n| TABLE        | ORDERS                       | NULL          | BASE_TABLE_SCHEMA_NAME   | TPCH_SF1                               |\n| TABLE        | ORDERS                       | NULL          | BASE_TABLE_NAME          | ORDERS                                 |\n| TABLE        | ORDERS                       | NULL          | SYNONYMS                 | [\"sales orders\"]                       |\n| TABLE        | ORDERS                       | NULL          | PRIMARY_KEY              | [\"O_ORDERKEY\"]                         |\n| TABLE        | ORDERS                       | NULL          | COMMENT                  | All orders table for the sales domain  |\n| RELATIONSHIP | ORDERS_TO_CUSTOMERS          | ORDERS        | TABLE                    | ORDERS                                 |\n| RELATIONSHIP | ORDERS_TO_CUSTOMERS          | ORDERS        | REF_TABLE                | CUSTOMERS                              |\n| RELATIONSHIP | ORDERS_TO_CUSTOMERS          | ORDERS        | FOREIGN_KEY              | [\"O_CUSTKEY\"]                          |\n| RELATIONSHIP | ORDERS_TO_CUSTOMERS          | ORDERS        | REF_KEY                  | [\"C_CUSTKEY\"]                          |\n| METRIC       | AVERAGE_LINE_ITEMS_PER_ORDER | ORDERS        | TABLE                    | ORDERS                                 |\n| METRIC       | AVERAGE_LINE_ITEMS_PER_ORDER | ORDERS        | EXPRESSION               | AVG(orders.count_line_items)           |\n| METRIC       | AVERAGE_LINE_ITEMS_PER_ORDER | ORDERS        | DATA_TYPE                | NUMBER(36,6)                           |\n| METRIC       | AVERAGE_LINE_ITEMS_PER_ORDER | ORDERS        | COMMENT                  | Average number of line items per order |\n| FACT         | COUNT_LINE_ITEMS             | ORDERS        | TABLE                    | ORDERS                                 |\n| FACT         | COUNT_LINE_ITEMS             | ORDERS        | EXPRESSION               | COUNT(line_items.line_item_id)         |\n| FACT         | COUNT_LINE_ITEMS             | ORDERS        | DATA_TYPE                | NUMBER(18,0)                           |\n| METRIC       | ORDER_AVERAGE_VALUE          | ORDERS        | TABLE                    | ORDERS                                 |\n| METRIC       | ORDER_AVERAGE_VALUE          | ORDERS        | EXPRESSION               | AVG(orders.o_totalprice)               |\n| METRIC       | ORDER_AVERAGE_VALUE          | ORDERS        | DATA_TYPE                | NUMBER(30,8)                           |\n| METRIC       | ORDER_AVERAGE_VALUE          | ORDERS        | COMMENT                  | Average order value across all orders  |\n| DIMENSION    | ORDER_DATE                   | ORDERS        | TABLE                    | ORDERS                                 |\n| DIMENSION    | ORDER_DATE                   | ORDERS        | EXPRESSION               | o_orderdate                            |\n| DIMENSION    | ORDER_DATE                   | ORDERS        | DATA_TYPE                | DATE                                   |\n| DIMENSION    | ORDER_DATE                   | ORDERS        | COMMENT                  | Date when the order was placed         |\n| DIMENSION    | ORDER_YEAR                   | ORDERS        | TABLE                    | ORDERS                                 |\n| DIMENSION    | ORDER_YEAR                   | ORDERS        | EXPRESSION               | YEAR(o_orderdate)                      |\n| DIMENSION    | ORDER_YEAR                   | ORDERS        | DATA_TYPE                | NUMBER(4,0)                            |\n| DIMENSION    | ORDER_YEAR                   | ORDERS        | COMMENT                  | Year when the order was placed         |\n+--------------+------------------------------+---------------+--------------------------+----------------------------------------+"
        ]
      }
    ]
  },
  {
    "category": "DROP SEMANTIC VIEW",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-semantic-view",
    "details": [
      {
        "heading": "DROP SEMANTIC VIEW",
        "description": "\nRemoves the specified semantic view from the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE SEMANTIC VIEW , DESCRIBE SEMANTIC VIEW , SHOW SEMANTIC VIEWS"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP SEMANTIC VIEW [ IF EXISTS ] <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the semantic view to drop. If the identifier contains spaces or special characters, the entire string must be enclosed in double quotes.\nIdentifiers enclosed in double quotes are also case-sensitive. For more information, see Identifier requirements."
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "OWNERSHIP",
                "Semantic view",
                "OWNERSHIP is a special privilege on an object that is automatically granted to the role that created the object, but can also be transferred using the GRANT OWNERSHIP command to a different role by the owning role (or any role with the MANAGE GRANTS privilege)."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Examples",
        "description": "\nThe following example drops the semantic view named my_semantic_view:\nOn this page\nSyntax\nParameters\nAccess control requirements\nExamples\nRelated content\nOverview of semantic views\nUsing SQL commands to create and manage semantic views\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP SEMANTIC VIEW my_semantic_view;"
        ]
      }
    ]
  },
  {
    "category": "SHOW SEMANTIC VIEWS",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-semantic-views",
    "details": [
      {
        "heading": "SHOW SEMANTIC VIEWS",
        "description": "\nLists the semantic views for which you have access privileges. You can list\nviews for the current or specified schema.\nThe output returns view metadata and properties, ordered lexicographically by database, schema, and semantic view name. This is\nimportant to note if you want to filter the results using the provided filters.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE SEMANTIC VIEW , DESCRIBE SEMANTIC VIEW , DROP SEMANTIC VIEW"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW [ TERSE ] SEMANTIC VIEWS [ LIKE '<pattern>' ]\n  [ IN\n    {\n      ACCOUNT                                         |\n\n      DATABASE                                        |\n      DATABASE <database_name>                        |\n\n      SCHEMA                                          |\n      SCHEMA <schema_name>                            |\n      <schema_name>\n    }\n  ]\n\n  [ STARTS WITH '<name_string>' ]\n  [ LIMIT <rows> [ FROM '<name_string>' ] ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "TERSE",
            "definition": "Returns only a subset of the output columns: created_on name kind The kind column value is always SEMANTIC_VIEW. database_name schema_name Default: No value (all columns are included in the output)"
          },
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "STARTS WITH 'name_string'",
            "definition": "Optionally filters the command output based on the characters that appear at the beginning of\nthe object name. The string must be enclosed in single quotes and is case sensitive. For example, the following strings return different results: . Default: No value (no filtering is applied to the output)"
          },
          {
            "term": "LIMIT rows [ FROM 'name_string' ]",
            "definition": "Optionally limits the maximum number of rows returned, while also enabling pagination of the results. The actual number of rows\nreturned might be less than the specified limit. For example, the number of existing objects is less than the specified limit. The optional FROM 'name_string' subclause effectively serves as a cursor for the results. This enables fetching the\nspecified number of rows following the first row whose object name matches the specified string: The string must be enclosed in single quotes and is case sensitive. The string does not have to include the full object name; partial names are supported. Default: No value (no limit is applied to the output) Note For SHOW commands that support both the FROM 'name_string' and STARTS WITH 'name_string' clauses, you can combine\nboth of these clauses in the same statement. However, both conditions must be met or they cancel out each other and no results are\nreturned. In addition, objects are returned in lexicographic order by name, so FROM 'name_string' only returns rows with a higher\nlexicographic value than the rows returned by STARTS WITH 'name_string'. For example: ... STARTS WITH 'A' LIMIT ... FROM 'B' would return no results. ... STARTS WITH 'B' LIMIT ... FROM 'A' would return no results. ... STARTS WITH 'A' LIMIT ... FROM 'AB' would return results (if any rows match the input strings)."
          }
        ]
      },
      {
        "heading": "Output",
        "tables": [
          {
            "headers": [
              "Column",
              "Description"
            ],
            "rows": [
              [
                "created_on",
                "Date and time when the semantic view was created."
              ],
              [
                "name",
                "Name of the semantic view."
              ],
              [
                "kind",
                "View type. This is always SEMANTIC_VIEW.\nThis column only appears in the output if you specify TERSE."
              ],
              [
                "database_name",
                "Database in which the semantic view is stored."
              ],
              [
                "schema_name",
                "Schema in which the semantic view is stored."
              ],
              [
                "comment",
                "Comment about the semantic view."
              ],
              [
                "owner",
                "Role that owns the semantic view."
              ],
              [
                "owner_role_type",
                "The type of role that owns the object, for example ROLE. . If a Snowflake Native App owns the object, the value is APPLICATION. . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "REFERENCES",
                "Semantic view",
                ""
              ],
              [
                "OWNERSHIP",
                "Semantic view",
                "OWNERSHIP is a special privilege on an object that is automatically granted to the role that created the object, but can also be transferred using the GRANT OWNERSHIP command to a different role by the owning role (or any role with the MANAGE GRANTS privilege)."
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema."
      },
      {
        "heading": "Examples",
        "description": "\nThe following example lists the semantic views in the database that is currently in use:\nThe following example includes only a subset of the output columns:\nThe following example displays the semantic views with names that have the string tpch:\nThe following example displays the semantic views with names that start with MY_SEMANTIC_VIEW:\nThe following example displays the first three semantic views with names that start with MY_SEMANTIC_VIEW:\nThe following example displays the three semantic views with names that start with MY_SEMANTIC_VIEW after the view named\nMY_SEMANTIC_VIEW_3:\nOn this page\nSyntax\nParameters\nOutput\nAccess control requirements\nUsage notes\nExamples\nRelated content\nOverview of semantic views\nUsing SQL commands to create and manage semantic views\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "SHOW SEMANTIC VIEWS;",
          "+-------------------------------+----------------------+---------------+-------------+---------+---------+-----------------+-----------+\n| created_on                    | name                 | database_name | schema_name | comment | owner   | owner_role_type | extension |\n|-------------------------------+----------------------+---------------+-------------+---------+---------+-----------------+-----------+\n| 2025-04-10 08:29:02.732 -0700 | MY_SEMANTIC_VIEW_1   | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:29:21.117 -0700 | MY_SEMANTIC_VIEW_2   | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:29:38.040 -0700 | MY_SEMANTIC_VIEW_3   | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:33.161 -0700 | MY_SEMANTIC_VIEW_4   | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:46.294 -0700 | MY_SEMANTIC_VIEW_5   | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:58.480 -0700 | MY_SEMANTIC_VIEW_6   | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-02-28 16:16:04.002 -0800 | O_TPCH_SEMANTIC_VIEW | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-03-21 07:03:54.120 -0700 | TPCH_REV_ANALYSIS    | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n+-------------------------------+----------------------+---------------+-------------+---------+---------+-----------------+-----------+",
          "SHOW TERSE SEMANTIC VIEWS;",
          "+-------------------------------+-----------------------+---------------+---------------+-------------------+\n| created_on                    | name                  | kind          | database_name | schema_name       |\n|-------------------------------+-----------------------+---------------+---------------+-------------------|\n| 2025-04-10 08:29:02.732 -0700 | MY_SEMANTIC_VIEW_1    | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n| 2025-04-10 08:29:21.117 -0700 | MY_SEMANTIC_VIEW_2    | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n| 2025-04-10 08:29:38.040 -0700 | MY_SEMANTIC_VIEW_3    | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n| 2025-04-10 08:47:33.161 -0700 | MY_SEMANTIC_VIEW_4    | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n| 2025-04-10 08:47:46.294 -0700 | MY_SEMANTIC_VIEW_5    | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n| 2025-04-10 08:47:58.480 -0700 | MY_SEMANTIC_VIEW_6    | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n| 2025-02-28 16:16:04.002 -0800 | O_TPCH_SEMANTIC_VIEW  | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n| 2025-03-21 07:03:54.120 -0700 | TPCH_REV_ANALYSIS     | SEMANTIC_VIEW | MY_DB         | MY_SCHEMA         |\n+-------------------------------+-----------------------+---------------+---------------+-------------------+",
          "SHOW SEMANTIC VIEWS LIKE '%tpch%';",
          "+-------------------------------+----------------------+---------------+-------------+---------+---------+-----------------+-----------+\n| created_on                    | name                 | database_name | schema_name | comment | owner   | owner_role_type | extension |\n|-------------------------------+----------------------+---------------+-------------+---------+---------+-----------------+-----------|\n| 2025-02-28 16:16:04.002 -0800 | O_TPCH_SEMANTIC_VIEW | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-03-21 07:03:54.120 -0700 | TPCH_REV_ANALYSIS    | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n+-------------------------------+----------------------+---------------+-------------+---------+---------+-----------------+-----------+",
          "SHOW SEMANTIC VIEWS STARTS WITH 'MY_SEMANTIC_VIEW';",
          "+-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------+\n| created_on                    | name               | database_name | schema_name | comment | owner   | owner_role_type | extension |\n|-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------|\n| 2025-04-10 08:29:02.732 -0700 | MY_SEMANTIC_VIEW_1 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:29:21.117 -0700 | MY_SEMANTIC_VIEW_2 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:29:38.040 -0700 | MY_SEMANTIC_VIEW_3 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:33.161 -0700 | MY_SEMANTIC_VIEW_4 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:46.294 -0700 | MY_SEMANTIC_VIEW_5 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:58.480 -0700 | MY_SEMANTIC_VIEW_6 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n+-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------+",
          "SHOW SEMANTIC VIEWS STARTS WITH 'MY_SEMANTIC_VIEW' LIMIT 3;",
          "+-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------+\n| created_on                    | name               | database_name | schema_name | comment | owner   | owner_role_type | extension |\n|-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------|\n| 2025-04-10 08:29:02.732 -0700 | MY_SEMANTIC_VIEW_1 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:29:21.117 -0700 | MY_SEMANTIC_VIEW_2 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:29:38.040 -0700 | MY_SEMANTIC_VIEW_3 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n+-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------+",
          "SHOW SEMANTIC VIEWS STARTS WITH 'MY_SEMANTIC_VIEW' LIMIT 3 FROM 'MY_SEMANTIC_VIEW_3';",
          "+-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------+\n| created_on                    | name               | database_name | schema_name | comment | owner   | owner_role_type | extension |\n|-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------|\n| 2025-04-10 08:47:33.161 -0700 | MY_SEMANTIC_VIEW_4 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:46.294 -0700 | MY_SEMANTIC_VIEW_5 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n| 2025-04-10 08:47:58.480 -0700 | MY_SEMANTIC_VIEW_6 | MY_DB         | MY_SCHEMA   |         | MY_ROLE | ROLE            | NULL      |\n+-------------------------------+--------------------+---------------+-------------+---------+---------+-----------------+-----------+"
        ]
      }
    ]
  },
  {
    "category": "CREATE SEQUENCE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/create-sequence",
    "details": [
      {
        "heading": "CREATE SEQUENCE",
        "description": "\nCreates a new sequence, which can be used for generating sequential, unique numbers.\nImportant\nSnowflake does not guarantee generating sequence numbers without gaps. The generated numbers are not necessarily contiguous.\nFor more details, see Using Sequences.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "DROP SEQUENCE , ALTER SEQUENCE , SHOW SEQUENCES , DESCRIBE SEQUENCE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "CREATE [ OR REPLACE ] SEQUENCE [ IF NOT EXISTS ] <name>\n  [ WITH ]\n  [ START [ WITH ] [ = ] <initial_value> ]\n  [ INCREMENT [ BY ] [ = ] <sequence_interval> ]\n  [ { ORDER | NOORDER } ]\n  [ COMMENT = '<string_literal>' ]"
        ]
      },
      {
        "heading": "Required parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the sequence; must be unique for the schema in which the sequence is created. In addition, the identifier must start with an alphabetic character and cannot contain spaces or special characters unless the\nentire identifier string is enclosed in double quotes (e.g. \"My object\"). Identifiers enclosed in double quotes are also\ncase-sensitive. For more details about identifiers, see Identifier requirements."
          }
        ]
      },
      {
        "heading": "Optional parameters",
        "definitions": [
          {
            "term": "START [ WITH ] [ = ] initial_value",
            "definition": "Specifies the first value returned by the sequence. Supported values are any value that can be represented by a 64-bit twos\ncomplement integer (from -2^63 to 2^63 - 1). Default: 1"
          },
          {
            "term": "INCREMENT [ BY ] [ = ] sequence_interval",
            "definition": "Specifies the step interval of the sequence: For positive sequence interval n, the next n-1 values are reserved by each sequence call. For negative sequence interval -n, the next n-1 lower values are reserved by each sequence call. Supported values are any non-zero value that can be represented by a 64-bit twos complement integer. Default: 1"
          },
          {
            "term": "{ ORDER | NOORDER }",
            "definition": "Specifies whether or not the values are generated for the sequence in\nincreasing or decreasing order. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2, the generated values might be\n1, 3, 5, 7, 9, etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2, the generated values might be 1, 3, 101, 5, 103, etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). Default: The NOORDER_SEQUENCE_AS_DEFAULT parameter determines which property is set by default."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Specifies a comment for the sequence. Default: No value"
          }
        ]
      },
      {
        "heading": "Access control requirements",
        "tables": [
          {
            "headers": [
              "Privilege",
              "Object",
              "Notes"
            ],
            "rows": [
              [
                "CREATE SEQUENCE",
                "Schema",
                ""
              ]
            ]
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe first/initial value for a sequence cannot be changed after the sequence is created.\nA sequence does not necessarily produce a gap-free sequence. Values increase (until the limit is reached) and are unique,\nbut are not necessarily contiguous. For more information, including the upper and lower limits, see Sequence Semantics.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe first/initial value for a sequence cannot be changed after the sequence is created.\nA sequence does not necessarily produce a gap-free sequence. Values increase (until the limit is reached) and are unique,\nbut are not necessarily contiguous. For more information, including the upper and lower limits, see Sequence Semantics.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction.\nThe OR REPLACE and IF NOT EXISTS clauses are mutually exclusive. They cant both be used in the same statement.\nCREATE OR REPLACE <object> statements are atomic. That is, when an object is replaced, the old object is deleted and the new object is created in a single transaction."
      },
      {
        "heading": "Examples",
        "description": "\nHere is a simple example of using sequences:\nRun the same query again; note how the sequence numbers change:\nNow use the sequence while inserting into a table:\nCreate a sequence that increments by 5 rather than by 1:\nRun the same query again; note how the sequence numbers change. You might expect that the next set of sequence numbers would start 5\nhigher than the previous statement left off. However, the next sequence number starts 20 higher (5 * 4, where 5 is the size of the\nincrement and 4 is the number of NEXTVAL operations in the statement):\nThis example demonstrates that you can use a sequence as a default value for a column to provide unique identifiers for each row in\na table:\nThis query shows that each row in the table has a distinct value:\nMore examples are available in Using Sequences.\nOn this page\nSyntax\nRequired parameters\nOptional parameters\nAccess control requirements\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "CREATE OR REPLACE SEQUENCE seq_01 START = 1 INCREMENT = 1;\nCREATE OR REPLACE TABLE sequence_test_table (i INTEGER);",
          "SELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       1 |\n+---------+",
          "SELECT seq_01.nextval;\n+---------+\n| NEXTVAL |\n|---------|\n|       2 |\n+---------+",
          "INSERT INTO sequence_test_table (i) VALUES (seq_01.nextval);",
          "SELECT i FROM sequence_test_table;\n+---+\n| I |\n|---|\n| 3 |\n+---+",
          "CREATE OR REPLACE SEQUENCE seq_5 START = 1 INCREMENT = 5;",
          "SELECT seq_5.nextval a, seq_5.nextval b, seq_5.nextval c, seq_5.nextval d;\n+---+---+----+----+\n| A | B |  C |  D |\n|---+---+----+----|\n| 1 | 6 | 11 | 16 |\n+---+---+----+----+",
          "SELECT seq_5.nextval a, seq_5.nextval b, seq_5.nextval c, seq_5.nextval d;\n+----+----+----+----+\n|  A |  B |  C |  D |\n|----+----+----+----|\n| 36 | 41 | 46 | 51 |\n+----+----+----+----+",
          "CREATE OR REPLACE SEQUENCE seq90;\nCREATE OR REPLACE TABLE sequence_demo (i INTEGER DEFAULT seq90.nextval, dummy SMALLINT);\nINSERT INTO sequence_demo (dummy) VALUES (0);\n\n-- Keep doubling the number of rows:\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;\nINSERT INTO sequence_demo (dummy) SELECT dummy FROM sequence_demo;",
          "SELECT i FROM sequence_demo ORDER BY i LIMIT 10;\n+----+\n|  I |\n|----|\n|  1 |\n|  2 |\n|  3 |\n|  4 |\n|  5 |\n|  6 |\n|  7 |\n|  8 |\n|  9 |\n| 10 |\n+----+",
          "SELECT COUNT(i), COUNT(DISTINCT i) FROM sequence_demo;\n+----------+-------------------+\n| COUNT(I) | COUNT(DISTINCT I) |\n|----------+-------------------|\n|     1024 |              1024 |\n+----------+-------------------+"
        ]
      }
    ]
  },
  {
    "category": "ALTER SEQUENCE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/alter-sequence",
    "details": [
      {
        "heading": "ALTER SEQUENCE",
        "description": "\nModifies the properties for an existing sequence.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE SEQUENCE , DROP SEQUENCE , SHOW SEQUENCES , DESCRIBE SEQUENCE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "ALTER SEQUENCE [ IF EXISTS ] <name> RENAME TO <new_name>\n\nALTER SEQUENCE [ IF EXISTS ] <name> [ SET ] [ INCREMENT [ BY ] [ = ] <sequence_interval> ]\n\nALTER SEQUENCE [ IF EXISTS ] <name> SET\n  [ { ORDER | NOORDER } ]\n  [ COMMENT = '<string_literal>' ]\n\nALTER SEQUENCE [ IF EXISTS ] <name> UNSET COMMENT"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the sequence to alter. If the identifier contains spaces or special characters, the entire string must be enclosed in\ndouble quotes. Identifiers enclosed in double quotes are also case-sensitive. For more details about identifiers, see Identifier requirements."
          },
          {
            "term": "RENAME TO new_name",
            "definition": "Specifies the new identifier for the sequence; must be unique for the schema. For more details about identifiers, see Identifier requirements. You can move the object to a different database and/or schema while optionally renaming the object. To do so, specify\na qualified new_name value that includes the new database and/or schema name in the form\ndb_name.schema_name.object_name or schema_name.object_name, respectively. Note The destination database and/or schema must already exist. In addition, an object with the same name cannot already\nexist in the new location; otherwise, the statement returns an error. Moving an object to a managed access schema is prohibited unless the object owner (that is, the role that has\nthe OWNERSHIP privilege on the object) also owns the target schema. When an object is renamed, other objects that reference it must be updated with the new name."
          },
          {
            "term": "SET...",
            "definition": "Specifies the properties to set for the sequence: Specifies the step interval of the sequence: For positive sequence interval n, the next n-1 values are reserved by each sequence call. For negative sequence interval -n, the next n-1 lower values are reserved by each sequence call. Supported values are any non-zero value that can be represented by a 64-bit twos complement integer. Specifies whether or not the values are generated for the sequence in\nincreasing order. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2, the generated values might be\n1, 3, 5, 7, 9, etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2, the generated values might be 1, 3, 101, 5, 103, etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). Note If a sequence is set to NOORDER, you cannot change the sequence to ORDER. Adds a comment or overwrites an existing comment for the sequence."
          },
          {
            "term": "[ INCREMENT [ BY ] sequence_interval ]",
            "definition": "Specifies the step interval of the sequence: For positive sequence interval n, the next n-1 values are reserved by each sequence call. For negative sequence interval -n, the next n-1 lower values are reserved by each sequence call. Supported values are any non-zero value that can be represented by a 64-bit twos complement integer."
          },
          {
            "term": "{ ORDER | NOORDER }",
            "definition": "Specifies whether or not the values are generated for the sequence in\nincreasing order. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2, the generated values might be\n1, 3, 5, 7, 9, etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2, the generated values might be 1, 3, 101, 5, 103, etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). Note If a sequence is set to NOORDER, you cannot change the sequence to ORDER."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites an existing comment for the sequence."
          },
          {
            "term": "UNSET ...",
            "definition": "Specifies the properties to unset for the sequence, which resets them to the defaults. Currently, the only property you can unset is COMMENT, which removes the comment, if one exists, for the sequence."
          },
          {
            "term": "[ INCREMENT [ BY ] sequence_interval ]",
            "definition": "Specifies the step interval of the sequence: For positive sequence interval n, the next n-1 values are reserved by each sequence call. For negative sequence interval -n, the next n-1 lower values are reserved by each sequence call. Supported values are any non-zero value that can be represented by a 64-bit twos complement integer."
          },
          {
            "term": "{ ORDER | NOORDER }",
            "definition": "Specifies whether or not the values are generated for the sequence in\nincreasing order. ORDER specifies that the values generated for a sequence or auto-incremented column are in increasing order (or, if the interval\nis a negative value, in decreasing order). For example, if a sequence or auto-incremented column has START 1 INCREMENT 2, the generated values might be\n1, 3, 5, 7, 9, etc. NOORDER specifies that the values are not guaranteed to be in increasing order. For example, if a sequence has START 1 INCREMENT 2, the generated values might be 1, 3, 101, 5, 103, etc. NOORDER can improve performance when multiple INSERT operations are performed concurrently (for example, when multiple\nclients are executing multiple INSERT statements). Note If a sequence is set to NOORDER, you cannot change the sequence to ORDER."
          },
          {
            "term": "COMMENT = 'string_literal'",
            "definition": "Adds a comment or overwrites an existing comment for the sequence."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe first/initial value for a sequence cannot be changed after the sequence is created.\nRegarding metadata:\n\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake.\nThe first/initial value for a sequence cannot be changed after the sequence is created.\nRegarding metadata:\nAttention\nCustomers should ensure that no personal data (other than for a User object), sensitive data, export-controlled data, or other regulated data is entered as metadata when using the Snowflake service. For more information, see Metadata fields in Snowflake."
      },
      {
        "heading": "Examples",
        "description": "\nRename sequence myseq to newseq:\nMore examples are available in Using Sequences.\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "ALTER SEQUENCE myseq RENAME TO newseq;"
        ]
      }
    ]
  },
  {
    "category": "DESCRIBE SEQUENCE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/desc-sequence",
    "details": [
      {
        "heading": "DESCRIBE SEQUENCE",
        "description": "\nDescribes a sequence, including the sequences interval.\nDESCRIBE can be abbreviated to DESC.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "ALTER SEQUENCE , CREATE SEQUENCE , DROP SEQUENCE , SHOW SEQUENCES"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DESC[RIBE] SEQUENCE <name>"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier for the sequence to describe."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query."
      },
      {
        "heading": "Examples",
        "description": "\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DESC SEQUENCE my_sequence;"
        ]
      }
    ]
  },
  {
    "category": "DROP SEQUENCE",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/drop-sequence",
    "details": [
      {
        "heading": "DROP SEQUENCE",
        "description": "\nRemoves a sequence from the current/specified schema.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "CREATE SEQUENCE , ALTER SEQUENCE , SHOW SEQUENCES , DESCRIBE SEQUENCE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "DROP SEQUENCE [ IF EXISTS ] <name> [ CASCADE | RESTRICT ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "name",
            "definition": "Specifies the identifier of the sequence to drop. If the sequence identifier is not fully-qualified (in the form of db_name.schema_name.sequence_name or\nschema_name.sequence_name), the command looks for the sequence in the current schema for the session."
          },
          {
            "term": "CASCADE | RESTRICT",
            "definition": "Snowflake allows the keywords CASCADE and RESTRICT syntactically, but does not act on them. For example,\ndropping a sequence with the CASCADE keyword does not actually drop a table that uses the sequence.\nDropping a sequence with the RESTRICT keyword does not issue a warning if a table is still using the sequence."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nTo drop a sequence, you must be using a role that has ownership privilege on the sequence.\nAfter dropping a sequence, creating a sequence with the same name creates a new version of the sequence. The\nnew sequence does not resume generating numbers where the old sequence left off.\nBefore dropping a sequence, verify that no tables or other database objects reference the sequence.\nIf the dropped sequence was referenced in the DEFAULT clause of a table, then calling GET_DDL() for that\ntable results in an error, rather than in the DDL that created the table.\nTo drop a sequence, you must be using a role that has ownership privilege on the sequence.\nAfter dropping a sequence, creating a sequence with the same name creates a new version of the sequence. The\nnew sequence does not resume generating numbers where the old sequence left off.\nBefore dropping a sequence, verify that no tables or other database objects reference the sequence.\nIf the dropped sequence was referenced in the DEFAULT clause of a table, then calling GET_DDL() for that\ntable results in an error, rather than in the DDL that created the table.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error.\nWhen the IF EXISTS clause is specified and the target object doesnt exist, the command completes successfully\nwithout returning an error."
      },
      {
        "heading": "Examples",
        "description": "\nDrop a sequence:\nOn this page\nSyntax\nParameters\nUsage notes\nExamples\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n",
        "syntax": [
          "DROP SEQUENCE IF EXISTS invoice_sequence_number;"
        ]
      }
    ]
  },
  {
    "category": "SHOW SEQUENCES",
    "url": "https://docs.snowflake.com/en/sql-reference/sql/show-sequences",
    "details": [
      {
        "heading": "SHOW SEQUENCES",
        "description": "\nLists all the sequences for which you have access privileges. This command can be used to list the sequences for a specified schema or\ndatabase (or the current schema/database for the session), or your entire account.",
        "definitions": [
          {
            "term": "See also:",
            "definition": "SEQUENCES view (Information Schema) , CREATE SEQUENCE , ALTER SEQUENCE , DROP SEQUENCE ,\nDESCRIBE SEQUENCE"
          }
        ]
      },
      {
        "heading": "Syntax",
        "syntax": [
          "SHOW SEQUENCES [ LIKE '<pattern>' ]\n               [ IN\n                    {\n                      ACCOUNT                                         |\n\n                      DATABASE                                        |\n                      DATABASE <database_name>                        |\n\n                      SCHEMA                                          |\n                      SCHEMA <schema_name>                            |\n                      <schema_name>\n\n                      APPLICATION <application_name>                  |\n                      APPLICATION PACKAGE <application_package_name>  |\n                    }\n               ]"
        ]
      },
      {
        "heading": "Parameters",
        "definitions": [
          {
            "term": "LIKE 'pattern'",
            "definition": "Optionally filters the command output by object name. The filter uses case-insensitive pattern matching, with support for SQL\nwildcard characters (% and _). For example, the following patterns return the same results: . Default: No value (no filtering is applied to the output)."
          },
          {
            "term": "[ IN ... ]",
            "definition": "Optionally specifies the scope of the command. Specify one of the following: Returns records for the entire account. Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables. Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output. Returns records for the named Snowflake Native App or application package. Default: Depends on whether the session currently has a database in use: Database: DATABASE is the default (that is, the command returns the objects you have privileges to view in the database). No database: ACCOUNT is the default (that is, the command returns the objects you have privileges to view in your account)."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          },
          {
            "term": "ACCOUNT",
            "definition": "Returns records for the entire account."
          },
          {
            "term": "DATABASE, . DATABASE db_name",
            "definition": "Returns records for the current database in use or for a specified database (db_name). If you specify DATABASE without db_name and no database is in use, the keyword has no effect on the output. Note Using SHOW commands without an IN clause in a database context can result in fewer than expected results. Objects with the same name are only displayed once if no IN clause is used. For example, if you have table t1 in\nschema1 and table t1 in schema2, and they are both in scope of the database context youve specified (that is, the database\nyouve selected is the parent of schema1 and schema2), then SHOW TABLES only displays one of the t1 tables."
          },
          {
            "term": "SCHEMA, . SCHEMA schema_name",
            "definition": "Returns records for the current schema in use or a specified schema (schema_name). SCHEMA is optional if a database is in use or if you specify the fully qualified schema_name (for example, db.schema). If no database is in use, specifying SCHEMA has no effect on the output."
          },
          {
            "term": "APPLICATION application_name, . APPLICATION PACKAGE application_package_name",
            "definition": "Returns records for the named Snowflake Native App or application package."
          }
        ]
      },
      {
        "heading": "Usage notes",
        "description": "\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nThe command doesnt require a running warehouse to execute.\nThe command only returns objects for which the current users current role has been granted at least one access privilege.\nThe MANAGE GRANTS access privilege implicitly allows its holder to see every object in the account. By default, only the account\nadministrator (users with the ACCOUNTADMIN role) and security administrator (users with the SECURITYADMIN role) have the\nMANAGE GRANTS privilege.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nTo post-process the output of this command, you can use the pipe operator\nor the RESULT_SCAN function. Both constructs treat the output as a result set that\nyou can query.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nThe command returns a maximum of ten thousand records for the specified object type, as dictated by the access privileges for the role\nused to execute the command. Any records above the ten thousand records limit arent returned, even with a filter applied.\nTo view results for which more than ten thousand records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.\nOn this page\nSyntax\nParameters\nUsage notes\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "English",
    "url": "https://docs.snowflake.com/en/sql-reference/commands-table",
    "details": [
      {
        "heading": "Table, view, and sequence commands",
        "description": "\nTable, view, and sequence commands enable you to manage your tables, views, and sequences."
      },
      {
        "heading": "General database objects",
        "description": "\nSHOW OBJECTS\nSHOW OBJECTS"
      },
      {
        "heading": "Table",
        "description": "\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE"
      },
      {
        "heading": "Dynamic table",
        "description": "\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES"
      },
      {
        "heading": "Event table",
        "description": "\nCREATE EVENT TABLE\nALTER TABLE (event tables)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE\nCREATE EVENT TABLE\nALTER TABLE (event tables)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE"
      },
      {
        "heading": "External table",
        "description": "\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES"
      },
      {
        "heading": "Hybrid table",
        "description": "\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES"
      },
      {
        "heading": "Apache Iceberg table",
        "description": "\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE"
      },
      {
        "heading": "View",
        "description": "\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS"
      },
      {
        "heading": "Materialized view",
        "description": "\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW"
      },
      {
        "heading": "Semantic view",
        "description": "\nCREATE SEMANTIC VIEW\nDESCRIBE SEMANTIC VIEW\nDROP SEMANTIC VIEW\nSHOW SEMANTIC VIEWS\nCREATE SEMANTIC VIEW\nDESCRIBE SEMANTIC VIEW\nDROP SEMANTIC VIEW\nSHOW SEMANTIC VIEWS"
      },
      {
        "heading": "Sequence",
        "description": "\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nOn this page\nGeneral database objects\nTable\nDynamic table\nEvent table\nExternal table\nHybrid table\nApache Iceberg table\nView\nMaterialized view\nSemantic view\nSequence\nWe use cookies to improve your experience on our site. By accepting, you agree to our privacy policy.\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "FranÃ§ais",
    "url": "https://docs.snowflake.com/fr/sql-reference/commands-table",
    "details": [
      {
        "heading": "Commandes de tables, de vues et de squences",
        "description": "\nLes commandes de tables, de vues et de squences vous permettent de grer vos tables, vues et squences.\nDans ce chapitre:"
      },
      {
        "heading": "Objets de bases de donnes gnraux",
        "description": "\nSHOW OBJECTS\nSHOW OBJECTS"
      },
      {
        "heading": "Table",
        "description": "\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE"
      },
      {
        "heading": "Table dynamique",
        "description": "\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES"
      },
      {
        "heading": "Table des vnements",
        "description": "\nCREATE EVENT TABLE\nALTER TABLE (tables dvnements)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE\nCREATE EVENT TABLE\nALTER TABLE (tables dvnements)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE"
      },
      {
        "heading": "Table externe",
        "description": "\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES"
      },
      {
        "heading": "Table hybride",
        "description": "\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES"
      },
      {
        "heading": "Table Apache Iceberg",
        "description": "\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE"
      },
      {
        "heading": "Vue",
        "description": "\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS"
      },
      {
        "heading": "Vue matrialise",
        "description": "\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW"
      },
      {
        "heading": "Squence",
        "description": "\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nSur cette page\nObjets de bases de donnes gnraux\nTable\nTable dynamique\nTable des vnements\nTable externe\nTable hybride\nTable Apache Iceberg\nVue\nVue matrialise\nSquence\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "Deutsch",
    "url": "https://docs.snowflake.com/de/sql-reference/commands-table",
    "details": [
      {
        "heading": "Befehle fr Tabellen, Ansichten und Sequenzen",
        "description": "\nMit den Befehlen fr Tabellen, Ansichten und Sequenzen knnen Sie Ihre Tabellen, Ansichten und Sequenzen verwalten.\nUnter diesem Thema:"
      },
      {
        "heading": "Allgemeine Datenbankobjekte",
        "description": "\nSHOW OBJECTS\nSHOW OBJECTS"
      },
      {
        "heading": "Tabelle",
        "description": "\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE"
      },
      {
        "heading": "Dynamische Tabelle",
        "description": "\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES"
      },
      {
        "heading": "Ereignistabelle",
        "description": "\nCREATE EVENT TABLE\nALTER TABLE (Ereignistabellen)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE\nCREATE EVENT TABLE\nALTER TABLE (Ereignistabellen)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE"
      },
      {
        "heading": "Externe Tabelle",
        "description": "\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES"
      },
      {
        "heading": "Hybridtabelle",
        "description": "\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES"
      },
      {
        "heading": "Apache Iceberg-Tabelle",
        "description": "\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE"
      },
      {
        "heading": "Ansicht",
        "description": "\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS"
      },
      {
        "heading": "Materialisierte Ansicht",
        "description": "\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW"
      },
      {
        "heading": "Sequenz",
        "description": "\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nAuf dieser Seite\nAllgemeine Datenbankobjekte\nTabelle\nDynamische Tabelle\nEreignistabelle\nExterne Tabelle\nHybridtabelle\nApache Iceberg-Tabelle\nAnsicht\nMaterialisierte Ansicht\nSequenz\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "æ—¥æœ¬èªž",
    "url": "https://docs.snowflake.com/ja/sql-reference/commands-table",
    "details": [
      {
        "description": "\n\n:"
      },
      {
        "description": "\nSHOW OBJECTS\nSHOW OBJECTS"
      },
      {
        "description": "\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE"
      },
      {
        "description": "\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES"
      },
      {
        "description": "\nCREATE EVENT TABLE\nALTER TABLE \nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE\nCREATE EVENT TABLE\nALTER TABLE \nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE"
      },
      {
        "description": "\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES"
      },
      {
        "description": "\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES"
      },
      {
        "heading": "Apache Iceberg ",
        "description": "\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE"
      },
      {
        "description": "\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS"
      },
      {
        "description": "\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW"
      },
      {
        "description": "\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\n\n\n\n\n\n\n\nApache Iceberg \n\n\n\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "í•œêµ­ì–´",
    "url": "https://docs.snowflake.com/ko/sql-reference/commands-table",
    "details": [
      {
        "heading": ",    ",
        "description": "\n,      , ,    .\n  :"
      },
      {
        "description": "\nSHOW OBJECTS\nSHOW OBJECTS"
      },
      {
        "description": "\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE"
      },
      {
        "description": "\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES"
      },
      {
        "description": "\nCREATE EVENT TABLE\nALTER TABLE( )\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE\nCREATE EVENT TABLE\nALTER TABLE( )\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE"
      },
      {
        "description": "\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES"
      },
      {
        "description": "\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES"
      },
      {
        "heading": "Apache Iceberg ",
        "description": "\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE"
      },
      {
        "description": "\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS"
      },
      {
        "description": "\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW"
      },
      {
        "description": "\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\n \n  \n\n \n \n \n \nApache Iceberg \n\n \n\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  },
  {
    "category": "PortuguÃªs",
    "url": "https://docs.snowflake.com/pt/sql-reference/commands-table",
    "details": [
      {
        "heading": "Comandos de tabela, exibio e sequncia",
        "description": "\nOs comandos de tabelas, exibies e sequncias permitem gerenciar suas tabelas, exibies e sequncias.\nNeste tpico:"
      },
      {
        "heading": "Objetos de banco de dados geral",
        "description": "\nSHOW OBJECTS\nSHOW OBJECTS"
      },
      {
        "heading": "Tabela",
        "description": "\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE\nCREATE TABLE\nALTER TABLE\nDESCRIBE TABLE\nDROP TABLE\nUNDROP TABLE\nSHOW TABLES\nSHOW COLUMNS\nSHOW PRIMARY KEYS\nDESCRIBE SEARCH OPTIMIZATION\nTRUNCATE TABLE"
      },
      {
        "heading": "Tabela dinmica",
        "description": "\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES"
      },
      {
        "heading": "Tabela de eventos",
        "description": "\nCREATE EVENT TABLE\nALTER TABLE (tabelas de eventos)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE\nCREATE EVENT TABLE\nALTER TABLE (tabelas de eventos)\nSHOW EVENT TABLES\nDESCRIBE EVENT TABLE"
      },
      {
        "heading": "Tabela externa",
        "description": "\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES\nCREATE EXTERNAL TABLE\nALTER EXTERNAL TABLE\nDESCRIBE EXTERNAL TABLE\nDROP EXTERNAL TABLE\nSHOW EXTERNAL TABLES"
      },
      {
        "heading": "Tabela hbrida",
        "description": "\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES\nCREATE HYBRID TABLE\nCREATE INDEX\nDROP INDEX\nSHOW HYBRID TABLES\nSHOW INDEXES"
      },
      {
        "heading": "Tabela Apache Iceberg",
        "description": "\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE\nCREATE ICEBERG TABLE\nALTER ICEBERG TABLE\nDROP ICEBERG TABLE\nUNDROP ICEBERG TABLE\nSHOW ICEBERG TABLES\nDESCRIBE ICEBERG TABLE"
      },
      {
        "heading": "Exibio",
        "description": "\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS\nCREATE VIEW\nALTER VIEW\nDESCRIBE VIEW\nDROP VIEW\nSHOW VIEWS"
      },
      {
        "heading": "Exibio materializada",
        "description": "\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW\nCREATE MATERIALIZED VIEW\nALTER MATERIALIZED VIEW\nDESCRIBE MATERIALIZED VIEW\nDROP MATERIALIZED VIEW\nSHOW MATERIALIZED VIEWS\nTRUNCATE MATERIALIZED VIEW"
      },
      {
        "heading": "Sequncia",
        "description": "\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nCREATE SEQUENCE\nALTER SEQUENCE\nDESCRIBE SEQUENCE\nDROP SEQUENCE\nSHOW SEQUENCES\nNesta pgina\nObjetos de banco de dados geral\nTabela\nTabela dinmica\nTabela de eventos\nTabela externa\nTabela hbrida\nTabela Apache Iceberg\nExibio\nExibio materializada\nSequncia\nEnglish\nFranais\nDeutsch\n\n\nPortugus\n"
      }
    ]
  }
]